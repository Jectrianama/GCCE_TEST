{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77793878",
   "metadata": {
    "id": "oAuRT75GdLFw",
    "papermill": {
     "duration": 0.00964,
     "end_time": "2023-01-03T00:14:23.133729",
     "exception": false,
     "start_time": "2023-01-03T00:14:23.124089",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cats vs. Dogs Class dataset for multiple annotators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e3de49",
   "metadata": {
    "id": "9rK94t33nwDC",
    "papermill": {
     "duration": 0.00796,
     "end_time": "2023-01-03T00:14:23.150121",
     "exception": false,
     "start_time": "2023-01-03T00:14:23.142161",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ecd5db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T00:14:23.168821Z",
     "iopub.status.busy": "2023-01-03T00:14:23.168332Z",
     "iopub.status.idle": "2023-01-03T00:14:29.986487Z",
     "shell.execute_reply": "2023-01-03T00:14:29.985446Z"
    },
    "id": "zSyMHuCVys-O",
    "papermill": {
     "duration": 6.830706,
     "end_time": "2023-01-03T00:14:29.989053",
     "exception": false,
     "start_time": "2023-01-03T00:14:23.158347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense,Conv2D,Flatten,MaxPooling2D,GlobalAveragePooling2D\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "319306c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T00:14:30.009218Z",
     "iopub.status.busy": "2023-01-03T00:14:30.008701Z",
     "iopub.status.idle": "2023-01-03T00:14:30.012999Z",
     "shell.execute_reply": "2023-01-03T00:14:30.012094Z"
    },
    "id": "-E1MJt8cxlwg",
    "outputId": "ea43c1c9-075f-44de-d2d8-e135799b6630",
    "papermill": {
     "duration": 0.015576,
     "end_time": "2023-01-03T00:14:30.014952",
     "exception": false,
     "start_time": "2023-01-03T00:14:29.999376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfde19ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T00:14:30.033169Z",
     "iopub.status.busy": "2023-01-03T00:14:30.032897Z",
     "iopub.status.idle": "2023-01-03T00:14:30.037334Z",
     "shell.execute_reply": "2023-01-03T00:14:30.036305Z"
    },
    "id": "QJPvjdZ-f8ca",
    "papermill": {
     "duration": 0.016081,
     "end_time": "2023-01-03T00:14:30.039507",
     "exception": false,
     "start_time": "2023-01-03T00:14:30.023426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# os.chdir('/content/drive/Shareddrives/Multiple Anotators/CrowdLayer/Notebooks')\n",
    "# cwd = os.getcwd()\n",
    "# sys.path.append(\"../Models\")\n",
    "\n",
    "\n",
    "# from Multiple_Annotators_C import MultipleAnnotators_Classification\n",
    "\n",
    "#import sys\n",
    "#sys.path.insert(1, '../input/multiple-annotators-c/')\n",
    "#os.chdir('/Multiple Anotators-c/')\n",
    "#cwd = os.getcwd()\n",
    "#sys.path.append('/input/multiple-annotators-c')\n",
    "#from Multiple_Annotators_C import MultipleAnnotators_Classification\n",
    "\n",
    "# seed_value= 12321 \n",
    "# from numpy.random import seed\n",
    "# seed(seed_value)\n",
    "# tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f876b021",
   "metadata": {
    "id": "6Un5nFWgnyem",
    "papermill": {
     "duration": 0.008693,
     "end_time": "2023-01-03T00:14:30.057052",
     "exception": false,
     "start_time": "2023-01-03T00:14:30.048359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Download and Prepare the Dataset\n",
    "\n",
    "We will use the [Cats vs Dogs](https://www.tensorflow.org/datasets/catalog/cats_vs_dogs) dataset and we can load it via Tensorflow Datasets. The images are labeled 0 for cats and 1 for dogs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee5091d",
   "metadata": {
    "id": "Gw6K2Uey06kh",
    "papermill": {
     "duration": 0.008463,
     "end_time": "2023-01-03T00:14:30.074131",
     "exception": false,
     "start_time": "2023-01-03T00:14:30.065668",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Multiple annotators model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c102e35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T00:14:30.092667Z",
     "iopub.status.busy": "2023-01-03T00:14:30.092389Z",
     "iopub.status.idle": "2023-01-03T00:14:33.046179Z",
     "shell.execute_reply": "2023-01-03T00:14:33.045182Z"
    },
    "id": "xam4REp209Sd",
    "papermill": {
     "duration": 2.966001,
     "end_time": "2023-01-03T00:14:33.048621",
     "exception": false,
     "start_time": "2023-01-03T00:14:30.082620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 00:14:30.189688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-03 00:14:30.287772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-03 00:14:30.288576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-03 00:14:30.290426: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-03 00:14:30.295288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-03 00:14:30.296015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-03 00:14:30.296696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-03 00:14:32.630570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-03 00:14:32.631470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-03 00:14:32.632118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-03 00:14:32.632718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "validation_data = tf.data.experimental.load('/kaggle/input/cat-vs-dog-ma-sin/cats_dogs_Te')\n",
    "train_data_MA = tf.data.experimental.load('/kaggle/input/cat-vs-dog-ma-sin/cats_dogs_MA_sin_Tr_1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fb421a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T00:14:33.068429Z",
     "iopub.status.busy": "2023-01-03T00:14:33.067567Z",
     "iopub.status.idle": "2023-01-03T00:14:33.078822Z",
     "shell.execute_reply": "2023-01-03T00:14:33.077896Z"
    },
    "id": "D_S0EJ3mFdfK",
    "outputId": "9ed3c2c7-50b4-4445-a01e-c9a3d780c403",
    "papermill": {
     "duration": 0.022794,
     "end_time": "2023-01-03T00:14:33.080803",
     "exception": false,
     "start_time": "2023-01-03T00:14:33.058009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18610"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_count = tf.data.experimental.cardinality(train_data_MA).numpy() # los datos de training son 18610 usar subconjunto de 5000\n",
    "image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a39e65ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T00:14:33.099393Z",
     "iopub.status.busy": "2023-01-03T00:14:33.098854Z",
     "iopub.status.idle": "2023-01-03T00:14:33.106285Z",
     "shell.execute_reply": "2023-01-03T00:14:33.105297Z"
    },
    "id": "ctjLei0TxcVh",
    "outputId": "6f578b73-ebdf-4465-91c7-2adb7d127174",
    "papermill": {
     "duration": 0.019315,
     "end_time": "2023-01-03T00:14:33.108629",
     "exception": false,
     "start_time": "2023-01-03T00:14:33.089314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4652"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_count1 = tf.data.experimental.cardinality(validation_data).numpy() # los datos de training son 18610\n",
    "image_count1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "832b90a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T00:14:33.127433Z",
     "iopub.status.busy": "2023-01-03T00:14:33.126900Z",
     "iopub.status.idle": "2023-01-03T00:14:48.303812Z",
     "shell.execute_reply": "2023-01-03T00:14:48.302791Z"
    },
    "id": "opk5MXl4IwjC",
    "papermill": {
     "duration": 15.188885,
     "end_time": "2023-01-03T00:14:48.306251",
     "exception": false,
     "start_time": "2023-01-03T00:14:33.117366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 00:14:33.157277: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "#X_test = [validation_data[i][0] for i in range(image_count1)]\n",
    "#Y_true_test = [validation_data[i][1] for i in range(image_count1)]\n",
    "Y_true_test = np.asarray([aux[1].numpy() for aux  in validation_data])\n",
    "X_test = np.asarray([aux[0].numpy() for aux  in validation_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83eecd8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T00:14:48.328081Z",
     "iopub.status.busy": "2023-01-03T00:14:48.327771Z",
     "iopub.status.idle": "2023-01-03T00:14:48.334844Z",
     "shell.execute_reply": "2023-01-03T00:14:48.333896Z"
    },
    "id": "-BydcVOQxcVh",
    "outputId": "8c1b4ed2-7c43-4675-f055-f9e4e3f5b3dd",
    "papermill": {
     "duration": 0.020797,
     "end_time": "2023-01-03T00:14:48.337749",
     "exception": false,
     "start_time": "2023-01-03T00:14:48.316952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18610"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc9f3295",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T00:14:48.356784Z",
     "iopub.status.busy": "2023-01-03T00:14:48.356515Z",
     "iopub.status.idle": "2023-01-03T00:14:48.363586Z",
     "shell.execute_reply": "2023-01-03T00:14:48.362757Z"
    },
    "id": "HdFme6fdxcVh",
    "papermill": {
     "duration": 0.018679,
     "end_time": "2023-01-03T00:14:48.365539",
     "exception": false,
     "start_time": "2023-01-03T00:14:48.346860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_size = int(image_count * 0.2)\n",
    "train_ds_MA = train_data_MA.skip(val_size)\n",
    "val_ds_MA = train_data_MA.take(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d29ddf62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T00:14:48.384469Z",
     "iopub.status.busy": "2023-01-03T00:14:48.383644Z",
     "iopub.status.idle": "2023-01-03T00:14:48.393566Z",
     "shell.execute_reply": "2023-01-03T00:14:48.392767Z"
    },
    "id": "aVHIlFpgxcVi",
    "papermill": {
     "duration": 0.021407,
     "end_time": "2023-01-03T00:14:48.395558",
     "exception": false,
     "start_time": "2023-01-03T00:14:48.374151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_batches_MA = train_ds_MA.shuffle(1024).batch(batch_size)\n",
    "val_batches_MA = val_ds_MA.shuffle(1024).batch(batch_size)\n",
    "test_batches_MA = validation_data.shuffle(1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db14ee89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T00:14:48.414373Z",
     "iopub.status.busy": "2023-01-03T00:14:48.413596Z",
     "iopub.status.idle": "2023-01-03T00:14:48.421439Z",
     "shell.execute_reply": "2023-01-03T00:14:48.420624Z"
    },
    "id": "GsB4EA2-xcVi",
    "outputId": "2d45809e-a9cc-408f-9a8b-745e8fe850e9",
    "papermill": {
     "duration": 0.019158,
     "end_time": "2023-01-03T00:14:48.423341",
     "exception": false,
     "start_time": "2023-01-03T00:14:48.404183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14888"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_count = tf.data.experimental.cardinality(train_ds_MA).numpy() # los datos de training son 18610 usar subconjunto de 5000\n",
    "image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac92cd0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T00:14:48.461120Z",
     "iopub.status.busy": "2023-01-03T00:14:48.460430Z",
     "iopub.status.idle": "2023-01-03T00:14:48.471820Z",
     "shell.execute_reply": "2023-01-03T00:14:48.471205Z"
    },
    "id": "Hk33DzwkxcVi",
    "outputId": "aad91eec-842c-4995-de90-5bb715539b6a",
    "papermill": {
     "duration": 0.038914,
     "end_time": "2023-01-03T00:14:48.473557",
     "exception": false,
     "start_time": "2023-01-03T00:14:48.434643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3722"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_count_val = tf.data.experimental.cardinality(val_ds_MA).numpy() # los datos de training son 18610 usar subconjunto de 5000\n",
    "image_count_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f926e31",
   "metadata": {
    "id": "UMeK3NG3xcVi",
    "papermill": {
     "duration": 0.008798,
     "end_time": "2023-01-03T00:14:48.491173",
     "exception": false,
     "start_time": "2023-01-03T00:14:48.482375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0246a32a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T00:14:48.510291Z",
     "iopub.status.busy": "2023-01-03T00:14:48.509518Z",
     "iopub.status.idle": "2023-01-03T00:15:03.937991Z",
     "shell.execute_reply": "2023-01-03T00:15:03.937123Z"
    },
    "id": "uvwc7eixxcVi",
    "outputId": "d7766078-8c40-41ed-fb01-66b5f62a07f1",
    "papermill": {
     "duration": 15.439996,
     "end_time": "2023-01-03T00:15:03.939989",
     "exception": false,
     "start_time": "2023-01-03T00:14:48.499993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 00:14:59.139476: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 47 of 1024\n",
      "2023-01-03 00:15:02.119370: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:228] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotator 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.86      0.84        63\n",
      "         1.0       0.86      0.83      0.84        65\n",
      "\n",
      "    accuracy                           0.84       128\n",
      "   macro avg       0.84      0.84      0.84       128\n",
      "weighted avg       0.84      0.84      0.84       128\n",
      "\n",
      "annotator 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.52      0.53        63\n",
      "         1.0       0.55      0.55      0.55        65\n",
      "\n",
      "    accuracy                           0.54       128\n",
      "   macro avg       0.54      0.54      0.54       128\n",
      "weighted avg       0.54      0.54      0.54       128\n",
      "\n",
      "annotator 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.63      0.63        63\n",
      "         1.0       0.64      0.63      0.64        65\n",
      "\n",
      "    accuracy                           0.63       128\n",
      "   macro avg       0.63      0.63      0.63       128\n",
      "weighted avg       0.63      0.63      0.63       128\n",
      "\n",
      "annotator 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.47      0.43      0.45        63\n",
      "         1.0       0.49      0.52      0.50        65\n",
      "\n",
      "    accuracy                           0.48       128\n",
      "   macro avg       0.48      0.48      0.47       128\n",
      "weighted avg       0.48      0.48      0.48       128\n",
      "\n",
      "annotator 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.26      0.30      0.28        63\n",
      "         1.0       0.21      0.18      0.20        65\n",
      "\n",
      "    accuracy                           0.24       128\n",
      "   macro avg       0.24      0.24      0.24       128\n",
      "weighted avg       0.24      0.24      0.24       128\n",
      "\n",
      "annotator 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82        66\n",
      "         1.0       0.81      0.81      0.81        62\n",
      "\n",
      "    accuracy                           0.81       128\n",
      "   macro avg       0.81      0.81      0.81       128\n",
      "weighted avg       0.81      0.81      0.81       128\n",
      "\n",
      "annotator 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.58      0.53      0.56        66\n",
      "         1.0       0.54      0.60      0.57        62\n",
      "\n",
      "    accuracy                           0.56       128\n",
      "   macro avg       0.56      0.56      0.56       128\n",
      "weighted avg       0.56      0.56      0.56       128\n",
      "\n",
      "annotator 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.68      0.65        66\n",
      "         1.0       0.62      0.55      0.58        62\n",
      "\n",
      "    accuracy                           0.62       128\n",
      "   macro avg       0.62      0.62      0.61       128\n",
      "weighted avg       0.62      0.62      0.62       128\n",
      "\n",
      "annotator 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.55      0.54        66\n",
      "         1.0       0.50      0.48      0.49        62\n",
      "\n",
      "    accuracy                           0.52       128\n",
      "   macro avg       0.51      0.51      0.51       128\n",
      "weighted avg       0.52      0.52      0.52       128\n",
      "\n",
      "annotator 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.26      0.27      0.27        66\n",
      "         1.0       0.19      0.18      0.18        62\n",
      "\n",
      "    accuracy                           0.23       128\n",
      "   macro avg       0.22      0.23      0.22       128\n",
      "weighted avg       0.22      0.23      0.23       128\n",
      "\n",
      "annotator 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.82      0.81        61\n",
      "         1.0       0.83      0.82      0.83        67\n",
      "\n",
      "    accuracy                           0.82       128\n",
      "   macro avg       0.82      0.82      0.82       128\n",
      "weighted avg       0.82      0.82      0.82       128\n",
      "\n",
      "annotator 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.51      0.52      0.52        61\n",
      "         1.0       0.55      0.54      0.55        67\n",
      "\n",
      "    accuracy                           0.53       128\n",
      "   macro avg       0.53      0.53      0.53       128\n",
      "weighted avg       0.53      0.53      0.53       128\n",
      "\n",
      "annotator 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.70      0.66        61\n",
      "         1.0       0.69      0.60      0.64        67\n",
      "\n",
      "    accuracy                           0.65       128\n",
      "   macro avg       0.65      0.65      0.65       128\n",
      "weighted avg       0.65      0.65      0.65       128\n",
      "\n",
      "annotator 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.64      0.60        61\n",
      "         1.0       0.63      0.55      0.59        67\n",
      "\n",
      "    accuracy                           0.59       128\n",
      "   macro avg       0.60      0.60      0.59       128\n",
      "weighted avg       0.60      0.59      0.59       128\n",
      "\n",
      "annotator 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.22      0.25      0.23        61\n",
      "         1.0       0.22      0.19      0.21        67\n",
      "\n",
      "    accuracy                           0.22       128\n",
      "   macro avg       0.22      0.22      0.22       128\n",
      "weighted avg       0.22      0.22      0.22       128\n",
      "\n",
      "annotator 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.80      0.83        64\n",
      "         1.0       0.81      0.88      0.84        64\n",
      "\n",
      "    accuracy                           0.84       128\n",
      "   macro avg       0.84      0.84      0.84       128\n",
      "weighted avg       0.84      0.84      0.84       128\n",
      "\n",
      "annotator 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.59      0.59        64\n",
      "         1.0       0.59      0.59      0.59        64\n",
      "\n",
      "    accuracy                           0.59       128\n",
      "   macro avg       0.59      0.59      0.59       128\n",
      "weighted avg       0.59      0.59      0.59       128\n",
      "\n",
      "annotator 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.61      0.61        64\n",
      "         1.0       0.61      0.61      0.61        64\n",
      "\n",
      "    accuracy                           0.61       128\n",
      "   macro avg       0.61      0.61      0.61       128\n",
      "weighted avg       0.61      0.61      0.61       128\n",
      "\n",
      "annotator 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.48      0.50        64\n",
      "         1.0       0.51      0.55      0.53        64\n",
      "\n",
      "    accuracy                           0.52       128\n",
      "   macro avg       0.52      0.52      0.52       128\n",
      "weighted avg       0.52      0.52      0.52       128\n",
      "\n",
      "annotator 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.22      0.23      0.23        64\n",
      "         1.0       0.20      0.19      0.19        64\n",
      "\n",
      "    accuracy                           0.21       128\n",
      "   macro avg       0.21      0.21      0.21       128\n",
      "weighted avg       0.21      0.21      0.21       128\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "i = 0\n",
    "fig, ax = plt.subplots(1, 4)\n",
    "for image, label, label2 in train_batches_MA.take(4):\n",
    "   # predictedLabel = int(predictions[i] >= 0.5)\n",
    "   # print(label2)\n",
    "    ax[i].axis('off')\n",
    "   # ax[i].set_title(classNames[label[i]])\n",
    "    ax[i].imshow(image[0])\n",
    "    i += 1\n",
    "    for j in range(label2.shape[1]):\n",
    "      print('annotator',j+1)\n",
    "      print(classification_report(label ,label2[:,j]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50fe91d",
   "metadata": {
    "id": "9AgOHREc1bmd",
    "papermill": {
     "duration": 0.010289,
     "end_time": "2023-01-03T00:15:03.960808",
     "exception": false,
     "start_time": "2023-01-03T00:15:03.950519",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Build the classifier from multiple annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4b62d73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T00:15:03.982751Z",
     "iopub.status.busy": "2023-01-03T00:15:03.982471Z",
     "iopub.status.idle": "2023-01-03T00:15:04.008695Z",
     "shell.execute_reply": "2023-01-03T00:15:04.007854Z"
    },
    "id": "k-ePr0-fxcVi",
    "papermill": {
     "duration": 0.039751,
     "end_time": "2023-01-03T00:15:04.010666",
     "exception": false,
     "start_time": "2023-01-03T00:15:03.970915",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense,Conv2D,Flatten,MaxPooling2D,GlobalAveragePooling2D\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "class MultipleAnnotators_Classification():\n",
    "    def __init__(self, output_dim, num_annotators, q= 0.0001):\n",
    "        self.K = output_dim\n",
    "        self.R = num_annotators\n",
    "        self.q = q\n",
    "        #self.callbacks #=callbacks\n",
    "        #self.l1_param=l1_param \n",
    "        #self.l2_param=l1_param\n",
    "\n",
    "    def CrowdLayer(self, input):\n",
    "       #x = keras.layers.Dense(self.R + self.K, kernel_regularizer=regularizers.L1L2(l1= 1e-2, l2=1e-3),  activation='tanh')(input)\n",
    "        output_cla = keras.layers.Dense(self.K,  activation='softmax')(input)\n",
    "        output_ann = keras.layers.Dense(self.R,  activation='sigmoid')(input)\n",
    "        output = keras.layers.Concatenate()([output_cla, output_ann])\n",
    "        \n",
    "        return output\n",
    "#RCDNN   \n",
    "    def loss(self):\n",
    "        def custom_loss(y_true, y_pred):\n",
    "            # print(y_true,y_pred)\n",
    "            pred = y_pred[:, :self.K]\n",
    "            pred = tf.clip_by_value(pred, clip_value_min=1e-9, clip_value_max=1-1e-9) #estabilidad numerica de la funcion de costo\n",
    "            ann_ = y_pred[:, self.K:]\n",
    "            Y_true = tf.one_hot(tf.cast(y_true, dtype=tf.int32), depth=self.K, axis=1)\n",
    "            Y_hat = tf.repeat(tf.expand_dims(pred,-1), self.R, axis = -1)\n",
    "            p_logreg = tf.math.reduce_prod(tf.math.pow(Y_hat, Y_true), axis=1)\n",
    "            temp1 = ann_*tf.math.log(p_logreg)  \n",
    "            temp2 = (1 - ann_)*tf.math.log(1/self.K)*tf.reduce_sum(Y_true,axis=1)\n",
    "            # temp2 = (tf.ones(tf.shape(ann_)) - ann_)*tf.math.log(1/K)\n",
    "            # print(tf.reduce_mean(Y_true,axis=1).numpy())\n",
    "            return -tf.math.reduce_sum((temp1 + temp2))\n",
    "        return custom_loss\n",
    "    \n",
    "#     def loss(self):\n",
    "#         def custom_loss(y_true, y_pred):\n",
    "#                # print(y_true,y_pred)\n",
    "#            # q = 0.1\n",
    "#             pred = y_pred[:, :self.K]\n",
    "#             pred = tf.clip_by_value(pred, clip_value_min=1e-9, clip_value_max=1)\n",
    "#             ann_ = y_pred[:, self.K:]\n",
    "#             # ann_ = tf.clip_by_value(ann_, clip_value_min=1e-9, clip_value_max=1-1e-9)\n",
    "#             Y_true = tf.one_hot(tf.cast(y_true, dtype=tf.int32), depth=self.K, axis=1)\n",
    "#             Y_hat = tf.repeat(tf.expand_dims(pred,-1), self.R, axis = -1)\n",
    "\n",
    "#             p_gcce = Y_true*(1 - Y_hat**self.q)/self.q\n",
    "#             temp1 = ann_*tf.math.reduce_sum(p_gcce, axis=1)\n",
    "#             temp2 = (1 - ann_)*(1-(1/self.K)**self.q)/self.q*tf.reduce_sum(Y_true,axis=1)\n",
    "#             return tf.math.reduce_sum((temp1 + temp2))\n",
    "#         return custom_loss\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, x, Y, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.model(x, training=True)\n",
    "            loss_value = self.loss_fn(Y, logits)\n",
    "        grads = tape.gradient(loss_value, self.model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "        self.train_acc_metric.update_state(y, logits[:, :self.K])\n",
    "        return loss_value\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, x, y):\n",
    "        val_logits = self.model(x, training=False)\n",
    "        self.val_acc_metric.update_state(y, val_logits[:,:self.K])\n",
    "\n",
    "    def fit(self, model, Data_tr, Data_Val, epochs):\n",
    "        self.model = model\n",
    "        #++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        # Instantiate an optimizer.\n",
    "        #self.optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-4)\n",
    "        self.optimizer =  tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        #self.optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4, clipnorm=1.0)\n",
    "        #++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        # Instantiate a loss function.\n",
    "        self.loss_fn = self.loss()\n",
    "        self.train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "        self.val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "        train_loss = np.zeros(epochs)\n",
    "        train_accur = np.zeros(epochs)\n",
    "        val_accur = np.zeros(epochs)\n",
    "        val_loss = np.zeros(epochs)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Iterate over the batches of the dataset.\n",
    "            for step, (x_batch_train, y_batch_train, Y_batch_train) in enumerate(Data_tr):\n",
    "                # print(y_batch_train, Y_batch_train)\n",
    "                loss_value = self.train_step(x_batch_train, Y_batch_train, y_batch_train)\n",
    "\n",
    "                # Log every 200 batches.\n",
    "                if step % 10 == 0:\n",
    "                    train_acc = self.train_acc_metric.result()\n",
    "                    print(\n",
    "                      \"Training loss (for one batch) at step %d: %.4f, Accuracy: %.4f\"\n",
    "                      % (step, float(loss_value), float(train_acc))\n",
    "                            )\n",
    "                # print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "\n",
    "\n",
    "\n",
    "            # Run a validation loop at the end of each epoch.\n",
    "            for x_batch_val, y_batch_val,Y_batch_val in Data_Val:\n",
    "\n",
    "                val_logits = model(x_batch_val, training=False)\n",
    "\n",
    "                val_loss_value = self.loss_fn(Y_batch_val, val_logits)\n",
    "\n",
    "                self.val_acc_metric.update_state(y_batch_val, val_logits[:,:self.K])\n",
    "                \n",
    "               # np.round(np.mean([model(x_batch_val, training= True) for sample in range(100)]), 2)\n",
    "\n",
    "\n",
    "             # Display metrics at the end of each epoch.\n",
    "            train_acc = self.train_acc_metric.result()\n",
    "            val_acc = self.val_acc_metric.result()\n",
    "\n",
    "\n",
    "            print('---- Training ----')\n",
    "            print(\"Training loss: %.4f\" % (float(loss_value),))\n",
    "            print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "            # Reset training metrics at the end of each epoch\n",
    "            self.train_acc_metric.reset_states()\n",
    "            self.val_acc_metric.reset_states()\n",
    "\n",
    "\n",
    "            train_loss[epoch] = float(loss_value)\n",
    "            train_accur[epoch] = float(train_acc)\n",
    "\n",
    "            val_accur[epoch] = float(val_acc)\n",
    "            val_loss[epoch] = float(val_loss_value) \n",
    "\n",
    "\n",
    "            print('---- Validation ----')\n",
    "            print(\"Validation loss: %.4f\" % (float(val_loss_value),))\n",
    "            print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "\n",
    "            print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.suptitle('Loss and accuracy')\n",
    "        ax1.plot(range(1,epochs+1),train_loss)\n",
    "        ax1.plot(range(1,epochs+1), val_loss)\n",
    "        ax2.plot(range(1,epochs+1),train_accur)\n",
    "        ax2.plot(range(1,epochs+1),val_accur)\n",
    "        #plt.figure(figsize=(16,9))\n",
    "        ax1.set(xlabel= 'Epoch', ylabel=\"Loss\")\n",
    "        ax2.set(xlabel= 'Epoch',ylabel=\"Accuracy\")\n",
    "        ax1.legend(['Training_loss', 'Validation_loss'])\n",
    "        ax2.legend(['Training', 'Validation'])\n",
    "        ax1.grid()\n",
    "        ax2.grid()\n",
    "        plt.show()\n",
    "        return self.model\n",
    "\n",
    "    def eval_model(self, Data):\n",
    "        self.val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "        for x_batch_val, y_batch_val in Data:\n",
    "            self.test_step(x_batch_val, y_batch_val)\n",
    "\n",
    "        val_acc = self.val_acc_metric.result()\n",
    "        self.val_acc_metric.reset_states()\n",
    "        return val_acc\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ac5193e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T00:15:04.032873Z",
     "iopub.status.busy": "2023-01-03T00:15:04.032573Z",
     "iopub.status.idle": "2023-01-03T00:15:04.039728Z",
     "shell.execute_reply": "2023-01-03T00:15:04.038609Z"
    },
    "id": "4l-_pkpaBkSv",
    "papermill": {
     "duration": 0.020981,
     "end_time": "2023-01-03T00:15:04.042274",
     "exception": false,
     "start_time": "2023-01-03T00:15:04.021293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "  # print(y_true,y_pred)\n",
    "  K = 2 #len(np.unique(y_true))\n",
    "  R = 5\n",
    "  q = 0.1\n",
    "  pred = y_pred[:, K]\n",
    "  pred = tf.clip_by_value(pred, clip_value_min=1e-9, clip_value_max=1)\n",
    "  ann_ = y_pred[:,  K:]\n",
    "  # ann_ = tf.clip_by_value(ann_, clip_value_min=1e-9, clip_value_max=1-1e-9)\n",
    "  Y_true = tf.one_hot(tf.cast(y_true, dtype=tf.int32), depth=K, axis=1)\n",
    "  Y_hat = tf.repeat(tf.expand_dims(pred,-1), R, axis = -1)\n",
    "\n",
    "  p_gcce = Y_true*(1 - Y_hat**q)/q\n",
    "  temp1 = ann_*tf.math.reduce_sum(p_gcce, axis=1)\n",
    "  temp2 = (1 - ann_)*(1-(1/K)**q)/q*tf.reduce_sum(Y_true,axis=1)\n",
    "  return tf.math.reduce_sum((temp1 + temp2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29d02696",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T00:15:04.064993Z",
     "iopub.status.busy": "2023-01-03T00:15:04.064718Z",
     "iopub.status.idle": "2023-01-03T00:15:04.075035Z",
     "shell.execute_reply": "2023-01-03T00:15:04.074107Z"
    },
    "id": "0I4Rrc5TxcVj",
    "papermill": {
     "duration": 0.023931,
     "end_time": "2023-01-03T00:15:04.077046",
     "exception": false,
     "start_time": "2023-01-03T00:15:04.053115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MA = MultipleAnnotators_Classification(2, 5, 0.001)\n",
    " \n",
    "def create_model():\n",
    "   \n",
    "    l1 = 1e-2\n",
    "    # Block 1\n",
    "    inputs = keras.layers.Input(shape=(150, 150, 3), name='entrada')\n",
    "    x = keras.layers.BatchNormalization()(inputs)\n",
    "    x = keras.layers.Conv2D(32, (3, 3), activation=\"relu\" , name=\"block1_conv1\")(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "   # x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name=\"block1_pool\")(x)\n",
    "\n",
    "\n",
    "    # Block 2\n",
    "   # x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Conv2D(32, (3, 3), activation=\"relu\", name=\"block2_conv1\")(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    #x = keras.layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name=\"block2_pool\")(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Conv2D(64, (3, 3), activation=\"relu\", name=\"block3_conv1\" )(x)             \n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "   # x = keras.layers.Dropout(0.2)(x)\n",
    "   \n",
    "   # x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name=\"block3_pool\")(x)\n",
    "    \n",
    "    # Block 4\n",
    "   # x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Conv2D(64, (3, 3), activation=\"relu\", name=\"block4_conv1\")(x)            \n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name=\"block4_pool\")(x)\n",
    "    #x = keras.layers.Dropout(0.2)(x)\n",
    "    \n",
    "    #x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "   \n",
    "    x = keras.layers.Flatten()(x)\n",
    "    #x = keras.layers.Dropout(0.5)(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    output = MA.CrowdLayer(x)\n",
    "    model = keras.Model(inputs=inputs,outputs=output)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ae3cc3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T00:15:04.099363Z",
     "iopub.status.busy": "2023-01-03T00:15:04.099076Z",
     "iopub.status.idle": "2023-01-03T00:15:04.102823Z",
     "shell.execute_reply": "2023-01-03T00:15:04.101953Z"
    },
    "id": "iZAxrNF3_hE_",
    "papermill": {
     "duration": 0.017336,
     "end_time": "2023-01-03T00:15:04.104932",
     "exception": false,
     "start_time": "2023-01-03T00:15:04.087596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "# callbacks = [\n",
    "#     EarlyStopping(patience=10, verbose=1),\n",
    "#     ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00001, verbose=1),\n",
    "#     ModelCheckpoint('model1.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719a00a6",
   "metadata": {
    "id": "Z-fV95n3GEqa",
    "papermill": {
     "duration": 0.010166,
     "end_time": "2023-01-03T00:15:04.125794",
     "exception": false,
     "start_time": "2023-01-03T00:15:04.115628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c87168fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T00:15:04.148015Z",
     "iopub.status.busy": "2023-01-03T00:15:04.147224Z",
     "iopub.status.idle": "2023-01-03T00:15:04.152096Z",
     "shell.execute_reply": "2023-01-03T00:15:04.151168Z"
    },
    "id": "_H_sb1cl1FC_",
    "outputId": "59d957da-9223-4a01-e4d9-33933f7a2f4a",
    "papermill": {
     "duration": 0.017943,
     "end_time": "2023-01-03T00:15:04.154106",
     "exception": false,
     "start_time": "2023-01-03T00:15:04.136163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# classification_report_r= []\n",
    "# model = create_model()\n",
    "# K=2\n",
    "# R=5\n",
    "# NUM_RUNS = 5\n",
    "# N_EPOCHS = 30\n",
    "# val_acc = np.zeros(NUM_RUNS)\n",
    "# for i in range(NUM_RUNS):\n",
    "#   MA = MultipleAnnotators_Classification(K, R, 0.1)\n",
    "#   model = create_model()\n",
    "#   optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, clipnorm=1.0)\n",
    "#   model.compile(optimizer=optimizer, loss= MA.loss())\n",
    "#   history_model = model.fit(train_batches_MA, validation_data=val_batches_MA, epochs= N_EPOCHS, callbacks=callbacks, verbose=0)\n",
    "#   #model = MA.fit(model, Data_train_MA, N_EPOCHS)\n",
    "#   pred_2 = model.predict(X_test)\n",
    "\n",
    "#   lambda_R_ = pred_2[:, K:] #annotators reliability prediction N x R   \n",
    "#   classification_report_r += [classification_report( pred_2[:,:K].argmax(axis=1),Y_true_test.ravel(),output_dict=True)]\n",
    "#   print(classification_report( pred_2[:,:K].argmax(axis=1),Y_true_test.ravel()))\n",
    "#   #val_acc[i] = MA.eval_model(test_batches_MA)\n",
    "#   #print(\"Validation acc: %.4f\" % (float(val_acc[i]),))\n",
    "#   # Create the history figure\n",
    "#   plt.figure(figsize=(16,9))\n",
    "#   for i in  history_model.history:\n",
    "#       plt.plot(history_model.history[i],label=i)\n",
    "#   plt.title('Model history')\n",
    "#   plt.legend()\n",
    "#   plt.grid()\n",
    "\n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame(val_acc)\n",
    "# #df.to_csimport pandas as pddf = pd.DataFrame(val_acc)#df.to_csv('/kaggle/working/CatDogs_MA_InceptionV3.csv',index=False) # save to notebook output​v('/kaggle/working/CatDogs_MA_InceptionV3.csv',index=False) # save to notebook output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16270fe9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T00:15:04.176596Z",
     "iopub.status.busy": "2023-01-03T00:15:04.176326Z",
     "iopub.status.idle": "2023-01-03T05:19:54.680119Z",
     "shell.execute_reply": "2023-01-03T05:19:54.678783Z"
    },
    "id": "Mu0lyAUIGSTB",
    "outputId": "cb82872d-c3ba-4d76-a28c-237eb266e78b",
    "papermill": {
     "duration": 18290.519763,
     "end_time": "2023-01-03T05:19:54.684861",
     "exception": false,
     "start_time": "2023-01-03T00:15:04.165098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 00:15:07.364570: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 0: 656.9564, Accuracy: 0.4766\n",
      "Training loss (for one batch) at step 10: 563.9164, Accuracy: 0.5057\n",
      "Training loss (for one batch) at step 20: 543.3098, Accuracy: 0.5126\n",
      "Training loss (for one batch) at step 30: 549.2102, Accuracy: 0.5129\n",
      "Training loss (for one batch) at step 40: 493.2834, Accuracy: 0.5072\n",
      "Training loss (for one batch) at step 50: 493.4080, Accuracy: 0.5034\n",
      "Training loss (for one batch) at step 60: 484.9579, Accuracy: 0.5092\n",
      "Training loss (for one batch) at step 70: 474.2472, Accuracy: 0.5077\n",
      "Training loss (for one batch) at step 80: 480.1964, Accuracy: 0.5099\n",
      "Training loss (for one batch) at step 90: 480.3363, Accuracy: 0.5097\n",
      "Training loss (for one batch) at step 100: 465.7234, Accuracy: 0.5102\n",
      "Training loss (for one batch) at step 110: 464.5514, Accuracy: 0.5083\n",
      "---- Training ----\n",
      "Training loss: 154.0789\n",
      "Training acc over epoch: 0.5091\n",
      "---- Validation ----\n",
      "Validation loss: 34.1488\n",
      "Validation acc: 0.5132\n",
      "Time taken: 64.82s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 478.2643, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 466.5909, Accuracy: 0.5227\n",
      "Training loss (for one batch) at step 20: 454.5555, Accuracy: 0.5167\n",
      "Training loss (for one batch) at step 30: 455.8902, Accuracy: 0.5098\n",
      "Training loss (for one batch) at step 40: 452.2205, Accuracy: 0.5088\n",
      "Training loss (for one batch) at step 50: 457.3409, Accuracy: 0.5066\n",
      "Training loss (for one batch) at step 60: 459.3384, Accuracy: 0.5088\n",
      "Training loss (for one batch) at step 70: 456.9925, Accuracy: 0.5106\n",
      "Training loss (for one batch) at step 80: 451.9912, Accuracy: 0.5106\n",
      "Training loss (for one batch) at step 90: 446.7426, Accuracy: 0.5112\n",
      "Training loss (for one batch) at step 100: 446.3700, Accuracy: 0.5128\n",
      "Training loss (for one batch) at step 110: 449.7564, Accuracy: 0.5117\n",
      "---- Training ----\n",
      "Training loss: 148.4933\n",
      "Training acc over epoch: 0.5116\n",
      "---- Validation ----\n",
      "Validation loss: 36.4663\n",
      "Validation acc: 0.5126\n",
      "Time taken: 21.00s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 452.2011, Accuracy: 0.4844\n",
      "Training loss (for one batch) at step 10: 450.2125, Accuracy: 0.5050\n",
      "Training loss (for one batch) at step 20: 447.9263, Accuracy: 0.5153\n",
      "Training loss (for one batch) at step 30: 449.4242, Accuracy: 0.5126\n",
      "Training loss (for one batch) at step 40: 450.4097, Accuracy: 0.5192\n",
      "Training loss (for one batch) at step 50: 447.3922, Accuracy: 0.5267\n",
      "Training loss (for one batch) at step 60: 447.1030, Accuracy: 0.5278\n",
      "Training loss (for one batch) at step 70: 445.3578, Accuracy: 0.5278\n",
      "Training loss (for one batch) at step 80: 444.2218, Accuracy: 0.5271\n",
      "Training loss (for one batch) at step 90: 447.2610, Accuracy: 0.5246\n",
      "Training loss (for one batch) at step 100: 448.4834, Accuracy: 0.5243\n",
      "Training loss (for one batch) at step 110: 446.3277, Accuracy: 0.5267\n",
      "---- Training ----\n",
      "Training loss: 138.4524\n",
      "Training acc over epoch: 0.5281\n",
      "---- Validation ----\n",
      "Validation loss: 34.8022\n",
      "Validation acc: 0.5543\n",
      "Time taken: 18.43s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 445.2908, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 443.2294, Accuracy: 0.5376\n",
      "Training loss (for one batch) at step 20: 445.3890, Accuracy: 0.5316\n",
      "Training loss (for one batch) at step 30: 443.3368, Accuracy: 0.5295\n",
      "Training loss (for one batch) at step 40: 445.3210, Accuracy: 0.5341\n",
      "Training loss (for one batch) at step 50: 442.8741, Accuracy: 0.5368\n",
      "Training loss (for one batch) at step 60: 444.8065, Accuracy: 0.5375\n",
      "Training loss (for one batch) at step 70: 443.4263, Accuracy: 0.5426\n",
      "Training loss (for one batch) at step 80: 447.3266, Accuracy: 0.5460\n",
      "Training loss (for one batch) at step 90: 441.6135, Accuracy: 0.5459\n",
      "Training loss (for one batch) at step 100: 439.0031, Accuracy: 0.5470\n",
      "Training loss (for one batch) at step 110: 442.0659, Accuracy: 0.5470\n",
      "---- Training ----\n",
      "Training loss: 138.6333\n",
      "Training acc over epoch: 0.5485\n",
      "---- Validation ----\n",
      "Validation loss: 34.6240\n",
      "Validation acc: 0.6024\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 445.1988, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 444.4843, Accuracy: 0.5646\n",
      "Training loss (for one batch) at step 20: 439.9179, Accuracy: 0.5640\n",
      "Training loss (for one batch) at step 30: 441.6695, Accuracy: 0.5643\n",
      "Training loss (for one batch) at step 40: 439.1671, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 50: 440.8353, Accuracy: 0.5732\n",
      "Training loss (for one batch) at step 60: 439.5266, Accuracy: 0.5779\n",
      "Training loss (for one batch) at step 70: 443.7099, Accuracy: 0.5798\n",
      "Training loss (for one batch) at step 80: 443.9932, Accuracy: 0.5762\n",
      "Training loss (for one batch) at step 90: 445.7957, Accuracy: 0.5768\n",
      "Training loss (for one batch) at step 100: 441.6473, Accuracy: 0.5790\n",
      "Training loss (for one batch) at step 110: 446.0453, Accuracy: 0.5791\n",
      "---- Training ----\n",
      "Training loss: 137.3264\n",
      "Training acc over epoch: 0.5801\n",
      "---- Validation ----\n",
      "Validation loss: 34.8732\n",
      "Validation acc: 0.5991\n",
      "Time taken: 18.08s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 441.8637, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 444.7586, Accuracy: 0.5675\n",
      "Training loss (for one batch) at step 20: 437.9290, Accuracy: 0.5621\n",
      "Training loss (for one batch) at step 30: 440.2354, Accuracy: 0.5789\n",
      "Training loss (for one batch) at step 40: 447.0769, Accuracy: 0.5800\n",
      "Training loss (for one batch) at step 50: 440.4658, Accuracy: 0.5908\n",
      "Training loss (for one batch) at step 60: 447.1488, Accuracy: 0.5912\n",
      "Training loss (for one batch) at step 70: 444.7661, Accuracy: 0.5951\n",
      "Training loss (for one batch) at step 80: 442.8295, Accuracy: 0.5961\n",
      "Training loss (for one batch) at step 90: 442.9269, Accuracy: 0.5919\n",
      "Training loss (for one batch) at step 100: 440.3583, Accuracy: 0.5946\n",
      "Training loss (for one batch) at step 110: 442.4408, Accuracy: 0.5965\n",
      "---- Training ----\n",
      "Training loss: 139.3313\n",
      "Training acc over epoch: 0.5977\n",
      "---- Validation ----\n",
      "Validation loss: 34.7094\n",
      "Validation acc: 0.6343\n",
      "Time taken: 20.25s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 443.5953, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 443.3701, Accuracy: 0.6165\n",
      "Training loss (for one batch) at step 20: 438.3666, Accuracy: 0.5971\n",
      "Training loss (for one batch) at step 30: 435.2464, Accuracy: 0.5993\n",
      "Training loss (for one batch) at step 40: 441.5586, Accuracy: 0.6000\n",
      "Training loss (for one batch) at step 50: 432.7784, Accuracy: 0.6059\n",
      "Training loss (for one batch) at step 60: 440.1534, Accuracy: 0.6082\n",
      "Training loss (for one batch) at step 70: 441.4347, Accuracy: 0.6085\n",
      "Training loss (for one batch) at step 80: 442.6855, Accuracy: 0.6078\n",
      "Training loss (for one batch) at step 90: 436.4860, Accuracy: 0.6060\n",
      "Training loss (for one batch) at step 100: 439.5043, Accuracy: 0.6067\n",
      "Training loss (for one batch) at step 110: 439.8595, Accuracy: 0.6082\n",
      "---- Training ----\n",
      "Training loss: 136.3828\n",
      "Training acc over epoch: 0.6090\n",
      "---- Validation ----\n",
      "Validation loss: 34.2378\n",
      "Validation acc: 0.6365\n",
      "Time taken: 18.08s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 439.8766, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 440.5597, Accuracy: 0.6143\n",
      "Training loss (for one batch) at step 20: 441.1482, Accuracy: 0.6049\n",
      "Training loss (for one batch) at step 30: 436.6145, Accuracy: 0.6003\n",
      "Training loss (for one batch) at step 40: 438.0146, Accuracy: 0.6090\n",
      "Training loss (for one batch) at step 50: 437.7910, Accuracy: 0.6147\n",
      "Training loss (for one batch) at step 60: 440.0766, Accuracy: 0.6209\n",
      "Training loss (for one batch) at step 70: 439.1937, Accuracy: 0.6244\n",
      "Training loss (for one batch) at step 80: 439.6217, Accuracy: 0.6233\n",
      "Training loss (for one batch) at step 90: 438.0356, Accuracy: 0.6226\n",
      "Training loss (for one batch) at step 100: 435.1191, Accuracy: 0.6230\n",
      "Training loss (for one batch) at step 110: 439.1363, Accuracy: 0.6244\n",
      "---- Training ----\n",
      "Training loss: 134.6425\n",
      "Training acc over epoch: 0.6243\n",
      "---- Validation ----\n",
      "Validation loss: 34.4195\n",
      "Validation acc: 0.6182\n",
      "Time taken: 18.14s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 435.4420, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 444.2797, Accuracy: 0.6222\n",
      "Training loss (for one batch) at step 20: 440.0511, Accuracy: 0.6120\n",
      "Training loss (for one batch) at step 30: 435.9523, Accuracy: 0.6149\n",
      "Training loss (for one batch) at step 40: 428.6259, Accuracy: 0.6208\n",
      "Training loss (for one batch) at step 50: 434.0531, Accuracy: 0.6230\n",
      "Training loss (for one batch) at step 60: 444.6940, Accuracy: 0.6313\n",
      "Training loss (for one batch) at step 70: 443.6179, Accuracy: 0.6353\n",
      "Training loss (for one batch) at step 80: 439.9921, Accuracy: 0.6322\n",
      "Training loss (for one batch) at step 90: 440.0175, Accuracy: 0.6290\n",
      "Training loss (for one batch) at step 100: 429.9513, Accuracy: 0.6313\n",
      "Training loss (for one batch) at step 110: 439.2825, Accuracy: 0.6313\n",
      "---- Training ----\n",
      "Training loss: 133.0781\n",
      "Training acc over epoch: 0.6331\n",
      "---- Validation ----\n",
      "Validation loss: 34.4315\n",
      "Validation acc: 0.6569\n",
      "Time taken: 18.40s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 438.9788, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 438.0002, Accuracy: 0.6278\n",
      "Training loss (for one batch) at step 20: 436.1595, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 30: 432.5573, Accuracy: 0.6280\n",
      "Training loss (for one batch) at step 40: 436.1678, Accuracy: 0.6359\n",
      "Training loss (for one batch) at step 50: 418.8699, Accuracy: 0.6409\n",
      "Training loss (for one batch) at step 60: 433.7995, Accuracy: 0.6473\n",
      "Training loss (for one batch) at step 70: 436.7140, Accuracy: 0.6516\n",
      "Training loss (for one batch) at step 80: 431.8406, Accuracy: 0.6462\n",
      "Training loss (for one batch) at step 90: 436.0688, Accuracy: 0.6455\n",
      "Training loss (for one batch) at step 100: 432.2090, Accuracy: 0.6471\n",
      "Training loss (for one batch) at step 110: 434.4649, Accuracy: 0.6485\n",
      "---- Training ----\n",
      "Training loss: 136.8590\n",
      "Training acc over epoch: 0.6482\n",
      "---- Validation ----\n",
      "Validation loss: 34.2408\n",
      "Validation acc: 0.6462\n",
      "Time taken: 18.28s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 442.0291, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 432.4736, Accuracy: 0.6463\n",
      "Training loss (for one batch) at step 20: 437.1864, Accuracy: 0.6350\n",
      "Training loss (for one batch) at step 30: 424.6157, Accuracy: 0.6439\n",
      "Training loss (for one batch) at step 40: 430.7028, Accuracy: 0.6496\n",
      "Training loss (for one batch) at step 50: 420.6181, Accuracy: 0.6573\n",
      "Training loss (for one batch) at step 60: 425.4001, Accuracy: 0.6628\n",
      "Training loss (for one batch) at step 70: 436.9254, Accuracy: 0.6660\n",
      "Training loss (for one batch) at step 80: 438.0911, Accuracy: 0.6625\n",
      "Training loss (for one batch) at step 90: 432.9799, Accuracy: 0.6572\n",
      "Training loss (for one batch) at step 100: 428.1252, Accuracy: 0.6583\n",
      "Training loss (for one batch) at step 110: 438.9662, Accuracy: 0.6591\n",
      "---- Training ----\n",
      "Training loss: 135.7170\n",
      "Training acc over epoch: 0.6585\n",
      "---- Validation ----\n",
      "Validation loss: 33.9700\n",
      "Validation acc: 0.6448\n",
      "Time taken: 18.11s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 437.2924, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 437.1500, Accuracy: 0.6506\n",
      "Training loss (for one batch) at step 20: 438.7097, Accuracy: 0.6510\n",
      "Training loss (for one batch) at step 30: 431.6166, Accuracy: 0.6550\n",
      "Training loss (for one batch) at step 40: 427.9998, Accuracy: 0.6618\n",
      "Training loss (for one batch) at step 50: 408.4163, Accuracy: 0.6743\n",
      "Training loss (for one batch) at step 60: 413.0212, Accuracy: 0.6828\n",
      "Training loss (for one batch) at step 70: 439.0441, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 80: 438.8060, Accuracy: 0.6807\n",
      "Training loss (for one batch) at step 90: 434.3466, Accuracy: 0.6776\n",
      "Training loss (for one batch) at step 100: 423.6349, Accuracy: 0.6765\n",
      "Training loss (for one batch) at step 110: 435.6768, Accuracy: 0.6774\n",
      "---- Training ----\n",
      "Training loss: 132.4863\n",
      "Training acc over epoch: 0.6775\n",
      "---- Validation ----\n",
      "Validation loss: 34.1970\n",
      "Validation acc: 0.6695\n",
      "Time taken: 18.13s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 439.6580, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 440.2312, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 20: 428.5217, Accuracy: 0.6514\n",
      "Training loss (for one batch) at step 30: 426.5986, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 40: 418.7849, Accuracy: 0.6759\n",
      "Training loss (for one batch) at step 50: 412.3283, Accuracy: 0.6829\n",
      "Training loss (for one batch) at step 60: 421.9102, Accuracy: 0.6904\n",
      "Training loss (for one batch) at step 70: 427.3528, Accuracy: 0.6893\n",
      "Training loss (for one batch) at step 80: 433.7603, Accuracy: 0.6856\n",
      "Training loss (for one batch) at step 90: 425.7829, Accuracy: 0.6846\n",
      "Training loss (for one batch) at step 100: 420.4962, Accuracy: 0.6829\n",
      "Training loss (for one batch) at step 110: 428.3873, Accuracy: 0.6826\n",
      "---- Training ----\n",
      "Training loss: 133.7870\n",
      "Training acc over epoch: 0.6830\n",
      "---- Validation ----\n",
      "Validation loss: 37.6650\n",
      "Validation acc: 0.6620\n",
      "Time taken: 18.19s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 442.0018, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 433.3278, Accuracy: 0.6847\n",
      "Training loss (for one batch) at step 20: 430.5785, Accuracy: 0.6838\n",
      "Training loss (for one batch) at step 30: 416.1050, Accuracy: 0.6860\n",
      "Training loss (for one batch) at step 40: 418.3253, Accuracy: 0.6885\n",
      "Training loss (for one batch) at step 50: 417.2584, Accuracy: 0.6965\n",
      "Training loss (for one batch) at step 60: 409.9238, Accuracy: 0.7050\n",
      "Training loss (for one batch) at step 70: 419.5822, Accuracy: 0.7035\n",
      "Training loss (for one batch) at step 80: 419.3517, Accuracy: 0.6970\n",
      "Training loss (for one batch) at step 90: 421.7477, Accuracy: 0.6958\n",
      "Training loss (for one batch) at step 100: 426.6942, Accuracy: 0.6960\n",
      "Training loss (for one batch) at step 110: 421.9911, Accuracy: 0.6981\n",
      "---- Training ----\n",
      "Training loss: 128.3657\n",
      "Training acc over epoch: 0.7000\n",
      "---- Validation ----\n",
      "Validation loss: 36.6148\n",
      "Validation acc: 0.6483\n",
      "Time taken: 18.48s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 456.4919, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 426.4175, Accuracy: 0.6847\n",
      "Training loss (for one batch) at step 20: 410.4835, Accuracy: 0.6734\n",
      "Training loss (for one batch) at step 30: 405.6211, Accuracy: 0.6885\n",
      "Training loss (for one batch) at step 40: 418.2940, Accuracy: 0.6966\n",
      "Training loss (for one batch) at step 50: 404.9901, Accuracy: 0.7080\n",
      "Training loss (for one batch) at step 60: 396.5921, Accuracy: 0.7147\n",
      "Training loss (for one batch) at step 70: 425.5796, Accuracy: 0.7143\n",
      "Training loss (for one batch) at step 80: 426.8210, Accuracy: 0.7089\n",
      "Training loss (for one batch) at step 90: 413.9116, Accuracy: 0.7059\n",
      "Training loss (for one batch) at step 100: 410.8226, Accuracy: 0.7082\n",
      "Training loss (for one batch) at step 110: 422.8261, Accuracy: 0.7080\n",
      "---- Training ----\n",
      "Training loss: 133.4287\n",
      "Training acc over epoch: 0.7063\n",
      "---- Validation ----\n",
      "Validation loss: 35.7707\n",
      "Validation acc: 0.6714\n",
      "Time taken: 21.00s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 425.7213, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 425.4881, Accuracy: 0.6662\n",
      "Training loss (for one batch) at step 20: 416.5614, Accuracy: 0.6737\n",
      "Training loss (for one batch) at step 30: 411.0530, Accuracy: 0.6865\n",
      "Training loss (for one batch) at step 40: 404.6544, Accuracy: 0.6966\n",
      "Training loss (for one batch) at step 50: 377.5548, Accuracy: 0.7091\n",
      "Training loss (for one batch) at step 60: 398.4655, Accuracy: 0.7180\n",
      "Training loss (for one batch) at step 70: 393.4411, Accuracy: 0.7186\n",
      "Training loss (for one batch) at step 80: 408.0128, Accuracy: 0.7149\n",
      "Training loss (for one batch) at step 90: 411.1075, Accuracy: 0.7123\n",
      "Training loss (for one batch) at step 100: 394.0298, Accuracy: 0.7147\n",
      "Training loss (for one batch) at step 110: 400.8701, Accuracy: 0.7152\n",
      "---- Training ----\n",
      "Training loss: 127.3718\n",
      "Training acc over epoch: 0.7154\n",
      "---- Validation ----\n",
      "Validation loss: 32.4529\n",
      "Validation acc: 0.6631\n",
      "Time taken: 23.72s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 430.7726, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 428.8160, Accuracy: 0.6534\n",
      "Training loss (for one batch) at step 20: 410.3550, Accuracy: 0.6708\n",
      "Training loss (for one batch) at step 30: 400.3215, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 40: 391.8932, Accuracy: 0.7060\n",
      "Training loss (for one batch) at step 50: 376.0266, Accuracy: 0.7157\n",
      "Training loss (for one batch) at step 60: 392.2616, Accuracy: 0.7245\n",
      "Training loss (for one batch) at step 70: 417.1331, Accuracy: 0.7224\n",
      "Training loss (for one batch) at step 80: 406.2221, Accuracy: 0.7144\n",
      "Training loss (for one batch) at step 90: 401.8911, Accuracy: 0.7123\n",
      "Training loss (for one batch) at step 100: 390.4509, Accuracy: 0.7164\n",
      "Training loss (for one batch) at step 110: 395.9827, Accuracy: 0.7192\n",
      "---- Training ----\n",
      "Training loss: 136.8128\n",
      "Training acc over epoch: 0.7186\n",
      "---- Validation ----\n",
      "Validation loss: 41.4644\n",
      "Validation acc: 0.6545\n",
      "Time taken: 20.55s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 428.6834, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 420.2577, Accuracy: 0.6896\n",
      "Training loss (for one batch) at step 20: 388.1905, Accuracy: 0.6860\n",
      "Training loss (for one batch) at step 30: 391.2732, Accuracy: 0.7056\n",
      "Training loss (for one batch) at step 40: 390.7155, Accuracy: 0.7197\n",
      "Training loss (for one batch) at step 50: 370.5406, Accuracy: 0.7296\n",
      "Training loss (for one batch) at step 60: 379.3691, Accuracy: 0.7376\n",
      "Training loss (for one batch) at step 70: 386.0126, Accuracy: 0.7325\n",
      "Training loss (for one batch) at step 80: 394.4793, Accuracy: 0.7276\n",
      "Training loss (for one batch) at step 90: 381.2128, Accuracy: 0.7236\n",
      "Training loss (for one batch) at step 100: 381.4289, Accuracy: 0.7269\n",
      "Training loss (for one batch) at step 110: 400.2967, Accuracy: 0.7274\n",
      "---- Training ----\n",
      "Training loss: 124.3920\n",
      "Training acc over epoch: 0.7268\n",
      "---- Validation ----\n",
      "Validation loss: 34.7108\n",
      "Validation acc: 0.6857\n",
      "Time taken: 18.33s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 416.0569, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 412.5198, Accuracy: 0.6847\n",
      "Training loss (for one batch) at step 20: 390.2849, Accuracy: 0.6983\n",
      "Training loss (for one batch) at step 30: 374.7392, Accuracy: 0.7218\n",
      "Training loss (for one batch) at step 40: 385.9436, Accuracy: 0.7332\n",
      "Training loss (for one batch) at step 50: 381.7018, Accuracy: 0.7414\n",
      "Training loss (for one batch) at step 60: 364.4783, Accuracy: 0.7487\n",
      "Training loss (for one batch) at step 70: 395.2302, Accuracy: 0.7421\n",
      "Training loss (for one batch) at step 80: 396.1593, Accuracy: 0.7358\n",
      "Training loss (for one batch) at step 90: 356.3455, Accuracy: 0.7327\n",
      "Training loss (for one batch) at step 100: 368.5134, Accuracy: 0.7335\n",
      "Training loss (for one batch) at step 110: 393.3732, Accuracy: 0.7348\n",
      "---- Training ----\n",
      "Training loss: 123.5594\n",
      "Training acc over epoch: 0.7343\n",
      "---- Validation ----\n",
      "Validation loss: 42.0594\n",
      "Validation acc: 0.6682\n",
      "Time taken: 23.73s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 413.3909, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 404.0007, Accuracy: 0.6925\n",
      "Training loss (for one batch) at step 20: 383.1841, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 30: 376.3630, Accuracy: 0.7251\n",
      "Training loss (for one batch) at step 40: 368.3793, Accuracy: 0.7384\n",
      "Training loss (for one batch) at step 50: 334.5588, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 60: 355.0172, Accuracy: 0.7592\n",
      "Training loss (for one batch) at step 70: 382.5366, Accuracy: 0.7520\n",
      "Training loss (for one batch) at step 80: 387.4615, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 90: 377.2375, Accuracy: 0.7375\n",
      "Training loss (for one batch) at step 100: 360.0405, Accuracy: 0.7396\n",
      "Training loss (for one batch) at step 110: 393.6882, Accuracy: 0.7406\n",
      "---- Training ----\n",
      "Training loss: 110.9839\n",
      "Training acc over epoch: 0.7394\n",
      "---- Validation ----\n",
      "Validation loss: 44.7015\n",
      "Validation acc: 0.6797\n",
      "Time taken: 19.20s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 395.8516, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 404.0408, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 20: 362.9857, Accuracy: 0.7076\n",
      "Training loss (for one batch) at step 30: 350.1375, Accuracy: 0.7326\n",
      "Training loss (for one batch) at step 40: 337.4070, Accuracy: 0.7485\n",
      "Training loss (for one batch) at step 50: 348.6978, Accuracy: 0.7606\n",
      "Training loss (for one batch) at step 60: 347.0988, Accuracy: 0.7652\n",
      "Training loss (for one batch) at step 70: 379.4744, Accuracy: 0.7587\n",
      "Training loss (for one batch) at step 80: 376.5567, Accuracy: 0.7505\n",
      "Training loss (for one batch) at step 90: 375.6095, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 100: 356.3623, Accuracy: 0.7517\n",
      "Training loss (for one batch) at step 110: 354.4178, Accuracy: 0.7516\n",
      "---- Training ----\n",
      "Training loss: 126.9092\n",
      "Training acc over epoch: 0.7509\n",
      "---- Validation ----\n",
      "Validation loss: 44.0817\n",
      "Validation acc: 0.6711\n",
      "Time taken: 18.28s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 386.2692, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 380.8206, Accuracy: 0.6861\n",
      "Training loss (for one batch) at step 20: 364.5771, Accuracy: 0.7057\n",
      "Training loss (for one batch) at step 30: 356.0305, Accuracy: 0.7324\n",
      "Training loss (for one batch) at step 40: 341.3352, Accuracy: 0.7553\n",
      "Training loss (for one batch) at step 50: 338.0036, Accuracy: 0.7684\n",
      "Training loss (for one batch) at step 60: 345.8356, Accuracy: 0.7764\n",
      "Training loss (for one batch) at step 70: 373.9965, Accuracy: 0.7667\n",
      "Training loss (for one batch) at step 80: 359.4945, Accuracy: 0.7554\n",
      "Training loss (for one batch) at step 90: 343.6505, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 100: 343.9450, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 110: 359.8390, Accuracy: 0.7550\n",
      "---- Training ----\n",
      "Training loss: 108.3959\n",
      "Training acc over epoch: 0.7543\n",
      "---- Validation ----\n",
      "Validation loss: 43.3217\n",
      "Validation acc: 0.6865\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 386.2764, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 377.3495, Accuracy: 0.6861\n",
      "Training loss (for one batch) at step 20: 336.3402, Accuracy: 0.7113\n",
      "Training loss (for one batch) at step 30: 339.8987, Accuracy: 0.7341\n",
      "Training loss (for one batch) at step 40: 320.9834, Accuracy: 0.7530\n",
      "Training loss (for one batch) at step 50: 315.0347, Accuracy: 0.7659\n",
      "Training loss (for one batch) at step 60: 342.0507, Accuracy: 0.7738\n",
      "Training loss (for one batch) at step 70: 372.2475, Accuracy: 0.7625\n",
      "Training loss (for one batch) at step 80: 367.8800, Accuracy: 0.7522\n",
      "Training loss (for one batch) at step 90: 345.2139, Accuracy: 0.7493\n",
      "Training loss (for one batch) at step 100: 320.1758, Accuracy: 0.7539\n",
      "Training loss (for one batch) at step 110: 357.1042, Accuracy: 0.7549\n",
      "---- Training ----\n",
      "Training loss: 118.9571\n",
      "Training acc over epoch: 0.7543\n",
      "---- Validation ----\n",
      "Validation loss: 52.6991\n",
      "Validation acc: 0.6757\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 378.3333, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 368.2653, Accuracy: 0.6882\n",
      "Training loss (for one batch) at step 20: 318.3153, Accuracy: 0.7143\n",
      "Training loss (for one batch) at step 30: 323.1917, Accuracy: 0.7424\n",
      "Training loss (for one batch) at step 40: 322.3529, Accuracy: 0.7597\n",
      "Training loss (for one batch) at step 50: 321.9817, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 60: 355.4247, Accuracy: 0.7786\n",
      "Training loss (for one batch) at step 70: 350.5674, Accuracy: 0.7647\n",
      "Training loss (for one batch) at step 80: 340.9678, Accuracy: 0.7548\n",
      "Training loss (for one batch) at step 90: 328.2758, Accuracy: 0.7509\n",
      "Training loss (for one batch) at step 100: 328.3828, Accuracy: 0.7555\n",
      "Training loss (for one batch) at step 110: 330.0909, Accuracy: 0.7567\n",
      "---- Training ----\n",
      "Training loss: 116.8110\n",
      "Training acc over epoch: 0.7559\n",
      "---- Validation ----\n",
      "Validation loss: 42.9133\n",
      "Validation acc: 0.6776\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 364.9119, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 353.8631, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 20: 319.4429, Accuracy: 0.7228\n",
      "Training loss (for one batch) at step 30: 323.1703, Accuracy: 0.7495\n",
      "Training loss (for one batch) at step 40: 320.6247, Accuracy: 0.7696\n",
      "Training loss (for one batch) at step 50: 321.4498, Accuracy: 0.7839\n",
      "Training loss (for one batch) at step 60: 346.0876, Accuracy: 0.7879\n",
      "Training loss (for one batch) at step 70: 350.6584, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 80: 346.2030, Accuracy: 0.7636\n",
      "Training loss (for one batch) at step 90: 332.9337, Accuracy: 0.7626\n",
      "Training loss (for one batch) at step 100: 327.6584, Accuracy: 0.7672\n",
      "Training loss (for one batch) at step 110: 327.3953, Accuracy: 0.7669\n",
      "---- Training ----\n",
      "Training loss: 99.7133\n",
      "Training acc over epoch: 0.7657\n",
      "---- Validation ----\n",
      "Validation loss: 39.6910\n",
      "Validation acc: 0.6617\n",
      "Time taken: 18.08s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 364.9976, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 375.6015, Accuracy: 0.7003\n",
      "Training loss (for one batch) at step 20: 350.0122, Accuracy: 0.7206\n",
      "Training loss (for one batch) at step 30: 327.5182, Accuracy: 0.7404\n",
      "Training loss (for one batch) at step 40: 299.2828, Accuracy: 0.7654\n",
      "Training loss (for one batch) at step 50: 298.8700, Accuracy: 0.7790\n",
      "Training loss (for one batch) at step 60: 309.1933, Accuracy: 0.7815\n",
      "Training loss (for one batch) at step 70: 333.4941, Accuracy: 0.7716\n",
      "Training loss (for one batch) at step 80: 361.0565, Accuracy: 0.7599\n",
      "Training loss (for one batch) at step 90: 326.0076, Accuracy: 0.7574\n",
      "Training loss (for one batch) at step 100: 315.5331, Accuracy: 0.7603\n",
      "Training loss (for one batch) at step 110: 331.9398, Accuracy: 0.7601\n",
      "---- Training ----\n",
      "Training loss: 96.5427\n",
      "Training acc over epoch: 0.7601\n",
      "---- Validation ----\n",
      "Validation loss: 59.8671\n",
      "Validation acc: 0.6730\n",
      "Time taken: 17.89s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 359.4496, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 340.6793, Accuracy: 0.6939\n",
      "Training loss (for one batch) at step 20: 330.1569, Accuracy: 0.7161\n",
      "Training loss (for one batch) at step 30: 303.8688, Accuracy: 0.7455\n",
      "Training loss (for one batch) at step 40: 299.3379, Accuracy: 0.7702\n",
      "Training loss (for one batch) at step 50: 331.4485, Accuracy: 0.7806\n",
      "Training loss (for one batch) at step 60: 319.6211, Accuracy: 0.7878\n",
      "Training loss (for one batch) at step 70: 369.4586, Accuracy: 0.7776\n",
      "Training loss (for one batch) at step 80: 341.7788, Accuracy: 0.7637\n",
      "Training loss (for one batch) at step 90: 311.8840, Accuracy: 0.7610\n",
      "Training loss (for one batch) at step 100: 309.6030, Accuracy: 0.7655\n",
      "Training loss (for one batch) at step 110: 324.2281, Accuracy: 0.7639\n",
      "---- Training ----\n",
      "Training loss: 97.0392\n",
      "Training acc over epoch: 0.7629\n",
      "---- Validation ----\n",
      "Validation loss: 50.8320\n",
      "Validation acc: 0.6784\n",
      "Time taken: 18.14s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 333.2180, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 349.1633, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 20: 300.7568, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 30: 305.9289, Accuracy: 0.7475\n",
      "Training loss (for one batch) at step 40: 314.7381, Accuracy: 0.7679\n",
      "Training loss (for one batch) at step 50: 304.4953, Accuracy: 0.7825\n",
      "Training loss (for one batch) at step 60: 313.0714, Accuracy: 0.7889\n",
      "Training loss (for one batch) at step 70: 323.4233, Accuracy: 0.7774\n",
      "Training loss (for one batch) at step 80: 341.4776, Accuracy: 0.7627\n",
      "Training loss (for one batch) at step 90: 295.4609, Accuracy: 0.7604\n",
      "Training loss (for one batch) at step 100: 304.5141, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 110: 311.2520, Accuracy: 0.7663\n",
      "---- Training ----\n",
      "Training loss: 110.6973\n",
      "Training acc over epoch: 0.7649\n",
      "---- Validation ----\n",
      "Validation loss: 43.2198\n",
      "Validation acc: 0.6730\n",
      "Time taken: 18.13s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 356.1946, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 340.7663, Accuracy: 0.6825\n",
      "Training loss (for one batch) at step 20: 302.2961, Accuracy: 0.7217\n",
      "Training loss (for one batch) at step 30: 297.9896, Accuracy: 0.7460\n",
      "Training loss (for one batch) at step 40: 290.7352, Accuracy: 0.7652\n",
      "Training loss (for one batch) at step 50: 285.9527, Accuracy: 0.7831\n",
      "Training loss (for one batch) at step 60: 325.0248, Accuracy: 0.7879\n",
      "Training loss (for one batch) at step 70: 313.3680, Accuracy: 0.7783\n",
      "Training loss (for one batch) at step 80: 345.6059, Accuracy: 0.7647\n",
      "Training loss (for one batch) at step 90: 296.2231, Accuracy: 0.7602\n",
      "Training loss (for one batch) at step 100: 285.8940, Accuracy: 0.7662\n",
      "Training loss (for one batch) at step 110: 308.6430, Accuracy: 0.7664\n",
      "---- Training ----\n",
      "Training loss: 123.6199\n",
      "Training acc over epoch: 0.7646\n",
      "---- Validation ----\n",
      "Validation loss: 40.0644\n",
      "Validation acc: 0.6824\n",
      "Time taken: 17.88s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 351.7749, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 325.0449, Accuracy: 0.6634\n",
      "Training loss (for one batch) at step 20: 300.0672, Accuracy: 0.7016\n",
      "Training loss (for one batch) at step 30: 283.1426, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 40: 286.1183, Accuracy: 0.7670\n",
      "Training loss (for one batch) at step 50: 267.7051, Accuracy: 0.7840\n",
      "Training loss (for one batch) at step 60: 302.9379, Accuracy: 0.7900\n",
      "Training loss (for one batch) at step 70: 343.3614, Accuracy: 0.7793\n",
      "Training loss (for one batch) at step 80: 325.5570, Accuracy: 0.7653\n",
      "Training loss (for one batch) at step 90: 304.6862, Accuracy: 0.7641\n",
      "Training loss (for one batch) at step 100: 291.6366, Accuracy: 0.7662\n",
      "Training loss (for one batch) at step 110: 312.6998, Accuracy: 0.7671\n",
      "---- Training ----\n",
      "Training loss: 86.6865\n",
      "Training acc over epoch: 0.7665\n",
      "---- Validation ----\n",
      "Validation loss: 63.8224\n",
      "Validation acc: 0.6851\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 349.4731, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 325.9449, Accuracy: 0.6854\n",
      "Training loss (for one batch) at step 20: 288.9043, Accuracy: 0.7210\n",
      "Training loss (for one batch) at step 30: 281.6122, Accuracy: 0.7510\n",
      "Training loss (for one batch) at step 40: 291.8077, Accuracy: 0.7731\n",
      "Training loss (for one batch) at step 50: 278.0418, Accuracy: 0.7862\n",
      "Training loss (for one batch) at step 60: 279.2211, Accuracy: 0.7929\n",
      "Training loss (for one batch) at step 70: 321.3862, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 80: 321.6725, Accuracy: 0.7678\n",
      "Training loss (for one batch) at step 90: 285.2805, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 100: 291.9345, Accuracy: 0.7683\n",
      "Training loss (for one batch) at step 110: 289.5250, Accuracy: 0.7684\n",
      "---- Training ----\n",
      "Training loss: 102.9881\n",
      "Training acc over epoch: 0.7668\n",
      "---- Validation ----\n",
      "Validation loss: 48.1284\n",
      "Validation acc: 0.6848\n",
      "Time taken: 17.88s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 336.1184, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 331.6360, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 20: 303.2971, Accuracy: 0.7147\n",
      "Training loss (for one batch) at step 30: 288.2803, Accuracy: 0.7510\n",
      "Training loss (for one batch) at step 40: 278.3253, Accuracy: 0.7713\n",
      "Training loss (for one batch) at step 50: 264.6089, Accuracy: 0.7868\n",
      "Training loss (for one batch) at step 60: 296.5911, Accuracy: 0.7911\n",
      "Training loss (for one batch) at step 70: 308.2276, Accuracy: 0.7828\n",
      "Training loss (for one batch) at step 80: 302.6302, Accuracy: 0.7650\n",
      "Training loss (for one batch) at step 90: 289.9225, Accuracy: 0.7609\n",
      "Training loss (for one batch) at step 100: 284.0532, Accuracy: 0.7664\n",
      "Training loss (for one batch) at step 110: 288.0968, Accuracy: 0.7680\n",
      "---- Training ----\n",
      "Training loss: 106.7730\n",
      "Training acc over epoch: 0.7663\n",
      "---- Validation ----\n",
      "Validation loss: 38.6720\n",
      "Validation acc: 0.6714\n",
      "Time taken: 18.08s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 323.4277, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 329.7865, Accuracy: 0.6847\n",
      "Training loss (for one batch) at step 20: 282.8899, Accuracy: 0.7199\n",
      "Training loss (for one batch) at step 30: 288.4915, Accuracy: 0.7523\n",
      "Training loss (for one batch) at step 40: 259.9701, Accuracy: 0.7731\n",
      "Training loss (for one batch) at step 50: 269.8147, Accuracy: 0.7886\n",
      "Training loss (for one batch) at step 60: 290.1815, Accuracy: 0.7957\n",
      "Training loss (for one batch) at step 70: 323.9929, Accuracy: 0.7848\n",
      "Training loss (for one batch) at step 80: 319.7749, Accuracy: 0.7689\n",
      "Training loss (for one batch) at step 90: 291.9925, Accuracy: 0.7655\n",
      "Training loss (for one batch) at step 100: 269.4650, Accuracy: 0.7690\n",
      "Training loss (for one batch) at step 110: 297.0981, Accuracy: 0.7692\n",
      "---- Training ----\n",
      "Training loss: 102.9689\n",
      "Training acc over epoch: 0.7685\n",
      "---- Validation ----\n",
      "Validation loss: 44.5489\n",
      "Validation acc: 0.6902\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 313.5228, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 321.0747, Accuracy: 0.6733\n",
      "Training loss (for one batch) at step 20: 295.6610, Accuracy: 0.7158\n",
      "Training loss (for one batch) at step 30: 281.0779, Accuracy: 0.7477\n",
      "Training loss (for one batch) at step 40: 272.8368, Accuracy: 0.7691\n",
      "Training loss (for one batch) at step 50: 282.0450, Accuracy: 0.7845\n",
      "Training loss (for one batch) at step 60: 286.4440, Accuracy: 0.7924\n",
      "Training loss (for one batch) at step 70: 296.3845, Accuracy: 0.7804\n",
      "Training loss (for one batch) at step 80: 304.0989, Accuracy: 0.7663\n",
      "Training loss (for one batch) at step 90: 274.4319, Accuracy: 0.7630\n",
      "Training loss (for one batch) at step 100: 280.5522, Accuracy: 0.7666\n",
      "Training loss (for one batch) at step 110: 285.1171, Accuracy: 0.7686\n",
      "---- Training ----\n",
      "Training loss: 85.4393\n",
      "Training acc over epoch: 0.7662\n",
      "---- Validation ----\n",
      "Validation loss: 53.1839\n",
      "Validation acc: 0.6639\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 324.7073, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 318.6005, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 20: 278.8847, Accuracy: 0.7210\n",
      "Training loss (for one batch) at step 30: 270.8764, Accuracy: 0.7523\n",
      "Training loss (for one batch) at step 40: 280.3217, Accuracy: 0.7715\n",
      "Training loss (for one batch) at step 50: 280.7772, Accuracy: 0.7881\n",
      "Training loss (for one batch) at step 60: 297.4620, Accuracy: 0.7918\n",
      "Training loss (for one batch) at step 70: 307.7444, Accuracy: 0.7800\n",
      "Training loss (for one batch) at step 80: 304.6161, Accuracy: 0.7679\n",
      "Training loss (for one batch) at step 90: 280.4308, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 100: 260.7419, Accuracy: 0.7693\n",
      "Training loss (for one batch) at step 110: 310.2597, Accuracy: 0.7689\n",
      "---- Training ----\n",
      "Training loss: 98.6544\n",
      "Training acc over epoch: 0.7677\n",
      "---- Validation ----\n",
      "Validation loss: 49.9996\n",
      "Validation acc: 0.6808\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 308.8494, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 310.8938, Accuracy: 0.6747\n",
      "Training loss (for one batch) at step 20: 276.3683, Accuracy: 0.7143\n",
      "Training loss (for one batch) at step 30: 267.9771, Accuracy: 0.7475\n",
      "Training loss (for one batch) at step 40: 262.4207, Accuracy: 0.7685\n",
      "Training loss (for one batch) at step 50: 255.3443, Accuracy: 0.7857\n",
      "Training loss (for one batch) at step 60: 269.0827, Accuracy: 0.7951\n",
      "Training loss (for one batch) at step 70: 305.0692, Accuracy: 0.7829\n",
      "Training loss (for one batch) at step 80: 306.8397, Accuracy: 0.7690\n",
      "Training loss (for one batch) at step 90: 283.0038, Accuracy: 0.7643\n",
      "Training loss (for one batch) at step 100: 270.8181, Accuracy: 0.7690\n",
      "Training loss (for one batch) at step 110: 309.2968, Accuracy: 0.7694\n",
      "---- Training ----\n",
      "Training loss: 92.0658\n",
      "Training acc over epoch: 0.7683\n",
      "---- Validation ----\n",
      "Validation loss: 40.0020\n",
      "Validation acc: 0.6859\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 307.4147, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 322.3271, Accuracy: 0.6712\n",
      "Training loss (for one batch) at step 20: 272.6988, Accuracy: 0.7135\n",
      "Training loss (for one batch) at step 30: 270.6826, Accuracy: 0.7505\n",
      "Training loss (for one batch) at step 40: 285.5607, Accuracy: 0.7692\n",
      "Training loss (for one batch) at step 50: 259.5805, Accuracy: 0.7881\n",
      "Training loss (for one batch) at step 60: 270.5269, Accuracy: 0.7938\n",
      "Training loss (for one batch) at step 70: 297.5824, Accuracy: 0.7801\n",
      "Training loss (for one batch) at step 80: 292.6119, Accuracy: 0.7636\n",
      "Training loss (for one batch) at step 90: 300.2136, Accuracy: 0.7624\n",
      "Training loss (for one batch) at step 100: 270.7522, Accuracy: 0.7664\n",
      "Training loss (for one batch) at step 110: 286.0112, Accuracy: 0.7677\n",
      "---- Training ----\n",
      "Training loss: 91.6346\n",
      "Training acc over epoch: 0.7669\n",
      "---- Validation ----\n",
      "Validation loss: 48.9818\n",
      "Validation acc: 0.6706\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 320.8252, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 319.8988, Accuracy: 0.6768\n",
      "Training loss (for one batch) at step 20: 283.3888, Accuracy: 0.7202\n",
      "Training loss (for one batch) at step 30: 279.5154, Accuracy: 0.7480\n",
      "Training loss (for one batch) at step 40: 268.4700, Accuracy: 0.7725\n",
      "Training loss (for one batch) at step 50: 260.3011, Accuracy: 0.7874\n",
      "Training loss (for one batch) at step 60: 280.4554, Accuracy: 0.7934\n",
      "Training loss (for one batch) at step 70: 310.7095, Accuracy: 0.7790\n",
      "Training loss (for one batch) at step 80: 332.7178, Accuracy: 0.7641\n",
      "Training loss (for one batch) at step 90: 272.1798, Accuracy: 0.7616\n",
      "Training loss (for one batch) at step 100: 256.9127, Accuracy: 0.7668\n",
      "Training loss (for one batch) at step 110: 276.7878, Accuracy: 0.7689\n",
      "---- Training ----\n",
      "Training loss: 86.7394\n",
      "Training acc over epoch: 0.7666\n",
      "---- Validation ----\n",
      "Validation loss: 51.4318\n",
      "Validation acc: 0.6875\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 308.7717, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 312.5245, Accuracy: 0.6634\n",
      "Training loss (for one batch) at step 20: 294.4157, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 30: 262.9295, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 40: 251.6290, Accuracy: 0.7689\n",
      "Training loss (for one batch) at step 50: 269.3477, Accuracy: 0.7871\n",
      "Training loss (for one batch) at step 60: 283.8010, Accuracy: 0.7924\n",
      "Training loss (for one batch) at step 70: 294.6024, Accuracy: 0.7772\n",
      "Training loss (for one batch) at step 80: 330.9677, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 90: 274.0151, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 100: 261.5614, Accuracy: 0.7669\n",
      "Training loss (for one batch) at step 110: 273.9283, Accuracy: 0.7681\n",
      "---- Training ----\n",
      "Training loss: 106.1119\n",
      "Training acc over epoch: 0.7666\n",
      "---- Validation ----\n",
      "Validation loss: 40.2421\n",
      "Validation acc: 0.6873\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 304.0107, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 294.6766, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 20: 264.9764, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 30: 266.5254, Accuracy: 0.7462\n",
      "Training loss (for one batch) at step 40: 245.2247, Accuracy: 0.7673\n",
      "Training loss (for one batch) at step 50: 271.4613, Accuracy: 0.7848\n",
      "Training loss (for one batch) at step 60: 294.6121, Accuracy: 0.7933\n",
      "Training loss (for one batch) at step 70: 286.6886, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 80: 301.9682, Accuracy: 0.7672\n",
      "Training loss (for one batch) at step 90: 253.9776, Accuracy: 0.7640\n",
      "Training loss (for one batch) at step 100: 267.3841, Accuracy: 0.7686\n",
      "Training loss (for one batch) at step 110: 270.2579, Accuracy: 0.7694\n",
      "---- Training ----\n",
      "Training loss: 91.2477\n",
      "Training acc over epoch: 0.7674\n",
      "---- Validation ----\n",
      "Validation loss: 53.2744\n",
      "Validation acc: 0.6897\n",
      "Time taken: 17.89s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 308.6800, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 314.4465, Accuracy: 0.6825\n",
      "Training loss (for one batch) at step 20: 272.4745, Accuracy: 0.7258\n",
      "Training loss (for one batch) at step 30: 272.7667, Accuracy: 0.7553\n",
      "Training loss (for one batch) at step 40: 249.5261, Accuracy: 0.7723\n",
      "Training loss (for one batch) at step 50: 250.9330, Accuracy: 0.7911\n",
      "Training loss (for one batch) at step 60: 261.2930, Accuracy: 0.7933\n",
      "Training loss (for one batch) at step 70: 280.0905, Accuracy: 0.7818\n",
      "Training loss (for one batch) at step 80: 269.8388, Accuracy: 0.7668\n",
      "Training loss (for one batch) at step 90: 268.6315, Accuracy: 0.7624\n",
      "Training loss (for one batch) at step 100: 256.2661, Accuracy: 0.7685\n",
      "Training loss (for one batch) at step 110: 281.4250, Accuracy: 0.7689\n",
      "---- Training ----\n",
      "Training loss: 107.8588\n",
      "Training acc over epoch: 0.7677\n",
      "---- Validation ----\n",
      "Validation loss: 39.5346\n",
      "Validation acc: 0.6959\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 288.1836, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 299.0655, Accuracy: 0.6811\n",
      "Training loss (for one batch) at step 20: 257.6551, Accuracy: 0.7232\n",
      "Training loss (for one batch) at step 30: 257.6852, Accuracy: 0.7540\n",
      "Training loss (for one batch) at step 40: 270.9167, Accuracy: 0.7742\n",
      "Training loss (for one batch) at step 50: 246.5199, Accuracy: 0.7892\n",
      "Training loss (for one batch) at step 60: 271.0572, Accuracy: 0.7956\n",
      "Training loss (for one batch) at step 70: 302.9778, Accuracy: 0.7838\n",
      "Training loss (for one batch) at step 80: 298.9622, Accuracy: 0.7688\n",
      "Training loss (for one batch) at step 90: 272.6339, Accuracy: 0.7664\n",
      "Training loss (for one batch) at step 100: 281.4617, Accuracy: 0.7687\n",
      "Training loss (for one batch) at step 110: 278.1960, Accuracy: 0.7700\n",
      "---- Training ----\n",
      "Training loss: 93.9245\n",
      "Training acc over epoch: 0.7694\n",
      "---- Validation ----\n",
      "Validation loss: 37.9995\n",
      "Validation acc: 0.6795\n",
      "Time taken: 18.07s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 316.7603, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 281.3156, Accuracy: 0.6676\n",
      "Training loss (for one batch) at step 20: 257.8405, Accuracy: 0.7128\n",
      "Training loss (for one batch) at step 30: 261.2148, Accuracy: 0.7460\n",
      "Training loss (for one batch) at step 40: 259.7714, Accuracy: 0.7727\n",
      "Training loss (for one batch) at step 50: 245.1487, Accuracy: 0.7875\n",
      "Training loss (for one batch) at step 60: 278.9635, Accuracy: 0.7955\n",
      "Training loss (for one batch) at step 70: 292.5336, Accuracy: 0.7824\n",
      "Training loss (for one batch) at step 80: 275.5865, Accuracy: 0.7680\n",
      "Training loss (for one batch) at step 90: 260.0099, Accuracy: 0.7651\n",
      "Training loss (for one batch) at step 100: 264.9591, Accuracy: 0.7688\n",
      "Training loss (for one batch) at step 110: 276.8064, Accuracy: 0.7692\n",
      "---- Training ----\n",
      "Training loss: 82.3105\n",
      "Training acc over epoch: 0.7675\n",
      "---- Validation ----\n",
      "Validation loss: 52.2831\n",
      "Validation acc: 0.6803\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 274.6815, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 288.4832, Accuracy: 0.6839\n",
      "Training loss (for one batch) at step 20: 258.6106, Accuracy: 0.7206\n",
      "Training loss (for one batch) at step 30: 264.8847, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 40: 260.7273, Accuracy: 0.7727\n",
      "Training loss (for one batch) at step 50: 250.8790, Accuracy: 0.7871\n",
      "Training loss (for one batch) at step 60: 264.4214, Accuracy: 0.7950\n",
      "Training loss (for one batch) at step 70: 293.6512, Accuracy: 0.7801\n",
      "Training loss (for one batch) at step 80: 275.4094, Accuracy: 0.7662\n",
      "Training loss (for one batch) at step 90: 271.3691, Accuracy: 0.7612\n",
      "Training loss (for one batch) at step 100: 268.7207, Accuracy: 0.7657\n",
      "Training loss (for one batch) at step 110: 264.6707, Accuracy: 0.7668\n",
      "---- Training ----\n",
      "Training loss: 93.6996\n",
      "Training acc over epoch: 0.7657\n",
      "---- Validation ----\n",
      "Validation loss: 36.1632\n",
      "Validation acc: 0.6932\n",
      "Time taken: 18.11s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 309.4751, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 299.1120, Accuracy: 0.6697\n",
      "Training loss (for one batch) at step 20: 260.9601, Accuracy: 0.7139\n",
      "Training loss (for one batch) at step 30: 254.4957, Accuracy: 0.7490\n",
      "Training loss (for one batch) at step 40: 255.4561, Accuracy: 0.7715\n",
      "Training loss (for one batch) at step 50: 264.3057, Accuracy: 0.7884\n",
      "Training loss (for one batch) at step 60: 272.5030, Accuracy: 0.7944\n",
      "Training loss (for one batch) at step 70: 307.3649, Accuracy: 0.7811\n",
      "Training loss (for one batch) at step 80: 291.4390, Accuracy: 0.7666\n",
      "Training loss (for one batch) at step 90: 258.0519, Accuracy: 0.7624\n",
      "Training loss (for one batch) at step 100: 257.7610, Accuracy: 0.7679\n",
      "Training loss (for one batch) at step 110: 273.0685, Accuracy: 0.7675\n",
      "---- Training ----\n",
      "Training loss: 88.4173\n",
      "Training acc over epoch: 0.7673\n",
      "---- Validation ----\n",
      "Validation loss: 51.7645\n",
      "Validation acc: 0.6824\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 289.7183, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 278.2802, Accuracy: 0.6847\n",
      "Training loss (for one batch) at step 20: 278.5250, Accuracy: 0.7225\n",
      "Training loss (for one batch) at step 30: 258.5912, Accuracy: 0.7530\n",
      "Training loss (for one batch) at step 40: 246.0534, Accuracy: 0.7740\n",
      "Training loss (for one batch) at step 50: 240.8869, Accuracy: 0.7875\n",
      "Training loss (for one batch) at step 60: 264.3648, Accuracy: 0.7944\n",
      "Training loss (for one batch) at step 70: 265.3736, Accuracy: 0.7817\n",
      "Training loss (for one batch) at step 80: 277.6261, Accuracy: 0.7644\n",
      "Training loss (for one batch) at step 90: 252.3959, Accuracy: 0.7624\n",
      "Training loss (for one batch) at step 100: 273.3804, Accuracy: 0.7683\n",
      "Training loss (for one batch) at step 110: 257.8666, Accuracy: 0.7680\n",
      "---- Training ----\n",
      "Training loss: 91.3931\n",
      "Training acc over epoch: 0.7671\n",
      "---- Validation ----\n",
      "Validation loss: 39.5829\n",
      "Validation acc: 0.6878\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 293.6413, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 277.3230, Accuracy: 0.6570\n",
      "Training loss (for one batch) at step 20: 248.7973, Accuracy: 0.7083\n",
      "Training loss (for one batch) at step 30: 246.7620, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 40: 247.8791, Accuracy: 0.7721\n",
      "Training loss (for one batch) at step 50: 243.6071, Accuracy: 0.7851\n",
      "Training loss (for one batch) at step 60: 261.8941, Accuracy: 0.7915\n",
      "Training loss (for one batch) at step 70: 304.3437, Accuracy: 0.7799\n",
      "Training loss (for one batch) at step 80: 275.7153, Accuracy: 0.7674\n",
      "Training loss (for one batch) at step 90: 261.1544, Accuracy: 0.7637\n",
      "Training loss (for one batch) at step 100: 253.9051, Accuracy: 0.7673\n",
      "Training loss (for one batch) at step 110: 257.5111, Accuracy: 0.7687\n",
      "---- Training ----\n",
      "Training loss: 91.3302\n",
      "Training acc over epoch: 0.7671\n",
      "---- Validation ----\n",
      "Validation loss: 41.2495\n",
      "Validation acc: 0.6865\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 267.3216, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 284.0676, Accuracy: 0.6783\n",
      "Training loss (for one batch) at step 20: 257.8635, Accuracy: 0.7083\n",
      "Training loss (for one batch) at step 30: 238.8941, Accuracy: 0.7462\n",
      "Training loss (for one batch) at step 40: 233.8937, Accuracy: 0.7698\n",
      "Training loss (for one batch) at step 50: 263.8920, Accuracy: 0.7871\n",
      "Training loss (for one batch) at step 60: 258.6975, Accuracy: 0.7924\n",
      "Training loss (for one batch) at step 70: 272.4599, Accuracy: 0.7822\n",
      "Training loss (for one batch) at step 80: 279.0822, Accuracy: 0.7660\n",
      "Training loss (for one batch) at step 90: 251.6193, Accuracy: 0.7614\n",
      "Training loss (for one batch) at step 100: 238.6138, Accuracy: 0.7667\n",
      "Training loss (for one batch) at step 110: 259.6150, Accuracy: 0.7685\n",
      "---- Training ----\n",
      "Training loss: 102.0790\n",
      "Training acc over epoch: 0.7659\n",
      "---- Validation ----\n",
      "Validation loss: 56.3111\n",
      "Validation acc: 0.6994\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 299.6488, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 278.7921, Accuracy: 0.6811\n",
      "Training loss (for one batch) at step 20: 258.8184, Accuracy: 0.7076\n",
      "Training loss (for one batch) at step 30: 241.7089, Accuracy: 0.7432\n",
      "Training loss (for one batch) at step 40: 235.2824, Accuracy: 0.7704\n",
      "Training loss (for one batch) at step 50: 237.1132, Accuracy: 0.7868\n",
      "Training loss (for one batch) at step 60: 272.9024, Accuracy: 0.7926\n",
      "Training loss (for one batch) at step 70: 290.5213, Accuracy: 0.7793\n",
      "Training loss (for one batch) at step 80: 279.2233, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 90: 256.5945, Accuracy: 0.7606\n",
      "Training loss (for one batch) at step 100: 248.3309, Accuracy: 0.7663\n",
      "Training loss (for one batch) at step 110: 267.0815, Accuracy: 0.7653\n",
      "---- Training ----\n",
      "Training loss: 75.0580\n",
      "Training acc over epoch: 0.7648\n",
      "---- Validation ----\n",
      "Validation loss: 43.8081\n",
      "Validation acc: 0.6797\n",
      "Time taken: 18.06s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 300.0682, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 256.2222, Accuracy: 0.6740\n",
      "Training loss (for one batch) at step 20: 247.4298, Accuracy: 0.7143\n",
      "Training loss (for one batch) at step 30: 241.4412, Accuracy: 0.7513\n",
      "Training loss (for one batch) at step 40: 223.9522, Accuracy: 0.7710\n",
      "Training loss (for one batch) at step 50: 240.9385, Accuracy: 0.7874\n",
      "Training loss (for one batch) at step 60: 259.2927, Accuracy: 0.7953\n",
      "Training loss (for one batch) at step 70: 284.2189, Accuracy: 0.7817\n",
      "Training loss (for one batch) at step 80: 290.3125, Accuracy: 0.7653\n",
      "Training loss (for one batch) at step 90: 237.3751, Accuracy: 0.7624\n",
      "Training loss (for one batch) at step 100: 253.1078, Accuracy: 0.7677\n",
      "Training loss (for one batch) at step 110: 262.3374, Accuracy: 0.7685\n",
      "---- Training ----\n",
      "Training loss: 92.0325\n",
      "Training acc over epoch: 0.7666\n",
      "---- Validation ----\n",
      "Validation loss: 54.6968\n",
      "Validation acc: 0.6840\n",
      "Time taken: 20.16s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 277.3662, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 299.1781, Accuracy: 0.6783\n",
      "Training loss (for one batch) at step 20: 252.4187, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 30: 245.4560, Accuracy: 0.7497\n",
      "Training loss (for one batch) at step 40: 242.9925, Accuracy: 0.7708\n",
      "Training loss (for one batch) at step 50: 235.9878, Accuracy: 0.7863\n",
      "Training loss (for one batch) at step 60: 247.8208, Accuracy: 0.7930\n",
      "Training loss (for one batch) at step 70: 273.4281, Accuracy: 0.7818\n",
      "Training loss (for one batch) at step 80: 273.2430, Accuracy: 0.7658\n",
      "Training loss (for one batch) at step 90: 275.6520, Accuracy: 0.7630\n",
      "Training loss (for one batch) at step 100: 247.7357, Accuracy: 0.7667\n",
      "Training loss (for one batch) at step 110: 260.2792, Accuracy: 0.7683\n",
      "---- Training ----\n",
      "Training loss: 83.0190\n",
      "Training acc over epoch: 0.7669\n",
      "---- Validation ----\n",
      "Validation loss: 62.1444\n",
      "Validation acc: 0.6867\n",
      "Time taken: 17.84s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 280.3835, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 274.0694, Accuracy: 0.6690\n",
      "Training loss (for one batch) at step 20: 251.8050, Accuracy: 0.7128\n",
      "Training loss (for one batch) at step 30: 259.0600, Accuracy: 0.7487\n",
      "Training loss (for one batch) at step 40: 250.7743, Accuracy: 0.7689\n",
      "Training loss (for one batch) at step 50: 248.4507, Accuracy: 0.7874\n",
      "Training loss (for one batch) at step 60: 269.2818, Accuracy: 0.7937\n",
      "Training loss (for one batch) at step 70: 267.1797, Accuracy: 0.7792\n",
      "Training loss (for one batch) at step 80: 264.5554, Accuracy: 0.7616\n",
      "Training loss (for one batch) at step 90: 248.3951, Accuracy: 0.7618\n",
      "Training loss (for one batch) at step 100: 230.7816, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 110: 232.4256, Accuracy: 0.7670\n",
      "---- Training ----\n",
      "Training loss: 85.9912\n",
      "Training acc over epoch: 0.7659\n",
      "---- Validation ----\n",
      "Validation loss: 44.7309\n",
      "Validation acc: 0.6644\n",
      "Time taken: 17.86s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 288.0092, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 280.1950, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 20: 247.5346, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 30: 241.1313, Accuracy: 0.7432\n",
      "Training loss (for one batch) at step 40: 233.2492, Accuracy: 0.7670\n",
      "Training loss (for one batch) at step 50: 258.9416, Accuracy: 0.7869\n",
      "Training loss (for one batch) at step 60: 249.9390, Accuracy: 0.7950\n",
      "Training loss (for one batch) at step 70: 268.9495, Accuracy: 0.7796\n",
      "Training loss (for one batch) at step 80: 291.2157, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 90: 248.3454, Accuracy: 0.7612\n",
      "Training loss (for one batch) at step 100: 278.2698, Accuracy: 0.7659\n",
      "Training loss (for one batch) at step 110: 245.6177, Accuracy: 0.7682\n",
      "---- Training ----\n",
      "Training loss: 100.4430\n",
      "Training acc over epoch: 0.7662\n",
      "---- Validation ----\n",
      "Validation loss: 42.8378\n",
      "Validation acc: 0.6803\n",
      "Time taken: 18.10s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 301.8186, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 260.7254, Accuracy: 0.6612\n",
      "Training loss (for one batch) at step 20: 237.7265, Accuracy: 0.7068\n",
      "Training loss (for one batch) at step 30: 229.7724, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 40: 236.5365, Accuracy: 0.7689\n",
      "Training loss (for one batch) at step 50: 229.1510, Accuracy: 0.7834\n",
      "Training loss (for one batch) at step 60: 239.3022, Accuracy: 0.7916\n",
      "Training loss (for one batch) at step 70: 251.3273, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 80: 284.7068, Accuracy: 0.7636\n",
      "Training loss (for one batch) at step 90: 242.6589, Accuracy: 0.7606\n",
      "Training loss (for one batch) at step 100: 247.2318, Accuracy: 0.7645\n",
      "Training loss (for one batch) at step 110: 264.7127, Accuracy: 0.7662\n",
      "---- Training ----\n",
      "Training loss: 77.2322\n",
      "Training acc over epoch: 0.7646\n",
      "---- Validation ----\n",
      "Validation loss: 52.7174\n",
      "Validation acc: 0.7007\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 280.9830, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 255.8221, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 20: 248.0957, Accuracy: 0.7065\n",
      "Training loss (for one batch) at step 30: 236.8151, Accuracy: 0.7495\n",
      "Training loss (for one batch) at step 40: 253.4368, Accuracy: 0.7742\n",
      "Training loss (for one batch) at step 50: 245.0179, Accuracy: 0.7911\n",
      "Training loss (for one batch) at step 60: 267.9294, Accuracy: 0.7962\n",
      "Training loss (for one batch) at step 70: 260.8560, Accuracy: 0.7820\n",
      "Training loss (for one batch) at step 80: 267.8417, Accuracy: 0.7666\n",
      "Training loss (for one batch) at step 90: 244.0941, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 100: 239.4998, Accuracy: 0.7679\n",
      "Training loss (for one batch) at step 110: 247.4745, Accuracy: 0.7684\n",
      "---- Training ----\n",
      "Training loss: 89.1947\n",
      "Training acc over epoch: 0.7679\n",
      "---- Validation ----\n",
      "Validation loss: 37.7945\n",
      "Validation acc: 0.6814\n",
      "Time taken: 17.88s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 263.8698, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 261.8621, Accuracy: 0.6555\n",
      "Training loss (for one batch) at step 20: 254.2669, Accuracy: 0.7094\n",
      "Training loss (for one batch) at step 30: 234.3457, Accuracy: 0.7467\n",
      "Training loss (for one batch) at step 40: 249.2195, Accuracy: 0.7685\n",
      "Training loss (for one batch) at step 50: 236.2799, Accuracy: 0.7872\n",
      "Training loss (for one batch) at step 60: 239.4684, Accuracy: 0.7934\n",
      "Training loss (for one batch) at step 70: 271.6683, Accuracy: 0.7807\n",
      "Training loss (for one batch) at step 80: 269.5569, Accuracy: 0.7662\n",
      "Training loss (for one batch) at step 90: 242.1373, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 100: 264.4132, Accuracy: 0.7677\n",
      "Training loss (for one batch) at step 110: 242.4337, Accuracy: 0.7682\n",
      "---- Training ----\n",
      "Training loss: 78.4392\n",
      "Training acc over epoch: 0.7664\n",
      "---- Validation ----\n",
      "Validation loss: 72.1541\n",
      "Validation acc: 0.6714\n",
      "Time taken: 20.16s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 270.2323, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 271.0722, Accuracy: 0.6825\n",
      "Training loss (for one batch) at step 20: 241.3592, Accuracy: 0.7176\n",
      "Training loss (for one batch) at step 30: 243.5736, Accuracy: 0.7513\n",
      "Training loss (for one batch) at step 40: 227.7066, Accuracy: 0.7748\n",
      "Training loss (for one batch) at step 50: 246.7110, Accuracy: 0.7881\n",
      "Training loss (for one batch) at step 60: 247.5526, Accuracy: 0.7948\n",
      "Training loss (for one batch) at step 70: 298.6355, Accuracy: 0.7810\n",
      "Training loss (for one batch) at step 80: 261.4401, Accuracy: 0.7641\n",
      "Training loss (for one batch) at step 90: 241.8102, Accuracy: 0.7627\n",
      "Training loss (for one batch) at step 100: 232.2341, Accuracy: 0.7671\n",
      "Training loss (for one batch) at step 110: 263.5288, Accuracy: 0.7681\n",
      "---- Training ----\n",
      "Training loss: 73.4674\n",
      "Training acc over epoch: 0.7671\n",
      "---- Validation ----\n",
      "Validation loss: 52.2510\n",
      "Validation acc: 0.6902\n",
      "Time taken: 17.87s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 263.5458, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 254.1264, Accuracy: 0.6705\n",
      "Training loss (for one batch) at step 20: 235.9148, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 30: 233.2956, Accuracy: 0.7455\n",
      "Training loss (for one batch) at step 40: 228.4990, Accuracy: 0.7698\n",
      "Training loss (for one batch) at step 50: 240.2003, Accuracy: 0.7874\n",
      "Training loss (for one batch) at step 60: 264.9592, Accuracy: 0.7961\n",
      "Training loss (for one batch) at step 70: 263.3239, Accuracy: 0.7815\n",
      "Training loss (for one batch) at step 80: 275.6232, Accuracy: 0.7661\n",
      "Training loss (for one batch) at step 90: 247.9341, Accuracy: 0.7618\n",
      "Training loss (for one batch) at step 100: 251.9253, Accuracy: 0.7666\n",
      "Training loss (for one batch) at step 110: 255.4810, Accuracy: 0.7676\n",
      "---- Training ----\n",
      "Training loss: 78.2115\n",
      "Training acc over epoch: 0.7667\n",
      "---- Validation ----\n",
      "Validation loss: 55.7583\n",
      "Validation acc: 0.6835\n",
      "Time taken: 17.86s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 251.4091, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 266.3872, Accuracy: 0.6818\n",
      "Training loss (for one batch) at step 20: 234.7513, Accuracy: 0.7087\n",
      "Training loss (for one batch) at step 30: 244.5955, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 40: 239.2447, Accuracy: 0.7706\n",
      "Training loss (for one batch) at step 50: 236.0239, Accuracy: 0.7842\n",
      "Training loss (for one batch) at step 60: 230.5643, Accuracy: 0.7928\n",
      "Training loss (for one batch) at step 70: 248.8298, Accuracy: 0.7806\n",
      "Training loss (for one batch) at step 80: 264.5735, Accuracy: 0.7639\n",
      "Training loss (for one batch) at step 90: 248.1739, Accuracy: 0.7609\n",
      "Training loss (for one batch) at step 100: 229.7583, Accuracy: 0.7653\n",
      "Training loss (for one batch) at step 110: 249.4811, Accuracy: 0.7681\n",
      "---- Training ----\n",
      "Training loss: 89.9842\n",
      "Training acc over epoch: 0.7665\n",
      "---- Validation ----\n",
      "Validation loss: 62.0368\n",
      "Validation acc: 0.6835\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 288.8742, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 255.6858, Accuracy: 0.6783\n",
      "Training loss (for one batch) at step 20: 230.5872, Accuracy: 0.7169\n",
      "Training loss (for one batch) at step 30: 236.4650, Accuracy: 0.7477\n",
      "Training loss (for one batch) at step 40: 239.4220, Accuracy: 0.7704\n",
      "Training loss (for one batch) at step 50: 229.7007, Accuracy: 0.7863\n",
      "Training loss (for one batch) at step 60: 256.0328, Accuracy: 0.7925\n",
      "Training loss (for one batch) at step 70: 261.3384, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 80: 282.3411, Accuracy: 0.7624\n",
      "Training loss (for one batch) at step 90: 265.5289, Accuracy: 0.7588\n",
      "Training loss (for one batch) at step 100: 231.2128, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 110: 258.7063, Accuracy: 0.7662\n",
      "---- Training ----\n",
      "Training loss: 81.1751\n",
      "Training acc over epoch: 0.7648\n",
      "---- Validation ----\n",
      "Validation loss: 55.2311\n",
      "Validation acc: 0.6878\n",
      "Time taken: 17.85s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 282.8985, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 273.4245, Accuracy: 0.6676\n",
      "Training loss (for one batch) at step 20: 237.2041, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 30: 230.9630, Accuracy: 0.7452\n",
      "Training loss (for one batch) at step 40: 242.1456, Accuracy: 0.7687\n",
      "Training loss (for one batch) at step 50: 231.5730, Accuracy: 0.7851\n",
      "Training loss (for one batch) at step 60: 233.0835, Accuracy: 0.7933\n",
      "Training loss (for one batch) at step 70: 276.0475, Accuracy: 0.7773\n",
      "Training loss (for one batch) at step 80: 270.9352, Accuracy: 0.7637\n",
      "Training loss (for one batch) at step 90: 231.6270, Accuracy: 0.7599\n",
      "Training loss (for one batch) at step 100: 229.4456, Accuracy: 0.7645\n",
      "Training loss (for one batch) at step 110: 248.5534, Accuracy: 0.7661\n",
      "---- Training ----\n",
      "Training loss: 80.4637\n",
      "Training acc over epoch: 0.7650\n",
      "---- Validation ----\n",
      "Validation loss: 52.8864\n",
      "Validation acc: 0.6795\n",
      "Time taken: 20.12s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 288.0201, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 256.4521, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 20: 253.5717, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 30: 219.7320, Accuracy: 0.7485\n",
      "Training loss (for one batch) at step 40: 229.1639, Accuracy: 0.7729\n",
      "Training loss (for one batch) at step 50: 227.1553, Accuracy: 0.7881\n",
      "Training loss (for one batch) at step 60: 225.0890, Accuracy: 0.7942\n",
      "Training loss (for one batch) at step 70: 261.4149, Accuracy: 0.7803\n",
      "Training loss (for one batch) at step 80: 265.7350, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 90: 240.7909, Accuracy: 0.7608\n",
      "Training loss (for one batch) at step 100: 241.5167, Accuracy: 0.7673\n",
      "Training loss (for one batch) at step 110: 253.3786, Accuracy: 0.7672\n",
      "---- Training ----\n",
      "Training loss: 83.2857\n",
      "Training acc over epoch: 0.7660\n",
      "---- Validation ----\n",
      "Validation loss: 74.1152\n",
      "Validation acc: 0.6679\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 258.8842, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 265.9191, Accuracy: 0.6591\n",
      "Training loss (for one batch) at step 20: 236.5614, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 30: 248.6070, Accuracy: 0.7419\n",
      "Training loss (for one batch) at step 40: 227.2580, Accuracy: 0.7679\n",
      "Training loss (for one batch) at step 50: 234.4982, Accuracy: 0.7857\n",
      "Training loss (for one batch) at step 60: 256.4838, Accuracy: 0.7898\n",
      "Training loss (for one batch) at step 70: 288.6283, Accuracy: 0.7778\n",
      "Training loss (for one batch) at step 80: 287.9516, Accuracy: 0.7608\n",
      "Training loss (for one batch) at step 90: 237.9473, Accuracy: 0.7587\n",
      "Training loss (for one batch) at step 100: 237.0598, Accuracy: 0.7652\n",
      "Training loss (for one batch) at step 110: 238.7722, Accuracy: 0.7664\n",
      "---- Training ----\n",
      "Training loss: 77.1026\n",
      "Training acc over epoch: 0.7649\n",
      "---- Validation ----\n",
      "Validation loss: 46.0568\n",
      "Validation acc: 0.6741\n",
      "Time taken: 17.87s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 271.6111, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 261.6994, Accuracy: 0.6705\n",
      "Training loss (for one batch) at step 20: 238.9698, Accuracy: 0.7080\n",
      "Training loss (for one batch) at step 30: 227.6002, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 40: 228.4865, Accuracy: 0.7679\n",
      "Training loss (for one batch) at step 50: 242.6560, Accuracy: 0.7849\n",
      "Training loss (for one batch) at step 60: 250.6332, Accuracy: 0.7912\n",
      "Training loss (for one batch) at step 70: 261.8885, Accuracy: 0.7786\n",
      "Training loss (for one batch) at step 80: 269.2032, Accuracy: 0.7645\n",
      "Training loss (for one batch) at step 90: 238.2079, Accuracy: 0.7616\n",
      "Training loss (for one batch) at step 100: 247.0576, Accuracy: 0.7669\n",
      "Training loss (for one batch) at step 110: 274.2833, Accuracy: 0.7663\n",
      "---- Training ----\n",
      "Training loss: 73.5712\n",
      "Training acc over epoch: 0.7656\n",
      "---- Validation ----\n",
      "Validation loss: 38.5358\n",
      "Validation acc: 0.6685\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 266.6269, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 250.6094, Accuracy: 0.6655\n",
      "Training loss (for one batch) at step 20: 249.2850, Accuracy: 0.7128\n",
      "Training loss (for one batch) at step 30: 244.9137, Accuracy: 0.7510\n",
      "Training loss (for one batch) at step 40: 235.0933, Accuracy: 0.7732\n",
      "Training loss (for one batch) at step 50: 228.1882, Accuracy: 0.7868\n",
      "Training loss (for one batch) at step 60: 234.5112, Accuracy: 0.7943\n",
      "Training loss (for one batch) at step 70: 260.9902, Accuracy: 0.7790\n",
      "Training loss (for one batch) at step 80: 244.4800, Accuracy: 0.7640\n",
      "Training loss (for one batch) at step 90: 244.5406, Accuracy: 0.7612\n",
      "Training loss (for one batch) at step 100: 234.4320, Accuracy: 0.7640\n",
      "Training loss (for one batch) at step 110: 252.9395, Accuracy: 0.7658\n",
      "---- Training ----\n",
      "Training loss: 86.3222\n",
      "Training acc over epoch: 0.7648\n",
      "---- Validation ----\n",
      "Validation loss: 67.5687\n",
      "Validation acc: 0.6730\n",
      "Time taken: 18.05s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 282.9450, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 241.0248, Accuracy: 0.6683\n",
      "Training loss (for one batch) at step 20: 221.7858, Accuracy: 0.7076\n",
      "Training loss (for one batch) at step 30: 241.0045, Accuracy: 0.7477\n",
      "Training loss (for one batch) at step 40: 222.1596, Accuracy: 0.7689\n",
      "Training loss (for one batch) at step 50: 237.7014, Accuracy: 0.7843\n",
      "Training loss (for one batch) at step 60: 234.2468, Accuracy: 0.7914\n",
      "Training loss (for one batch) at step 70: 273.3868, Accuracy: 0.7803\n",
      "Training loss (for one batch) at step 80: 250.1328, Accuracy: 0.7632\n",
      "Training loss (for one batch) at step 90: 242.0988, Accuracy: 0.7590\n",
      "Training loss (for one batch) at step 100: 241.5235, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 110: 248.2374, Accuracy: 0.7665\n",
      "---- Training ----\n",
      "Training loss: 84.3194\n",
      "Training acc over epoch: 0.7649\n",
      "---- Validation ----\n",
      "Validation loss: 61.7291\n",
      "Validation acc: 0.6792\n",
      "Time taken: 17.87s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 266.6678, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 259.2228, Accuracy: 0.6697\n",
      "Training loss (for one batch) at step 20: 245.4007, Accuracy: 0.7135\n",
      "Training loss (for one batch) at step 30: 232.3126, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 40: 226.2316, Accuracy: 0.7687\n",
      "Training loss (for one batch) at step 50: 239.3259, Accuracy: 0.7835\n",
      "Training loss (for one batch) at step 60: 251.0092, Accuracy: 0.7903\n",
      "Training loss (for one batch) at step 70: 269.0286, Accuracy: 0.7808\n",
      "Training loss (for one batch) at step 80: 263.7823, Accuracy: 0.7625\n",
      "Training loss (for one batch) at step 90: 221.7041, Accuracy: 0.7597\n",
      "Training loss (for one batch) at step 100: 227.2141, Accuracy: 0.7655\n",
      "Training loss (for one batch) at step 110: 248.7441, Accuracy: 0.7670\n",
      "---- Training ----\n",
      "Training loss: 84.8164\n",
      "Training acc over epoch: 0.7650\n",
      "---- Validation ----\n",
      "Validation loss: 71.5095\n",
      "Validation acc: 0.6693\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 252.7652, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 245.2078, Accuracy: 0.6783\n",
      "Training loss (for one batch) at step 20: 223.6452, Accuracy: 0.7061\n",
      "Training loss (for one batch) at step 30: 222.8486, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 40: 226.9998, Accuracy: 0.7679\n",
      "Training loss (for one batch) at step 50: 251.7941, Accuracy: 0.7865\n",
      "Training loss (for one batch) at step 60: 231.2002, Accuracy: 0.7934\n",
      "Training loss (for one batch) at step 70: 254.9308, Accuracy: 0.7806\n",
      "Training loss (for one batch) at step 80: 261.1698, Accuracy: 0.7647\n",
      "Training loss (for one batch) at step 90: 221.6067, Accuracy: 0.7627\n",
      "Training loss (for one batch) at step 100: 231.2860, Accuracy: 0.7682\n",
      "Training loss (for one batch) at step 110: 258.9424, Accuracy: 0.7690\n",
      "---- Training ----\n",
      "Training loss: 76.4710\n",
      "Training acc over epoch: 0.7669\n",
      "---- Validation ----\n",
      "Validation loss: 62.1118\n",
      "Validation acc: 0.6722\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 254.7724, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 267.3886, Accuracy: 0.6662\n",
      "Training loss (for one batch) at step 20: 242.8092, Accuracy: 0.7080\n",
      "Training loss (for one batch) at step 30: 217.1750, Accuracy: 0.7394\n",
      "Training loss (for one batch) at step 40: 230.0118, Accuracy: 0.7641\n",
      "Training loss (for one batch) at step 50: 242.9233, Accuracy: 0.7817\n",
      "Training loss (for one batch) at step 60: 237.6187, Accuracy: 0.7903\n",
      "Training loss (for one batch) at step 70: 253.9326, Accuracy: 0.7778\n",
      "Training loss (for one batch) at step 80: 244.4121, Accuracy: 0.7609\n",
      "Training loss (for one batch) at step 90: 238.4483, Accuracy: 0.7569\n",
      "Training loss (for one batch) at step 100: 256.5916, Accuracy: 0.7624\n",
      "Training loss (for one batch) at step 110: 247.5793, Accuracy: 0.7641\n",
      "---- Training ----\n",
      "Training loss: 84.4572\n",
      "Training acc over epoch: 0.7637\n",
      "---- Validation ----\n",
      "Validation loss: 64.9015\n",
      "Validation acc: 0.6803\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 253.4011, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 274.8616, Accuracy: 0.6733\n",
      "Training loss (for one batch) at step 20: 227.8882, Accuracy: 0.7072\n",
      "Training loss (for one batch) at step 30: 234.7724, Accuracy: 0.7455\n",
      "Training loss (for one batch) at step 40: 228.6675, Accuracy: 0.7673\n",
      "Training loss (for one batch) at step 50: 235.0391, Accuracy: 0.7840\n",
      "Training loss (for one batch) at step 60: 265.1957, Accuracy: 0.7912\n",
      "Training loss (for one batch) at step 70: 260.0123, Accuracy: 0.7799\n",
      "Training loss (for one batch) at step 80: 248.4610, Accuracy: 0.7637\n",
      "Training loss (for one batch) at step 90: 225.4622, Accuracy: 0.7582\n",
      "Training loss (for one batch) at step 100: 242.7490, Accuracy: 0.7643\n",
      "Training loss (for one batch) at step 110: 254.3250, Accuracy: 0.7660\n",
      "---- Training ----\n",
      "Training loss: 77.7302\n",
      "Training acc over epoch: 0.7645\n",
      "---- Validation ----\n",
      "Validation loss: 75.8123\n",
      "Validation acc: 0.6951\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 251.2824, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 258.7733, Accuracy: 0.6697\n",
      "Training loss (for one batch) at step 20: 244.2798, Accuracy: 0.7094\n",
      "Training loss (for one batch) at step 30: 219.8370, Accuracy: 0.7472\n",
      "Training loss (for one batch) at step 40: 228.3716, Accuracy: 0.7702\n",
      "Training loss (for one batch) at step 50: 233.1934, Accuracy: 0.7834\n",
      "Training loss (for one batch) at step 60: 228.3565, Accuracy: 0.7919\n",
      "Training loss (for one batch) at step 70: 268.0353, Accuracy: 0.7784\n",
      "Training loss (for one batch) at step 80: 257.6274, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 90: 227.6430, Accuracy: 0.7604\n",
      "Training loss (for one batch) at step 100: 232.9267, Accuracy: 0.7644\n",
      "Training loss (for one batch) at step 110: 230.9732, Accuracy: 0.7669\n",
      "---- Training ----\n",
      "Training loss: 80.8627\n",
      "Training acc over epoch: 0.7653\n",
      "---- Validation ----\n",
      "Validation loss: 44.8467\n",
      "Validation acc: 0.6967\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 276.5366, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 240.9540, Accuracy: 0.6676\n",
      "Training loss (for one batch) at step 20: 251.9937, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 30: 244.1232, Accuracy: 0.7475\n",
      "Training loss (for one batch) at step 40: 239.6693, Accuracy: 0.7702\n",
      "Training loss (for one batch) at step 50: 210.6975, Accuracy: 0.7869\n",
      "Training loss (for one batch) at step 60: 231.3228, Accuracy: 0.7938\n",
      "Training loss (for one batch) at step 70: 250.2668, Accuracy: 0.7790\n",
      "Training loss (for one batch) at step 80: 260.3293, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 90: 254.6605, Accuracy: 0.7599\n",
      "Training loss (for one batch) at step 100: 239.4295, Accuracy: 0.7647\n",
      "Training loss (for one batch) at step 110: 245.0102, Accuracy: 0.7663\n",
      "---- Training ----\n",
      "Training loss: 87.5340\n",
      "Training acc over epoch: 0.7642\n",
      "---- Validation ----\n",
      "Validation loss: 40.5214\n",
      "Validation acc: 0.6894\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 278.9302, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 246.0303, Accuracy: 0.6669\n",
      "Training loss (for one batch) at step 20: 234.2800, Accuracy: 0.7094\n",
      "Training loss (for one batch) at step 30: 218.1601, Accuracy: 0.7417\n",
      "Training loss (for one batch) at step 40: 227.7504, Accuracy: 0.7679\n",
      "Training loss (for one batch) at step 50: 223.1554, Accuracy: 0.7834\n",
      "Training loss (for one batch) at step 60: 232.5927, Accuracy: 0.7929\n",
      "Training loss (for one batch) at step 70: 259.3692, Accuracy: 0.7784\n",
      "Training loss (for one batch) at step 80: 241.3510, Accuracy: 0.7631\n",
      "Training loss (for one batch) at step 90: 229.5661, Accuracy: 0.7593\n",
      "Training loss (for one batch) at step 100: 239.2247, Accuracy: 0.7642\n",
      "Training loss (for one batch) at step 110: 273.2400, Accuracy: 0.7637\n",
      "---- Training ----\n",
      "Training loss: 84.9021\n",
      "Training acc over epoch: 0.7632\n",
      "---- Validation ----\n",
      "Validation loss: 54.9109\n",
      "Validation acc: 0.6784\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 247.1717, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 234.6711, Accuracy: 0.6754\n",
      "Training loss (for one batch) at step 20: 228.2160, Accuracy: 0.7094\n",
      "Training loss (for one batch) at step 30: 242.4550, Accuracy: 0.7477\n",
      "Training loss (for one batch) at step 40: 227.4465, Accuracy: 0.7679\n",
      "Training loss (for one batch) at step 50: 221.6317, Accuracy: 0.7848\n",
      "Training loss (for one batch) at step 60: 252.0062, Accuracy: 0.7911\n",
      "Training loss (for one batch) at step 70: 228.0093, Accuracy: 0.7786\n",
      "Training loss (for one batch) at step 80: 251.6436, Accuracy: 0.7632\n",
      "Training loss (for one batch) at step 90: 242.6086, Accuracy: 0.7600\n",
      "Training loss (for one batch) at step 100: 229.0276, Accuracy: 0.7648\n",
      "Training loss (for one batch) at step 110: 250.1018, Accuracy: 0.7662\n",
      "---- Training ----\n",
      "Training loss: 83.0857\n",
      "Training acc over epoch: 0.7648\n",
      "---- Validation ----\n",
      "Validation loss: 51.5566\n",
      "Validation acc: 0.6741\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 261.6784, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 242.6539, Accuracy: 0.6669\n",
      "Training loss (for one batch) at step 20: 244.7443, Accuracy: 0.7076\n",
      "Training loss (for one batch) at step 30: 223.4578, Accuracy: 0.7447\n",
      "Training loss (for one batch) at step 40: 214.2304, Accuracy: 0.7692\n",
      "Training loss (for one batch) at step 50: 217.8886, Accuracy: 0.7863\n",
      "Training loss (for one batch) at step 60: 237.5563, Accuracy: 0.7941\n",
      "Training loss (for one batch) at step 70: 274.5981, Accuracy: 0.7790\n",
      "Training loss (for one batch) at step 80: 255.5439, Accuracy: 0.7631\n",
      "Training loss (for one batch) at step 90: 253.7948, Accuracy: 0.7593\n",
      "Training loss (for one batch) at step 100: 225.3230, Accuracy: 0.7642\n",
      "Training loss (for one batch) at step 110: 223.1782, Accuracy: 0.7656\n",
      "---- Training ----\n",
      "Training loss: 79.5156\n",
      "Training acc over epoch: 0.7644\n",
      "---- Validation ----\n",
      "Validation loss: 42.2415\n",
      "Validation acc: 0.6722\n",
      "Time taken: 18.05s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 263.0840, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 252.7392, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 20: 241.7100, Accuracy: 0.7094\n",
      "Training loss (for one batch) at step 30: 229.0527, Accuracy: 0.7460\n",
      "Training loss (for one batch) at step 40: 224.1140, Accuracy: 0.7713\n",
      "Training loss (for one batch) at step 50: 210.5434, Accuracy: 0.7868\n",
      "Training loss (for one batch) at step 60: 226.9368, Accuracy: 0.7948\n",
      "Training loss (for one batch) at step 70: 265.9708, Accuracy: 0.7807\n",
      "Training loss (for one batch) at step 80: 249.3001, Accuracy: 0.7630\n",
      "Training loss (for one batch) at step 90: 225.5314, Accuracy: 0.7604\n",
      "Training loss (for one batch) at step 100: 231.6667, Accuracy: 0.7636\n",
      "Training loss (for one batch) at step 110: 249.3882, Accuracy: 0.7658\n",
      "---- Training ----\n",
      "Training loss: 75.3542\n",
      "Training acc over epoch: 0.7649\n",
      "---- Validation ----\n",
      "Validation loss: 52.1732\n",
      "Validation acc: 0.6711\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 244.1485, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 250.4908, Accuracy: 0.6534\n",
      "Training loss (for one batch) at step 20: 227.9817, Accuracy: 0.7113\n",
      "Training loss (for one batch) at step 30: 234.1905, Accuracy: 0.7462\n",
      "Training loss (for one batch) at step 40: 234.1702, Accuracy: 0.7683\n",
      "Training loss (for one batch) at step 50: 231.1207, Accuracy: 0.7845\n",
      "Training loss (for one batch) at step 60: 224.3094, Accuracy: 0.7909\n",
      "Training loss (for one batch) at step 70: 236.4364, Accuracy: 0.7761\n",
      "Training loss (for one batch) at step 80: 251.8230, Accuracy: 0.7606\n",
      "Training loss (for one batch) at step 90: 236.2907, Accuracy: 0.7580\n",
      "Training loss (for one batch) at step 100: 235.0329, Accuracy: 0.7629\n",
      "Training loss (for one batch) at step 110: 239.5783, Accuracy: 0.7646\n",
      "---- Training ----\n",
      "Training loss: 96.2623\n",
      "Training acc over epoch: 0.7633\n",
      "---- Validation ----\n",
      "Validation loss: 49.7495\n",
      "Validation acc: 0.6773\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 243.2417, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 250.9620, Accuracy: 0.6577\n",
      "Training loss (for one batch) at step 20: 229.4727, Accuracy: 0.7065\n",
      "Training loss (for one batch) at step 30: 215.4574, Accuracy: 0.7440\n",
      "Training loss (for one batch) at step 40: 255.6651, Accuracy: 0.7702\n",
      "Training loss (for one batch) at step 50: 238.3911, Accuracy: 0.7826\n",
      "Training loss (for one batch) at step 60: 228.4839, Accuracy: 0.7932\n",
      "Training loss (for one batch) at step 70: 247.5642, Accuracy: 0.7781\n",
      "Training loss (for one batch) at step 80: 257.2968, Accuracy: 0.7609\n",
      "Training loss (for one batch) at step 90: 227.7831, Accuracy: 0.7588\n",
      "Training loss (for one batch) at step 100: 235.7441, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 110: 242.3687, Accuracy: 0.7664\n",
      "---- Training ----\n",
      "Training loss: 77.7034\n",
      "Training acc over epoch: 0.7649\n",
      "---- Validation ----\n",
      "Validation loss: 73.1774\n",
      "Validation acc: 0.6808\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 256.3096, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 245.5598, Accuracy: 0.6705\n",
      "Training loss (for one batch) at step 20: 228.2073, Accuracy: 0.7046\n",
      "Training loss (for one batch) at step 30: 221.6056, Accuracy: 0.7371\n",
      "Training loss (for one batch) at step 40: 219.8479, Accuracy: 0.7652\n",
      "Training loss (for one batch) at step 50: 216.6829, Accuracy: 0.7822\n",
      "Training loss (for one batch) at step 60: 237.9122, Accuracy: 0.7893\n",
      "Training loss (for one batch) at step 70: 263.7119, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 80: 238.8129, Accuracy: 0.7624\n",
      "Training loss (for one batch) at step 90: 231.3395, Accuracy: 0.7571\n",
      "Training loss (for one batch) at step 100: 232.9607, Accuracy: 0.7618\n",
      "Training loss (for one batch) at step 110: 238.8998, Accuracy: 0.7632\n",
      "---- Training ----\n",
      "Training loss: 83.4934\n",
      "Training acc over epoch: 0.7622\n",
      "---- Validation ----\n",
      "Validation loss: 42.5859\n",
      "Validation acc: 0.6706\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 256.0878, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 235.1956, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 20: 232.4126, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 30: 237.7987, Accuracy: 0.7462\n",
      "Training loss (for one batch) at step 40: 221.3090, Accuracy: 0.7694\n",
      "Training loss (for one batch) at step 50: 217.1411, Accuracy: 0.7849\n",
      "Training loss (for one batch) at step 60: 223.2142, Accuracy: 0.7915\n",
      "Training loss (for one batch) at step 70: 244.5612, Accuracy: 0.7782\n",
      "Training loss (for one batch) at step 80: 264.0649, Accuracy: 0.7642\n",
      "Training loss (for one batch) at step 90: 246.9969, Accuracy: 0.7618\n",
      "Training loss (for one batch) at step 100: 244.4633, Accuracy: 0.7658\n",
      "Training loss (for one batch) at step 110: 231.9417, Accuracy: 0.7657\n",
      "---- Training ----\n",
      "Training loss: 83.4356\n",
      "Training acc over epoch: 0.7661\n",
      "---- Validation ----\n",
      "Validation loss: 64.5850\n",
      "Validation acc: 0.6736\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 270.5356, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 228.2043, Accuracy: 0.6655\n",
      "Training loss (for one batch) at step 20: 244.6507, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 30: 229.9241, Accuracy: 0.7384\n",
      "Training loss (for one batch) at step 40: 223.0532, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 50: 228.2805, Accuracy: 0.7829\n",
      "Training loss (for one batch) at step 60: 229.9163, Accuracy: 0.7905\n",
      "Training loss (for one batch) at step 70: 234.7730, Accuracy: 0.7765\n",
      "Training loss (for one batch) at step 80: 225.6733, Accuracy: 0.7600\n",
      "Training loss (for one batch) at step 90: 234.5912, Accuracy: 0.7576\n",
      "Training loss (for one batch) at step 100: 238.4351, Accuracy: 0.7641\n",
      "Training loss (for one batch) at step 110: 230.9907, Accuracy: 0.7653\n",
      "---- Training ----\n",
      "Training loss: 66.0328\n",
      "Training acc over epoch: 0.7641\n",
      "---- Validation ----\n",
      "Validation loss: 48.1251\n",
      "Validation acc: 0.6894\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 250.1434, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 258.9962, Accuracy: 0.6612\n",
      "Training loss (for one batch) at step 20: 229.3794, Accuracy: 0.7080\n",
      "Training loss (for one batch) at step 30: 222.3592, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 40: 231.0602, Accuracy: 0.7700\n",
      "Training loss (for one batch) at step 50: 202.8674, Accuracy: 0.7858\n",
      "Training loss (for one batch) at step 60: 232.9497, Accuracy: 0.7907\n",
      "Training loss (for one batch) at step 70: 256.7946, Accuracy: 0.7759\n",
      "Training loss (for one batch) at step 80: 260.7233, Accuracy: 0.7617\n",
      "Training loss (for one batch) at step 90: 224.7705, Accuracy: 0.7576\n",
      "Training loss (for one batch) at step 100: 237.4210, Accuracy: 0.7638\n",
      "Training loss (for one batch) at step 110: 224.6435, Accuracy: 0.7665\n",
      "---- Training ----\n",
      "Training loss: 76.7604\n",
      "Training acc over epoch: 0.7640\n",
      "---- Validation ----\n",
      "Validation loss: 50.2266\n",
      "Validation acc: 0.6889\n",
      "Time taken: 18.05s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 254.6179, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 241.8348, Accuracy: 0.6712\n",
      "Training loss (for one batch) at step 20: 242.8485, Accuracy: 0.7102\n",
      "Training loss (for one batch) at step 30: 216.8499, Accuracy: 0.7432\n",
      "Training loss (for one batch) at step 40: 231.8018, Accuracy: 0.7639\n",
      "Training loss (for one batch) at step 50: 219.1073, Accuracy: 0.7811\n",
      "Training loss (for one batch) at step 60: 221.3275, Accuracy: 0.7887\n",
      "Training loss (for one batch) at step 70: 255.1317, Accuracy: 0.7763\n",
      "Training loss (for one batch) at step 80: 247.4396, Accuracy: 0.7601\n",
      "Training loss (for one batch) at step 90: 214.2454, Accuracy: 0.7576\n",
      "Training loss (for one batch) at step 100: 237.8029, Accuracy: 0.7625\n",
      "Training loss (for one batch) at step 110: 231.1298, Accuracy: 0.7648\n",
      "---- Training ----\n",
      "Training loss: 82.6086\n",
      "Training acc over epoch: 0.7636\n",
      "---- Validation ----\n",
      "Validation loss: 71.1355\n",
      "Validation acc: 0.6985\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 256.0929, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 249.2653, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 20: 218.1080, Accuracy: 0.7072\n",
      "Training loss (for one batch) at step 30: 233.9773, Accuracy: 0.7465\n",
      "Training loss (for one batch) at step 40: 229.5668, Accuracy: 0.7689\n",
      "Training loss (for one batch) at step 50: 231.0108, Accuracy: 0.7845\n",
      "Training loss (for one batch) at step 60: 234.2426, Accuracy: 0.7907\n",
      "Training loss (for one batch) at step 70: 243.6874, Accuracy: 0.7773\n",
      "Training loss (for one batch) at step 80: 250.1797, Accuracy: 0.7610\n",
      "Training loss (for one batch) at step 90: 228.5507, Accuracy: 0.7583\n",
      "Training loss (for one batch) at step 100: 229.1939, Accuracy: 0.7625\n",
      "Training loss (for one batch) at step 110: 227.5831, Accuracy: 0.7646\n",
      "---- Training ----\n",
      "Training loss: 71.7931\n",
      "Training acc over epoch: 0.7641\n",
      "---- Validation ----\n",
      "Validation loss: 62.9335\n",
      "Validation acc: 0.6900\n",
      "Time taken: 18.06s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 232.0774, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 237.1555, Accuracy: 0.6662\n",
      "Training loss (for one batch) at step 20: 232.8249, Accuracy: 0.7087\n",
      "Training loss (for one batch) at step 30: 213.0248, Accuracy: 0.7447\n",
      "Training loss (for one batch) at step 40: 220.3478, Accuracy: 0.7694\n",
      "Training loss (for one batch) at step 50: 220.8061, Accuracy: 0.7846\n",
      "Training loss (for one batch) at step 60: 227.3591, Accuracy: 0.7920\n",
      "Training loss (for one batch) at step 70: 237.2865, Accuracy: 0.7803\n",
      "Training loss (for one batch) at step 80: 265.0648, Accuracy: 0.7622\n",
      "Training loss (for one batch) at step 90: 224.7885, Accuracy: 0.7586\n",
      "Training loss (for one batch) at step 100: 224.2174, Accuracy: 0.7632\n",
      "Training loss (for one batch) at step 110: 236.3833, Accuracy: 0.7643\n",
      "---- Training ----\n",
      "Training loss: 83.0503\n",
      "Training acc over epoch: 0.7642\n",
      "---- Validation ----\n",
      "Validation loss: 42.7223\n",
      "Validation acc: 0.6875\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 233.7691, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 260.9542, Accuracy: 0.6761\n",
      "Training loss (for one batch) at step 20: 231.5830, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 30: 223.8778, Accuracy: 0.7520\n",
      "Training loss (for one batch) at step 40: 221.7580, Accuracy: 0.7719\n",
      "Training loss (for one batch) at step 50: 216.9125, Accuracy: 0.7866\n",
      "Training loss (for one batch) at step 60: 221.3868, Accuracy: 0.7929\n",
      "Training loss (for one batch) at step 70: 247.3134, Accuracy: 0.7815\n",
      "Training loss (for one batch) at step 80: 259.3249, Accuracy: 0.7644\n",
      "Training loss (for one batch) at step 90: 247.1995, Accuracy: 0.7607\n",
      "Training loss (for one batch) at step 100: 240.9636, Accuracy: 0.7648\n",
      "Training loss (for one batch) at step 110: 243.8100, Accuracy: 0.7668\n",
      "---- Training ----\n",
      "Training loss: 70.5678\n",
      "Training acc over epoch: 0.7643\n",
      "---- Validation ----\n",
      "Validation loss: 55.1467\n",
      "Validation acc: 0.6752\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 252.4776, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 230.9643, Accuracy: 0.6705\n",
      "Training loss (for one batch) at step 20: 232.0438, Accuracy: 0.7076\n",
      "Training loss (for one batch) at step 30: 226.4580, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 40: 216.4225, Accuracy: 0.7727\n",
      "Training loss (for one batch) at step 50: 225.8512, Accuracy: 0.7865\n",
      "Training loss (for one batch) at step 60: 250.2789, Accuracy: 0.7912\n",
      "Training loss (for one batch) at step 70: 262.6779, Accuracy: 0.7793\n",
      "Training loss (for one batch) at step 80: 241.7718, Accuracy: 0.7643\n",
      "Training loss (for one batch) at step 90: 226.8276, Accuracy: 0.7606\n",
      "Training loss (for one batch) at step 100: 226.5321, Accuracy: 0.7657\n",
      "Training loss (for one batch) at step 110: 205.9585, Accuracy: 0.7665\n",
      "---- Training ----\n",
      "Training loss: 78.8349\n",
      "Training acc over epoch: 0.7651\n",
      "---- Validation ----\n",
      "Validation loss: 72.1420\n",
      "Validation acc: 0.6913\n",
      "Time taken: 20.15s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 243.6912, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 232.2189, Accuracy: 0.6705\n",
      "Training loss (for one batch) at step 20: 219.1292, Accuracy: 0.7128\n",
      "Training loss (for one batch) at step 30: 220.3555, Accuracy: 0.7462\n",
      "Training loss (for one batch) at step 40: 218.9611, Accuracy: 0.7692\n",
      "Training loss (for one batch) at step 50: 241.7155, Accuracy: 0.7852\n",
      "Training loss (for one batch) at step 60: 225.9140, Accuracy: 0.7910\n",
      "Training loss (for one batch) at step 70: 232.2492, Accuracy: 0.7788\n",
      "Training loss (for one batch) at step 80: 234.1510, Accuracy: 0.7639\n",
      "Training loss (for one batch) at step 90: 220.4689, Accuracy: 0.7613\n",
      "Training loss (for one batch) at step 100: 218.7712, Accuracy: 0.7664\n",
      "Training loss (for one batch) at step 110: 262.7164, Accuracy: 0.7663\n",
      "---- Training ----\n",
      "Training loss: 67.0757\n",
      "Training acc over epoch: 0.7654\n",
      "---- Validation ----\n",
      "Validation loss: 61.4438\n",
      "Validation acc: 0.6722\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 242.5543, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 231.9908, Accuracy: 0.6605\n",
      "Training loss (for one batch) at step 20: 234.2725, Accuracy: 0.7087\n",
      "Training loss (for one batch) at step 30: 226.9456, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 40: 219.9454, Accuracy: 0.7675\n",
      "Training loss (for one batch) at step 50: 222.5581, Accuracy: 0.7845\n",
      "Training loss (for one batch) at step 60: 238.1498, Accuracy: 0.7898\n",
      "Training loss (for one batch) at step 70: 244.9127, Accuracy: 0.7778\n",
      "Training loss (for one batch) at step 80: 258.6364, Accuracy: 0.7634\n",
      "Training loss (for one batch) at step 90: 235.3322, Accuracy: 0.7599\n",
      "Training loss (for one batch) at step 100: 242.0792, Accuracy: 0.7638\n",
      "Training loss (for one batch) at step 110: 245.0026, Accuracy: 0.7659\n",
      "---- Training ----\n",
      "Training loss: 75.6722\n",
      "Training acc over epoch: 0.7643\n",
      "---- Validation ----\n",
      "Validation loss: 44.0909\n",
      "Validation acc: 0.6744\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 235.5649, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 252.9297, Accuracy: 0.6740\n",
      "Training loss (for one batch) at step 20: 235.3772, Accuracy: 0.7121\n",
      "Training loss (for one batch) at step 30: 217.9708, Accuracy: 0.7460\n",
      "Training loss (for one batch) at step 40: 213.1759, Accuracy: 0.7710\n",
      "Training loss (for one batch) at step 50: 218.1497, Accuracy: 0.7866\n",
      "Training loss (for one batch) at step 60: 223.1716, Accuracy: 0.7925\n",
      "Training loss (for one batch) at step 70: 247.5743, Accuracy: 0.7805\n",
      "Training loss (for one batch) at step 80: 275.4541, Accuracy: 0.7623\n",
      "Training loss (for one batch) at step 90: 233.1130, Accuracy: 0.7579\n",
      "Training loss (for one batch) at step 100: 224.6017, Accuracy: 0.7640\n",
      "Training loss (for one batch) at step 110: 268.0637, Accuracy: 0.7649\n",
      "---- Training ----\n",
      "Training loss: 79.2782\n",
      "Training acc over epoch: 0.7638\n",
      "---- Validation ----\n",
      "Validation loss: 61.3260\n",
      "Validation acc: 0.6889\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 242.2244, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 236.9577, Accuracy: 0.6548\n",
      "Training loss (for one batch) at step 20: 229.3664, Accuracy: 0.7065\n",
      "Training loss (for one batch) at step 30: 214.0772, Accuracy: 0.7447\n",
      "Training loss (for one batch) at step 40: 237.1404, Accuracy: 0.7681\n",
      "Training loss (for one batch) at step 50: 213.3722, Accuracy: 0.7840\n",
      "Training loss (for one batch) at step 60: 250.4261, Accuracy: 0.7924\n",
      "Training loss (for one batch) at step 70: 238.9248, Accuracy: 0.7793\n",
      "Training loss (for one batch) at step 80: 235.1786, Accuracy: 0.7628\n",
      "Training loss (for one batch) at step 90: 236.3157, Accuracy: 0.7598\n",
      "Training loss (for one batch) at step 100: 218.4260, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 110: 244.6026, Accuracy: 0.7677\n",
      "---- Training ----\n",
      "Training loss: 76.3621\n",
      "Training acc over epoch: 0.7644\n",
      "---- Validation ----\n",
      "Validation loss: 83.5124\n",
      "Validation acc: 0.6652\n",
      "Time taken: 18.09s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 251.0152, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 236.6540, Accuracy: 0.6648\n",
      "Training loss (for one batch) at step 20: 245.6449, Accuracy: 0.7046\n",
      "Training loss (for one batch) at step 30: 222.1449, Accuracy: 0.7424\n",
      "Training loss (for one batch) at step 40: 234.0680, Accuracy: 0.7671\n",
      "Training loss (for one batch) at step 50: 212.1102, Accuracy: 0.7846\n",
      "Training loss (for one batch) at step 60: 224.1045, Accuracy: 0.7925\n",
      "Training loss (for one batch) at step 70: 229.4211, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 80: 243.4304, Accuracy: 0.7624\n",
      "Training loss (for one batch) at step 90: 247.0964, Accuracy: 0.7610\n",
      "Training loss (for one batch) at step 100: 236.0962, Accuracy: 0.7650\n",
      "Training loss (for one batch) at step 110: 235.3967, Accuracy: 0.7665\n",
      "---- Training ----\n",
      "Training loss: 67.9808\n",
      "Training acc over epoch: 0.7646\n",
      "---- Validation ----\n",
      "Validation loss: 38.7199\n",
      "Validation acc: 0.6918\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 254.2786, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 242.6513, Accuracy: 0.6534\n",
      "Training loss (for one batch) at step 20: 233.1929, Accuracy: 0.7091\n",
      "Training loss (for one batch) at step 30: 229.0827, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 40: 209.8082, Accuracy: 0.7683\n",
      "Training loss (for one batch) at step 50: 219.7891, Accuracy: 0.7857\n",
      "Training loss (for one batch) at step 60: 230.7575, Accuracy: 0.7928\n",
      "Training loss (for one batch) at step 70: 240.9825, Accuracy: 0.7789\n",
      "Training loss (for one batch) at step 80: 230.7332, Accuracy: 0.7624\n",
      "Training loss (for one batch) at step 90: 208.4348, Accuracy: 0.7588\n",
      "Training loss (for one batch) at step 100: 211.3724, Accuracy: 0.7638\n",
      "Training loss (for one batch) at step 110: 227.3758, Accuracy: 0.7657\n",
      "---- Training ----\n",
      "Training loss: 74.8410\n",
      "Training acc over epoch: 0.7644\n",
      "---- Validation ----\n",
      "Validation loss: 46.7197\n",
      "Validation acc: 0.6951\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 241.9778, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 239.5810, Accuracy: 0.6733\n",
      "Training loss (for one batch) at step 20: 213.3073, Accuracy: 0.7091\n",
      "Training loss (for one batch) at step 30: 216.0879, Accuracy: 0.7485\n",
      "Training loss (for one batch) at step 40: 206.1554, Accuracy: 0.7706\n",
      "Training loss (for one batch) at step 50: 207.1868, Accuracy: 0.7854\n",
      "Training loss (for one batch) at step 60: 207.8293, Accuracy: 0.7914\n",
      "Training loss (for one batch) at step 70: 226.8816, Accuracy: 0.7782\n",
      "Training loss (for one batch) at step 80: 235.2237, Accuracy: 0.7628\n",
      "Training loss (for one batch) at step 90: 216.3689, Accuracy: 0.7596\n",
      "Training loss (for one batch) at step 100: 231.4167, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 110: 232.7925, Accuracy: 0.7658\n",
      "---- Training ----\n",
      "Training loss: 77.3683\n",
      "Training acc over epoch: 0.7650\n",
      "---- Validation ----\n",
      "Validation loss: 43.3109\n",
      "Validation acc: 0.6843\n",
      "Time taken: 20.14s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 251.8058, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 255.7190, Accuracy: 0.6740\n",
      "Training loss (for one batch) at step 20: 217.5507, Accuracy: 0.7046\n",
      "Training loss (for one batch) at step 30: 214.3152, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 40: 204.6390, Accuracy: 0.7681\n",
      "Training loss (for one batch) at step 50: 222.2480, Accuracy: 0.7839\n",
      "Training loss (for one batch) at step 60: 226.9852, Accuracy: 0.7915\n",
      "Training loss (for one batch) at step 70: 233.1720, Accuracy: 0.7766\n",
      "Training loss (for one batch) at step 80: 251.4024, Accuracy: 0.7613\n",
      "Training loss (for one batch) at step 90: 223.6107, Accuracy: 0.7574\n",
      "Training loss (for one batch) at step 100: 229.1673, Accuracy: 0.7638\n",
      "Training loss (for one batch) at step 110: 238.1108, Accuracy: 0.7653\n",
      "---- Training ----\n",
      "Training loss: 75.3753\n",
      "Training acc over epoch: 0.7630\n",
      "---- Validation ----\n",
      "Validation loss: 54.9105\n",
      "Validation acc: 0.6959\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 256.3737, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 243.4805, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 20: 214.9275, Accuracy: 0.6987\n",
      "Training loss (for one batch) at step 30: 221.2038, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 40: 216.5519, Accuracy: 0.7719\n",
      "Training loss (for one batch) at step 50: 227.2967, Accuracy: 0.7857\n",
      "Training loss (for one batch) at step 60: 235.8345, Accuracy: 0.7924\n",
      "Training loss (for one batch) at step 70: 241.7606, Accuracy: 0.7797\n",
      "Training loss (for one batch) at step 80: 228.2150, Accuracy: 0.7624\n",
      "Training loss (for one batch) at step 90: 226.9653, Accuracy: 0.7587\n",
      "Training loss (for one batch) at step 100: 221.8599, Accuracy: 0.7645\n",
      "Training loss (for one batch) at step 110: 227.8419, Accuracy: 0.7658\n",
      "---- Training ----\n",
      "Training loss: 77.1814\n",
      "Training acc over epoch: 0.7642\n",
      "---- Validation ----\n",
      "Validation loss: 42.6673\n",
      "Validation acc: 0.7106\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 250.8911, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 244.9353, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 20: 220.7314, Accuracy: 0.7083\n",
      "Training loss (for one batch) at step 30: 227.4053, Accuracy: 0.7470\n",
      "Training loss (for one batch) at step 40: 239.4897, Accuracy: 0.7704\n",
      "Training loss (for one batch) at step 50: 211.5153, Accuracy: 0.7840\n",
      "Training loss (for one batch) at step 60: 234.6064, Accuracy: 0.7919\n",
      "Training loss (for one batch) at step 70: 241.1664, Accuracy: 0.7789\n",
      "Training loss (for one batch) at step 80: 252.0311, Accuracy: 0.7607\n",
      "Training loss (for one batch) at step 90: 226.2534, Accuracy: 0.7582\n",
      "Training loss (for one batch) at step 100: 218.1729, Accuracy: 0.7634\n",
      "Training loss (for one batch) at step 110: 239.1035, Accuracy: 0.7651\n",
      "---- Training ----\n",
      "Training loss: 77.1161\n",
      "Training acc over epoch: 0.7641\n",
      "---- Validation ----\n",
      "Validation loss: 38.4786\n",
      "Validation acc: 0.6918\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 231.8325, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 228.1842, Accuracy: 0.6733\n",
      "Training loss (for one batch) at step 20: 210.9321, Accuracy: 0.7128\n",
      "Training loss (for one batch) at step 30: 228.9319, Accuracy: 0.7487\n",
      "Training loss (for one batch) at step 40: 218.4210, Accuracy: 0.7719\n",
      "Training loss (for one batch) at step 50: 235.8960, Accuracy: 0.7858\n",
      "Training loss (for one batch) at step 60: 216.2577, Accuracy: 0.7909\n",
      "Training loss (for one batch) at step 70: 234.9303, Accuracy: 0.7778\n",
      "Training loss (for one batch) at step 80: 254.7829, Accuracy: 0.7616\n",
      "Training loss (for one batch) at step 90: 222.7762, Accuracy: 0.7594\n",
      "Training loss (for one batch) at step 100: 245.1638, Accuracy: 0.7642\n",
      "Training loss (for one batch) at step 110: 253.8967, Accuracy: 0.7661\n",
      "---- Training ----\n",
      "Training loss: 70.9284\n",
      "Training acc over epoch: 0.7640\n",
      "---- Validation ----\n",
      "Validation loss: 77.1283\n",
      "Validation acc: 0.6996\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 228.9114, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 223.0793, Accuracy: 0.6605\n",
      "Training loss (for one batch) at step 20: 215.8608, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 30: 209.7930, Accuracy: 0.7407\n",
      "Training loss (for one batch) at step 40: 242.5298, Accuracy: 0.7677\n",
      "Training loss (for one batch) at step 50: 237.9931, Accuracy: 0.7854\n",
      "Training loss (for one batch) at step 60: 223.9851, Accuracy: 0.7898\n",
      "Training loss (for one batch) at step 70: 271.9988, Accuracy: 0.7776\n",
      "Training loss (for one batch) at step 80: 246.1119, Accuracy: 0.7612\n",
      "Training loss (for one batch) at step 90: 220.2798, Accuracy: 0.7587\n",
      "Training loss (for one batch) at step 100: 229.8538, Accuracy: 0.7630\n",
      "Training loss (for one batch) at step 110: 220.9713, Accuracy: 0.7654\n",
      "---- Training ----\n",
      "Training loss: 75.2656\n",
      "Training acc over epoch: 0.7628\n",
      "---- Validation ----\n",
      "Validation loss: 69.3987\n",
      "Validation acc: 0.6900\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 236.6562, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 225.8292, Accuracy: 0.6776\n",
      "Training loss (for one batch) at step 20: 233.8216, Accuracy: 0.7091\n",
      "Training loss (for one batch) at step 30: 226.0730, Accuracy: 0.7452\n",
      "Training loss (for one batch) at step 40: 205.9475, Accuracy: 0.7691\n",
      "Training loss (for one batch) at step 50: 225.5119, Accuracy: 0.7840\n",
      "Training loss (for one batch) at step 60: 222.4022, Accuracy: 0.7901\n",
      "Training loss (for one batch) at step 70: 249.3260, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 80: 245.6704, Accuracy: 0.7619\n",
      "Training loss (for one batch) at step 90: 234.9077, Accuracy: 0.7574\n",
      "Training loss (for one batch) at step 100: 237.7416, Accuracy: 0.7622\n",
      "Training loss (for one batch) at step 110: 226.1701, Accuracy: 0.7641\n",
      "---- Training ----\n",
      "Training loss: 72.6668\n",
      "Training acc over epoch: 0.7632\n",
      "---- Validation ----\n",
      "Validation loss: 44.2863\n",
      "Validation acc: 0.6811\n",
      "Time taken: 17.93s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.6939\n",
      "Validation AUC: 0.6961\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 693.8319, Accuracy: 0.4766\n",
      "Training loss (for one batch) at step 10: 578.8286, Accuracy: 0.5142\n",
      "Training loss (for one batch) at step 20: 541.0170, Accuracy: 0.5153\n",
      "Training loss (for one batch) at step 30: 535.6951, Accuracy: 0.5189\n",
      "Training loss (for one batch) at step 40: 485.5503, Accuracy: 0.5160\n",
      "Training loss (for one batch) at step 50: 499.8648, Accuracy: 0.5195\n",
      "Training loss (for one batch) at step 60: 480.9888, Accuracy: 0.5195\n",
      "Training loss (for one batch) at step 70: 493.3076, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 80: 482.5076, Accuracy: 0.5183\n",
      "Training loss (for one batch) at step 90: 474.3747, Accuracy: 0.5192\n",
      "Training loss (for one batch) at step 100: 465.2343, Accuracy: 0.5176\n",
      "Training loss (for one batch) at step 110: 465.3364, Accuracy: 0.5161\n",
      "---- Training ----\n",
      "Training loss: 144.2230\n",
      "Training acc over epoch: 0.5161\n",
      "---- Validation ----\n",
      "Validation loss: 34.8887\n",
      "Validation acc: 0.5134\n",
      "Time taken: 30.09s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 458.7129, Accuracy: 0.4844\n",
      "Training loss (for one batch) at step 10: 456.0429, Accuracy: 0.5206\n",
      "Training loss (for one batch) at step 20: 449.0010, Accuracy: 0.5056\n",
      "Training loss (for one batch) at step 30: 450.3559, Accuracy: 0.5111\n",
      "Training loss (for one batch) at step 40: 452.0355, Accuracy: 0.5149\n",
      "Training loss (for one batch) at step 50: 452.6594, Accuracy: 0.5190\n",
      "Training loss (for one batch) at step 60: 446.0669, Accuracy: 0.5214\n",
      "Training loss (for one batch) at step 70: 447.7134, Accuracy: 0.5200\n",
      "Training loss (for one batch) at step 80: 450.7342, Accuracy: 0.5227\n",
      "Training loss (for one batch) at step 90: 452.4210, Accuracy: 0.5182\n",
      "Training loss (for one batch) at step 100: 445.8461, Accuracy: 0.5187\n",
      "Training loss (for one batch) at step 110: 451.4225, Accuracy: 0.5201\n",
      "---- Training ----\n",
      "Training loss: 143.1171\n",
      "Training acc over epoch: 0.5208\n",
      "---- Validation ----\n",
      "Validation loss: 34.8969\n",
      "Validation acc: 0.4911\n",
      "Time taken: 20.16s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 446.1776, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 444.3346, Accuracy: 0.5490\n",
      "Training loss (for one batch) at step 20: 447.1324, Accuracy: 0.5294\n",
      "Training loss (for one batch) at step 30: 446.7937, Accuracy: 0.5333\n",
      "Training loss (for one batch) at step 40: 447.2906, Accuracy: 0.5311\n",
      "Training loss (for one batch) at step 50: 438.9971, Accuracy: 0.5374\n",
      "Training loss (for one batch) at step 60: 440.5791, Accuracy: 0.5379\n",
      "Training loss (for one batch) at step 70: 444.8068, Accuracy: 0.5393\n",
      "Training loss (for one batch) at step 80: 443.4572, Accuracy: 0.5415\n",
      "Training loss (for one batch) at step 90: 443.8892, Accuracy: 0.5380\n",
      "Training loss (for one batch) at step 100: 441.6845, Accuracy: 0.5361\n",
      "Training loss (for one batch) at step 110: 444.6098, Accuracy: 0.5366\n",
      "---- Training ----\n",
      "Training loss: 140.5131\n",
      "Training acc over epoch: 0.5379\n",
      "---- Validation ----\n",
      "Validation loss: 34.8819\n",
      "Validation acc: 0.5234\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 449.8148, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 445.0024, Accuracy: 0.5582\n",
      "Training loss (for one batch) at step 20: 444.2803, Accuracy: 0.5688\n",
      "Training loss (for one batch) at step 30: 443.7596, Accuracy: 0.5660\n",
      "Training loss (for one batch) at step 40: 440.1797, Accuracy: 0.5539\n",
      "Training loss (for one batch) at step 50: 444.2085, Accuracy: 0.5492\n",
      "Training loss (for one batch) at step 60: 441.8239, Accuracy: 0.5493\n",
      "Training loss (for one batch) at step 70: 444.2537, Accuracy: 0.5540\n",
      "Training loss (for one batch) at step 80: 445.4799, Accuracy: 0.5525\n",
      "Training loss (for one batch) at step 90: 443.5029, Accuracy: 0.5552\n",
      "Training loss (for one batch) at step 100: 441.7421, Accuracy: 0.5532\n",
      "Training loss (for one batch) at step 110: 445.0609, Accuracy: 0.5553\n",
      "---- Training ----\n",
      "Training loss: 135.7363\n",
      "Training acc over epoch: 0.5563\n",
      "---- Validation ----\n",
      "Validation loss: 34.7393\n",
      "Validation acc: 0.5873\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 443.9319, Accuracy: 0.4453\n",
      "Training loss (for one batch) at step 10: 443.2758, Accuracy: 0.5526\n",
      "Training loss (for one batch) at step 20: 441.5692, Accuracy: 0.5450\n",
      "Training loss (for one batch) at step 30: 439.1108, Accuracy: 0.5557\n",
      "Training loss (for one batch) at step 40: 442.9347, Accuracy: 0.5520\n",
      "Training loss (for one batch) at step 50: 438.9758, Accuracy: 0.5570\n",
      "Training loss (for one batch) at step 60: 437.0731, Accuracy: 0.5661\n",
      "Training loss (for one batch) at step 70: 446.7549, Accuracy: 0.5710\n",
      "Training loss (for one batch) at step 80: 441.8263, Accuracy: 0.5717\n",
      "Training loss (for one batch) at step 90: 443.4816, Accuracy: 0.5673\n",
      "Training loss (for one batch) at step 100: 443.3816, Accuracy: 0.5655\n",
      "Training loss (for one batch) at step 110: 447.4166, Accuracy: 0.5674\n",
      "---- Training ----\n",
      "Training loss: 140.2926\n",
      "Training acc over epoch: 0.5680\n",
      "---- Validation ----\n",
      "Validation loss: 34.5960\n",
      "Validation acc: 0.5922\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 442.9457, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 443.3331, Accuracy: 0.5746\n",
      "Training loss (for one batch) at step 20: 440.6797, Accuracy: 0.5785\n",
      "Training loss (for one batch) at step 30: 441.4820, Accuracy: 0.5794\n",
      "Training loss (for one batch) at step 40: 440.3064, Accuracy: 0.5764\n",
      "Training loss (for one batch) at step 50: 447.1341, Accuracy: 0.5846\n",
      "Training loss (for one batch) at step 60: 441.0235, Accuracy: 0.5875\n",
      "Training loss (for one batch) at step 70: 447.7545, Accuracy: 0.5920\n",
      "Training loss (for one batch) at step 80: 441.8349, Accuracy: 0.5901\n",
      "Training loss (for one batch) at step 90: 439.5413, Accuracy: 0.5893\n",
      "Training loss (for one batch) at step 100: 441.0999, Accuracy: 0.5900\n",
      "Training loss (for one batch) at step 110: 445.2476, Accuracy: 0.5906\n",
      "---- Training ----\n",
      "Training loss: 138.7029\n",
      "Training acc over epoch: 0.5913\n",
      "---- Validation ----\n",
      "Validation loss: 34.7859\n",
      "Validation acc: 0.6059\n",
      "Time taken: 18.05s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 441.1640, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 442.5770, Accuracy: 0.5817\n",
      "Training loss (for one batch) at step 20: 437.8253, Accuracy: 0.5911\n",
      "Training loss (for one batch) at step 30: 437.0786, Accuracy: 0.5983\n",
      "Training loss (for one batch) at step 40: 437.5409, Accuracy: 0.6027\n",
      "Training loss (for one batch) at step 50: 442.6502, Accuracy: 0.6081\n",
      "Training loss (for one batch) at step 60: 434.2805, Accuracy: 0.6126\n",
      "Training loss (for one batch) at step 70: 439.3834, Accuracy: 0.6169\n",
      "Training loss (for one batch) at step 80: 443.7715, Accuracy: 0.6148\n",
      "Training loss (for one batch) at step 90: 438.1442, Accuracy: 0.6104\n",
      "Training loss (for one batch) at step 100: 437.0510, Accuracy: 0.6082\n",
      "Training loss (for one batch) at step 110: 441.6110, Accuracy: 0.6056\n",
      "---- Training ----\n",
      "Training loss: 137.1739\n",
      "Training acc over epoch: 0.6052\n",
      "---- Validation ----\n",
      "Validation loss: 35.2323\n",
      "Validation acc: 0.5994\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 438.9006, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 444.8048, Accuracy: 0.5902\n",
      "Training loss (for one batch) at step 20: 435.4604, Accuracy: 0.6001\n",
      "Training loss (for one batch) at step 30: 441.1749, Accuracy: 0.6121\n",
      "Training loss (for one batch) at step 40: 433.8369, Accuracy: 0.6132\n",
      "Training loss (for one batch) at step 50: 435.1411, Accuracy: 0.6219\n",
      "Training loss (for one batch) at step 60: 435.0533, Accuracy: 0.6317\n",
      "Training loss (for one batch) at step 70: 444.6738, Accuracy: 0.6317\n",
      "Training loss (for one batch) at step 80: 442.7163, Accuracy: 0.6292\n",
      "Training loss (for one batch) at step 90: 441.4874, Accuracy: 0.6247\n",
      "Training loss (for one batch) at step 100: 436.6136, Accuracy: 0.6220\n",
      "Training loss (for one batch) at step 110: 438.6960, Accuracy: 0.6231\n",
      "---- Training ----\n",
      "Training loss: 138.6608\n",
      "Training acc over epoch: 0.6239\n",
      "---- Validation ----\n",
      "Validation loss: 34.8111\n",
      "Validation acc: 0.6244\n",
      "Time taken: 18.11s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 443.4044, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 442.2557, Accuracy: 0.6179\n",
      "Training loss (for one batch) at step 20: 440.8506, Accuracy: 0.6146\n",
      "Training loss (for one batch) at step 30: 433.7144, Accuracy: 0.6255\n",
      "Training loss (for one batch) at step 40: 434.6021, Accuracy: 0.6330\n",
      "Training loss (for one batch) at step 50: 426.2261, Accuracy: 0.6394\n",
      "Training loss (for one batch) at step 60: 425.5984, Accuracy: 0.6474\n",
      "Training loss (for one batch) at step 70: 437.3113, Accuracy: 0.6487\n",
      "Training loss (for one batch) at step 80: 444.8821, Accuracy: 0.6423\n",
      "Training loss (for one batch) at step 90: 438.8327, Accuracy: 0.6362\n",
      "Training loss (for one batch) at step 100: 433.2438, Accuracy: 0.6333\n",
      "Training loss (for one batch) at step 110: 440.6369, Accuracy: 0.6327\n",
      "---- Training ----\n",
      "Training loss: 139.0836\n",
      "Training acc over epoch: 0.6343\n",
      "---- Validation ----\n",
      "Validation loss: 34.0336\n",
      "Validation acc: 0.6636\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 447.0099, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 442.1823, Accuracy: 0.6264\n",
      "Training loss (for one batch) at step 20: 432.3986, Accuracy: 0.6097\n",
      "Training loss (for one batch) at step 30: 435.1661, Accuracy: 0.6144\n",
      "Training loss (for one batch) at step 40: 424.7435, Accuracy: 0.6252\n",
      "Training loss (for one batch) at step 50: 421.5714, Accuracy: 0.6405\n",
      "Training loss (for one batch) at step 60: 427.4063, Accuracy: 0.6496\n",
      "Training loss (for one batch) at step 70: 440.6994, Accuracy: 0.6544\n",
      "Training loss (for one batch) at step 80: 436.2881, Accuracy: 0.6508\n",
      "Training loss (for one batch) at step 90: 434.2492, Accuracy: 0.6505\n",
      "Training loss (for one batch) at step 100: 430.6529, Accuracy: 0.6522\n",
      "Training loss (for one batch) at step 110: 430.9286, Accuracy: 0.6553\n",
      "---- Training ----\n",
      "Training loss: 140.0373\n",
      "Training acc over epoch: 0.6562\n",
      "---- Validation ----\n",
      "Validation loss: 35.1512\n",
      "Validation acc: 0.6521\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 435.9579, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 440.3675, Accuracy: 0.6399\n",
      "Training loss (for one batch) at step 20: 434.7461, Accuracy: 0.6332\n",
      "Training loss (for one batch) at step 30: 430.3944, Accuracy: 0.6356\n",
      "Training loss (for one batch) at step 40: 416.6550, Accuracy: 0.6481\n",
      "Training loss (for one batch) at step 50: 435.4634, Accuracy: 0.6601\n",
      "Training loss (for one batch) at step 60: 425.1606, Accuracy: 0.6674\n",
      "Training loss (for one batch) at step 70: 438.6981, Accuracy: 0.6677\n",
      "Training loss (for one batch) at step 80: 438.8778, Accuracy: 0.6614\n",
      "Training loss (for one batch) at step 90: 428.7582, Accuracy: 0.6582\n",
      "Training loss (for one batch) at step 100: 426.2607, Accuracy: 0.6590\n",
      "Training loss (for one batch) at step 110: 434.7274, Accuracy: 0.6603\n",
      "---- Training ----\n",
      "Training loss: 143.4257\n",
      "Training acc over epoch: 0.6598\n",
      "---- Validation ----\n",
      "Validation loss: 33.7103\n",
      "Validation acc: 0.6650\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 436.2758, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 428.5864, Accuracy: 0.6286\n",
      "Training loss (for one batch) at step 20: 439.5765, Accuracy: 0.6414\n",
      "Training loss (for one batch) at step 30: 427.6170, Accuracy: 0.6447\n",
      "Training loss (for one batch) at step 40: 409.8906, Accuracy: 0.6603\n",
      "Training loss (for one batch) at step 50: 398.9565, Accuracy: 0.6742\n",
      "Training loss (for one batch) at step 60: 426.0149, Accuracy: 0.6861\n",
      "Training loss (for one batch) at step 70: 439.0347, Accuracy: 0.6860\n",
      "Training loss (for one batch) at step 80: 434.3072, Accuracy: 0.6756\n",
      "Training loss (for one batch) at step 90: 430.7913, Accuracy: 0.6712\n",
      "Training loss (for one batch) at step 100: 423.7235, Accuracy: 0.6710\n",
      "Training loss (for one batch) at step 110: 431.6017, Accuracy: 0.6751\n",
      "---- Training ----\n",
      "Training loss: 138.1339\n",
      "Training acc over epoch: 0.6762\n",
      "---- Validation ----\n",
      "Validation loss: 36.3515\n",
      "Validation acc: 0.6668\n",
      "Time taken: 18.10s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 432.3207, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 430.4108, Accuracy: 0.6662\n",
      "Training loss (for one batch) at step 20: 436.2975, Accuracy: 0.6462\n",
      "Training loss (for one batch) at step 30: 427.4378, Accuracy: 0.6547\n",
      "Training loss (for one batch) at step 40: 403.6696, Accuracy: 0.6723\n",
      "Training loss (for one batch) at step 50: 393.7485, Accuracy: 0.6898\n",
      "Training loss (for one batch) at step 60: 413.3548, Accuracy: 0.7026\n",
      "Training loss (for one batch) at step 70: 427.8707, Accuracy: 0.7002\n",
      "Training loss (for one batch) at step 80: 430.1137, Accuracy: 0.6940\n",
      "Training loss (for one batch) at step 90: 420.1633, Accuracy: 0.6914\n",
      "Training loss (for one batch) at step 100: 421.8605, Accuracy: 0.6917\n",
      "Training loss (for one batch) at step 110: 416.1699, Accuracy: 0.6926\n",
      "---- Training ----\n",
      "Training loss: 134.6611\n",
      "Training acc over epoch: 0.6932\n",
      "---- Validation ----\n",
      "Validation loss: 34.5633\n",
      "Validation acc: 0.6854\n",
      "Time taken: 18.29s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 428.7951, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 422.6199, Accuracy: 0.6634\n",
      "Training loss (for one batch) at step 20: 422.9890, Accuracy: 0.6529\n",
      "Training loss (for one batch) at step 30: 419.3094, Accuracy: 0.6656\n",
      "Training loss (for one batch) at step 40: 397.2134, Accuracy: 0.6785\n",
      "Training loss (for one batch) at step 50: 395.2310, Accuracy: 0.6935\n",
      "Training loss (for one batch) at step 60: 398.4807, Accuracy: 0.7043\n",
      "Training loss (for one batch) at step 70: 411.3386, Accuracy: 0.7050\n",
      "Training loss (for one batch) at step 80: 407.0803, Accuracy: 0.7000\n",
      "Training loss (for one batch) at step 90: 412.3009, Accuracy: 0.6966\n",
      "Training loss (for one batch) at step 100: 410.7234, Accuracy: 0.6949\n",
      "Training loss (for one batch) at step 110: 410.7073, Accuracy: 0.6976\n",
      "---- Training ----\n",
      "Training loss: 133.8924\n",
      "Training acc over epoch: 0.6983\n",
      "---- Validation ----\n",
      "Validation loss: 37.2739\n",
      "Validation acc: 0.6757\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 432.9213, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 422.9985, Accuracy: 0.6662\n",
      "Training loss (for one batch) at step 20: 422.6384, Accuracy: 0.6663\n",
      "Training loss (for one batch) at step 30: 397.7690, Accuracy: 0.6815\n",
      "Training loss (for one batch) at step 40: 389.1991, Accuracy: 0.7020\n",
      "Training loss (for one batch) at step 50: 369.4370, Accuracy: 0.7128\n",
      "Training loss (for one batch) at step 60: 385.5922, Accuracy: 0.7258\n",
      "Training loss (for one batch) at step 70: 420.7922, Accuracy: 0.7222\n",
      "Training loss (for one batch) at step 80: 418.2406, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 90: 398.1774, Accuracy: 0.7084\n",
      "Training loss (for one batch) at step 100: 389.1957, Accuracy: 0.7078\n",
      "Training loss (for one batch) at step 110: 411.3567, Accuracy: 0.7100\n",
      "---- Training ----\n",
      "Training loss: 131.8957\n",
      "Training acc over epoch: 0.7100\n",
      "---- Validation ----\n",
      "Validation loss: 32.1885\n",
      "Validation acc: 0.6744\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 430.7615, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 408.4712, Accuracy: 0.6768\n",
      "Training loss (for one batch) at step 20: 409.6055, Accuracy: 0.6722\n",
      "Training loss (for one batch) at step 30: 392.7196, Accuracy: 0.6923\n",
      "Training loss (for one batch) at step 40: 361.1751, Accuracy: 0.7088\n",
      "Training loss (for one batch) at step 50: 357.1020, Accuracy: 0.7247\n",
      "Training loss (for one batch) at step 60: 367.6886, Accuracy: 0.7342\n",
      "Training loss (for one batch) at step 70: 399.4591, Accuracy: 0.7263\n",
      "Training loss (for one batch) at step 80: 398.1112, Accuracy: 0.7169\n",
      "Training loss (for one batch) at step 90: 401.7001, Accuracy: 0.7121\n",
      "Training loss (for one batch) at step 100: 402.5151, Accuracy: 0.7149\n",
      "Training loss (for one batch) at step 110: 387.4726, Accuracy: 0.7168\n",
      "---- Training ----\n",
      "Training loss: 121.3013\n",
      "Training acc over epoch: 0.7170\n",
      "---- Validation ----\n",
      "Validation loss: 46.5410\n",
      "Validation acc: 0.6647\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 405.3212, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 420.1821, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 20: 389.2810, Accuracy: 0.6894\n",
      "Training loss (for one batch) at step 30: 390.7637, Accuracy: 0.7054\n",
      "Training loss (for one batch) at step 40: 365.8321, Accuracy: 0.7159\n",
      "Training loss (for one batch) at step 50: 340.9454, Accuracy: 0.7330\n",
      "Training loss (for one batch) at step 60: 372.4478, Accuracy: 0.7431\n",
      "Training loss (for one batch) at step 70: 396.3237, Accuracy: 0.7381\n",
      "Training loss (for one batch) at step 80: 408.5246, Accuracy: 0.7281\n",
      "Training loss (for one batch) at step 90: 370.8815, Accuracy: 0.7245\n",
      "Training loss (for one batch) at step 100: 374.2885, Accuracy: 0.7260\n",
      "Training loss (for one batch) at step 110: 388.5907, Accuracy: 0.7262\n",
      "---- Training ----\n",
      "Training loss: 124.8743\n",
      "Training acc over epoch: 0.7250\n",
      "---- Validation ----\n",
      "Validation loss: 33.6047\n",
      "Validation acc: 0.6658\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 393.7145, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 412.6837, Accuracy: 0.6591\n",
      "Training loss (for one batch) at step 20: 397.9097, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 30: 367.6277, Accuracy: 0.7014\n",
      "Training loss (for one batch) at step 40: 348.2932, Accuracy: 0.7214\n",
      "Training loss (for one batch) at step 50: 326.7181, Accuracy: 0.7353\n",
      "Training loss (for one batch) at step 60: 359.8905, Accuracy: 0.7456\n",
      "Training loss (for one batch) at step 70: 377.4142, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 80: 373.2870, Accuracy: 0.7253\n",
      "Training loss (for one batch) at step 90: 365.7659, Accuracy: 0.7233\n",
      "Training loss (for one batch) at step 100: 354.4149, Accuracy: 0.7270\n",
      "Training loss (for one batch) at step 110: 376.1592, Accuracy: 0.7256\n",
      "---- Training ----\n",
      "Training loss: 123.4936\n",
      "Training acc over epoch: 0.7247\n",
      "---- Validation ----\n",
      "Validation loss: 45.3983\n",
      "Validation acc: 0.6572\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 408.0768, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 383.1143, Accuracy: 0.6619\n",
      "Training loss (for one batch) at step 20: 367.3595, Accuracy: 0.6890\n",
      "Training loss (for one batch) at step 30: 357.9695, Accuracy: 0.7145\n",
      "Training loss (for one batch) at step 40: 346.4331, Accuracy: 0.7298\n",
      "Training loss (for one batch) at step 50: 325.5233, Accuracy: 0.7442\n",
      "Training loss (for one batch) at step 60: 366.0883, Accuracy: 0.7510\n",
      "Training loss (for one batch) at step 70: 375.3494, Accuracy: 0.7402\n",
      "Training loss (for one batch) at step 80: 376.1807, Accuracy: 0.7277\n",
      "Training loss (for one batch) at step 90: 368.3167, Accuracy: 0.7237\n",
      "Training loss (for one batch) at step 100: 359.1347, Accuracy: 0.7279\n",
      "Training loss (for one batch) at step 110: 358.1592, Accuracy: 0.7282\n",
      "---- Training ----\n",
      "Training loss: 120.8760\n",
      "Training acc over epoch: 0.7282\n",
      "---- Validation ----\n",
      "Validation loss: 49.7766\n",
      "Validation acc: 0.6550\n",
      "Time taken: 18.07s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 394.5957, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 385.2067, Accuracy: 0.6669\n",
      "Training loss (for one batch) at step 20: 358.3064, Accuracy: 0.6890\n",
      "Training loss (for one batch) at step 30: 348.4927, Accuracy: 0.7150\n",
      "Training loss (for one batch) at step 40: 347.0925, Accuracy: 0.7369\n",
      "Training loss (for one batch) at step 50: 319.6099, Accuracy: 0.7508\n",
      "Training loss (for one batch) at step 60: 354.4948, Accuracy: 0.7572\n",
      "Training loss (for one batch) at step 70: 388.1727, Accuracy: 0.7467\n",
      "Training loss (for one batch) at step 80: 387.0106, Accuracy: 0.7334\n",
      "Training loss (for one batch) at step 90: 340.1116, Accuracy: 0.7287\n",
      "Training loss (for one batch) at step 100: 342.2595, Accuracy: 0.7328\n",
      "Training loss (for one batch) at step 110: 374.7706, Accuracy: 0.7354\n",
      "---- Training ----\n",
      "Training loss: 109.2016\n",
      "Training acc over epoch: 0.7350\n",
      "---- Validation ----\n",
      "Validation loss: 36.9404\n",
      "Validation acc: 0.6612\n",
      "Time taken: 18.14s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 388.6912, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 375.0770, Accuracy: 0.6548\n",
      "Training loss (for one batch) at step 20: 340.4626, Accuracy: 0.6842\n",
      "Training loss (for one batch) at step 30: 340.1834, Accuracy: 0.7082\n",
      "Training loss (for one batch) at step 40: 331.6562, Accuracy: 0.7309\n",
      "Training loss (for one batch) at step 50: 306.5287, Accuracy: 0.7512\n",
      "Training loss (for one batch) at step 60: 328.1519, Accuracy: 0.7568\n",
      "Training loss (for one batch) at step 70: 363.6413, Accuracy: 0.7465\n",
      "Training loss (for one batch) at step 80: 367.1862, Accuracy: 0.7319\n",
      "Training loss (for one batch) at step 90: 337.7716, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 100: 329.3091, Accuracy: 0.7325\n",
      "Training loss (for one batch) at step 110: 357.2152, Accuracy: 0.7336\n",
      "---- Training ----\n",
      "Training loss: 108.2835\n",
      "Training acc over epoch: 0.7319\n",
      "---- Validation ----\n",
      "Validation loss: 47.3995\n",
      "Validation acc: 0.6580\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 372.1504, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 374.8963, Accuracy: 0.6470\n",
      "Training loss (for one batch) at step 20: 347.6117, Accuracy: 0.6793\n",
      "Training loss (for one batch) at step 30: 322.7620, Accuracy: 0.7147\n",
      "Training loss (for one batch) at step 40: 322.5483, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 50: 325.5187, Accuracy: 0.7537\n",
      "Training loss (for one batch) at step 60: 335.7926, Accuracy: 0.7605\n",
      "Training loss (for one batch) at step 70: 360.4067, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 80: 353.2938, Accuracy: 0.7325\n",
      "Training loss (for one batch) at step 90: 338.9417, Accuracy: 0.7291\n",
      "Training loss (for one batch) at step 100: 311.0120, Accuracy: 0.7349\n",
      "Training loss (for one batch) at step 110: 341.5278, Accuracy: 0.7345\n",
      "---- Training ----\n",
      "Training loss: 98.1460\n",
      "Training acc over epoch: 0.7342\n",
      "---- Validation ----\n",
      "Validation loss: 38.8132\n",
      "Validation acc: 0.6720\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 363.4810, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 385.3913, Accuracy: 0.6392\n",
      "Training loss (for one batch) at step 20: 328.5565, Accuracy: 0.6819\n",
      "Training loss (for one batch) at step 30: 325.1212, Accuracy: 0.7122\n",
      "Training loss (for one batch) at step 40: 309.1544, Accuracy: 0.7340\n",
      "Training loss (for one batch) at step 50: 302.2928, Accuracy: 0.7526\n",
      "Training loss (for one batch) at step 60: 298.0633, Accuracy: 0.7591\n",
      "Training loss (for one batch) at step 70: 339.2231, Accuracy: 0.7472\n",
      "Training loss (for one batch) at step 80: 348.4034, Accuracy: 0.7324\n",
      "Training loss (for one batch) at step 90: 341.0229, Accuracy: 0.7309\n",
      "Training loss (for one batch) at step 100: 315.5308, Accuracy: 0.7360\n",
      "Training loss (for one batch) at step 110: 326.3843, Accuracy: 0.7378\n",
      "---- Training ----\n",
      "Training loss: 108.6993\n",
      "Training acc over epoch: 0.7361\n",
      "---- Validation ----\n",
      "Validation loss: 51.7133\n",
      "Validation acc: 0.6642\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 352.1302, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 350.1132, Accuracy: 0.6342\n",
      "Training loss (for one batch) at step 20: 335.2948, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 30: 293.1429, Accuracy: 0.7132\n",
      "Training loss (for one batch) at step 40: 307.2565, Accuracy: 0.7332\n",
      "Training loss (for one batch) at step 50: 284.3579, Accuracy: 0.7498\n",
      "Training loss (for one batch) at step 60: 315.3356, Accuracy: 0.7611\n",
      "Training loss (for one batch) at step 70: 327.0085, Accuracy: 0.7489\n",
      "Training loss (for one batch) at step 80: 350.6882, Accuracy: 0.7323\n",
      "Training loss (for one batch) at step 90: 315.0164, Accuracy: 0.7305\n",
      "Training loss (for one batch) at step 100: 308.8405, Accuracy: 0.7353\n",
      "Training loss (for one batch) at step 110: 322.4040, Accuracy: 0.7363\n",
      "---- Training ----\n",
      "Training loss: 111.9085\n",
      "Training acc over epoch: 0.7352\n",
      "---- Validation ----\n",
      "Validation loss: 45.3432\n",
      "Validation acc: 0.6738\n",
      "Time taken: 20.14s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 356.3714, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 332.5909, Accuracy: 0.6314\n",
      "Training loss (for one batch) at step 20: 323.7647, Accuracy: 0.6696\n",
      "Training loss (for one batch) at step 30: 300.8541, Accuracy: 0.7026\n",
      "Training loss (for one batch) at step 40: 304.1332, Accuracy: 0.7233\n",
      "Training loss (for one batch) at step 50: 312.1866, Accuracy: 0.7453\n",
      "Training loss (for one batch) at step 60: 333.8023, Accuracy: 0.7540\n",
      "Training loss (for one batch) at step 70: 333.1472, Accuracy: 0.7432\n",
      "Training loss (for one batch) at step 80: 319.2832, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 90: 309.8372, Accuracy: 0.7212\n",
      "Training loss (for one batch) at step 100: 321.6318, Accuracy: 0.7276\n",
      "Training loss (for one batch) at step 110: 325.0603, Accuracy: 0.7302\n",
      "---- Training ----\n",
      "Training loss: 99.4449\n",
      "Training acc over epoch: 0.7296\n",
      "---- Validation ----\n",
      "Validation loss: 46.8126\n",
      "Validation acc: 0.6690\n",
      "Time taken: 20.14s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 359.5710, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 356.2142, Accuracy: 0.6392\n",
      "Training loss (for one batch) at step 20: 324.1500, Accuracy: 0.6763\n",
      "Training loss (for one batch) at step 30: 290.0391, Accuracy: 0.7114\n",
      "Training loss (for one batch) at step 40: 284.7956, Accuracy: 0.7342\n",
      "Training loss (for one batch) at step 50: 285.9388, Accuracy: 0.7471\n",
      "Training loss (for one batch) at step 60: 286.9856, Accuracy: 0.7596\n",
      "Training loss (for one batch) at step 70: 316.0435, Accuracy: 0.7482\n",
      "Training loss (for one batch) at step 80: 346.4648, Accuracy: 0.7300\n",
      "Training loss (for one batch) at step 90: 291.5886, Accuracy: 0.7256\n",
      "Training loss (for one batch) at step 100: 294.4063, Accuracy: 0.7313\n",
      "Training loss (for one batch) at step 110: 323.4933, Accuracy: 0.7326\n",
      "---- Training ----\n",
      "Training loss: 96.5440\n",
      "Training acc over epoch: 0.7311\n",
      "---- Validation ----\n",
      "Validation loss: 42.2028\n",
      "Validation acc: 0.6634\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 350.6580, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 318.7199, Accuracy: 0.6200\n",
      "Training loss (for one batch) at step 20: 311.8261, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 30: 284.9354, Accuracy: 0.7039\n",
      "Training loss (for one batch) at step 40: 280.5967, Accuracy: 0.7319\n",
      "Training loss (for one batch) at step 50: 290.4516, Accuracy: 0.7472\n",
      "Training loss (for one batch) at step 60: 311.6674, Accuracy: 0.7555\n",
      "Training loss (for one batch) at step 70: 333.4341, Accuracy: 0.7459\n",
      "Training loss (for one batch) at step 80: 352.7996, Accuracy: 0.7276\n",
      "Training loss (for one batch) at step 90: 300.7159, Accuracy: 0.7262\n",
      "Training loss (for one batch) at step 100: 308.6264, Accuracy: 0.7316\n",
      "Training loss (for one batch) at step 110: 286.2496, Accuracy: 0.7331\n",
      "---- Training ----\n",
      "Training loss: 95.5209\n",
      "Training acc over epoch: 0.7335\n",
      "---- Validation ----\n",
      "Validation loss: 49.1779\n",
      "Validation acc: 0.6805\n",
      "Time taken: 18.10s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 333.9500, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 335.9767, Accuracy: 0.6278\n",
      "Training loss (for one batch) at step 20: 311.4966, Accuracy: 0.6745\n",
      "Training loss (for one batch) at step 30: 285.1138, Accuracy: 0.7140\n",
      "Training loss (for one batch) at step 40: 278.5863, Accuracy: 0.7374\n",
      "Training loss (for one batch) at step 50: 288.8022, Accuracy: 0.7547\n",
      "Training loss (for one batch) at step 60: 308.8149, Accuracy: 0.7600\n",
      "Training loss (for one batch) at step 70: 311.3759, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 80: 321.1294, Accuracy: 0.7297\n",
      "Training loss (for one batch) at step 90: 288.8709, Accuracy: 0.7270\n",
      "Training loss (for one batch) at step 100: 299.6544, Accuracy: 0.7336\n",
      "Training loss (for one batch) at step 110: 332.9878, Accuracy: 0.7338\n",
      "---- Training ----\n",
      "Training loss: 93.5403\n",
      "Training acc over epoch: 0.7343\n",
      "---- Validation ----\n",
      "Validation loss: 41.8403\n",
      "Validation acc: 0.6760\n",
      "Time taken: 18.06s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 362.7625, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 344.9315, Accuracy: 0.6435\n",
      "Training loss (for one batch) at step 20: 298.4422, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 30: 287.9828, Accuracy: 0.7132\n",
      "Training loss (for one batch) at step 40: 275.4218, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 50: 261.2892, Accuracy: 0.7537\n",
      "Training loss (for one batch) at step 60: 289.3281, Accuracy: 0.7617\n",
      "Training loss (for one batch) at step 70: 329.5473, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 80: 318.0505, Accuracy: 0.7321\n",
      "Training loss (for one batch) at step 90: 287.3702, Accuracy: 0.7272\n",
      "Training loss (for one batch) at step 100: 286.3597, Accuracy: 0.7331\n",
      "Training loss (for one batch) at step 110: 280.8675, Accuracy: 0.7354\n",
      "---- Training ----\n",
      "Training loss: 95.4984\n",
      "Training acc over epoch: 0.7339\n",
      "---- Validation ----\n",
      "Validation loss: 35.8039\n",
      "Validation acc: 0.6685\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 332.2545, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 344.7891, Accuracy: 0.6243\n",
      "Training loss (for one batch) at step 20: 282.4707, Accuracy: 0.6752\n",
      "Training loss (for one batch) at step 30: 289.0502, Accuracy: 0.7167\n",
      "Training loss (for one batch) at step 40: 267.6532, Accuracy: 0.7418\n",
      "Training loss (for one batch) at step 50: 276.3935, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 60: 296.9877, Accuracy: 0.7666\n",
      "Training loss (for one batch) at step 70: 300.8938, Accuracy: 0.7507\n",
      "Training loss (for one batch) at step 80: 311.6570, Accuracy: 0.7329\n",
      "Training loss (for one batch) at step 90: 273.4638, Accuracy: 0.7298\n",
      "Training loss (for one batch) at step 100: 276.0465, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 110: 302.0126, Accuracy: 0.7370\n",
      "---- Training ----\n",
      "Training loss: 97.5000\n",
      "Training acc over epoch: 0.7362\n",
      "---- Validation ----\n",
      "Validation loss: 38.9277\n",
      "Validation acc: 0.6744\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 324.8134, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 314.0105, Accuracy: 0.6314\n",
      "Training loss (for one batch) at step 20: 288.7646, Accuracy: 0.6715\n",
      "Training loss (for one batch) at step 30: 279.9773, Accuracy: 0.7112\n",
      "Training loss (for one batch) at step 40: 265.2794, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 50: 269.8333, Accuracy: 0.7532\n",
      "Training loss (for one batch) at step 60: 266.5865, Accuracy: 0.7604\n",
      "Training loss (for one batch) at step 70: 307.0157, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 80: 326.3623, Accuracy: 0.7296\n",
      "Training loss (for one batch) at step 90: 280.9935, Accuracy: 0.7260\n",
      "Training loss (for one batch) at step 100: 263.8004, Accuracy: 0.7327\n",
      "Training loss (for one batch) at step 110: 281.8934, Accuracy: 0.7334\n",
      "---- Training ----\n",
      "Training loss: 83.3547\n",
      "Training acc over epoch: 0.7332\n",
      "---- Validation ----\n",
      "Validation loss: 40.7561\n",
      "Validation acc: 0.6730\n",
      "Time taken: 18.07s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 334.0827, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 335.5573, Accuracy: 0.6179\n",
      "Training loss (for one batch) at step 20: 285.6065, Accuracy: 0.6763\n",
      "Training loss (for one batch) at step 30: 282.3363, Accuracy: 0.7122\n",
      "Training loss (for one batch) at step 40: 264.0428, Accuracy: 0.7361\n",
      "Training loss (for one batch) at step 50: 260.6171, Accuracy: 0.7546\n",
      "Training loss (for one batch) at step 60: 295.4984, Accuracy: 0.7631\n",
      "Training loss (for one batch) at step 70: 318.6865, Accuracy: 0.7486\n",
      "Training loss (for one batch) at step 80: 296.4776, Accuracy: 0.7331\n",
      "Training loss (for one batch) at step 90: 304.1954, Accuracy: 0.7286\n",
      "Training loss (for one batch) at step 100: 288.0779, Accuracy: 0.7357\n",
      "Training loss (for one batch) at step 110: 305.4253, Accuracy: 0.7375\n",
      "---- Training ----\n",
      "Training loss: 102.6598\n",
      "Training acc over epoch: 0.7364\n",
      "---- Validation ----\n",
      "Validation loss: 45.7217\n",
      "Validation acc: 0.6652\n",
      "Time taken: 18.06s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 314.4362, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 301.1328, Accuracy: 0.6193\n",
      "Training loss (for one batch) at step 20: 288.7067, Accuracy: 0.6689\n",
      "Training loss (for one batch) at step 30: 285.5426, Accuracy: 0.7079\n",
      "Training loss (for one batch) at step 40: 265.6410, Accuracy: 0.7330\n",
      "Training loss (for one batch) at step 50: 260.5348, Accuracy: 0.7486\n",
      "Training loss (for one batch) at step 60: 290.2430, Accuracy: 0.7541\n",
      "Training loss (for one batch) at step 70: 327.6735, Accuracy: 0.7423\n",
      "Training loss (for one batch) at step 80: 298.5986, Accuracy: 0.7281\n",
      "Training loss (for one batch) at step 90: 296.0860, Accuracy: 0.7251\n",
      "Training loss (for one batch) at step 100: 262.1840, Accuracy: 0.7326\n",
      "Training loss (for one batch) at step 110: 290.5993, Accuracy: 0.7337\n",
      "---- Training ----\n",
      "Training loss: 90.1529\n",
      "Training acc over epoch: 0.7331\n",
      "---- Validation ----\n",
      "Validation loss: 39.4644\n",
      "Validation acc: 0.6628\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 327.4676, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 301.8840, Accuracy: 0.6179\n",
      "Training loss (for one batch) at step 20: 272.6480, Accuracy: 0.6663\n",
      "Training loss (for one batch) at step 30: 276.9036, Accuracy: 0.7104\n",
      "Training loss (for one batch) at step 40: 265.7097, Accuracy: 0.7367\n",
      "Training loss (for one batch) at step 50: 256.4788, Accuracy: 0.7554\n",
      "Training loss (for one batch) at step 60: 286.4617, Accuracy: 0.7614\n",
      "Training loss (for one batch) at step 70: 305.3733, Accuracy: 0.7470\n",
      "Training loss (for one batch) at step 80: 290.9389, Accuracy: 0.7319\n",
      "Training loss (for one batch) at step 90: 275.8895, Accuracy: 0.7282\n",
      "Training loss (for one batch) at step 100: 274.1186, Accuracy: 0.7350\n",
      "Training loss (for one batch) at step 110: 283.3315, Accuracy: 0.7366\n",
      "---- Training ----\n",
      "Training loss: 97.6522\n",
      "Training acc over epoch: 0.7362\n",
      "---- Validation ----\n",
      "Validation loss: 43.1875\n",
      "Validation acc: 0.6687\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 348.4651, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 298.9598, Accuracy: 0.6143\n",
      "Training loss (for one batch) at step 20: 281.3884, Accuracy: 0.6685\n",
      "Training loss (for one batch) at step 30: 268.5636, Accuracy: 0.7087\n",
      "Training loss (for one batch) at step 40: 253.0052, Accuracy: 0.7357\n",
      "Training loss (for one batch) at step 50: 258.3399, Accuracy: 0.7506\n",
      "Training loss (for one batch) at step 60: 269.4371, Accuracy: 0.7605\n",
      "Training loss (for one batch) at step 70: 286.2202, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 80: 285.0649, Accuracy: 0.7301\n",
      "Training loss (for one batch) at step 90: 264.6158, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 100: 261.6670, Accuracy: 0.7340\n",
      "Training loss (for one batch) at step 110: 283.3381, Accuracy: 0.7349\n",
      "---- Training ----\n",
      "Training loss: 93.0867\n",
      "Training acc over epoch: 0.7332\n",
      "---- Validation ----\n",
      "Validation loss: 39.1371\n",
      "Validation acc: 0.6741\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 312.4792, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 323.3719, Accuracy: 0.6257\n",
      "Training loss (for one batch) at step 20: 289.1647, Accuracy: 0.6715\n",
      "Training loss (for one batch) at step 30: 263.0962, Accuracy: 0.7054\n",
      "Training loss (for one batch) at step 40: 258.4232, Accuracy: 0.7372\n",
      "Training loss (for one batch) at step 50: 251.9614, Accuracy: 0.7525\n",
      "Training loss (for one batch) at step 60: 265.1209, Accuracy: 0.7602\n",
      "Training loss (for one batch) at step 70: 300.4441, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 80: 290.2983, Accuracy: 0.7299\n",
      "Training loss (for one batch) at step 90: 281.4047, Accuracy: 0.7254\n",
      "Training loss (for one batch) at step 100: 260.7041, Accuracy: 0.7317\n",
      "Training loss (for one batch) at step 110: 268.7218, Accuracy: 0.7339\n",
      "---- Training ----\n",
      "Training loss: 84.8731\n",
      "Training acc over epoch: 0.7325\n",
      "---- Validation ----\n",
      "Validation loss: 66.1381\n",
      "Validation acc: 0.6827\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 343.7998, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 301.9059, Accuracy: 0.6463\n",
      "Training loss (for one batch) at step 20: 264.7396, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 30: 267.2330, Accuracy: 0.7177\n",
      "Training loss (for one batch) at step 40: 281.1953, Accuracy: 0.7416\n",
      "Training loss (for one batch) at step 50: 259.9223, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 60: 264.1716, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 70: 314.2877, Accuracy: 0.7477\n",
      "Training loss (for one batch) at step 80: 286.6757, Accuracy: 0.7322\n",
      "Training loss (for one batch) at step 90: 255.8257, Accuracy: 0.7279\n",
      "Training loss (for one batch) at step 100: 278.9139, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 110: 282.1435, Accuracy: 0.7363\n",
      "---- Training ----\n",
      "Training loss: 82.7249\n",
      "Training acc over epoch: 0.7349\n",
      "---- Validation ----\n",
      "Validation loss: 43.8236\n",
      "Validation acc: 0.6830\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 327.4507, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 294.8449, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 20: 279.1517, Accuracy: 0.6737\n",
      "Training loss (for one batch) at step 30: 264.7506, Accuracy: 0.7114\n",
      "Training loss (for one batch) at step 40: 264.6236, Accuracy: 0.7376\n",
      "Training loss (for one batch) at step 50: 254.3700, Accuracy: 0.7555\n",
      "Training loss (for one batch) at step 60: 282.6351, Accuracy: 0.7591\n",
      "Training loss (for one batch) at step 70: 312.6461, Accuracy: 0.7470\n",
      "Training loss (for one batch) at step 80: 283.7863, Accuracy: 0.7309\n",
      "Training loss (for one batch) at step 90: 264.0996, Accuracy: 0.7288\n",
      "Training loss (for one batch) at step 100: 252.5220, Accuracy: 0.7352\n",
      "Training loss (for one batch) at step 110: 289.0972, Accuracy: 0.7365\n",
      "---- Training ----\n",
      "Training loss: 101.8739\n",
      "Training acc over epoch: 0.7351\n",
      "---- Validation ----\n",
      "Validation loss: 40.3210\n",
      "Validation acc: 0.6760\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 309.3672, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 300.2851, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 20: 274.8929, Accuracy: 0.6700\n",
      "Training loss (for one batch) at step 30: 241.3596, Accuracy: 0.7145\n",
      "Training loss (for one batch) at step 40: 247.7764, Accuracy: 0.7374\n",
      "Training loss (for one batch) at step 50: 260.5645, Accuracy: 0.7537\n",
      "Training loss (for one batch) at step 60: 273.6425, Accuracy: 0.7624\n",
      "Training loss (for one batch) at step 70: 279.6369, Accuracy: 0.7497\n",
      "Training loss (for one batch) at step 80: 284.5350, Accuracy: 0.7317\n",
      "Training loss (for one batch) at step 90: 293.9455, Accuracy: 0.7299\n",
      "Training loss (for one batch) at step 100: 266.7988, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 110: 285.0224, Accuracy: 0.7370\n",
      "---- Training ----\n",
      "Training loss: 89.9998\n",
      "Training acc over epoch: 0.7356\n",
      "---- Validation ----\n",
      "Validation loss: 52.4651\n",
      "Validation acc: 0.6720\n",
      "Time taken: 18.35s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 304.3166, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 318.0947, Accuracy: 0.6115\n",
      "Training loss (for one batch) at step 20: 247.5686, Accuracy: 0.6711\n",
      "Training loss (for one batch) at step 30: 255.6625, Accuracy: 0.7145\n",
      "Training loss (for one batch) at step 40: 262.8396, Accuracy: 0.7399\n",
      "Training loss (for one batch) at step 50: 256.3131, Accuracy: 0.7566\n",
      "Training loss (for one batch) at step 60: 269.0353, Accuracy: 0.7610\n",
      "Training loss (for one batch) at step 70: 295.9885, Accuracy: 0.7467\n",
      "Training loss (for one batch) at step 80: 286.1374, Accuracy: 0.7290\n",
      "Training loss (for one batch) at step 90: 271.8483, Accuracy: 0.7254\n",
      "Training loss (for one batch) at step 100: 250.9432, Accuracy: 0.7322\n",
      "Training loss (for one batch) at step 110: 281.0250, Accuracy: 0.7338\n",
      "---- Training ----\n",
      "Training loss: 96.7570\n",
      "Training acc over epoch: 0.7333\n",
      "---- Validation ----\n",
      "Validation loss: 47.0485\n",
      "Validation acc: 0.6706\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 314.6522, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 281.9155, Accuracy: 0.6129\n",
      "Training loss (for one batch) at step 20: 267.6509, Accuracy: 0.6696\n",
      "Training loss (for one batch) at step 30: 250.8266, Accuracy: 0.7157\n",
      "Training loss (for one batch) at step 40: 251.2887, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 50: 234.8146, Accuracy: 0.7526\n",
      "Training loss (for one batch) at step 60: 253.3032, Accuracy: 0.7615\n",
      "Training loss (for one batch) at step 70: 295.2501, Accuracy: 0.7482\n",
      "Training loss (for one batch) at step 80: 291.0609, Accuracy: 0.7306\n",
      "Training loss (for one batch) at step 90: 268.3444, Accuracy: 0.7272\n",
      "Training loss (for one batch) at step 100: 275.9906, Accuracy: 0.7330\n",
      "Training loss (for one batch) at step 110: 298.9855, Accuracy: 0.7343\n",
      "---- Training ----\n",
      "Training loss: 93.4121\n",
      "Training acc over epoch: 0.7343\n",
      "---- Validation ----\n",
      "Validation loss: 59.6969\n",
      "Validation acc: 0.6690\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 304.2068, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 305.4233, Accuracy: 0.6207\n",
      "Training loss (for one batch) at step 20: 248.2330, Accuracy: 0.6674\n",
      "Training loss (for one batch) at step 30: 244.3251, Accuracy: 0.7077\n",
      "Training loss (for one batch) at step 40: 248.0322, Accuracy: 0.7367\n",
      "Training loss (for one batch) at step 50: 245.4363, Accuracy: 0.7517\n",
      "Training loss (for one batch) at step 60: 258.3894, Accuracy: 0.7620\n",
      "Training loss (for one batch) at step 70: 268.1687, Accuracy: 0.7464\n",
      "Training loss (for one batch) at step 80: 274.0329, Accuracy: 0.7301\n",
      "Training loss (for one batch) at step 90: 269.7916, Accuracy: 0.7259\n",
      "Training loss (for one batch) at step 100: 249.2962, Accuracy: 0.7332\n",
      "Training loss (for one batch) at step 110: 281.7200, Accuracy: 0.7349\n",
      "---- Training ----\n",
      "Training loss: 91.7317\n",
      "Training acc over epoch: 0.7333\n",
      "---- Validation ----\n",
      "Validation loss: 44.9157\n",
      "Validation acc: 0.6749\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 299.4251, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 296.1126, Accuracy: 0.6151\n",
      "Training loss (for one batch) at step 20: 268.6091, Accuracy: 0.6629\n",
      "Training loss (for one batch) at step 30: 245.4344, Accuracy: 0.7087\n",
      "Training loss (for one batch) at step 40: 250.1125, Accuracy: 0.7315\n",
      "Training loss (for one batch) at step 50: 231.7657, Accuracy: 0.7512\n",
      "Training loss (for one batch) at step 60: 273.6341, Accuracy: 0.7596\n",
      "Training loss (for one batch) at step 70: 264.1588, Accuracy: 0.7467\n",
      "Training loss (for one batch) at step 80: 281.2949, Accuracy: 0.7283\n",
      "Training loss (for one batch) at step 90: 249.2584, Accuracy: 0.7248\n",
      "Training loss (for one batch) at step 100: 236.9504, Accuracy: 0.7317\n",
      "Training loss (for one batch) at step 110: 274.1672, Accuracy: 0.7343\n",
      "---- Training ----\n",
      "Training loss: 111.7232\n",
      "Training acc over epoch: 0.7331\n",
      "---- Validation ----\n",
      "Validation loss: 47.9805\n",
      "Validation acc: 0.6609\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 289.2541, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 282.9433, Accuracy: 0.6136\n",
      "Training loss (for one batch) at step 20: 254.7530, Accuracy: 0.6659\n",
      "Training loss (for one batch) at step 30: 246.8575, Accuracy: 0.7127\n",
      "Training loss (for one batch) at step 40: 240.1164, Accuracy: 0.7311\n",
      "Training loss (for one batch) at step 50: 246.4660, Accuracy: 0.7505\n",
      "Training loss (for one batch) at step 60: 247.8211, Accuracy: 0.7569\n",
      "Training loss (for one batch) at step 70: 269.4098, Accuracy: 0.7456\n",
      "Training loss (for one batch) at step 80: 297.0081, Accuracy: 0.7279\n",
      "Training loss (for one batch) at step 90: 258.5405, Accuracy: 0.7241\n",
      "Training loss (for one batch) at step 100: 241.8406, Accuracy: 0.7300\n",
      "Training loss (for one batch) at step 110: 286.0860, Accuracy: 0.7328\n",
      "---- Training ----\n",
      "Training loss: 74.3607\n",
      "Training acc over epoch: 0.7315\n",
      "---- Validation ----\n",
      "Validation loss: 45.7878\n",
      "Validation acc: 0.6668\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 301.8853, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 300.4248, Accuracy: 0.6207\n",
      "Training loss (for one batch) at step 20: 265.2379, Accuracy: 0.6689\n",
      "Training loss (for one batch) at step 30: 242.2393, Accuracy: 0.7084\n",
      "Training loss (for one batch) at step 40: 242.4613, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 50: 253.0305, Accuracy: 0.7540\n",
      "Training loss (for one batch) at step 60: 268.2615, Accuracy: 0.7626\n",
      "Training loss (for one batch) at step 70: 282.8102, Accuracy: 0.7482\n",
      "Training loss (for one batch) at step 80: 276.0205, Accuracy: 0.7316\n",
      "Training loss (for one batch) at step 90: 246.3582, Accuracy: 0.7271\n",
      "Training loss (for one batch) at step 100: 243.5699, Accuracy: 0.7334\n",
      "Training loss (for one batch) at step 110: 281.5729, Accuracy: 0.7357\n",
      "---- Training ----\n",
      "Training loss: 91.6687\n",
      "Training acc over epoch: 0.7338\n",
      "---- Validation ----\n",
      "Validation loss: 45.2407\n",
      "Validation acc: 0.6738\n",
      "Time taken: 20.19s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 276.6348, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 282.3279, Accuracy: 0.6271\n",
      "Training loss (for one batch) at step 20: 253.3264, Accuracy: 0.6711\n",
      "Training loss (for one batch) at step 30: 226.5566, Accuracy: 0.7107\n",
      "Training loss (for one batch) at step 40: 246.9935, Accuracy: 0.7367\n",
      "Training loss (for one batch) at step 50: 238.6486, Accuracy: 0.7541\n",
      "Training loss (for one batch) at step 60: 255.2132, Accuracy: 0.7597\n",
      "Training loss (for one batch) at step 70: 269.0602, Accuracy: 0.7434\n",
      "Training loss (for one batch) at step 80: 289.1927, Accuracy: 0.7275\n",
      "Training loss (for one batch) at step 90: 248.0659, Accuracy: 0.7252\n",
      "Training loss (for one batch) at step 100: 274.6574, Accuracy: 0.7321\n",
      "Training loss (for one batch) at step 110: 261.4805, Accuracy: 0.7343\n",
      "---- Training ----\n",
      "Training loss: 99.3071\n",
      "Training acc over epoch: 0.7336\n",
      "---- Validation ----\n",
      "Validation loss: 41.3181\n",
      "Validation acc: 0.6663\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 276.0754, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 297.5645, Accuracy: 0.5881\n",
      "Training loss (for one batch) at step 20: 259.9270, Accuracy: 0.6566\n",
      "Training loss (for one batch) at step 30: 247.9608, Accuracy: 0.7026\n",
      "Training loss (for one batch) at step 40: 251.1844, Accuracy: 0.7292\n",
      "Training loss (for one batch) at step 50: 236.5786, Accuracy: 0.7497\n",
      "Training loss (for one batch) at step 60: 239.3513, Accuracy: 0.7579\n",
      "Training loss (for one batch) at step 70: 282.6354, Accuracy: 0.7425\n",
      "Training loss (for one batch) at step 80: 287.7553, Accuracy: 0.7242\n",
      "Training loss (for one batch) at step 90: 242.8656, Accuracy: 0.7226\n",
      "Training loss (for one batch) at step 100: 268.2302, Accuracy: 0.7287\n",
      "Training loss (for one batch) at step 110: 262.8200, Accuracy: 0.7302\n",
      "---- Training ----\n",
      "Training loss: 102.8200\n",
      "Training acc over epoch: 0.7298\n",
      "---- Validation ----\n",
      "Validation loss: 44.6292\n",
      "Validation acc: 0.6666\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 299.0158, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 258.3351, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 20: 269.3008, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 30: 242.4809, Accuracy: 0.7051\n",
      "Training loss (for one batch) at step 40: 232.3306, Accuracy: 0.7304\n",
      "Training loss (for one batch) at step 50: 262.4618, Accuracy: 0.7475\n",
      "Training loss (for one batch) at step 60: 251.4781, Accuracy: 0.7533\n",
      "Training loss (for one batch) at step 70: 279.5421, Accuracy: 0.7427\n",
      "Training loss (for one batch) at step 80: 247.9468, Accuracy: 0.7291\n",
      "Training loss (for one batch) at step 90: 256.4184, Accuracy: 0.7228\n",
      "Training loss (for one batch) at step 100: 260.3380, Accuracy: 0.7310\n",
      "Training loss (for one batch) at step 110: 254.6940, Accuracy: 0.7327\n",
      "---- Training ----\n",
      "Training loss: 91.8560\n",
      "Training acc over epoch: 0.7319\n",
      "---- Validation ----\n",
      "Validation loss: 41.5358\n",
      "Validation acc: 0.6709\n",
      "Time taken: 18.06s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 310.4317, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 279.6387, Accuracy: 0.5973\n",
      "Training loss (for one batch) at step 20: 235.5393, Accuracy: 0.6559\n",
      "Training loss (for one batch) at step 30: 249.2675, Accuracy: 0.6961\n",
      "Training loss (for one batch) at step 40: 240.7428, Accuracy: 0.7243\n",
      "Training loss (for one batch) at step 50: 239.5095, Accuracy: 0.7426\n",
      "Training loss (for one batch) at step 60: 255.0455, Accuracy: 0.7541\n",
      "Training loss (for one batch) at step 70: 258.0096, Accuracy: 0.7414\n",
      "Training loss (for one batch) at step 80: 275.3750, Accuracy: 0.7237\n",
      "Training loss (for one batch) at step 90: 248.4954, Accuracy: 0.7216\n",
      "Training loss (for one batch) at step 100: 246.0792, Accuracy: 0.7285\n",
      "Training loss (for one batch) at step 110: 288.4892, Accuracy: 0.7317\n",
      "---- Training ----\n",
      "Training loss: 84.7998\n",
      "Training acc over epoch: 0.7299\n",
      "---- Validation ----\n",
      "Validation loss: 62.4607\n",
      "Validation acc: 0.6728\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 279.8397, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 269.3421, Accuracy: 0.6101\n",
      "Training loss (for one batch) at step 20: 254.8147, Accuracy: 0.6674\n",
      "Training loss (for one batch) at step 30: 238.1495, Accuracy: 0.7130\n",
      "Training loss (for one batch) at step 40: 242.4006, Accuracy: 0.7374\n",
      "Training loss (for one batch) at step 50: 228.5643, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 60: 239.6795, Accuracy: 0.7617\n",
      "Training loss (for one batch) at step 70: 291.0190, Accuracy: 0.7467\n",
      "Training loss (for one batch) at step 80: 265.8681, Accuracy: 0.7302\n",
      "Training loss (for one batch) at step 90: 251.7367, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 100: 247.7779, Accuracy: 0.7334\n",
      "Training loss (for one batch) at step 110: 247.9591, Accuracy: 0.7346\n",
      "---- Training ----\n",
      "Training loss: 78.7393\n",
      "Training acc over epoch: 0.7330\n",
      "---- Validation ----\n",
      "Validation loss: 53.9048\n",
      "Validation acc: 0.6711\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 289.2317, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 264.1588, Accuracy: 0.6151\n",
      "Training loss (for one batch) at step 20: 224.8312, Accuracy: 0.6618\n",
      "Training loss (for one batch) at step 30: 238.1612, Accuracy: 0.7051\n",
      "Training loss (for one batch) at step 40: 255.5183, Accuracy: 0.7340\n",
      "Training loss (for one batch) at step 50: 242.7906, Accuracy: 0.7506\n",
      "Training loss (for one batch) at step 60: 253.6523, Accuracy: 0.7602\n",
      "Training loss (for one batch) at step 70: 288.1310, Accuracy: 0.7456\n",
      "Training loss (for one batch) at step 80: 305.6022, Accuracy: 0.7274\n",
      "Training loss (for one batch) at step 90: 239.7043, Accuracy: 0.7236\n",
      "Training loss (for one batch) at step 100: 248.9688, Accuracy: 0.7315\n",
      "Training loss (for one batch) at step 110: 269.4106, Accuracy: 0.7340\n",
      "---- Training ----\n",
      "Training loss: 80.9364\n",
      "Training acc over epoch: 0.7322\n",
      "---- Validation ----\n",
      "Validation loss: 45.0401\n",
      "Validation acc: 0.6830\n",
      "Time taken: 18.18s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 307.4480, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 282.2933, Accuracy: 0.5994\n",
      "Training loss (for one batch) at step 20: 254.9691, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 30: 231.6061, Accuracy: 0.6993\n",
      "Training loss (for one batch) at step 40: 238.0834, Accuracy: 0.7264\n",
      "Training loss (for one batch) at step 50: 226.1721, Accuracy: 0.7471\n",
      "Training loss (for one batch) at step 60: 250.6712, Accuracy: 0.7579\n",
      "Training loss (for one batch) at step 70: 259.3004, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 80: 283.8770, Accuracy: 0.7262\n",
      "Training loss (for one batch) at step 90: 230.1041, Accuracy: 0.7236\n",
      "Training loss (for one batch) at step 100: 237.1788, Accuracy: 0.7290\n",
      "Training loss (for one batch) at step 110: 242.8736, Accuracy: 0.7307\n",
      "---- Training ----\n",
      "Training loss: 93.6656\n",
      "Training acc over epoch: 0.7302\n",
      "---- Validation ----\n",
      "Validation loss: 65.0021\n",
      "Validation acc: 0.6652\n",
      "Time taken: 18.16s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 270.2591, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 277.2714, Accuracy: 0.6044\n",
      "Training loss (for one batch) at step 20: 246.9953, Accuracy: 0.6745\n",
      "Training loss (for one batch) at step 30: 253.1296, Accuracy: 0.7097\n",
      "Training loss (for one batch) at step 40: 237.2192, Accuracy: 0.7380\n",
      "Training loss (for one batch) at step 50: 230.2096, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 60: 247.2787, Accuracy: 0.7638\n",
      "Training loss (for one batch) at step 70: 287.0320, Accuracy: 0.7499\n",
      "Training loss (for one batch) at step 80: 264.5187, Accuracy: 0.7290\n",
      "Training loss (for one batch) at step 90: 252.7373, Accuracy: 0.7270\n",
      "Training loss (for one batch) at step 100: 240.0471, Accuracy: 0.7323\n",
      "Training loss (for one batch) at step 110: 252.1644, Accuracy: 0.7332\n",
      "---- Training ----\n",
      "Training loss: 88.4654\n",
      "Training acc over epoch: 0.7327\n",
      "---- Validation ----\n",
      "Validation loss: 48.9605\n",
      "Validation acc: 0.6706\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 297.0359, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 265.5046, Accuracy: 0.6058\n",
      "Training loss (for one batch) at step 20: 235.0212, Accuracy: 0.6644\n",
      "Training loss (for one batch) at step 30: 237.3075, Accuracy: 0.7051\n",
      "Training loss (for one batch) at step 40: 235.0401, Accuracy: 0.7302\n",
      "Training loss (for one batch) at step 50: 227.3659, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 60: 256.8999, Accuracy: 0.7576\n",
      "Training loss (for one batch) at step 70: 254.4990, Accuracy: 0.7430\n",
      "Training loss (for one batch) at step 80: 272.6587, Accuracy: 0.7257\n",
      "Training loss (for one batch) at step 90: 257.2883, Accuracy: 0.7211\n",
      "Training loss (for one batch) at step 100: 236.2638, Accuracy: 0.7271\n",
      "Training loss (for one batch) at step 110: 242.0483, Accuracy: 0.7294\n",
      "---- Training ----\n",
      "Training loss: 87.9467\n",
      "Training acc over epoch: 0.7290\n",
      "---- Validation ----\n",
      "Validation loss: 45.4656\n",
      "Validation acc: 0.6733\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 278.1996, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 272.3606, Accuracy: 0.6236\n",
      "Training loss (for one batch) at step 20: 253.3046, Accuracy: 0.6592\n",
      "Training loss (for one batch) at step 30: 234.7987, Accuracy: 0.7054\n",
      "Training loss (for one batch) at step 40: 237.3791, Accuracy: 0.7296\n",
      "Training loss (for one batch) at step 50: 221.9213, Accuracy: 0.7494\n",
      "Training loss (for one batch) at step 60: 244.6929, Accuracy: 0.7567\n",
      "Training loss (for one batch) at step 70: 271.4237, Accuracy: 0.7455\n",
      "Training loss (for one batch) at step 80: 248.4193, Accuracy: 0.7269\n",
      "Training loss (for one batch) at step 90: 250.3959, Accuracy: 0.7244\n",
      "Training loss (for one batch) at step 100: 235.8037, Accuracy: 0.7306\n",
      "Training loss (for one batch) at step 110: 250.4647, Accuracy: 0.7333\n",
      "---- Training ----\n",
      "Training loss: 78.4889\n",
      "Training acc over epoch: 0.7311\n",
      "---- Validation ----\n",
      "Validation loss: 45.0771\n",
      "Validation acc: 0.6744\n",
      "Time taken: 18.12s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 266.9235, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 289.8834, Accuracy: 0.5923\n",
      "Training loss (for one batch) at step 20: 238.4147, Accuracy: 0.6566\n",
      "Training loss (for one batch) at step 30: 237.5977, Accuracy: 0.7019\n",
      "Training loss (for one batch) at step 40: 226.1022, Accuracy: 0.7317\n",
      "Training loss (for one batch) at step 50: 236.6702, Accuracy: 0.7506\n",
      "Training loss (for one batch) at step 60: 232.4310, Accuracy: 0.7587\n",
      "Training loss (for one batch) at step 70: 266.6169, Accuracy: 0.7430\n",
      "Training loss (for one batch) at step 80: 267.0105, Accuracy: 0.7256\n",
      "Training loss (for one batch) at step 90: 244.8150, Accuracy: 0.7229\n",
      "Training loss (for one batch) at step 100: 225.0792, Accuracy: 0.7314\n",
      "Training loss (for one batch) at step 110: 243.5612, Accuracy: 0.7323\n",
      "---- Training ----\n",
      "Training loss: 84.9514\n",
      "Training acc over epoch: 0.7309\n",
      "---- Validation ----\n",
      "Validation loss: 34.4197\n",
      "Validation acc: 0.6693\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 288.4394, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 251.3864, Accuracy: 0.6293\n",
      "Training loss (for one batch) at step 20: 229.5657, Accuracy: 0.6663\n",
      "Training loss (for one batch) at step 30: 238.4685, Accuracy: 0.7074\n",
      "Training loss (for one batch) at step 40: 243.2198, Accuracy: 0.7329\n",
      "Training loss (for one batch) at step 50: 229.1639, Accuracy: 0.7514\n",
      "Training loss (for one batch) at step 60: 243.8877, Accuracy: 0.7600\n",
      "Training loss (for one batch) at step 70: 272.8102, Accuracy: 0.7464\n",
      "Training loss (for one batch) at step 80: 264.8325, Accuracy: 0.7268\n",
      "Training loss (for one batch) at step 90: 226.9125, Accuracy: 0.7244\n",
      "Training loss (for one batch) at step 100: 246.9133, Accuracy: 0.7328\n",
      "Training loss (for one batch) at step 110: 251.0526, Accuracy: 0.7340\n",
      "---- Training ----\n",
      "Training loss: 80.6690\n",
      "Training acc over epoch: 0.7324\n",
      "---- Validation ----\n",
      "Validation loss: 50.6533\n",
      "Validation acc: 0.6749\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 292.5882, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 269.5507, Accuracy: 0.6087\n",
      "Training loss (for one batch) at step 20: 234.7696, Accuracy: 0.6652\n",
      "Training loss (for one batch) at step 30: 239.6112, Accuracy: 0.7034\n",
      "Training loss (for one batch) at step 40: 211.0296, Accuracy: 0.7287\n",
      "Training loss (for one batch) at step 50: 229.6431, Accuracy: 0.7498\n",
      "Training loss (for one batch) at step 60: 266.1151, Accuracy: 0.7582\n",
      "Training loss (for one batch) at step 70: 261.6131, Accuracy: 0.7436\n",
      "Training loss (for one batch) at step 80: 273.3205, Accuracy: 0.7249\n",
      "Training loss (for one batch) at step 90: 245.4172, Accuracy: 0.7230\n",
      "Training loss (for one batch) at step 100: 223.2811, Accuracy: 0.7297\n",
      "Training loss (for one batch) at step 110: 254.5451, Accuracy: 0.7312\n",
      "---- Training ----\n",
      "Training loss: 75.4972\n",
      "Training acc over epoch: 0.7301\n",
      "---- Validation ----\n",
      "Validation loss: 41.5555\n",
      "Validation acc: 0.6709\n",
      "Time taken: 20.15s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 290.9301, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 271.6748, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 20: 240.8582, Accuracy: 0.6559\n",
      "Training loss (for one batch) at step 30: 231.2151, Accuracy: 0.7014\n",
      "Training loss (for one batch) at step 40: 235.4726, Accuracy: 0.7325\n",
      "Training loss (for one batch) at step 50: 216.1745, Accuracy: 0.7505\n",
      "Training loss (for one batch) at step 60: 233.3742, Accuracy: 0.7574\n",
      "Training loss (for one batch) at step 70: 260.3916, Accuracy: 0.7460\n",
      "Training loss (for one batch) at step 80: 267.0157, Accuracy: 0.7269\n",
      "Training loss (for one batch) at step 90: 249.6258, Accuracy: 0.7235\n",
      "Training loss (for one batch) at step 100: 224.6669, Accuracy: 0.7296\n",
      "Training loss (for one batch) at step 110: 260.2881, Accuracy: 0.7321\n",
      "---- Training ----\n",
      "Training loss: 70.1059\n",
      "Training acc over epoch: 0.7306\n",
      "---- Validation ----\n",
      "Validation loss: 50.5817\n",
      "Validation acc: 0.6674\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 300.8198, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 264.2276, Accuracy: 0.6051\n",
      "Training loss (for one batch) at step 20: 261.5918, Accuracy: 0.6615\n",
      "Training loss (for one batch) at step 30: 219.2567, Accuracy: 0.7016\n",
      "Training loss (for one batch) at step 40: 252.1324, Accuracy: 0.7319\n",
      "Training loss (for one batch) at step 50: 221.5139, Accuracy: 0.7466\n",
      "Training loss (for one batch) at step 60: 240.6353, Accuracy: 0.7576\n",
      "Training loss (for one batch) at step 70: 266.6003, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 80: 250.1534, Accuracy: 0.7236\n",
      "Training loss (for one batch) at step 90: 237.4018, Accuracy: 0.7200\n",
      "Training loss (for one batch) at step 100: 236.6328, Accuracy: 0.7259\n",
      "Training loss (for one batch) at step 110: 272.0706, Accuracy: 0.7293\n",
      "---- Training ----\n",
      "Training loss: 88.5442\n",
      "Training acc over epoch: 0.7269\n",
      "---- Validation ----\n",
      "Validation loss: 50.4498\n",
      "Validation acc: 0.6803\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 270.9448, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 254.3375, Accuracy: 0.6030\n",
      "Training loss (for one batch) at step 20: 237.1410, Accuracy: 0.6585\n",
      "Training loss (for one batch) at step 30: 239.9079, Accuracy: 0.7021\n",
      "Training loss (for one batch) at step 40: 220.6981, Accuracy: 0.7277\n",
      "Training loss (for one batch) at step 50: 230.2524, Accuracy: 0.7463\n",
      "Training loss (for one batch) at step 60: 250.5602, Accuracy: 0.7560\n",
      "Training loss (for one batch) at step 70: 248.4346, Accuracy: 0.7415\n",
      "Training loss (for one batch) at step 80: 282.1243, Accuracy: 0.7262\n",
      "Training loss (for one batch) at step 90: 219.8293, Accuracy: 0.7254\n",
      "Training loss (for one batch) at step 100: 238.3612, Accuracy: 0.7297\n",
      "Training loss (for one batch) at step 110: 257.8369, Accuracy: 0.7331\n",
      "---- Training ----\n",
      "Training loss: 90.4404\n",
      "Training acc over epoch: 0.7314\n",
      "---- Validation ----\n",
      "Validation loss: 57.3440\n",
      "Validation acc: 0.6623\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 276.7389, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 265.5734, Accuracy: 0.6030\n",
      "Training loss (for one batch) at step 20: 224.1981, Accuracy: 0.6600\n",
      "Training loss (for one batch) at step 30: 228.6504, Accuracy: 0.7041\n",
      "Training loss (for one batch) at step 40: 228.9243, Accuracy: 0.7292\n",
      "Training loss (for one batch) at step 50: 222.2101, Accuracy: 0.7525\n",
      "Training loss (for one batch) at step 60: 239.2079, Accuracy: 0.7583\n",
      "Training loss (for one batch) at step 70: 263.4000, Accuracy: 0.7439\n",
      "Training loss (for one batch) at step 80: 265.6284, Accuracy: 0.7257\n",
      "Training loss (for one batch) at step 90: 226.5764, Accuracy: 0.7226\n",
      "Training loss (for one batch) at step 100: 217.2957, Accuracy: 0.7290\n",
      "Training loss (for one batch) at step 110: 246.9055, Accuracy: 0.7307\n",
      "---- Training ----\n",
      "Training loss: 73.1746\n",
      "Training acc over epoch: 0.7304\n",
      "---- Validation ----\n",
      "Validation loss: 39.3675\n",
      "Validation acc: 0.6760\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 270.9253, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 261.0266, Accuracy: 0.5994\n",
      "Training loss (for one batch) at step 20: 240.8524, Accuracy: 0.6510\n",
      "Training loss (for one batch) at step 30: 227.6040, Accuracy: 0.6986\n",
      "Training loss (for one batch) at step 40: 228.3141, Accuracy: 0.7285\n",
      "Training loss (for one batch) at step 50: 219.6068, Accuracy: 0.7485\n",
      "Training loss (for one batch) at step 60: 255.3660, Accuracy: 0.7540\n",
      "Training loss (for one batch) at step 70: 240.7178, Accuracy: 0.7404\n",
      "Training loss (for one batch) at step 80: 239.1142, Accuracy: 0.7233\n",
      "Training loss (for one batch) at step 90: 245.4248, Accuracy: 0.7200\n",
      "Training loss (for one batch) at step 100: 216.9448, Accuracy: 0.7259\n",
      "Training loss (for one batch) at step 110: 244.1903, Accuracy: 0.7280\n",
      "---- Training ----\n",
      "Training loss: 91.5529\n",
      "Training acc over epoch: 0.7268\n",
      "---- Validation ----\n",
      "Validation loss: 50.7193\n",
      "Validation acc: 0.6795\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 257.4811, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 259.4599, Accuracy: 0.6051\n",
      "Training loss (for one batch) at step 20: 223.9299, Accuracy: 0.6577\n",
      "Training loss (for one batch) at step 30: 243.6918, Accuracy: 0.7004\n",
      "Training loss (for one batch) at step 40: 231.1599, Accuracy: 0.7302\n",
      "Training loss (for one batch) at step 50: 222.5677, Accuracy: 0.7489\n",
      "Training loss (for one batch) at step 60: 249.0410, Accuracy: 0.7582\n",
      "Training loss (for one batch) at step 70: 270.4703, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 80: 254.1969, Accuracy: 0.7261\n",
      "Training loss (for one batch) at step 90: 221.4753, Accuracy: 0.7211\n",
      "Training loss (for one batch) at step 100: 250.1722, Accuracy: 0.7273\n",
      "Training loss (for one batch) at step 110: 251.6508, Accuracy: 0.7281\n",
      "---- Training ----\n",
      "Training loss: 77.6450\n",
      "Training acc over epoch: 0.7268\n",
      "---- Validation ----\n",
      "Validation loss: 46.3965\n",
      "Validation acc: 0.6757\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 270.4916, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 264.3297, Accuracy: 0.6051\n",
      "Training loss (for one batch) at step 20: 246.9701, Accuracy: 0.6652\n",
      "Training loss (for one batch) at step 30: 239.0779, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 40: 244.5722, Accuracy: 0.7321\n",
      "Training loss (for one batch) at step 50: 230.8726, Accuracy: 0.7512\n",
      "Training loss (for one batch) at step 60: 227.9355, Accuracy: 0.7576\n",
      "Training loss (for one batch) at step 70: 248.9123, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 80: 263.0815, Accuracy: 0.7243\n",
      "Training loss (for one batch) at step 90: 238.8779, Accuracy: 0.7220\n",
      "Training loss (for one batch) at step 100: 251.0829, Accuracy: 0.7287\n",
      "Training loss (for one batch) at step 110: 233.1156, Accuracy: 0.7316\n",
      "---- Training ----\n",
      "Training loss: 85.1828\n",
      "Training acc over epoch: 0.7298\n",
      "---- Validation ----\n",
      "Validation loss: 42.6348\n",
      "Validation acc: 0.6754\n",
      "Time taken: 18.39s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 273.3458, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 275.3929, Accuracy: 0.6037\n",
      "Training loss (for one batch) at step 20: 226.9977, Accuracy: 0.6596\n",
      "Training loss (for one batch) at step 30: 234.2115, Accuracy: 0.6991\n",
      "Training loss (for one batch) at step 40: 243.9524, Accuracy: 0.7271\n",
      "Training loss (for one batch) at step 50: 223.4665, Accuracy: 0.7465\n",
      "Training loss (for one batch) at step 60: 254.7136, Accuracy: 0.7569\n",
      "Training loss (for one batch) at step 70: 243.2376, Accuracy: 0.7435\n",
      "Training loss (for one batch) at step 80: 252.6259, Accuracy: 0.7262\n",
      "Training loss (for one batch) at step 90: 240.2280, Accuracy: 0.7218\n",
      "Training loss (for one batch) at step 100: 225.7263, Accuracy: 0.7271\n",
      "Training loss (for one batch) at step 110: 248.3229, Accuracy: 0.7307\n",
      "---- Training ----\n",
      "Training loss: 88.3359\n",
      "Training acc over epoch: 0.7287\n",
      "---- Validation ----\n",
      "Validation loss: 56.3245\n",
      "Validation acc: 0.6435\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 264.1248, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 261.3588, Accuracy: 0.6342\n",
      "Training loss (for one batch) at step 20: 241.8217, Accuracy: 0.6752\n",
      "Training loss (for one batch) at step 30: 225.5870, Accuracy: 0.7097\n",
      "Training loss (for one batch) at step 40: 219.5087, Accuracy: 0.7359\n",
      "Training loss (for one batch) at step 50: 232.6579, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 60: 234.9474, Accuracy: 0.7587\n",
      "Training loss (for one batch) at step 70: 249.3585, Accuracy: 0.7448\n",
      "Training loss (for one batch) at step 80: 262.6877, Accuracy: 0.7261\n",
      "Training loss (for one batch) at step 90: 237.5180, Accuracy: 0.7231\n",
      "Training loss (for one batch) at step 100: 239.3685, Accuracy: 0.7303\n",
      "Training loss (for one batch) at step 110: 259.9783, Accuracy: 0.7312\n",
      "---- Training ----\n",
      "Training loss: 79.7686\n",
      "Training acc over epoch: 0.7310\n",
      "---- Validation ----\n",
      "Validation loss: 47.1195\n",
      "Validation acc: 0.6784\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 265.0633, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 266.7031, Accuracy: 0.6143\n",
      "Training loss (for one batch) at step 20: 239.1860, Accuracy: 0.6592\n",
      "Training loss (for one batch) at step 30: 229.7864, Accuracy: 0.7059\n",
      "Training loss (for one batch) at step 40: 227.8675, Accuracy: 0.7298\n",
      "Training loss (for one batch) at step 50: 226.4475, Accuracy: 0.7488\n",
      "Training loss (for one batch) at step 60: 236.4155, Accuracy: 0.7569\n",
      "Training loss (for one batch) at step 70: 239.5814, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 80: 254.2998, Accuracy: 0.7234\n",
      "Training loss (for one batch) at step 90: 224.7704, Accuracy: 0.7204\n",
      "Training loss (for one batch) at step 100: 235.9114, Accuracy: 0.7284\n",
      "Training loss (for one batch) at step 110: 238.8189, Accuracy: 0.7290\n",
      "---- Training ----\n",
      "Training loss: 76.1658\n",
      "Training acc over epoch: 0.7280\n",
      "---- Validation ----\n",
      "Validation loss: 64.4646\n",
      "Validation acc: 0.6762\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 290.5989, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 253.1551, Accuracy: 0.6044\n",
      "Training loss (for one batch) at step 20: 221.2715, Accuracy: 0.6570\n",
      "Training loss (for one batch) at step 30: 231.1609, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 40: 221.3325, Accuracy: 0.7334\n",
      "Training loss (for one batch) at step 50: 238.3813, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 60: 246.3796, Accuracy: 0.7599\n",
      "Training loss (for one batch) at step 70: 260.6134, Accuracy: 0.7441\n",
      "Training loss (for one batch) at step 80: 251.5985, Accuracy: 0.7285\n",
      "Training loss (for one batch) at step 90: 222.2461, Accuracy: 0.7248\n",
      "Training loss (for one batch) at step 100: 233.8738, Accuracy: 0.7302\n",
      "Training loss (for one batch) at step 110: 256.3009, Accuracy: 0.7325\n",
      "---- Training ----\n",
      "Training loss: 78.3837\n",
      "Training acc over epoch: 0.7315\n",
      "---- Validation ----\n",
      "Validation loss: 47.0141\n",
      "Validation acc: 0.6636\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 261.7474, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 258.1432, Accuracy: 0.6009\n",
      "Training loss (for one batch) at step 20: 238.5032, Accuracy: 0.6544\n",
      "Training loss (for one batch) at step 30: 220.9539, Accuracy: 0.6988\n",
      "Training loss (for one batch) at step 40: 223.7939, Accuracy: 0.7275\n",
      "Training loss (for one batch) at step 50: 229.7465, Accuracy: 0.7497\n",
      "Training loss (for one batch) at step 60: 231.6482, Accuracy: 0.7570\n",
      "Training loss (for one batch) at step 70: 262.5956, Accuracy: 0.7413\n",
      "Training loss (for one batch) at step 80: 249.9537, Accuracy: 0.7239\n",
      "Training loss (for one batch) at step 90: 232.6385, Accuracy: 0.7211\n",
      "Training loss (for one batch) at step 100: 239.6448, Accuracy: 0.7269\n",
      "Training loss (for one batch) at step 110: 247.9805, Accuracy: 0.7293\n",
      "---- Training ----\n",
      "Training loss: 72.8821\n",
      "Training acc over epoch: 0.7281\n",
      "---- Validation ----\n",
      "Validation loss: 71.3120\n",
      "Validation acc: 0.6752\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 262.7821, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 246.0013, Accuracy: 0.6143\n",
      "Training loss (for one batch) at step 20: 241.5100, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 30: 232.0872, Accuracy: 0.6988\n",
      "Training loss (for one batch) at step 40: 227.5626, Accuracy: 0.7313\n",
      "Training loss (for one batch) at step 50: 225.9779, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 60: 220.9730, Accuracy: 0.7582\n",
      "Training loss (for one batch) at step 70: 245.8412, Accuracy: 0.7420\n",
      "Training loss (for one batch) at step 80: 251.7054, Accuracy: 0.7217\n",
      "Training loss (for one batch) at step 90: 254.1419, Accuracy: 0.7204\n",
      "Training loss (for one batch) at step 100: 217.6983, Accuracy: 0.7276\n",
      "Training loss (for one batch) at step 110: 237.6123, Accuracy: 0.7299\n",
      "---- Training ----\n",
      "Training loss: 93.1867\n",
      "Training acc over epoch: 0.7280\n",
      "---- Validation ----\n",
      "Validation loss: 53.7379\n",
      "Validation acc: 0.6771\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 254.3502, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 271.4310, Accuracy: 0.5994\n",
      "Training loss (for one batch) at step 20: 227.3579, Accuracy: 0.6574\n",
      "Training loss (for one batch) at step 30: 231.8418, Accuracy: 0.7009\n",
      "Training loss (for one batch) at step 40: 217.8224, Accuracy: 0.7233\n",
      "Training loss (for one batch) at step 50: 212.8913, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 60: 229.3507, Accuracy: 0.7524\n",
      "Training loss (for one batch) at step 70: 244.1182, Accuracy: 0.7384\n",
      "Training loss (for one batch) at step 80: 252.0517, Accuracy: 0.7217\n",
      "Training loss (for one batch) at step 90: 233.2991, Accuracy: 0.7201\n",
      "Training loss (for one batch) at step 100: 223.7198, Accuracy: 0.7259\n",
      "Training loss (for one batch) at step 110: 228.7656, Accuracy: 0.7295\n",
      "---- Training ----\n",
      "Training loss: 81.0890\n",
      "Training acc over epoch: 0.7278\n",
      "---- Validation ----\n",
      "Validation loss: 46.9278\n",
      "Validation acc: 0.6784\n",
      "Time taken: 18.09s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 261.3260, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 253.6551, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 20: 221.9535, Accuracy: 0.6529\n",
      "Training loss (for one batch) at step 30: 228.4570, Accuracy: 0.7016\n",
      "Training loss (for one batch) at step 40: 222.2919, Accuracy: 0.7281\n",
      "Training loss (for one batch) at step 50: 214.8669, Accuracy: 0.7472\n",
      "Training loss (for one batch) at step 60: 248.4587, Accuracy: 0.7551\n",
      "Training loss (for one batch) at step 70: 237.5244, Accuracy: 0.7402\n",
      "Training loss (for one batch) at step 80: 260.1394, Accuracy: 0.7213\n",
      "Training loss (for one batch) at step 90: 229.7362, Accuracy: 0.7179\n",
      "Training loss (for one batch) at step 100: 219.8969, Accuracy: 0.7249\n",
      "Training loss (for one batch) at step 110: 243.0224, Accuracy: 0.7272\n",
      "---- Training ----\n",
      "Training loss: 79.7682\n",
      "Training acc over epoch: 0.7267\n",
      "---- Validation ----\n",
      "Validation loss: 49.9137\n",
      "Validation acc: 0.6730\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 268.3859, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 254.9407, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 20: 239.1004, Accuracy: 0.6585\n",
      "Training loss (for one batch) at step 30: 221.6417, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 40: 225.1855, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 50: 202.9217, Accuracy: 0.7497\n",
      "Training loss (for one batch) at step 60: 247.7728, Accuracy: 0.7590\n",
      "Training loss (for one batch) at step 70: 272.1729, Accuracy: 0.7423\n",
      "Training loss (for one batch) at step 80: 245.5586, Accuracy: 0.7229\n",
      "Training loss (for one batch) at step 90: 248.9612, Accuracy: 0.7213\n",
      "Training loss (for one batch) at step 100: 224.2402, Accuracy: 0.7269\n",
      "Training loss (for one batch) at step 110: 239.2332, Accuracy: 0.7293\n",
      "---- Training ----\n",
      "Training loss: 71.1262\n",
      "Training acc over epoch: 0.7291\n",
      "---- Validation ----\n",
      "Validation loss: 63.8750\n",
      "Validation acc: 0.6612\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 280.4508, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 243.7896, Accuracy: 0.6072\n",
      "Training loss (for one batch) at step 20: 251.5655, Accuracy: 0.6581\n",
      "Training loss (for one batch) at step 30: 230.4696, Accuracy: 0.7001\n",
      "Training loss (for one batch) at step 40: 226.9410, Accuracy: 0.7268\n",
      "Training loss (for one batch) at step 50: 211.0948, Accuracy: 0.7465\n",
      "Training loss (for one batch) at step 60: 235.1460, Accuracy: 0.7549\n",
      "Training loss (for one batch) at step 70: 234.8489, Accuracy: 0.7432\n",
      "Training loss (for one batch) at step 80: 262.1573, Accuracy: 0.7244\n",
      "Training loss (for one batch) at step 90: 238.9559, Accuracy: 0.7208\n",
      "Training loss (for one batch) at step 100: 228.4549, Accuracy: 0.7268\n",
      "Training loss (for one batch) at step 110: 241.7033, Accuracy: 0.7295\n",
      "---- Training ----\n",
      "Training loss: 79.8038\n",
      "Training acc over epoch: 0.7288\n",
      "---- Validation ----\n",
      "Validation loss: 47.8578\n",
      "Validation acc: 0.6787\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 264.0356, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 248.8785, Accuracy: 0.5952\n",
      "Training loss (for one batch) at step 20: 228.2377, Accuracy: 0.6555\n",
      "Training loss (for one batch) at step 30: 215.9398, Accuracy: 0.7004\n",
      "Training loss (for one batch) at step 40: 209.9843, Accuracy: 0.7283\n",
      "Training loss (for one batch) at step 50: 224.9638, Accuracy: 0.7482\n",
      "Training loss (for one batch) at step 60: 217.0601, Accuracy: 0.7549\n",
      "Training loss (for one batch) at step 70: 253.7840, Accuracy: 0.7388\n",
      "Training loss (for one batch) at step 80: 241.5397, Accuracy: 0.7224\n",
      "Training loss (for one batch) at step 90: 224.1265, Accuracy: 0.7195\n",
      "Training loss (for one batch) at step 100: 226.3541, Accuracy: 0.7250\n",
      "Training loss (for one batch) at step 110: 231.3806, Accuracy: 0.7285\n",
      "---- Training ----\n",
      "Training loss: 86.0788\n",
      "Training acc over epoch: 0.7276\n",
      "---- Validation ----\n",
      "Validation loss: 48.8713\n",
      "Validation acc: 0.6574\n",
      "Time taken: 18.08s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 258.3701, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 238.9784, Accuracy: 0.5952\n",
      "Training loss (for one batch) at step 20: 227.8381, Accuracy: 0.6566\n",
      "Training loss (for one batch) at step 30: 223.9971, Accuracy: 0.7097\n",
      "Training loss (for one batch) at step 40: 231.2955, Accuracy: 0.7338\n",
      "Training loss (for one batch) at step 50: 216.3754, Accuracy: 0.7523\n",
      "Training loss (for one batch) at step 60: 225.9417, Accuracy: 0.7585\n",
      "Training loss (for one batch) at step 70: 252.1360, Accuracy: 0.7430\n",
      "Training loss (for one batch) at step 80: 263.6237, Accuracy: 0.7245\n",
      "Training loss (for one batch) at step 90: 237.7663, Accuracy: 0.7225\n",
      "Training loss (for one batch) at step 100: 234.6481, Accuracy: 0.7290\n",
      "Training loss (for one batch) at step 110: 248.0607, Accuracy: 0.7297\n",
      "---- Training ----\n",
      "Training loss: 76.9476\n",
      "Training acc over epoch: 0.7287\n",
      "---- Validation ----\n",
      "Validation loss: 60.1178\n",
      "Validation acc: 0.6612\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 254.1200, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 249.8493, Accuracy: 0.5959\n",
      "Training loss (for one batch) at step 20: 224.5831, Accuracy: 0.6533\n",
      "Training loss (for one batch) at step 30: 219.5768, Accuracy: 0.6993\n",
      "Training loss (for one batch) at step 40: 232.6712, Accuracy: 0.7298\n",
      "Training loss (for one batch) at step 50: 218.0314, Accuracy: 0.7480\n",
      "Training loss (for one batch) at step 60: 241.1587, Accuracy: 0.7569\n",
      "Training loss (for one batch) at step 70: 253.7407, Accuracy: 0.7435\n",
      "Training loss (for one batch) at step 80: 228.2092, Accuracy: 0.7253\n",
      "Training loss (for one batch) at step 90: 237.4000, Accuracy: 0.7218\n",
      "Training loss (for one batch) at step 100: 223.5882, Accuracy: 0.7274\n",
      "Training loss (for one batch) at step 110: 251.8554, Accuracy: 0.7294\n",
      "---- Training ----\n",
      "Training loss: 82.7018\n",
      "Training acc over epoch: 0.7287\n",
      "---- Validation ----\n",
      "Validation loss: 38.2945\n",
      "Validation acc: 0.6558\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 247.4195, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 240.6046, Accuracy: 0.6129\n",
      "Training loss (for one batch) at step 20: 230.9354, Accuracy: 0.6585\n",
      "Training loss (for one batch) at step 30: 210.7253, Accuracy: 0.6966\n",
      "Training loss (for one batch) at step 40: 234.0068, Accuracy: 0.7269\n",
      "Training loss (for one batch) at step 50: 220.8551, Accuracy: 0.7466\n",
      "Training loss (for one batch) at step 60: 223.8509, Accuracy: 0.7569\n",
      "Training loss (for one batch) at step 70: 253.3872, Accuracy: 0.7399\n",
      "Training loss (for one batch) at step 80: 240.6765, Accuracy: 0.7202\n",
      "Training loss (for one batch) at step 90: 221.3922, Accuracy: 0.7168\n",
      "Training loss (for one batch) at step 100: 231.0601, Accuracy: 0.7235\n",
      "Training loss (for one batch) at step 110: 240.9008, Accuracy: 0.7259\n",
      "---- Training ----\n",
      "Training loss: 81.5344\n",
      "Training acc over epoch: 0.7257\n",
      "---- Validation ----\n",
      "Validation loss: 44.0082\n",
      "Validation acc: 0.6620\n",
      "Time taken: 18.18s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 270.4208, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 246.8571, Accuracy: 0.6037\n",
      "Training loss (for one batch) at step 20: 212.7658, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 30: 215.1188, Accuracy: 0.6971\n",
      "Training loss (for one batch) at step 40: 229.8816, Accuracy: 0.7269\n",
      "Training loss (for one batch) at step 50: 219.8412, Accuracy: 0.7462\n",
      "Training loss (for one batch) at step 60: 224.8225, Accuracy: 0.7565\n",
      "Training loss (for one batch) at step 70: 250.3885, Accuracy: 0.7421\n",
      "Training loss (for one batch) at step 80: 258.6790, Accuracy: 0.7226\n",
      "Training loss (for one batch) at step 90: 222.9471, Accuracy: 0.7198\n",
      "Training loss (for one batch) at step 100: 225.5332, Accuracy: 0.7268\n",
      "Training loss (for one batch) at step 110: 245.2908, Accuracy: 0.7287\n",
      "---- Training ----\n",
      "Training loss: 95.0063\n",
      "Training acc over epoch: 0.7284\n",
      "---- Validation ----\n",
      "Validation loss: 36.3881\n",
      "Validation acc: 0.6647\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 260.5190, Accuracy: 0.4922\n",
      "Training loss (for one batch) at step 10: 241.4236, Accuracy: 0.6044\n",
      "Training loss (for one batch) at step 20: 226.5376, Accuracy: 0.6488\n",
      "Training loss (for one batch) at step 30: 217.4881, Accuracy: 0.7014\n",
      "Training loss (for one batch) at step 40: 218.8050, Accuracy: 0.7302\n",
      "Training loss (for one batch) at step 50: 208.3275, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 60: 222.2036, Accuracy: 0.7568\n",
      "Training loss (for one batch) at step 70: 248.1577, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 80: 246.0486, Accuracy: 0.7237\n",
      "Training loss (for one batch) at step 90: 226.0110, Accuracy: 0.7200\n",
      "Training loss (for one batch) at step 100: 228.1451, Accuracy: 0.7276\n",
      "Training loss (for one batch) at step 110: 243.8521, Accuracy: 0.7292\n",
      "---- Training ----\n",
      "Training loss: 89.8516\n",
      "Training acc over epoch: 0.7286\n",
      "---- Validation ----\n",
      "Validation loss: 52.4424\n",
      "Validation acc: 0.6714\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 269.6824, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 254.3008, Accuracy: 0.5923\n",
      "Training loss (for one batch) at step 20: 254.1674, Accuracy: 0.6481\n",
      "Training loss (for one batch) at step 30: 210.2773, Accuracy: 0.6961\n",
      "Training loss (for one batch) at step 40: 228.2354, Accuracy: 0.7235\n",
      "Training loss (for one batch) at step 50: 232.2842, Accuracy: 0.7448\n",
      "Training loss (for one batch) at step 60: 231.5778, Accuracy: 0.7556\n",
      "Training loss (for one batch) at step 70: 234.1084, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 80: 241.5125, Accuracy: 0.7210\n",
      "Training loss (for one batch) at step 90: 222.2558, Accuracy: 0.7176\n",
      "Training loss (for one batch) at step 100: 219.7547, Accuracy: 0.7242\n",
      "Training loss (for one batch) at step 110: 221.6481, Accuracy: 0.7280\n",
      "---- Training ----\n",
      "Training loss: 79.3332\n",
      "Training acc over epoch: 0.7265\n",
      "---- Validation ----\n",
      "Validation loss: 40.9310\n",
      "Validation acc: 0.6779\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 271.7874, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 279.3186, Accuracy: 0.5966\n",
      "Training loss (for one batch) at step 20: 240.5910, Accuracy: 0.6536\n",
      "Training loss (for one batch) at step 30: 221.4084, Accuracy: 0.6983\n",
      "Training loss (for one batch) at step 40: 237.8659, Accuracy: 0.7281\n",
      "Training loss (for one batch) at step 50: 205.3881, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 60: 221.7974, Accuracy: 0.7590\n",
      "Training loss (for one batch) at step 70: 250.2455, Accuracy: 0.7431\n",
      "Training loss (for one batch) at step 80: 224.5183, Accuracy: 0.7233\n",
      "Training loss (for one batch) at step 90: 228.3074, Accuracy: 0.7186\n",
      "Training loss (for one batch) at step 100: 222.2273, Accuracy: 0.7252\n",
      "Training loss (for one batch) at step 110: 223.6533, Accuracy: 0.7289\n",
      "---- Training ----\n",
      "Training loss: 85.9546\n",
      "Training acc over epoch: 0.7272\n",
      "---- Validation ----\n",
      "Validation loss: 61.1281\n",
      "Validation acc: 0.6628\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 255.6126, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 258.7971, Accuracy: 0.5923\n",
      "Training loss (for one batch) at step 20: 225.6767, Accuracy: 0.6447\n",
      "Training loss (for one batch) at step 30: 218.3577, Accuracy: 0.6961\n",
      "Training loss (for one batch) at step 40: 219.9444, Accuracy: 0.7273\n",
      "Training loss (for one batch) at step 50: 214.4482, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 60: 220.9886, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 70: 258.8556, Accuracy: 0.7404\n",
      "Training loss (for one batch) at step 80: 238.9910, Accuracy: 0.7220\n",
      "Training loss (for one batch) at step 90: 238.8790, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 100: 213.8104, Accuracy: 0.7268\n",
      "Training loss (for one batch) at step 110: 229.3429, Accuracy: 0.7274\n",
      "---- Training ----\n",
      "Training loss: 73.0222\n",
      "Training acc over epoch: 0.7271\n",
      "---- Validation ----\n",
      "Validation loss: 47.2053\n",
      "Validation acc: 0.6757\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 255.1733, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 252.6456, Accuracy: 0.6080\n",
      "Training loss (for one batch) at step 20: 213.9032, Accuracy: 0.6611\n",
      "Training loss (for one batch) at step 30: 220.5885, Accuracy: 0.6998\n",
      "Training loss (for one batch) at step 40: 217.0783, Accuracy: 0.7302\n",
      "Training loss (for one batch) at step 50: 222.9676, Accuracy: 0.7492\n",
      "Training loss (for one batch) at step 60: 225.1963, Accuracy: 0.7573\n",
      "Training loss (for one batch) at step 70: 263.0013, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 80: 227.5359, Accuracy: 0.7230\n",
      "Training loss (for one batch) at step 90: 228.7301, Accuracy: 0.7206\n",
      "Training loss (for one batch) at step 100: 209.4876, Accuracy: 0.7270\n",
      "Training loss (for one batch) at step 110: 241.2060, Accuracy: 0.7292\n",
      "---- Training ----\n",
      "Training loss: 88.4801\n",
      "Training acc over epoch: 0.7275\n",
      "---- Validation ----\n",
      "Validation loss: 50.5860\n",
      "Validation acc: 0.6703\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 251.8994, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 247.4255, Accuracy: 0.5973\n",
      "Training loss (for one batch) at step 20: 223.4089, Accuracy: 0.6499\n",
      "Training loss (for one batch) at step 30: 215.8984, Accuracy: 0.6958\n",
      "Training loss (for one batch) at step 40: 217.8748, Accuracy: 0.7248\n",
      "Training loss (for one batch) at step 50: 218.6256, Accuracy: 0.7472\n",
      "Training loss (for one batch) at step 60: 226.2416, Accuracy: 0.7564\n",
      "Training loss (for one batch) at step 70: 225.2556, Accuracy: 0.7404\n",
      "Training loss (for one batch) at step 80: 235.0038, Accuracy: 0.7238\n",
      "Training loss (for one batch) at step 90: 229.0886, Accuracy: 0.7205\n",
      "Training loss (for one batch) at step 100: 231.2695, Accuracy: 0.7260\n",
      "Training loss (for one batch) at step 110: 215.2301, Accuracy: 0.7284\n",
      "---- Training ----\n",
      "Training loss: 89.0814\n",
      "Training acc over epoch: 0.7266\n",
      "---- Validation ----\n",
      "Validation loss: 37.2508\n",
      "Validation acc: 0.6617\n",
      "Time taken: 18.20s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 242.7662, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 243.6527, Accuracy: 0.6072\n",
      "Training loss (for one batch) at step 20: 232.6261, Accuracy: 0.6581\n",
      "Training loss (for one batch) at step 30: 231.0775, Accuracy: 0.7049\n",
      "Training loss (for one batch) at step 40: 209.4017, Accuracy: 0.7348\n",
      "Training loss (for one batch) at step 50: 210.9995, Accuracy: 0.7498\n",
      "Training loss (for one batch) at step 60: 220.8902, Accuracy: 0.7579\n",
      "Training loss (for one batch) at step 70: 231.3445, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 80: 261.3029, Accuracy: 0.7223\n",
      "Training loss (for one batch) at step 90: 228.7572, Accuracy: 0.7195\n",
      "Training loss (for one batch) at step 100: 233.8329, Accuracy: 0.7259\n",
      "Training loss (for one batch) at step 110: 241.2590, Accuracy: 0.7269\n",
      "---- Training ----\n",
      "Training loss: 81.7678\n",
      "Training acc over epoch: 0.7267\n",
      "---- Validation ----\n",
      "Validation loss: 52.9056\n",
      "Validation acc: 0.6797\n",
      "Time taken: 18.05s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 273.1202, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 236.8044, Accuracy: 0.6087\n",
      "Training loss (for one batch) at step 20: 233.0020, Accuracy: 0.6536\n",
      "Training loss (for one batch) at step 30: 222.3835, Accuracy: 0.6991\n",
      "Training loss (for one batch) at step 40: 222.1325, Accuracy: 0.7294\n",
      "Training loss (for one batch) at step 50: 206.9861, Accuracy: 0.7485\n",
      "Training loss (for one batch) at step 60: 235.2120, Accuracy: 0.7554\n",
      "Training loss (for one batch) at step 70: 232.8845, Accuracy: 0.7427\n",
      "Training loss (for one batch) at step 80: 248.4769, Accuracy: 0.7234\n",
      "Training loss (for one batch) at step 90: 229.5871, Accuracy: 0.7197\n",
      "Training loss (for one batch) at step 100: 225.5533, Accuracy: 0.7277\n",
      "Training loss (for one batch) at step 110: 241.7519, Accuracy: 0.7291\n",
      "---- Training ----\n",
      "Training loss: 82.7075\n",
      "Training acc over epoch: 0.7274\n",
      "---- Validation ----\n",
      "Validation loss: 56.2894\n",
      "Validation acc: 0.6776\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 255.2614, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 251.8296, Accuracy: 0.5923\n",
      "Training loss (for one batch) at step 20: 222.2868, Accuracy: 0.6481\n",
      "Training loss (for one batch) at step 30: 251.4248, Accuracy: 0.6951\n",
      "Training loss (for one batch) at step 40: 222.1472, Accuracy: 0.7264\n",
      "Training loss (for one batch) at step 50: 210.7363, Accuracy: 0.7442\n",
      "Training loss (for one batch) at step 60: 219.5963, Accuracy: 0.7538\n",
      "Training loss (for one batch) at step 70: 247.5561, Accuracy: 0.7388\n",
      "Training loss (for one batch) at step 80: 250.0476, Accuracy: 0.7206\n",
      "Training loss (for one batch) at step 90: 222.0244, Accuracy: 0.7161\n",
      "Training loss (for one batch) at step 100: 223.5517, Accuracy: 0.7230\n",
      "Training loss (for one batch) at step 110: 215.0102, Accuracy: 0.7252\n",
      "---- Training ----\n",
      "Training loss: 87.8230\n",
      "Training acc over epoch: 0.7247\n",
      "---- Validation ----\n",
      "Validation loss: 65.6205\n",
      "Validation acc: 0.6757\n",
      "Time taken: 17.89s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 252.1774, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 267.4261, Accuracy: 0.5987\n",
      "Training loss (for one batch) at step 20: 216.2896, Accuracy: 0.6544\n",
      "Training loss (for one batch) at step 30: 209.5893, Accuracy: 0.7041\n",
      "Training loss (for one batch) at step 40: 213.6160, Accuracy: 0.7292\n",
      "Training loss (for one batch) at step 50: 210.0182, Accuracy: 0.7489\n",
      "Training loss (for one batch) at step 60: 211.5663, Accuracy: 0.7581\n",
      "Training loss (for one batch) at step 70: 236.2466, Accuracy: 0.7416\n",
      "Training loss (for one batch) at step 80: 239.1496, Accuracy: 0.7220\n",
      "Training loss (for one batch) at step 90: 226.2187, Accuracy: 0.7176\n",
      "Training loss (for one batch) at step 100: 215.6387, Accuracy: 0.7250\n",
      "Training loss (for one batch) at step 110: 240.1365, Accuracy: 0.7284\n",
      "---- Training ----\n",
      "Training loss: 85.0135\n",
      "Training acc over epoch: 0.7267\n",
      "---- Validation ----\n",
      "Validation loss: 41.3468\n",
      "Validation acc: 0.6805\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 250.1156, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 235.3542, Accuracy: 0.5987\n",
      "Training loss (for one batch) at step 20: 226.7130, Accuracy: 0.6540\n",
      "Training loss (for one batch) at step 30: 210.3029, Accuracy: 0.6996\n",
      "Training loss (for one batch) at step 40: 217.4639, Accuracy: 0.7308\n",
      "Training loss (for one batch) at step 50: 206.1318, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 60: 210.1645, Accuracy: 0.7565\n",
      "Training loss (for one batch) at step 70: 255.3015, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 80: 246.5447, Accuracy: 0.7215\n",
      "Training loss (for one batch) at step 90: 222.2997, Accuracy: 0.7176\n",
      "Training loss (for one batch) at step 100: 220.7480, Accuracy: 0.7248\n",
      "Training loss (for one batch) at step 110: 223.2975, Accuracy: 0.7274\n",
      "---- Training ----\n",
      "Training loss: 85.0786\n",
      "Training acc over epoch: 0.7259\n",
      "---- Validation ----\n",
      "Validation loss: 56.0173\n",
      "Validation acc: 0.6711\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 242.8257, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 229.2790, Accuracy: 0.6044\n",
      "Training loss (for one batch) at step 20: 231.8773, Accuracy: 0.6536\n",
      "Training loss (for one batch) at step 30: 224.8221, Accuracy: 0.6923\n",
      "Training loss (for one batch) at step 40: 205.3397, Accuracy: 0.7260\n",
      "Training loss (for one batch) at step 50: 210.6649, Accuracy: 0.7459\n",
      "Training loss (for one batch) at step 60: 221.0808, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 70: 245.6350, Accuracy: 0.7393\n",
      "Training loss (for one batch) at step 80: 245.7439, Accuracy: 0.7227\n",
      "Training loss (for one batch) at step 90: 245.2964, Accuracy: 0.7176\n",
      "Training loss (for one batch) at step 100: 223.2480, Accuracy: 0.7235\n",
      "Training loss (for one batch) at step 110: 267.2917, Accuracy: 0.7264\n",
      "---- Training ----\n",
      "Training loss: 86.7469\n",
      "Training acc over epoch: 0.7254\n",
      "---- Validation ----\n",
      "Validation loss: 59.6316\n",
      "Validation acc: 0.6604\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 245.6783, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 238.5178, Accuracy: 0.6037\n",
      "Training loss (for one batch) at step 20: 233.8889, Accuracy: 0.6455\n",
      "Training loss (for one batch) at step 30: 216.1879, Accuracy: 0.6981\n",
      "Training loss (for one batch) at step 40: 215.9303, Accuracy: 0.7258\n",
      "Training loss (for one batch) at step 50: 203.8731, Accuracy: 0.7451\n",
      "Training loss (for one batch) at step 60: 235.8778, Accuracy: 0.7527\n",
      "Training loss (for one batch) at step 70: 241.7670, Accuracy: 0.7380\n",
      "Training loss (for one batch) at step 80: 248.4068, Accuracy: 0.7217\n",
      "Training loss (for one batch) at step 90: 215.4799, Accuracy: 0.7177\n",
      "Training loss (for one batch) at step 100: 224.2685, Accuracy: 0.7252\n",
      "Training loss (for one batch) at step 110: 226.2072, Accuracy: 0.7267\n",
      "---- Training ----\n",
      "Training loss: 78.7120\n",
      "Training acc over epoch: 0.7251\n",
      "---- Validation ----\n",
      "Validation loss: 48.6522\n",
      "Validation acc: 0.6902\n",
      "Time taken: 18.15s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 258.1297, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 253.2093, Accuracy: 0.5973\n",
      "Training loss (for one batch) at step 20: 225.2367, Accuracy: 0.6548\n",
      "Training loss (for one batch) at step 30: 206.9244, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 40: 217.5856, Accuracy: 0.7283\n",
      "Training loss (for one batch) at step 50: 218.1015, Accuracy: 0.7486\n",
      "Training loss (for one batch) at step 60: 221.0078, Accuracy: 0.7561\n",
      "Training loss (for one batch) at step 70: 226.7225, Accuracy: 0.7416\n",
      "Training loss (for one batch) at step 80: 239.5661, Accuracy: 0.7222\n",
      "Training loss (for one batch) at step 90: 226.3889, Accuracy: 0.7186\n",
      "Training loss (for one batch) at step 100: 209.7887, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 110: 246.2310, Accuracy: 0.7277\n",
      "---- Training ----\n",
      "Training loss: 77.0249\n",
      "Training acc over epoch: 0.7270\n",
      "---- Validation ----\n",
      "Validation loss: 49.7392\n",
      "Validation acc: 0.6765\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 228.0559, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 252.8779, Accuracy: 0.5888\n",
      "Training loss (for one batch) at step 20: 237.2963, Accuracy: 0.6529\n",
      "Training loss (for one batch) at step 30: 232.1133, Accuracy: 0.6981\n",
      "Training loss (for one batch) at step 40: 235.7133, Accuracy: 0.7275\n",
      "Training loss (for one batch) at step 50: 223.9236, Accuracy: 0.7485\n",
      "Training loss (for one batch) at step 60: 215.4968, Accuracy: 0.7565\n",
      "Training loss (for one batch) at step 70: 235.9409, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 80: 230.7281, Accuracy: 0.7214\n",
      "Training loss (for one batch) at step 90: 213.6667, Accuracy: 0.7194\n",
      "Training loss (for one batch) at step 100: 213.3870, Accuracy: 0.7253\n",
      "Training loss (for one batch) at step 110: 231.8779, Accuracy: 0.7270\n",
      "---- Training ----\n",
      "Training loss: 98.3849\n",
      "Training acc over epoch: 0.7265\n",
      "---- Validation ----\n",
      "Validation loss: 57.1112\n",
      "Validation acc: 0.6832\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 254.0480, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 253.9267, Accuracy: 0.6037\n",
      "Training loss (for one batch) at step 20: 223.6221, Accuracy: 0.6589\n",
      "Training loss (for one batch) at step 30: 222.3254, Accuracy: 0.7006\n",
      "Training loss (for one batch) at step 40: 208.0274, Accuracy: 0.7269\n",
      "Training loss (for one batch) at step 50: 223.2981, Accuracy: 0.7451\n",
      "Training loss (for one batch) at step 60: 214.6329, Accuracy: 0.7564\n",
      "Training loss (for one batch) at step 70: 238.3045, Accuracy: 0.7426\n",
      "Training loss (for one batch) at step 80: 244.7668, Accuracy: 0.7213\n",
      "Training loss (for one batch) at step 90: 247.0041, Accuracy: 0.7176\n",
      "Training loss (for one batch) at step 100: 221.1019, Accuracy: 0.7250\n",
      "Training loss (for one batch) at step 110: 260.1063, Accuracy: 0.7259\n",
      "---- Training ----\n",
      "Training loss: 75.3795\n",
      "Training acc over epoch: 0.7253\n",
      "---- Validation ----\n",
      "Validation loss: 61.0428\n",
      "Validation acc: 0.6857\n",
      "Time taken: 20.13s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 237.1830, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 224.6910, Accuracy: 0.6030\n",
      "Training loss (for one batch) at step 20: 211.5289, Accuracy: 0.6548\n",
      "Training loss (for one batch) at step 30: 215.2979, Accuracy: 0.7014\n",
      "Training loss (for one batch) at step 40: 228.0822, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 50: 241.4048, Accuracy: 0.7485\n",
      "Training loss (for one batch) at step 60: 214.5828, Accuracy: 0.7554\n",
      "Training loss (for one batch) at step 70: 252.3248, Accuracy: 0.7392\n",
      "Training loss (for one batch) at step 80: 242.9193, Accuracy: 0.7219\n",
      "Training loss (for one batch) at step 90: 223.4645, Accuracy: 0.7185\n",
      "Training loss (for one batch) at step 100: 226.2546, Accuracy: 0.7246\n",
      "Training loss (for one batch) at step 110: 223.1882, Accuracy: 0.7272\n",
      "---- Training ----\n",
      "Training loss: 85.7179\n",
      "Training acc over epoch: 0.7258\n",
      "---- Validation ----\n",
      "Validation loss: 43.2559\n",
      "Validation acc: 0.6625\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 242.0179, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 236.6642, Accuracy: 0.5817\n",
      "Training loss (for one batch) at step 20: 217.6436, Accuracy: 0.6347\n",
      "Training loss (for one batch) at step 30: 211.2847, Accuracy: 0.6903\n",
      "Training loss (for one batch) at step 40: 205.3544, Accuracy: 0.7207\n",
      "Training loss (for one batch) at step 50: 221.5050, Accuracy: 0.7439\n",
      "Training loss (for one batch) at step 60: 231.9662, Accuracy: 0.7546\n",
      "Training loss (for one batch) at step 70: 241.0300, Accuracy: 0.7367\n",
      "Training loss (for one batch) at step 80: 240.4069, Accuracy: 0.7182\n",
      "Training loss (for one batch) at step 90: 218.6809, Accuracy: 0.7142\n",
      "Training loss (for one batch) at step 100: 209.7216, Accuracy: 0.7201\n",
      "Training loss (for one batch) at step 110: 237.7721, Accuracy: 0.7240\n",
      "---- Training ----\n",
      "Training loss: 69.0547\n",
      "Training acc over epoch: 0.7230\n",
      "---- Validation ----\n",
      "Validation loss: 39.0421\n",
      "Validation acc: 0.6711\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 231.3645, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 241.1403, Accuracy: 0.5760\n",
      "Training loss (for one batch) at step 20: 225.8519, Accuracy: 0.6421\n",
      "Training loss (for one batch) at step 30: 213.7898, Accuracy: 0.6928\n",
      "Training loss (for one batch) at step 40: 229.1316, Accuracy: 0.7233\n",
      "Training loss (for one batch) at step 50: 215.1418, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 60: 207.4792, Accuracy: 0.7518\n",
      "Training loss (for one batch) at step 70: 228.0951, Accuracy: 0.7389\n",
      "Training loss (for one batch) at step 80: 239.5570, Accuracy: 0.7184\n",
      "Training loss (for one batch) at step 90: 210.1299, Accuracy: 0.7164\n",
      "Training loss (for one batch) at step 100: 213.3306, Accuracy: 0.7225\n",
      "Training loss (for one batch) at step 110: 220.8002, Accuracy: 0.7261\n",
      "---- Training ----\n",
      "Training loss: 72.4681\n",
      "Training acc over epoch: 0.7240\n",
      "---- Validation ----\n",
      "Validation loss: 41.4215\n",
      "Validation acc: 0.6754\n",
      "Time taken: 18.21s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 248.1956, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 240.8067, Accuracy: 0.5753\n",
      "Training loss (for one batch) at step 20: 223.2286, Accuracy: 0.6347\n",
      "Training loss (for one batch) at step 30: 215.4100, Accuracy: 0.6888\n",
      "Training loss (for one batch) at step 40: 212.0416, Accuracy: 0.7260\n",
      "Training loss (for one batch) at step 50: 212.8112, Accuracy: 0.7448\n",
      "Training loss (for one batch) at step 60: 238.1574, Accuracy: 0.7519\n",
      "Training loss (for one batch) at step 70: 221.4715, Accuracy: 0.7359\n",
      "Training loss (for one batch) at step 80: 247.2505, Accuracy: 0.7198\n",
      "Training loss (for one batch) at step 90: 210.4779, Accuracy: 0.7145\n",
      "Training loss (for one batch) at step 100: 227.1608, Accuracy: 0.7214\n",
      "Training loss (for one batch) at step 110: 234.1918, Accuracy: 0.7253\n",
      "---- Training ----\n",
      "Training loss: 84.2290\n",
      "Training acc over epoch: 0.7234\n",
      "---- Validation ----\n",
      "Validation loss: 43.0078\n",
      "Validation acc: 0.6636\n",
      "Time taken: 17.97s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.6535\n",
      "Validation AUC: 0.6545\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 630.4998, Accuracy: 0.4375\n",
      "Training loss (for one batch) at step 10: 564.4643, Accuracy: 0.5043\n",
      "Training loss (for one batch) at step 20: 523.3744, Accuracy: 0.5145\n",
      "Training loss (for one batch) at step 30: 495.0690, Accuracy: 0.5149\n",
      "Training loss (for one batch) at step 40: 489.2289, Accuracy: 0.5109\n",
      "Training loss (for one batch) at step 50: 501.5422, Accuracy: 0.5130\n",
      "Training loss (for one batch) at step 60: 490.3276, Accuracy: 0.5147\n",
      "Training loss (for one batch) at step 70: 471.0157, Accuracy: 0.5133\n",
      "Training loss (for one batch) at step 80: 465.9543, Accuracy: 0.5143\n",
      "Training loss (for one batch) at step 90: 458.3671, Accuracy: 0.5153\n",
      "Training loss (for one batch) at step 100: 455.4453, Accuracy: 0.5128\n",
      "Training loss (for one batch) at step 110: 460.1530, Accuracy: 0.5165\n",
      "---- Training ----\n",
      "Training loss: 145.7478\n",
      "Training acc over epoch: 0.5177\n",
      "---- Validation ----\n",
      "Validation loss: 34.1110\n",
      "Validation acc: 0.5132\n",
      "Time taken: 19.57s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 461.1322, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 454.3400, Accuracy: 0.5376\n",
      "Training loss (for one batch) at step 20: 455.6190, Accuracy: 0.5283\n",
      "Training loss (for one batch) at step 30: 449.2257, Accuracy: 0.5333\n",
      "Training loss (for one batch) at step 40: 450.5366, Accuracy: 0.5341\n",
      "Training loss (for one batch) at step 50: 455.3456, Accuracy: 0.5343\n",
      "Training loss (for one batch) at step 60: 443.4549, Accuracy: 0.5328\n",
      "Training loss (for one batch) at step 70: 448.6848, Accuracy: 0.5339\n",
      "Training loss (for one batch) at step 80: 446.6769, Accuracy: 0.5324\n",
      "Training loss (for one batch) at step 90: 449.2770, Accuracy: 0.5330\n",
      "Training loss (for one batch) at step 100: 451.4648, Accuracy: 0.5306\n",
      "Training loss (for one batch) at step 110: 445.8960, Accuracy: 0.5308\n",
      "---- Training ----\n",
      "Training loss: 142.5723\n",
      "Training acc over epoch: 0.5302\n",
      "---- Validation ----\n",
      "Validation loss: 34.1073\n",
      "Validation acc: 0.5137\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 451.5584, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 441.6916, Accuracy: 0.5717\n",
      "Training loss (for one batch) at step 20: 444.7712, Accuracy: 0.5666\n",
      "Training loss (for one batch) at step 30: 448.5600, Accuracy: 0.5592\n",
      "Training loss (for one batch) at step 40: 446.7529, Accuracy: 0.5549\n",
      "Training loss (for one batch) at step 50: 444.4415, Accuracy: 0.5513\n",
      "Training loss (for one batch) at step 60: 445.3210, Accuracy: 0.5543\n",
      "Training loss (for one batch) at step 70: 445.5340, Accuracy: 0.5571\n",
      "Training loss (for one batch) at step 80: 444.7906, Accuracy: 0.5584\n",
      "Training loss (for one batch) at step 90: 445.6267, Accuracy: 0.5597\n",
      "Training loss (for one batch) at step 100: 442.3926, Accuracy: 0.5601\n",
      "Training loss (for one batch) at step 110: 443.2942, Accuracy: 0.5605\n",
      "---- Training ----\n",
      "Training loss: 138.7153\n",
      "Training acc over epoch: 0.5593\n",
      "---- Validation ----\n",
      "Validation loss: 34.9497\n",
      "Validation acc: 0.5742\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 444.2526, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 443.9428, Accuracy: 0.5710\n",
      "Training loss (for one batch) at step 20: 442.1813, Accuracy: 0.5897\n",
      "Training loss (for one batch) at step 30: 444.5651, Accuracy: 0.5907\n",
      "Training loss (for one batch) at step 40: 444.2772, Accuracy: 0.5871\n",
      "Training loss (for one batch) at step 50: 443.1678, Accuracy: 0.5858\n",
      "Training loss (for one batch) at step 60: 441.9981, Accuracy: 0.5831\n",
      "Training loss (for one batch) at step 70: 445.2392, Accuracy: 0.5874\n",
      "Training loss (for one batch) at step 80: 444.6962, Accuracy: 0.5899\n",
      "Training loss (for one batch) at step 90: 443.2034, Accuracy: 0.5907\n",
      "Training loss (for one batch) at step 100: 445.3429, Accuracy: 0.5912\n",
      "Training loss (for one batch) at step 110: 442.8979, Accuracy: 0.5887\n",
      "---- Training ----\n",
      "Training loss: 139.4368\n",
      "Training acc over epoch: 0.5881\n",
      "---- Validation ----\n",
      "Validation loss: 34.6620\n",
      "Validation acc: 0.6163\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 442.5544, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 439.8414, Accuracy: 0.6122\n",
      "Training loss (for one batch) at step 20: 441.7905, Accuracy: 0.6101\n",
      "Training loss (for one batch) at step 30: 442.2792, Accuracy: 0.6084\n",
      "Training loss (for one batch) at step 40: 438.6817, Accuracy: 0.6046\n",
      "Training loss (for one batch) at step 50: 438.6329, Accuracy: 0.6066\n",
      "Training loss (for one batch) at step 60: 439.7604, Accuracy: 0.6099\n",
      "Training loss (for one batch) at step 70: 445.1360, Accuracy: 0.6121\n",
      "Training loss (for one batch) at step 80: 441.8199, Accuracy: 0.6112\n",
      "Training loss (for one batch) at step 90: 441.7162, Accuracy: 0.6104\n",
      "Training loss (for one batch) at step 100: 442.3801, Accuracy: 0.6101\n",
      "Training loss (for one batch) at step 110: 444.8739, Accuracy: 0.6128\n",
      "---- Training ----\n",
      "Training loss: 136.4648\n",
      "Training acc over epoch: 0.6124\n",
      "---- Validation ----\n",
      "Validation loss: 34.5374\n",
      "Validation acc: 0.6354\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 443.8144, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 442.5863, Accuracy: 0.6378\n",
      "Training loss (for one batch) at step 20: 437.0597, Accuracy: 0.6410\n",
      "Training loss (for one batch) at step 30: 438.1098, Accuracy: 0.6414\n",
      "Training loss (for one batch) at step 40: 436.5737, Accuracy: 0.6429\n",
      "Training loss (for one batch) at step 50: 440.7360, Accuracy: 0.6389\n",
      "Training loss (for one batch) at step 60: 439.8378, Accuracy: 0.6423\n",
      "Training loss (for one batch) at step 70: 442.8634, Accuracy: 0.6439\n",
      "Training loss (for one batch) at step 80: 442.3127, Accuracy: 0.6414\n",
      "Training loss (for one batch) at step 90: 439.8636, Accuracy: 0.6404\n",
      "Training loss (for one batch) at step 100: 439.6661, Accuracy: 0.6416\n",
      "Training loss (for one batch) at step 110: 442.0187, Accuracy: 0.6415\n",
      "---- Training ----\n",
      "Training loss: 138.8198\n",
      "Training acc over epoch: 0.6395\n",
      "---- Validation ----\n",
      "Validation loss: 34.6726\n",
      "Validation acc: 0.6083\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 442.7130, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 447.3644, Accuracy: 0.6307\n",
      "Training loss (for one batch) at step 20: 441.5259, Accuracy: 0.6235\n",
      "Training loss (for one batch) at step 30: 436.8690, Accuracy: 0.6318\n",
      "Training loss (for one batch) at step 40: 435.1299, Accuracy: 0.6387\n",
      "Training loss (for one batch) at step 50: 433.4930, Accuracy: 0.6425\n",
      "Training loss (for one batch) at step 60: 434.6473, Accuracy: 0.6456\n",
      "Training loss (for one batch) at step 70: 439.1293, Accuracy: 0.6510\n",
      "Training loss (for one batch) at step 80: 439.2748, Accuracy: 0.6503\n",
      "Training loss (for one batch) at step 90: 443.2322, Accuracy: 0.6446\n",
      "Training loss (for one batch) at step 100: 435.1738, Accuracy: 0.6446\n",
      "Training loss (for one batch) at step 110: 438.5527, Accuracy: 0.6453\n",
      "---- Training ----\n",
      "Training loss: 136.2975\n",
      "Training acc over epoch: 0.6456\n",
      "---- Validation ----\n",
      "Validation loss: 35.8472\n",
      "Validation acc: 0.6529\n",
      "Time taken: 18.05s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 445.1726, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 443.0143, Accuracy: 0.6733\n",
      "Training loss (for one batch) at step 20: 440.9739, Accuracy: 0.6585\n",
      "Training loss (for one batch) at step 30: 433.2089, Accuracy: 0.6633\n",
      "Training loss (for one batch) at step 40: 428.9461, Accuracy: 0.6620\n",
      "Training loss (for one batch) at step 50: 430.3946, Accuracy: 0.6673\n",
      "Training loss (for one batch) at step 60: 432.6626, Accuracy: 0.6698\n",
      "Training loss (for one batch) at step 70: 448.5459, Accuracy: 0.6674\n",
      "Training loss (for one batch) at step 80: 438.6194, Accuracy: 0.6656\n",
      "Training loss (for one batch) at step 90: 436.9087, Accuracy: 0.6579\n",
      "Training loss (for one batch) at step 100: 437.4493, Accuracy: 0.6533\n",
      "Training loss (for one batch) at step 110: 437.0743, Accuracy: 0.6521\n",
      "---- Training ----\n",
      "Training loss: 135.2713\n",
      "Training acc over epoch: 0.6530\n",
      "---- Validation ----\n",
      "Validation loss: 34.2721\n",
      "Validation acc: 0.6623\n",
      "Time taken: 18.05s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 448.8000, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 442.3048, Accuracy: 0.6740\n",
      "Training loss (for one batch) at step 20: 440.3251, Accuracy: 0.6581\n",
      "Training loss (for one batch) at step 30: 434.0820, Accuracy: 0.6673\n",
      "Training loss (for one batch) at step 40: 423.9687, Accuracy: 0.6723\n",
      "Training loss (for one batch) at step 50: 426.2980, Accuracy: 0.6820\n",
      "Training loss (for one batch) at step 60: 431.8596, Accuracy: 0.6856\n",
      "Training loss (for one batch) at step 70: 441.6639, Accuracy: 0.6898\n",
      "Training loss (for one batch) at step 80: 444.6172, Accuracy: 0.6867\n",
      "Training loss (for one batch) at step 90: 435.4643, Accuracy: 0.6810\n",
      "Training loss (for one batch) at step 100: 438.9484, Accuracy: 0.6779\n",
      "Training loss (for one batch) at step 110: 434.1982, Accuracy: 0.6772\n",
      "---- Training ----\n",
      "Training loss: 136.6982\n",
      "Training acc over epoch: 0.6775\n",
      "---- Validation ----\n",
      "Validation loss: 34.8161\n",
      "Validation acc: 0.6883\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 445.8256, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 436.9836, Accuracy: 0.6776\n",
      "Training loss (for one batch) at step 20: 437.9922, Accuracy: 0.6749\n",
      "Training loss (for one batch) at step 30: 430.8422, Accuracy: 0.6890\n",
      "Training loss (for one batch) at step 40: 418.5230, Accuracy: 0.6940\n",
      "Training loss (for one batch) at step 50: 422.0038, Accuracy: 0.7025\n",
      "Training loss (for one batch) at step 60: 436.1200, Accuracy: 0.7111\n",
      "Training loss (for one batch) at step 70: 441.0880, Accuracy: 0.7097\n",
      "Training loss (for one batch) at step 80: 436.6514, Accuracy: 0.7019\n",
      "Training loss (for one batch) at step 90: 435.6992, Accuracy: 0.6951\n",
      "Training loss (for one batch) at step 100: 433.4904, Accuracy: 0.6921\n",
      "Training loss (for one batch) at step 110: 428.0224, Accuracy: 0.6909\n",
      "---- Training ----\n",
      "Training loss: 133.5742\n",
      "Training acc over epoch: 0.6908\n",
      "---- Validation ----\n",
      "Validation loss: 34.0221\n",
      "Validation acc: 0.6857\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 445.8574, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 438.1979, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 20: 431.7788, Accuracy: 0.6864\n",
      "Training loss (for one batch) at step 30: 419.8816, Accuracy: 0.6958\n",
      "Training loss (for one batch) at step 40: 424.6116, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 50: 412.3489, Accuracy: 0.7091\n",
      "Training loss (for one batch) at step 60: 437.8099, Accuracy: 0.7205\n",
      "Training loss (for one batch) at step 70: 435.2081, Accuracy: 0.7214\n",
      "Training loss (for one batch) at step 80: 431.8131, Accuracy: 0.7149\n",
      "Training loss (for one batch) at step 90: 437.0564, Accuracy: 0.7064\n",
      "Training loss (for one batch) at step 100: 427.3318, Accuracy: 0.7061\n",
      "Training loss (for one batch) at step 110: 435.1713, Accuracy: 0.7044\n",
      "---- Training ----\n",
      "Training loss: 134.5319\n",
      "Training acc over epoch: 0.7048\n",
      "---- Validation ----\n",
      "Validation loss: 33.8280\n",
      "Validation acc: 0.6773\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 447.4057, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 439.3651, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 20: 423.8306, Accuracy: 0.6838\n",
      "Training loss (for one batch) at step 30: 421.5585, Accuracy: 0.6958\n",
      "Training loss (for one batch) at step 40: 416.5939, Accuracy: 0.7067\n",
      "Training loss (for one batch) at step 50: 405.7238, Accuracy: 0.7198\n",
      "Training loss (for one batch) at step 60: 411.8466, Accuracy: 0.7308\n",
      "Training loss (for one batch) at step 70: 426.8261, Accuracy: 0.7295\n",
      "Training loss (for one batch) at step 80: 436.6991, Accuracy: 0.7228\n",
      "Training loss (for one batch) at step 90: 434.4787, Accuracy: 0.7164\n",
      "Training loss (for one batch) at step 100: 432.3293, Accuracy: 0.7141\n",
      "Training loss (for one batch) at step 110: 432.9340, Accuracy: 0.7146\n",
      "---- Training ----\n",
      "Training loss: 135.3230\n",
      "Training acc over epoch: 0.7139\n",
      "---- Validation ----\n",
      "Validation loss: 32.9508\n",
      "Validation acc: 0.7002\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 439.5758, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 437.9786, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 20: 421.8856, Accuracy: 0.6808\n",
      "Training loss (for one batch) at step 30: 413.8262, Accuracy: 0.6943\n",
      "Training loss (for one batch) at step 40: 407.2457, Accuracy: 0.7134\n",
      "Training loss (for one batch) at step 50: 405.0039, Accuracy: 0.7281\n",
      "Training loss (for one batch) at step 60: 403.0897, Accuracy: 0.7387\n",
      "Training loss (for one batch) at step 70: 430.8972, Accuracy: 0.7365\n",
      "Training loss (for one batch) at step 80: 434.9446, Accuracy: 0.7283\n",
      "Training loss (for one batch) at step 90: 425.2777, Accuracy: 0.7236\n",
      "Training loss (for one batch) at step 100: 416.3270, Accuracy: 0.7269\n",
      "Training loss (for one batch) at step 110: 424.2734, Accuracy: 0.7247\n",
      "---- Training ----\n",
      "Training loss: 131.0286\n",
      "Training acc over epoch: 0.7243\n",
      "---- Validation ----\n",
      "Validation loss: 33.6655\n",
      "Validation acc: 0.6878\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 434.5534, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 434.8701, Accuracy: 0.6818\n",
      "Training loss (for one batch) at step 20: 411.0366, Accuracy: 0.6778\n",
      "Training loss (for one batch) at step 30: 404.7301, Accuracy: 0.7011\n",
      "Training loss (for one batch) at step 40: 389.8036, Accuracy: 0.7197\n",
      "Training loss (for one batch) at step 50: 380.6217, Accuracy: 0.7373\n",
      "Training loss (for one batch) at step 60: 395.2296, Accuracy: 0.7462\n",
      "Training loss (for one batch) at step 70: 429.8923, Accuracy: 0.7458\n",
      "Training loss (for one batch) at step 80: 411.0306, Accuracy: 0.7380\n",
      "Training loss (for one batch) at step 90: 421.6769, Accuracy: 0.7358\n",
      "Training loss (for one batch) at step 100: 411.4873, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 110: 418.3221, Accuracy: 0.7384\n",
      "---- Training ----\n",
      "Training loss: 136.7473\n",
      "Training acc over epoch: 0.7372\n",
      "---- Validation ----\n",
      "Validation loss: 36.4191\n",
      "Validation acc: 0.6964\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 434.1523, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 430.7146, Accuracy: 0.6889\n",
      "Training loss (for one batch) at step 20: 412.4352, Accuracy: 0.7009\n",
      "Training loss (for one batch) at step 30: 387.9505, Accuracy: 0.7253\n",
      "Training loss (for one batch) at step 40: 387.5694, Accuracy: 0.7428\n",
      "Training loss (for one batch) at step 50: 379.6956, Accuracy: 0.7592\n",
      "Training loss (for one batch) at step 60: 391.2275, Accuracy: 0.7677\n",
      "Training loss (for one batch) at step 70: 414.7449, Accuracy: 0.7642\n",
      "Training loss (for one batch) at step 80: 418.9654, Accuracy: 0.7555\n",
      "Training loss (for one batch) at step 90: 399.8544, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 100: 390.2610, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 110: 420.9343, Accuracy: 0.7540\n",
      "---- Training ----\n",
      "Training loss: 124.7363\n",
      "Training acc over epoch: 0.7534\n",
      "---- Validation ----\n",
      "Validation loss: 34.5670\n",
      "Validation acc: 0.6854\n",
      "Time taken: 18.14s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 438.6723, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 431.6561, Accuracy: 0.6932\n",
      "Training loss (for one batch) at step 20: 403.5464, Accuracy: 0.6979\n",
      "Training loss (for one batch) at step 30: 386.1628, Accuracy: 0.7223\n",
      "Training loss (for one batch) at step 40: 388.4587, Accuracy: 0.7420\n",
      "Training loss (for one batch) at step 50: 374.6096, Accuracy: 0.7593\n",
      "Training loss (for one batch) at step 60: 386.2604, Accuracy: 0.7727\n",
      "Training loss (for one batch) at step 70: 396.6110, Accuracy: 0.7677\n",
      "Training loss (for one batch) at step 80: 400.6024, Accuracy: 0.7575\n",
      "Training loss (for one batch) at step 90: 384.3640, Accuracy: 0.7523\n",
      "Training loss (for one batch) at step 100: 381.8436, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 110: 392.1219, Accuracy: 0.7552\n",
      "---- Training ----\n",
      "Training loss: 124.3503\n",
      "Training acc over epoch: 0.7542\n",
      "---- Validation ----\n",
      "Validation loss: 37.5321\n",
      "Validation acc: 0.7071\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 422.5936, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 412.8956, Accuracy: 0.6648\n",
      "Training loss (for one batch) at step 20: 384.1652, Accuracy: 0.7094\n",
      "Training loss (for one batch) at step 30: 374.9335, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 40: 366.2732, Accuracy: 0.7571\n",
      "Training loss (for one batch) at step 50: 347.2221, Accuracy: 0.7748\n",
      "Training loss (for one batch) at step 60: 379.1438, Accuracy: 0.7832\n",
      "Training loss (for one batch) at step 70: 399.4987, Accuracy: 0.7748\n",
      "Training loss (for one batch) at step 80: 407.3347, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 90: 380.9491, Accuracy: 0.7583\n",
      "Training loss (for one batch) at step 100: 377.4408, Accuracy: 0.7607\n",
      "Training loss (for one batch) at step 110: 398.6965, Accuracy: 0.7608\n",
      "---- Training ----\n",
      "Training loss: 125.7727\n",
      "Training acc over epoch: 0.7603\n",
      "---- Validation ----\n",
      "Validation loss: 42.2686\n",
      "Validation acc: 0.7077\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 413.3560, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 405.2128, Accuracy: 0.6918\n",
      "Training loss (for one batch) at step 20: 379.7203, Accuracy: 0.7102\n",
      "Training loss (for one batch) at step 30: 368.2667, Accuracy: 0.7359\n",
      "Training loss (for one batch) at step 40: 354.1767, Accuracy: 0.7603\n",
      "Training loss (for one batch) at step 50: 342.4417, Accuracy: 0.7750\n",
      "Training loss (for one batch) at step 60: 372.5846, Accuracy: 0.7862\n",
      "Training loss (for one batch) at step 70: 394.7918, Accuracy: 0.7776\n",
      "Training loss (for one batch) at step 80: 383.1670, Accuracy: 0.7677\n",
      "Training loss (for one batch) at step 90: 378.8656, Accuracy: 0.7640\n",
      "Training loss (for one batch) at step 100: 362.8836, Accuracy: 0.7662\n",
      "Training loss (for one batch) at step 110: 377.8279, Accuracy: 0.7681\n",
      "---- Training ----\n",
      "Training loss: 118.2065\n",
      "Training acc over epoch: 0.7672\n",
      "---- Validation ----\n",
      "Validation loss: 42.9222\n",
      "Validation acc: 0.6961\n",
      "Time taken: 18.05s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 429.8277, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 416.9563, Accuracy: 0.6534\n",
      "Training loss (for one batch) at step 20: 375.0599, Accuracy: 0.6927\n",
      "Training loss (for one batch) at step 30: 346.7744, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 40: 359.2373, Accuracy: 0.7563\n",
      "Training loss (for one batch) at step 50: 334.2683, Accuracy: 0.7728\n",
      "Training loss (for one batch) at step 60: 367.2919, Accuracy: 0.7836\n",
      "Training loss (for one batch) at step 70: 390.2657, Accuracy: 0.7768\n",
      "Training loss (for one batch) at step 80: 388.0223, Accuracy: 0.7645\n",
      "Training loss (for one batch) at step 90: 363.2290, Accuracy: 0.7613\n",
      "Training loss (for one batch) at step 100: 362.7792, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 110: 378.7365, Accuracy: 0.7667\n",
      "---- Training ----\n",
      "Training loss: 125.9094\n",
      "Training acc over epoch: 0.7663\n",
      "---- Validation ----\n",
      "Validation loss: 49.2826\n",
      "Validation acc: 0.6916\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 409.6105, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 408.1414, Accuracy: 0.6499\n",
      "Training loss (for one batch) at step 20: 349.1862, Accuracy: 0.6942\n",
      "Training loss (for one batch) at step 30: 329.7625, Accuracy: 0.7359\n",
      "Training loss (for one batch) at step 40: 330.9153, Accuracy: 0.7599\n",
      "Training loss (for one batch) at step 50: 327.6288, Accuracy: 0.7805\n",
      "Training loss (for one batch) at step 60: 349.0493, Accuracy: 0.7914\n",
      "Training loss (for one batch) at step 70: 406.4444, Accuracy: 0.7814\n",
      "Training loss (for one batch) at step 80: 383.0273, Accuracy: 0.7683\n",
      "Training loss (for one batch) at step 90: 342.2055, Accuracy: 0.7654\n",
      "Training loss (for one batch) at step 100: 339.6897, Accuracy: 0.7702\n",
      "Training loss (for one batch) at step 110: 375.8654, Accuracy: 0.7706\n",
      "---- Training ----\n",
      "Training loss: 125.9068\n",
      "Training acc over epoch: 0.7675\n",
      "---- Validation ----\n",
      "Validation loss: 36.6947\n",
      "Validation acc: 0.6768\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 412.7205, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 400.2346, Accuracy: 0.6477\n",
      "Training loss (for one batch) at step 20: 353.8302, Accuracy: 0.6983\n",
      "Training loss (for one batch) at step 30: 330.5117, Accuracy: 0.7387\n",
      "Training loss (for one batch) at step 40: 309.1426, Accuracy: 0.7706\n",
      "Training loss (for one batch) at step 50: 309.7888, Accuracy: 0.7888\n",
      "Training loss (for one batch) at step 60: 345.0110, Accuracy: 0.7985\n",
      "Training loss (for one batch) at step 70: 370.2088, Accuracy: 0.7880\n",
      "Training loss (for one batch) at step 80: 348.2213, Accuracy: 0.7711\n",
      "Training loss (for one batch) at step 90: 345.2233, Accuracy: 0.7693\n",
      "Training loss (for one batch) at step 100: 339.5601, Accuracy: 0.7733\n",
      "Training loss (for one batch) at step 110: 360.0471, Accuracy: 0.7739\n",
      "---- Training ----\n",
      "Training loss: 112.5037\n",
      "Training acc over epoch: 0.7728\n",
      "---- Validation ----\n",
      "Validation loss: 43.6235\n",
      "Validation acc: 0.6964\n",
      "Time taken: 18.11s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 373.0954, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 383.6244, Accuracy: 0.6477\n",
      "Training loss (for one batch) at step 20: 350.8081, Accuracy: 0.6994\n",
      "Training loss (for one batch) at step 30: 329.6828, Accuracy: 0.7434\n",
      "Training loss (for one batch) at step 40: 304.2505, Accuracy: 0.7698\n",
      "Training loss (for one batch) at step 50: 315.9874, Accuracy: 0.7909\n",
      "Training loss (for one batch) at step 60: 336.4212, Accuracy: 0.8016\n",
      "Training loss (for one batch) at step 70: 352.0267, Accuracy: 0.7858\n",
      "Training loss (for one batch) at step 80: 349.0226, Accuracy: 0.7693\n",
      "Training loss (for one batch) at step 90: 334.6573, Accuracy: 0.7645\n",
      "Training loss (for one batch) at step 100: 343.5969, Accuracy: 0.7702\n",
      "Training loss (for one batch) at step 110: 333.7448, Accuracy: 0.7713\n",
      "---- Training ----\n",
      "Training loss: 123.9031\n",
      "Training acc over epoch: 0.7693\n",
      "---- Validation ----\n",
      "Validation loss: 37.2555\n",
      "Validation acc: 0.7088\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 383.9704, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 374.3281, Accuracy: 0.6207\n",
      "Training loss (for one batch) at step 20: 314.2334, Accuracy: 0.6801\n",
      "Training loss (for one batch) at step 30: 324.8073, Accuracy: 0.7273\n",
      "Training loss (for one batch) at step 40: 291.7697, Accuracy: 0.7601\n",
      "Training loss (for one batch) at step 50: 302.4088, Accuracy: 0.7817\n",
      "Training loss (for one batch) at step 60: 337.7407, Accuracy: 0.7932\n",
      "Training loss (for one batch) at step 70: 355.4493, Accuracy: 0.7798\n",
      "Training loss (for one batch) at step 80: 368.2194, Accuracy: 0.7644\n",
      "Training loss (for one batch) at step 90: 319.1775, Accuracy: 0.7615\n",
      "Training loss (for one batch) at step 100: 295.7095, Accuracy: 0.7652\n",
      "Training loss (for one batch) at step 110: 323.8268, Accuracy: 0.7673\n",
      "---- Training ----\n",
      "Training loss: 123.8732\n",
      "Training acc over epoch: 0.7655\n",
      "---- Validation ----\n",
      "Validation loss: 39.7103\n",
      "Validation acc: 0.7010\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 373.9296, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 361.8053, Accuracy: 0.6321\n",
      "Training loss (for one batch) at step 20: 336.8345, Accuracy: 0.6801\n",
      "Training loss (for one batch) at step 30: 310.2216, Accuracy: 0.7283\n",
      "Training loss (for one batch) at step 40: 310.8767, Accuracy: 0.7605\n",
      "Training loss (for one batch) at step 50: 291.6000, Accuracy: 0.7814\n",
      "Training loss (for one batch) at step 60: 326.8195, Accuracy: 0.7953\n",
      "Training loss (for one batch) at step 70: 337.5576, Accuracy: 0.7819\n",
      "Training loss (for one batch) at step 80: 354.5165, Accuracy: 0.7671\n",
      "Training loss (for one batch) at step 90: 331.8181, Accuracy: 0.7657\n",
      "Training loss (for one batch) at step 100: 291.8943, Accuracy: 0.7710\n",
      "Training loss (for one batch) at step 110: 326.1320, Accuracy: 0.7717\n",
      "---- Training ----\n",
      "Training loss: 93.7465\n",
      "Training acc over epoch: 0.7690\n",
      "---- Validation ----\n",
      "Validation loss: 55.1279\n",
      "Validation acc: 0.6948\n",
      "Time taken: 18.06s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 378.3833, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 351.0808, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 20: 313.8651, Accuracy: 0.6782\n",
      "Training loss (for one batch) at step 30: 299.6757, Accuracy: 0.7324\n",
      "Training loss (for one batch) at step 40: 291.6973, Accuracy: 0.7645\n",
      "Training loss (for one batch) at step 50: 283.6465, Accuracy: 0.7855\n",
      "Training loss (for one batch) at step 60: 320.6071, Accuracy: 0.7973\n",
      "Training loss (for one batch) at step 70: 331.2857, Accuracy: 0.7842\n",
      "Training loss (for one batch) at step 80: 346.2415, Accuracy: 0.7680\n",
      "Training loss (for one batch) at step 90: 311.3788, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 100: 309.6717, Accuracy: 0.7689\n",
      "Training loss (for one batch) at step 110: 319.3253, Accuracy: 0.7706\n",
      "---- Training ----\n",
      "Training loss: 113.8696\n",
      "Training acc over epoch: 0.7694\n",
      "---- Validation ----\n",
      "Validation loss: 39.5826\n",
      "Validation acc: 0.7106\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 359.0734, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 362.3367, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 20: 290.4379, Accuracy: 0.6812\n",
      "Training loss (for one batch) at step 30: 295.5839, Accuracy: 0.7291\n",
      "Training loss (for one batch) at step 40: 289.2639, Accuracy: 0.7622\n",
      "Training loss (for one batch) at step 50: 275.3909, Accuracy: 0.7834\n",
      "Training loss (for one batch) at step 60: 310.3622, Accuracy: 0.7952\n",
      "Training loss (for one batch) at step 70: 339.7950, Accuracy: 0.7822\n",
      "Training loss (for one batch) at step 80: 354.1402, Accuracy: 0.7680\n",
      "Training loss (for one batch) at step 90: 331.2243, Accuracy: 0.7643\n",
      "Training loss (for one batch) at step 100: 324.7283, Accuracy: 0.7714\n",
      "Training loss (for one batch) at step 110: 315.8335, Accuracy: 0.7746\n",
      "---- Training ----\n",
      "Training loss: 100.8507\n",
      "Training acc over epoch: 0.7734\n",
      "---- Validation ----\n",
      "Validation loss: 68.4919\n",
      "Validation acc: 0.7010\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 345.8151, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 331.7178, Accuracy: 0.6378\n",
      "Training loss (for one batch) at step 20: 294.6330, Accuracy: 0.6879\n",
      "Training loss (for one batch) at step 30: 286.3185, Accuracy: 0.7359\n",
      "Training loss (for one batch) at step 40: 287.1172, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 50: 287.2876, Accuracy: 0.7898\n",
      "Training loss (for one batch) at step 60: 298.2874, Accuracy: 0.8011\n",
      "Training loss (for one batch) at step 70: 331.5976, Accuracy: 0.7833\n",
      "Training loss (for one batch) at step 80: 343.8599, Accuracy: 0.7673\n",
      "Training loss (for one batch) at step 90: 291.2805, Accuracy: 0.7634\n",
      "Training loss (for one batch) at step 100: 286.8674, Accuracy: 0.7700\n",
      "Training loss (for one batch) at step 110: 323.6677, Accuracy: 0.7718\n",
      "---- Training ----\n",
      "Training loss: 93.7547\n",
      "Training acc over epoch: 0.7704\n",
      "---- Validation ----\n",
      "Validation loss: 38.0848\n",
      "Validation acc: 0.7085\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 364.5968, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 335.5841, Accuracy: 0.6286\n",
      "Training loss (for one batch) at step 20: 298.1695, Accuracy: 0.6823\n",
      "Training loss (for one batch) at step 30: 281.7505, Accuracy: 0.7359\n",
      "Training loss (for one batch) at step 40: 287.1378, Accuracy: 0.7628\n",
      "Training loss (for one batch) at step 50: 298.1646, Accuracy: 0.7834\n",
      "Training loss (for one batch) at step 60: 288.8481, Accuracy: 0.7961\n",
      "Training loss (for one batch) at step 70: 341.6813, Accuracy: 0.7825\n",
      "Training loss (for one batch) at step 80: 324.3619, Accuracy: 0.7644\n",
      "Training loss (for one batch) at step 90: 288.9108, Accuracy: 0.7629\n",
      "Training loss (for one batch) at step 100: 323.0327, Accuracy: 0.7706\n",
      "Training loss (for one batch) at step 110: 309.1061, Accuracy: 0.7709\n",
      "---- Training ----\n",
      "Training loss: 101.8612\n",
      "Training acc over epoch: 0.7691\n",
      "---- Validation ----\n",
      "Validation loss: 41.8143\n",
      "Validation acc: 0.6943\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 341.8055, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 344.5363, Accuracy: 0.6158\n",
      "Training loss (for one batch) at step 20: 302.5469, Accuracy: 0.6700\n",
      "Training loss (for one batch) at step 30: 282.9228, Accuracy: 0.7278\n",
      "Training loss (for one batch) at step 40: 269.5897, Accuracy: 0.7611\n",
      "Training loss (for one batch) at step 50: 278.2330, Accuracy: 0.7825\n",
      "Training loss (for one batch) at step 60: 283.2661, Accuracy: 0.7939\n",
      "Training loss (for one batch) at step 70: 350.5413, Accuracy: 0.7815\n",
      "Training loss (for one batch) at step 80: 337.0430, Accuracy: 0.7647\n",
      "Training loss (for one batch) at step 90: 293.7372, Accuracy: 0.7614\n",
      "Training loss (for one batch) at step 100: 276.0709, Accuracy: 0.7674\n",
      "Training loss (for one batch) at step 110: 305.0062, Accuracy: 0.7701\n",
      "---- Training ----\n",
      "Training loss: 93.7352\n",
      "Training acc over epoch: 0.7680\n",
      "---- Validation ----\n",
      "Validation loss: 36.6846\n",
      "Validation acc: 0.7055\n",
      "Time taken: 18.15s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 338.7526, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 325.0517, Accuracy: 0.6222\n",
      "Training loss (for one batch) at step 20: 302.0801, Accuracy: 0.6700\n",
      "Training loss (for one batch) at step 30: 282.8174, Accuracy: 0.7276\n",
      "Training loss (for one batch) at step 40: 278.0357, Accuracy: 0.7626\n",
      "Training loss (for one batch) at step 50: 259.2485, Accuracy: 0.7857\n",
      "Training loss (for one batch) at step 60: 298.3919, Accuracy: 0.7978\n",
      "Training loss (for one batch) at step 70: 337.7634, Accuracy: 0.7818\n",
      "Training loss (for one batch) at step 80: 344.6241, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 90: 282.2983, Accuracy: 0.7627\n",
      "Training loss (for one batch) at step 100: 271.7590, Accuracy: 0.7693\n",
      "Training loss (for one batch) at step 110: 298.8201, Accuracy: 0.7707\n",
      "---- Training ----\n",
      "Training loss: 115.8538\n",
      "Training acc over epoch: 0.7687\n",
      "---- Validation ----\n",
      "Validation loss: 35.5216\n",
      "Validation acc: 0.7061\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 318.0937, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 350.7191, Accuracy: 0.6136\n",
      "Training loss (for one batch) at step 20: 285.8428, Accuracy: 0.6749\n",
      "Training loss (for one batch) at step 30: 277.4097, Accuracy: 0.7281\n",
      "Training loss (for one batch) at step 40: 271.6349, Accuracy: 0.7590\n",
      "Training loss (for one batch) at step 50: 284.4301, Accuracy: 0.7837\n",
      "Training loss (for one batch) at step 60: 268.0300, Accuracy: 0.7973\n",
      "Training loss (for one batch) at step 70: 320.8387, Accuracy: 0.7815\n",
      "Training loss (for one batch) at step 80: 307.9252, Accuracy: 0.7638\n",
      "Training loss (for one batch) at step 90: 266.9068, Accuracy: 0.7599\n",
      "Training loss (for one batch) at step 100: 297.9141, Accuracy: 0.7658\n",
      "Training loss (for one batch) at step 110: 275.3922, Accuracy: 0.7682\n",
      "---- Training ----\n",
      "Training loss: 101.3910\n",
      "Training acc over epoch: 0.7674\n",
      "---- Validation ----\n",
      "Validation loss: 34.5357\n",
      "Validation acc: 0.6988\n",
      "Time taken: 18.06s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 359.4044, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 324.6801, Accuracy: 0.6122\n",
      "Training loss (for one batch) at step 20: 276.6191, Accuracy: 0.6789\n",
      "Training loss (for one batch) at step 30: 260.2597, Accuracy: 0.7303\n",
      "Training loss (for one batch) at step 40: 250.0106, Accuracy: 0.7628\n",
      "Training loss (for one batch) at step 50: 277.3029, Accuracy: 0.7832\n",
      "Training loss (for one batch) at step 60: 289.4461, Accuracy: 0.7973\n",
      "Training loss (for one batch) at step 70: 318.4886, Accuracy: 0.7817\n",
      "Training loss (for one batch) at step 80: 302.0118, Accuracy: 0.7660\n",
      "Training loss (for one batch) at step 90: 273.4803, Accuracy: 0.7638\n",
      "Training loss (for one batch) at step 100: 271.8000, Accuracy: 0.7702\n",
      "Training loss (for one batch) at step 110: 295.3017, Accuracy: 0.7701\n",
      "---- Training ----\n",
      "Training loss: 83.0548\n",
      "Training acc over epoch: 0.7685\n",
      "---- Validation ----\n",
      "Validation loss: 44.8747\n",
      "Validation acc: 0.7069\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 347.0740, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 329.4998, Accuracy: 0.6023\n",
      "Training loss (for one batch) at step 20: 300.4714, Accuracy: 0.6696\n",
      "Training loss (for one batch) at step 30: 261.3321, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 40: 269.3927, Accuracy: 0.7603\n",
      "Training loss (for one batch) at step 50: 256.0883, Accuracy: 0.7826\n",
      "Training loss (for one batch) at step 60: 283.8260, Accuracy: 0.7951\n",
      "Training loss (for one batch) at step 70: 324.3250, Accuracy: 0.7818\n",
      "Training loss (for one batch) at step 80: 322.3997, Accuracy: 0.7659\n",
      "Training loss (for one batch) at step 90: 278.8088, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 100: 270.9573, Accuracy: 0.7692\n",
      "Training loss (for one batch) at step 110: 287.8639, Accuracy: 0.7713\n",
      "---- Training ----\n",
      "Training loss: 97.8693\n",
      "Training acc over epoch: 0.7693\n",
      "---- Validation ----\n",
      "Validation loss: 51.9923\n",
      "Validation acc: 0.6967\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 343.1033, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 314.5448, Accuracy: 0.6065\n",
      "Training loss (for one batch) at step 20: 268.0498, Accuracy: 0.6715\n",
      "Training loss (for one batch) at step 30: 276.6401, Accuracy: 0.7278\n",
      "Training loss (for one batch) at step 40: 253.8507, Accuracy: 0.7603\n",
      "Training loss (for one batch) at step 50: 245.0419, Accuracy: 0.7845\n",
      "Training loss (for one batch) at step 60: 271.3185, Accuracy: 0.7966\n",
      "Training loss (for one batch) at step 70: 316.4227, Accuracy: 0.7824\n",
      "Training loss (for one batch) at step 80: 334.9918, Accuracy: 0.7647\n",
      "Training loss (for one batch) at step 90: 276.0044, Accuracy: 0.7616\n",
      "Training loss (for one batch) at step 100: 305.3944, Accuracy: 0.7679\n",
      "Training loss (for one batch) at step 110: 311.1530, Accuracy: 0.7690\n",
      "---- Training ----\n",
      "Training loss: 101.6766\n",
      "Training acc over epoch: 0.7671\n",
      "---- Validation ----\n",
      "Validation loss: 41.9229\n",
      "Validation acc: 0.7152\n",
      "Time taken: 17.89s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 323.0806, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 312.4136, Accuracy: 0.6456\n",
      "Training loss (for one batch) at step 20: 292.2674, Accuracy: 0.6864\n",
      "Training loss (for one batch) at step 30: 268.8327, Accuracy: 0.7384\n",
      "Training loss (for one batch) at step 40: 262.1165, Accuracy: 0.7685\n",
      "Training loss (for one batch) at step 50: 268.7026, Accuracy: 0.7900\n",
      "Training loss (for one batch) at step 60: 277.7309, Accuracy: 0.7997\n",
      "Training loss (for one batch) at step 70: 303.7276, Accuracy: 0.7851\n",
      "Training loss (for one batch) at step 80: 315.0715, Accuracy: 0.7689\n",
      "Training loss (for one batch) at step 90: 264.6717, Accuracy: 0.7659\n",
      "Training loss (for one batch) at step 100: 259.8358, Accuracy: 0.7722\n",
      "Training loss (for one batch) at step 110: 292.1750, Accuracy: 0.7736\n",
      "---- Training ----\n",
      "Training loss: 94.3670\n",
      "Training acc over epoch: 0.7714\n",
      "---- Validation ----\n",
      "Validation loss: 37.4921\n",
      "Validation acc: 0.7053\n",
      "Time taken: 18.06s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 316.2551, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 304.2216, Accuracy: 0.6151\n",
      "Training loss (for one batch) at step 20: 265.6817, Accuracy: 0.6763\n",
      "Training loss (for one batch) at step 30: 255.0350, Accuracy: 0.7273\n",
      "Training loss (for one batch) at step 40: 270.3626, Accuracy: 0.7603\n",
      "Training loss (for one batch) at step 50: 249.1711, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 60: 274.7522, Accuracy: 0.7921\n",
      "Training loss (for one batch) at step 70: 290.8593, Accuracy: 0.7801\n",
      "Training loss (for one batch) at step 80: 308.0180, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 90: 261.2110, Accuracy: 0.7617\n",
      "Training loss (for one batch) at step 100: 248.7844, Accuracy: 0.7665\n",
      "Training loss (for one batch) at step 110: 279.2379, Accuracy: 0.7679\n",
      "---- Training ----\n",
      "Training loss: 86.8904\n",
      "Training acc over epoch: 0.7668\n",
      "---- Validation ----\n",
      "Validation loss: 47.0205\n",
      "Validation acc: 0.7066\n",
      "Time taken: 18.14s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 310.9629, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 331.2155, Accuracy: 0.6030\n",
      "Training loss (for one batch) at step 20: 261.3430, Accuracy: 0.6737\n",
      "Training loss (for one batch) at step 30: 253.6758, Accuracy: 0.7273\n",
      "Training loss (for one batch) at step 40: 249.9673, Accuracy: 0.7584\n",
      "Training loss (for one batch) at step 50: 253.7319, Accuracy: 0.7823\n",
      "Training loss (for one batch) at step 60: 294.8479, Accuracy: 0.7944\n",
      "Training loss (for one batch) at step 70: 307.7335, Accuracy: 0.7809\n",
      "Training loss (for one batch) at step 80: 305.4918, Accuracy: 0.7637\n",
      "Training loss (for one batch) at step 90: 261.0876, Accuracy: 0.7611\n",
      "Training loss (for one batch) at step 100: 265.0826, Accuracy: 0.7660\n",
      "Training loss (for one batch) at step 110: 290.6774, Accuracy: 0.7681\n",
      "---- Training ----\n",
      "Training loss: 96.1068\n",
      "Training acc over epoch: 0.7671\n",
      "---- Validation ----\n",
      "Validation loss: 53.8512\n",
      "Validation acc: 0.7045\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 326.8246, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 333.2947, Accuracy: 0.6108\n",
      "Training loss (for one batch) at step 20: 274.1964, Accuracy: 0.6711\n",
      "Training loss (for one batch) at step 30: 268.1490, Accuracy: 0.7278\n",
      "Training loss (for one batch) at step 40: 254.6693, Accuracy: 0.7622\n",
      "Training loss (for one batch) at step 50: 240.2947, Accuracy: 0.7843\n",
      "Training loss (for one batch) at step 60: 258.6625, Accuracy: 0.7971\n",
      "Training loss (for one batch) at step 70: 324.1054, Accuracy: 0.7826\n",
      "Training loss (for one batch) at step 80: 299.4229, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 90: 256.3359, Accuracy: 0.7604\n",
      "Training loss (for one batch) at step 100: 249.4690, Accuracy: 0.7678\n",
      "Training loss (for one batch) at step 110: 278.8282, Accuracy: 0.7678\n",
      "---- Training ----\n",
      "Training loss: 95.6582\n",
      "Training acc over epoch: 0.7669\n",
      "---- Validation ----\n",
      "Validation loss: 47.6256\n",
      "Validation acc: 0.7179\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 309.0670, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 323.8961, Accuracy: 0.6229\n",
      "Training loss (for one batch) at step 20: 269.1942, Accuracy: 0.6737\n",
      "Training loss (for one batch) at step 30: 245.3100, Accuracy: 0.7331\n",
      "Training loss (for one batch) at step 40: 257.1967, Accuracy: 0.7628\n",
      "Training loss (for one batch) at step 50: 250.9802, Accuracy: 0.7857\n",
      "Training loss (for one batch) at step 60: 261.0947, Accuracy: 0.7974\n",
      "Training loss (for one batch) at step 70: 296.3542, Accuracy: 0.7809\n",
      "Training loss (for one batch) at step 80: 304.7505, Accuracy: 0.7620\n",
      "Training loss (for one batch) at step 90: 277.3807, Accuracy: 0.7594\n",
      "Training loss (for one batch) at step 100: 268.3961, Accuracy: 0.7652\n",
      "Training loss (for one batch) at step 110: 291.7587, Accuracy: 0.7654\n",
      "---- Training ----\n",
      "Training loss: 106.2082\n",
      "Training acc over epoch: 0.7652\n",
      "---- Validation ----\n",
      "Validation loss: 55.2269\n",
      "Validation acc: 0.7028\n",
      "Time taken: 18.07s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 324.8199, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 287.4309, Accuracy: 0.5902\n",
      "Training loss (for one batch) at step 20: 278.3268, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 30: 251.0072, Accuracy: 0.7245\n",
      "Training loss (for one batch) at step 40: 244.0108, Accuracy: 0.7580\n",
      "Training loss (for one batch) at step 50: 266.7738, Accuracy: 0.7840\n",
      "Training loss (for one batch) at step 60: 274.5375, Accuracy: 0.7944\n",
      "Training loss (for one batch) at step 70: 298.9634, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 80: 293.4287, Accuracy: 0.7645\n",
      "Training loss (for one batch) at step 90: 255.9986, Accuracy: 0.7602\n",
      "Training loss (for one batch) at step 100: 267.1905, Accuracy: 0.7676\n",
      "Training loss (for one batch) at step 110: 296.7640, Accuracy: 0.7688\n",
      "---- Training ----\n",
      "Training loss: 85.9389\n",
      "Training acc over epoch: 0.7675\n",
      "---- Validation ----\n",
      "Validation loss: 47.2094\n",
      "Validation acc: 0.7093\n",
      "Time taken: 18.05s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 304.9951, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 315.7814, Accuracy: 0.6214\n",
      "Training loss (for one batch) at step 20: 265.4638, Accuracy: 0.6789\n",
      "Training loss (for one batch) at step 30: 245.0565, Accuracy: 0.7336\n",
      "Training loss (for one batch) at step 40: 260.1551, Accuracy: 0.7666\n",
      "Training loss (for one batch) at step 50: 271.0566, Accuracy: 0.7883\n",
      "Training loss (for one batch) at step 60: 276.3021, Accuracy: 0.7988\n",
      "Training loss (for one batch) at step 70: 306.4398, Accuracy: 0.7837\n",
      "Training loss (for one batch) at step 80: 304.3159, Accuracy: 0.7644\n",
      "Training loss (for one batch) at step 90: 259.4853, Accuracy: 0.7609\n",
      "Training loss (for one batch) at step 100: 253.3392, Accuracy: 0.7676\n",
      "Training loss (for one batch) at step 110: 278.3833, Accuracy: 0.7689\n",
      "---- Training ----\n",
      "Training loss: 88.0902\n",
      "Training acc over epoch: 0.7677\n",
      "---- Validation ----\n",
      "Validation loss: 33.9003\n",
      "Validation acc: 0.7149\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 334.7083, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 306.2886, Accuracy: 0.5966\n",
      "Training loss (for one batch) at step 20: 259.3738, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 30: 256.7313, Accuracy: 0.7314\n",
      "Training loss (for one batch) at step 40: 246.1642, Accuracy: 0.7668\n",
      "Training loss (for one batch) at step 50: 244.4141, Accuracy: 0.7871\n",
      "Training loss (for one batch) at step 60: 267.7396, Accuracy: 0.7955\n",
      "Training loss (for one batch) at step 70: 291.3092, Accuracy: 0.7789\n",
      "Training loss (for one batch) at step 80: 295.4092, Accuracy: 0.7622\n",
      "Training loss (for one batch) at step 90: 253.2890, Accuracy: 0.7585\n",
      "Training loss (for one batch) at step 100: 256.4640, Accuracy: 0.7658\n",
      "Training loss (for one batch) at step 110: 272.4705, Accuracy: 0.7668\n",
      "---- Training ----\n",
      "Training loss: 90.0670\n",
      "Training acc over epoch: 0.7666\n",
      "---- Validation ----\n",
      "Validation loss: 48.6703\n",
      "Validation acc: 0.7031\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 314.1217, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 312.9404, Accuracy: 0.6193\n",
      "Training loss (for one batch) at step 20: 267.4709, Accuracy: 0.6708\n",
      "Training loss (for one batch) at step 30: 253.4245, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 40: 254.6333, Accuracy: 0.7599\n",
      "Training loss (for one batch) at step 50: 235.1582, Accuracy: 0.7832\n",
      "Training loss (for one batch) at step 60: 268.7664, Accuracy: 0.7952\n",
      "Training loss (for one batch) at step 70: 307.3668, Accuracy: 0.7793\n",
      "Training loss (for one batch) at step 80: 306.6136, Accuracy: 0.7613\n",
      "Training loss (for one batch) at step 90: 243.5819, Accuracy: 0.7586\n",
      "Training loss (for one batch) at step 100: 237.7103, Accuracy: 0.7645\n",
      "Training loss (for one batch) at step 110: 273.0241, Accuracy: 0.7674\n",
      "---- Training ----\n",
      "Training loss: 80.6199\n",
      "Training acc over epoch: 0.7652\n",
      "---- Validation ----\n",
      "Validation loss: 52.3663\n",
      "Validation acc: 0.7088\n",
      "Time taken: 18.40s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 294.3486, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 290.0827, Accuracy: 0.6087\n",
      "Training loss (for one batch) at step 20: 247.4284, Accuracy: 0.6708\n",
      "Training loss (for one batch) at step 30: 252.1369, Accuracy: 0.7248\n",
      "Training loss (for one batch) at step 40: 255.5463, Accuracy: 0.7609\n",
      "Training loss (for one batch) at step 50: 247.5736, Accuracy: 0.7837\n",
      "Training loss (for one batch) at step 60: 274.3250, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 70: 292.9627, Accuracy: 0.7832\n",
      "Training loss (for one batch) at step 80: 296.4134, Accuracy: 0.7634\n",
      "Training loss (for one batch) at step 90: 277.9486, Accuracy: 0.7594\n",
      "Training loss (for one batch) at step 100: 256.5583, Accuracy: 0.7668\n",
      "Training loss (for one batch) at step 110: 268.7787, Accuracy: 0.7680\n",
      "---- Training ----\n",
      "Training loss: 84.5640\n",
      "Training acc over epoch: 0.7661\n",
      "---- Validation ----\n",
      "Validation loss: 43.9178\n",
      "Validation acc: 0.7109\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 337.9994, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 308.0280, Accuracy: 0.6051\n",
      "Training loss (for one batch) at step 20: 263.4162, Accuracy: 0.6741\n",
      "Training loss (for one batch) at step 30: 282.5698, Accuracy: 0.7314\n",
      "Training loss (for one batch) at step 40: 240.8722, Accuracy: 0.7651\n",
      "Training loss (for one batch) at step 50: 246.2300, Accuracy: 0.7828\n",
      "Training loss (for one batch) at step 60: 274.8034, Accuracy: 0.7957\n",
      "Training loss (for one batch) at step 70: 298.5766, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 80: 292.1841, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 90: 255.6824, Accuracy: 0.7577\n",
      "Training loss (for one batch) at step 100: 261.3915, Accuracy: 0.7648\n",
      "Training loss (for one batch) at step 110: 262.1880, Accuracy: 0.7658\n",
      "---- Training ----\n",
      "Training loss: 89.7429\n",
      "Training acc over epoch: 0.7651\n",
      "---- Validation ----\n",
      "Validation loss: 39.4307\n",
      "Validation acc: 0.7147\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 316.7649, Accuracy: 0.4922\n",
      "Training loss (for one batch) at step 10: 296.3359, Accuracy: 0.5952\n",
      "Training loss (for one batch) at step 20: 239.6219, Accuracy: 0.6696\n",
      "Training loss (for one batch) at step 30: 262.0134, Accuracy: 0.7225\n",
      "Training loss (for one batch) at step 40: 245.1562, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 50: 239.0113, Accuracy: 0.7808\n",
      "Training loss (for one batch) at step 60: 280.7769, Accuracy: 0.7944\n",
      "Training loss (for one batch) at step 70: 279.1136, Accuracy: 0.7788\n",
      "Training loss (for one batch) at step 80: 283.7932, Accuracy: 0.7603\n",
      "Training loss (for one batch) at step 90: 269.3221, Accuracy: 0.7573\n",
      "Training loss (for one batch) at step 100: 249.5267, Accuracy: 0.7643\n",
      "Training loss (for one batch) at step 110: 272.2562, Accuracy: 0.7665\n",
      "---- Training ----\n",
      "Training loss: 89.0493\n",
      "Training acc over epoch: 0.7648\n",
      "---- Validation ----\n",
      "Validation loss: 54.8092\n",
      "Validation acc: 0.7117\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 281.5336, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 311.1859, Accuracy: 0.6151\n",
      "Training loss (for one batch) at step 20: 233.3733, Accuracy: 0.6782\n",
      "Training loss (for one batch) at step 30: 257.6910, Accuracy: 0.7301\n",
      "Training loss (for one batch) at step 40: 249.6695, Accuracy: 0.7652\n",
      "Training loss (for one batch) at step 50: 255.4572, Accuracy: 0.7851\n",
      "Training loss (for one batch) at step 60: 261.3685, Accuracy: 0.7987\n",
      "Training loss (for one batch) at step 70: 283.8143, Accuracy: 0.7865\n",
      "Training loss (for one batch) at step 80: 291.8087, Accuracy: 0.7667\n",
      "Training loss (for one batch) at step 90: 253.7370, Accuracy: 0.7622\n",
      "Training loss (for one batch) at step 100: 239.8719, Accuracy: 0.7686\n",
      "Training loss (for one batch) at step 110: 262.1520, Accuracy: 0.7697\n",
      "---- Training ----\n",
      "Training loss: 94.0531\n",
      "Training acc over epoch: 0.7683\n",
      "---- Validation ----\n",
      "Validation loss: 40.5327\n",
      "Validation acc: 0.7045\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 289.0898, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 286.7381, Accuracy: 0.6087\n",
      "Training loss (for one batch) at step 20: 244.8110, Accuracy: 0.6767\n",
      "Training loss (for one batch) at step 30: 251.7053, Accuracy: 0.7316\n",
      "Training loss (for one batch) at step 40: 236.5501, Accuracy: 0.7628\n",
      "Training loss (for one batch) at step 50: 242.1700, Accuracy: 0.7849\n",
      "Training loss (for one batch) at step 60: 252.8053, Accuracy: 0.7975\n",
      "Training loss (for one batch) at step 70: 282.4451, Accuracy: 0.7838\n",
      "Training loss (for one batch) at step 80: 294.2011, Accuracy: 0.7628\n",
      "Training loss (for one batch) at step 90: 262.3047, Accuracy: 0.7584\n",
      "Training loss (for one batch) at step 100: 245.9314, Accuracy: 0.7666\n",
      "Training loss (for one batch) at step 110: 288.2980, Accuracy: 0.7698\n",
      "---- Training ----\n",
      "Training loss: 97.2510\n",
      "Training acc over epoch: 0.7677\n",
      "---- Validation ----\n",
      "Validation loss: 37.0761\n",
      "Validation acc: 0.6991\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 318.0931, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 276.1111, Accuracy: 0.6037\n",
      "Training loss (for one batch) at step 20: 254.9265, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 30: 245.3551, Accuracy: 0.7293\n",
      "Training loss (for one batch) at step 40: 230.1269, Accuracy: 0.7614\n",
      "Training loss (for one batch) at step 50: 242.4416, Accuracy: 0.7839\n",
      "Training loss (for one batch) at step 60: 259.6149, Accuracy: 0.7953\n",
      "Training loss (for one batch) at step 70: 274.0243, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 80: 285.2527, Accuracy: 0.7613\n",
      "Training loss (for one batch) at step 90: 261.7137, Accuracy: 0.7567\n",
      "Training loss (for one batch) at step 100: 254.8700, Accuracy: 0.7630\n",
      "Training loss (for one batch) at step 110: 268.0085, Accuracy: 0.7646\n",
      "---- Training ----\n",
      "Training loss: 77.4359\n",
      "Training acc over epoch: 0.7638\n",
      "---- Validation ----\n",
      "Validation loss: 38.6293\n",
      "Validation acc: 0.7026\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 288.3525, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 301.6261, Accuracy: 0.6087\n",
      "Training loss (for one batch) at step 20: 244.5601, Accuracy: 0.6656\n",
      "Training loss (for one batch) at step 30: 238.8160, Accuracy: 0.7251\n",
      "Training loss (for one batch) at step 40: 237.8945, Accuracy: 0.7614\n",
      "Training loss (for one batch) at step 50: 237.2505, Accuracy: 0.7846\n",
      "Training loss (for one batch) at step 60: 259.4271, Accuracy: 0.7960\n",
      "Training loss (for one batch) at step 70: 270.2141, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 80: 283.8318, Accuracy: 0.7623\n",
      "Training loss (for one batch) at step 90: 254.3894, Accuracy: 0.7575\n",
      "Training loss (for one batch) at step 100: 247.7845, Accuracy: 0.7640\n",
      "Training loss (for one batch) at step 110: 262.3318, Accuracy: 0.7658\n",
      "---- Training ----\n",
      "Training loss: 85.2761\n",
      "Training acc over epoch: 0.7644\n",
      "---- Validation ----\n",
      "Validation loss: 46.7387\n",
      "Validation acc: 0.7020\n",
      "Time taken: 18.14s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 288.6509, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 286.3469, Accuracy: 0.6072\n",
      "Training loss (for one batch) at step 20: 233.6448, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 30: 235.0683, Accuracy: 0.7253\n",
      "Training loss (for one batch) at step 40: 243.1075, Accuracy: 0.7576\n",
      "Training loss (for one batch) at step 50: 236.0951, Accuracy: 0.7782\n",
      "Training loss (for one batch) at step 60: 254.1109, Accuracy: 0.7911\n",
      "Training loss (for one batch) at step 70: 284.0672, Accuracy: 0.7824\n",
      "Training loss (for one batch) at step 80: 267.7755, Accuracy: 0.7624\n",
      "Training loss (for one batch) at step 90: 247.2094, Accuracy: 0.7583\n",
      "Training loss (for one batch) at step 100: 252.0981, Accuracy: 0.7641\n",
      "Training loss (for one batch) at step 110: 245.2855, Accuracy: 0.7662\n",
      "---- Training ----\n",
      "Training loss: 86.1339\n",
      "Training acc over epoch: 0.7648\n",
      "---- Validation ----\n",
      "Validation loss: 47.9418\n",
      "Validation acc: 0.6980\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 299.6281, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 292.9203, Accuracy: 0.6023\n",
      "Training loss (for one batch) at step 20: 277.3507, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 30: 240.4743, Accuracy: 0.7281\n",
      "Training loss (for one batch) at step 40: 235.1594, Accuracy: 0.7584\n",
      "Training loss (for one batch) at step 50: 242.2437, Accuracy: 0.7837\n",
      "Training loss (for one batch) at step 60: 255.1391, Accuracy: 0.7967\n",
      "Training loss (for one batch) at step 70: 279.4001, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 80: 302.7805, Accuracy: 0.7588\n",
      "Training loss (for one batch) at step 90: 243.2256, Accuracy: 0.7555\n",
      "Training loss (for one batch) at step 100: 242.1080, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 110: 278.9512, Accuracy: 0.7641\n",
      "---- Training ----\n",
      "Training loss: 85.2389\n",
      "Training acc over epoch: 0.7630\n",
      "---- Validation ----\n",
      "Validation loss: 33.6257\n",
      "Validation acc: 0.7028\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 290.0497, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 288.7114, Accuracy: 0.6001\n",
      "Training loss (for one batch) at step 20: 268.0208, Accuracy: 0.6689\n",
      "Training loss (for one batch) at step 30: 232.3773, Accuracy: 0.7268\n",
      "Training loss (for one batch) at step 40: 240.4016, Accuracy: 0.7593\n",
      "Training loss (for one batch) at step 50: 241.5460, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 60: 256.4986, Accuracy: 0.7964\n",
      "Training loss (for one batch) at step 70: 279.8257, Accuracy: 0.7819\n",
      "Training loss (for one batch) at step 80: 287.8341, Accuracy: 0.7604\n",
      "Training loss (for one batch) at step 90: 266.0994, Accuracy: 0.7588\n",
      "Training loss (for one batch) at step 100: 245.7630, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 110: 259.9245, Accuracy: 0.7679\n",
      "---- Training ----\n",
      "Training loss: 79.3901\n",
      "Training acc over epoch: 0.7665\n",
      "---- Validation ----\n",
      "Validation loss: 37.0654\n",
      "Validation acc: 0.6964\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 288.4565, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 296.6099, Accuracy: 0.6072\n",
      "Training loss (for one batch) at step 20: 251.7145, Accuracy: 0.6670\n",
      "Training loss (for one batch) at step 30: 234.7273, Accuracy: 0.7251\n",
      "Training loss (for one batch) at step 40: 243.1457, Accuracy: 0.7593\n",
      "Training loss (for one batch) at step 50: 237.3784, Accuracy: 0.7803\n",
      "Training loss (for one batch) at step 60: 250.5960, Accuracy: 0.7932\n",
      "Training loss (for one batch) at step 70: 264.0650, Accuracy: 0.7785\n",
      "Training loss (for one batch) at step 80: 280.2059, Accuracy: 0.7583\n",
      "Training loss (for one batch) at step 90: 262.2635, Accuracy: 0.7561\n",
      "Training loss (for one batch) at step 100: 245.6987, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 110: 267.8279, Accuracy: 0.7649\n",
      "---- Training ----\n",
      "Training loss: 99.9339\n",
      "Training acc over epoch: 0.7632\n",
      "---- Validation ----\n",
      "Validation loss: 40.0537\n",
      "Validation acc: 0.7039\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 297.7356, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 271.4307, Accuracy: 0.6058\n",
      "Training loss (for one batch) at step 20: 254.4099, Accuracy: 0.6670\n",
      "Training loss (for one batch) at step 30: 249.6310, Accuracy: 0.7208\n",
      "Training loss (for one batch) at step 40: 245.9639, Accuracy: 0.7572\n",
      "Training loss (for one batch) at step 50: 229.3450, Accuracy: 0.7817\n",
      "Training loss (for one batch) at step 60: 256.1209, Accuracy: 0.7950\n",
      "Training loss (for one batch) at step 70: 282.6437, Accuracy: 0.7786\n",
      "Training loss (for one batch) at step 80: 295.2516, Accuracy: 0.7585\n",
      "Training loss (for one batch) at step 90: 245.6484, Accuracy: 0.7555\n",
      "Training loss (for one batch) at step 100: 256.6035, Accuracy: 0.7647\n",
      "Training loss (for one batch) at step 110: 275.1861, Accuracy: 0.7645\n",
      "---- Training ----\n",
      "Training loss: 84.7130\n",
      "Training acc over epoch: 0.7631\n",
      "---- Validation ----\n",
      "Validation loss: 41.3165\n",
      "Validation acc: 0.7004\n",
      "Time taken: 17.86s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 300.5453, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 293.5767, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 20: 245.9914, Accuracy: 0.6559\n",
      "Training loss (for one batch) at step 30: 244.5418, Accuracy: 0.7167\n",
      "Training loss (for one batch) at step 40: 231.4109, Accuracy: 0.7572\n",
      "Training loss (for one batch) at step 50: 215.8636, Accuracy: 0.7817\n",
      "Training loss (for one batch) at step 60: 270.5344, Accuracy: 0.7921\n",
      "Training loss (for one batch) at step 70: 276.9672, Accuracy: 0.7765\n",
      "Training loss (for one batch) at step 80: 296.6644, Accuracy: 0.7568\n",
      "Training loss (for one batch) at step 90: 232.1766, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 100: 243.1152, Accuracy: 0.7611\n",
      "Training loss (for one batch) at step 110: 246.6888, Accuracy: 0.7634\n",
      "---- Training ----\n",
      "Training loss: 80.4451\n",
      "Training acc over epoch: 0.7613\n",
      "---- Validation ----\n",
      "Validation loss: 60.7877\n",
      "Validation acc: 0.7010\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 292.8437, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 269.9344, Accuracy: 0.5945\n",
      "Training loss (for one batch) at step 20: 245.6286, Accuracy: 0.6629\n",
      "Training loss (for one batch) at step 30: 237.3475, Accuracy: 0.7238\n",
      "Training loss (for one batch) at step 40: 223.6079, Accuracy: 0.7588\n",
      "Training loss (for one batch) at step 50: 253.1010, Accuracy: 0.7826\n",
      "Training loss (for one batch) at step 60: 256.6575, Accuracy: 0.7926\n",
      "Training loss (for one batch) at step 70: 267.8843, Accuracy: 0.7757\n",
      "Training loss (for one batch) at step 80: 267.5409, Accuracy: 0.7581\n",
      "Training loss (for one batch) at step 90: 236.1445, Accuracy: 0.7542\n",
      "Training loss (for one batch) at step 100: 265.1542, Accuracy: 0.7614\n",
      "Training loss (for one batch) at step 110: 270.8329, Accuracy: 0.7639\n",
      "---- Training ----\n",
      "Training loss: 101.1324\n",
      "Training acc over epoch: 0.7624\n",
      "---- Validation ----\n",
      "Validation loss: 56.9331\n",
      "Validation acc: 0.7045\n",
      "Time taken: 18.24s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 298.9304, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 286.2588, Accuracy: 0.6009\n",
      "Training loss (for one batch) at step 20: 260.3032, Accuracy: 0.6700\n",
      "Training loss (for one batch) at step 30: 257.6581, Accuracy: 0.7258\n",
      "Training loss (for one batch) at step 40: 230.4126, Accuracy: 0.7609\n",
      "Training loss (for one batch) at step 50: 241.8979, Accuracy: 0.7819\n",
      "Training loss (for one batch) at step 60: 257.0594, Accuracy: 0.7941\n",
      "Training loss (for one batch) at step 70: 262.8422, Accuracy: 0.7774\n",
      "Training loss (for one batch) at step 80: 268.1425, Accuracy: 0.7580\n",
      "Training loss (for one batch) at step 90: 241.3318, Accuracy: 0.7542\n",
      "Training loss (for one batch) at step 100: 236.4399, Accuracy: 0.7614\n",
      "Training loss (for one batch) at step 110: 260.8463, Accuracy: 0.7640\n",
      "---- Training ----\n",
      "Training loss: 75.8512\n",
      "Training acc over epoch: 0.7621\n",
      "---- Validation ----\n",
      "Validation loss: 41.7046\n",
      "Validation acc: 0.6956\n",
      "Time taken: 18.07s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 297.4438, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 303.2360, Accuracy: 0.5994\n",
      "Training loss (for one batch) at step 20: 237.1284, Accuracy: 0.6652\n",
      "Training loss (for one batch) at step 30: 243.5455, Accuracy: 0.7225\n",
      "Training loss (for one batch) at step 40: 228.9282, Accuracy: 0.7597\n",
      "Training loss (for one batch) at step 50: 224.4805, Accuracy: 0.7806\n",
      "Training loss (for one batch) at step 60: 246.1701, Accuracy: 0.7947\n",
      "Training loss (for one batch) at step 70: 278.8438, Accuracy: 0.7817\n",
      "Training loss (for one batch) at step 80: 268.8109, Accuracy: 0.7623\n",
      "Training loss (for one batch) at step 90: 246.4231, Accuracy: 0.7552\n",
      "Training loss (for one batch) at step 100: 260.6136, Accuracy: 0.7634\n",
      "Training loss (for one batch) at step 110: 276.0018, Accuracy: 0.7665\n",
      "---- Training ----\n",
      "Training loss: 86.1128\n",
      "Training acc over epoch: 0.7640\n",
      "---- Validation ----\n",
      "Validation loss: 44.5382\n",
      "Validation acc: 0.7028\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 263.5126, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 274.7639, Accuracy: 0.6115\n",
      "Training loss (for one batch) at step 20: 264.1913, Accuracy: 0.6711\n",
      "Training loss (for one batch) at step 30: 236.5256, Accuracy: 0.7240\n",
      "Training loss (for one batch) at step 40: 227.6678, Accuracy: 0.7595\n",
      "Training loss (for one batch) at step 50: 240.4766, Accuracy: 0.7842\n",
      "Training loss (for one batch) at step 60: 250.9769, Accuracy: 0.7953\n",
      "Training loss (for one batch) at step 70: 267.5362, Accuracy: 0.7821\n",
      "Training loss (for one batch) at step 80: 266.8426, Accuracy: 0.7620\n",
      "Training loss (for one batch) at step 90: 256.6757, Accuracy: 0.7569\n",
      "Training loss (for one batch) at step 100: 252.4941, Accuracy: 0.7639\n",
      "Training loss (for one batch) at step 110: 244.1211, Accuracy: 0.7654\n",
      "---- Training ----\n",
      "Training loss: 89.2051\n",
      "Training acc over epoch: 0.7630\n",
      "---- Validation ----\n",
      "Validation loss: 46.2646\n",
      "Validation acc: 0.6891\n",
      "Time taken: 17.87s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 289.1241, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 271.1596, Accuracy: 0.5803\n",
      "Training loss (for one batch) at step 20: 230.6368, Accuracy: 0.6659\n",
      "Training loss (for one batch) at step 30: 229.2481, Accuracy: 0.7175\n",
      "Training loss (for one batch) at step 40: 241.8550, Accuracy: 0.7536\n",
      "Training loss (for one batch) at step 50: 237.1953, Accuracy: 0.7790\n",
      "Training loss (for one batch) at step 60: 246.8842, Accuracy: 0.7929\n",
      "Training loss (for one batch) at step 70: 287.0457, Accuracy: 0.7766\n",
      "Training loss (for one batch) at step 80: 279.3199, Accuracy: 0.7570\n",
      "Training loss (for one batch) at step 90: 230.6269, Accuracy: 0.7531\n",
      "Training loss (for one batch) at step 100: 240.8704, Accuracy: 0.7616\n",
      "Training loss (for one batch) at step 110: 231.9208, Accuracy: 0.7624\n",
      "---- Training ----\n",
      "Training loss: 85.2952\n",
      "Training acc over epoch: 0.7621\n",
      "---- Validation ----\n",
      "Validation loss: 48.9280\n",
      "Validation acc: 0.7071\n",
      "Time taken: 18.08s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 276.7085, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 264.4242, Accuracy: 0.5994\n",
      "Training loss (for one batch) at step 20: 240.5732, Accuracy: 0.6633\n",
      "Training loss (for one batch) at step 30: 245.9151, Accuracy: 0.7175\n",
      "Training loss (for one batch) at step 40: 227.4074, Accuracy: 0.7567\n",
      "Training loss (for one batch) at step 50: 226.7812, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 60: 245.0804, Accuracy: 0.7944\n",
      "Training loss (for one batch) at step 70: 279.2834, Accuracy: 0.7794\n",
      "Training loss (for one batch) at step 80: 271.5670, Accuracy: 0.7577\n",
      "Training loss (for one batch) at step 90: 240.4149, Accuracy: 0.7539\n",
      "Training loss (for one batch) at step 100: 229.2631, Accuracy: 0.7625\n",
      "Training loss (for one batch) at step 110: 241.8208, Accuracy: 0.7641\n",
      "---- Training ----\n",
      "Training loss: 77.6881\n",
      "Training acc over epoch: 0.7627\n",
      "---- Validation ----\n",
      "Validation loss: 33.9675\n",
      "Validation acc: 0.6870\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 269.3631, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 275.8563, Accuracy: 0.6115\n",
      "Training loss (for one batch) at step 20: 232.6734, Accuracy: 0.6648\n",
      "Training loss (for one batch) at step 30: 235.9554, Accuracy: 0.7263\n",
      "Training loss (for one batch) at step 40: 248.4839, Accuracy: 0.7588\n",
      "Training loss (for one batch) at step 50: 224.8677, Accuracy: 0.7825\n",
      "Training loss (for one batch) at step 60: 243.1242, Accuracy: 0.7937\n",
      "Training loss (for one batch) at step 70: 271.3727, Accuracy: 0.7789\n",
      "Training loss (for one batch) at step 80: 272.8149, Accuracy: 0.7584\n",
      "Training loss (for one batch) at step 90: 228.4085, Accuracy: 0.7544\n",
      "Training loss (for one batch) at step 100: 241.6475, Accuracy: 0.7608\n",
      "Training loss (for one batch) at step 110: 242.3636, Accuracy: 0.7625\n",
      "---- Training ----\n",
      "Training loss: 75.8883\n",
      "Training acc over epoch: 0.7626\n",
      "---- Validation ----\n",
      "Validation loss: 44.4516\n",
      "Validation acc: 0.6967\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 283.8685, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 259.1029, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 20: 250.5248, Accuracy: 0.6544\n",
      "Training loss (for one batch) at step 30: 246.6305, Accuracy: 0.7180\n",
      "Training loss (for one batch) at step 40: 235.2532, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 50: 224.0847, Accuracy: 0.7794\n",
      "Training loss (for one batch) at step 60: 249.2561, Accuracy: 0.7911\n",
      "Training loss (for one batch) at step 70: 258.2067, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 80: 288.1790, Accuracy: 0.7589\n",
      "Training loss (for one batch) at step 90: 239.6329, Accuracy: 0.7544\n",
      "Training loss (for one batch) at step 100: 243.2310, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 110: 262.0071, Accuracy: 0.7637\n",
      "---- Training ----\n",
      "Training loss: 79.3938\n",
      "Training acc over epoch: 0.7611\n",
      "---- Validation ----\n",
      "Validation loss: 57.0927\n",
      "Validation acc: 0.7037\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 265.5257, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 293.9608, Accuracy: 0.5930\n",
      "Training loss (for one batch) at step 20: 243.8593, Accuracy: 0.6678\n",
      "Training loss (for one batch) at step 30: 240.3568, Accuracy: 0.7245\n",
      "Training loss (for one batch) at step 40: 248.9968, Accuracy: 0.7593\n",
      "Training loss (for one batch) at step 50: 237.3920, Accuracy: 0.7828\n",
      "Training loss (for one batch) at step 60: 250.2842, Accuracy: 0.7960\n",
      "Training loss (for one batch) at step 70: 282.3692, Accuracy: 0.7790\n",
      "Training loss (for one batch) at step 80: 272.4670, Accuracy: 0.7584\n",
      "Training loss (for one batch) at step 90: 261.0015, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 100: 244.8222, Accuracy: 0.7630\n",
      "Training loss (for one batch) at step 110: 294.6438, Accuracy: 0.7649\n",
      "---- Training ----\n",
      "Training loss: 98.0664\n",
      "Training acc over epoch: 0.7620\n",
      "---- Validation ----\n",
      "Validation loss: 34.2948\n",
      "Validation acc: 0.6937\n",
      "Time taken: 18.11s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 287.5809, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 268.3570, Accuracy: 0.5959\n",
      "Training loss (for one batch) at step 20: 235.3697, Accuracy: 0.6548\n",
      "Training loss (for one batch) at step 30: 223.6288, Accuracy: 0.7182\n",
      "Training loss (for one batch) at step 40: 231.2392, Accuracy: 0.7571\n",
      "Training loss (for one batch) at step 50: 229.5000, Accuracy: 0.7811\n",
      "Training loss (for one batch) at step 60: 242.0203, Accuracy: 0.7932\n",
      "Training loss (for one batch) at step 70: 266.6208, Accuracy: 0.7759\n",
      "Training loss (for one batch) at step 80: 266.6577, Accuracy: 0.7572\n",
      "Training loss (for one batch) at step 90: 240.5963, Accuracy: 0.7539\n",
      "Training loss (for one batch) at step 100: 246.1490, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 110: 258.9261, Accuracy: 0.7634\n",
      "---- Training ----\n",
      "Training loss: 76.8757\n",
      "Training acc over epoch: 0.7617\n",
      "---- Validation ----\n",
      "Validation loss: 47.8134\n",
      "Validation acc: 0.7071\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 268.9335, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 276.1435, Accuracy: 0.5824\n",
      "Training loss (for one batch) at step 20: 228.0561, Accuracy: 0.6589\n",
      "Training loss (for one batch) at step 30: 225.4405, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 40: 225.3089, Accuracy: 0.7479\n",
      "Training loss (for one batch) at step 50: 231.5083, Accuracy: 0.7745\n",
      "Training loss (for one batch) at step 60: 234.6671, Accuracy: 0.7887\n",
      "Training loss (for one batch) at step 70: 244.5278, Accuracy: 0.7737\n",
      "Training loss (for one batch) at step 80: 276.6393, Accuracy: 0.7538\n",
      "Training loss (for one batch) at step 90: 239.0815, Accuracy: 0.7496\n",
      "Training loss (for one batch) at step 100: 231.4268, Accuracy: 0.7567\n",
      "Training loss (for one batch) at step 110: 256.1704, Accuracy: 0.7609\n",
      "---- Training ----\n",
      "Training loss: 76.5958\n",
      "Training acc over epoch: 0.7575\n",
      "---- Validation ----\n",
      "Validation loss: 67.6810\n",
      "Validation acc: 0.7023\n",
      "Time taken: 17.86s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 280.3999, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 281.2846, Accuracy: 0.5838\n",
      "Training loss (for one batch) at step 20: 236.2599, Accuracy: 0.6607\n",
      "Training loss (for one batch) at step 30: 231.9622, Accuracy: 0.7205\n",
      "Training loss (for one batch) at step 40: 235.3933, Accuracy: 0.7563\n",
      "Training loss (for one batch) at step 50: 239.6576, Accuracy: 0.7793\n",
      "Training loss (for one batch) at step 60: 239.7668, Accuracy: 0.7911\n",
      "Training loss (for one batch) at step 70: 250.8150, Accuracy: 0.7775\n",
      "Training loss (for one batch) at step 80: 245.8407, Accuracy: 0.7582\n",
      "Training loss (for one batch) at step 90: 242.3423, Accuracy: 0.7545\n",
      "Training loss (for one batch) at step 100: 233.7887, Accuracy: 0.7620\n",
      "Training loss (for one batch) at step 110: 237.5924, Accuracy: 0.7631\n",
      "---- Training ----\n",
      "Training loss: 83.8628\n",
      "Training acc over epoch: 0.7616\n",
      "---- Validation ----\n",
      "Validation loss: 56.8891\n",
      "Validation acc: 0.7010\n",
      "Time taken: 20.22s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 287.1680, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 271.2208, Accuracy: 0.5746\n",
      "Training loss (for one batch) at step 20: 235.3221, Accuracy: 0.6652\n",
      "Training loss (for one batch) at step 30: 234.4232, Accuracy: 0.7235\n",
      "Training loss (for one batch) at step 40: 229.0789, Accuracy: 0.7591\n",
      "Training loss (for one batch) at step 50: 231.8864, Accuracy: 0.7817\n",
      "Training loss (for one batch) at step 60: 242.1317, Accuracy: 0.7944\n",
      "Training loss (for one batch) at step 70: 286.3609, Accuracy: 0.7783\n",
      "Training loss (for one batch) at step 80: 286.4446, Accuracy: 0.7585\n",
      "Training loss (for one batch) at step 90: 226.8434, Accuracy: 0.7556\n",
      "Training loss (for one batch) at step 100: 223.2287, Accuracy: 0.7620\n",
      "Training loss (for one batch) at step 110: 246.0817, Accuracy: 0.7655\n",
      "---- Training ----\n",
      "Training loss: 92.9101\n",
      "Training acc over epoch: 0.7620\n",
      "---- Validation ----\n",
      "Validation loss: 33.6699\n",
      "Validation acc: 0.7028\n",
      "Time taken: 17.86s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 284.0515, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 267.3139, Accuracy: 0.5845\n",
      "Training loss (for one batch) at step 20: 237.7817, Accuracy: 0.6551\n",
      "Training loss (for one batch) at step 30: 244.1028, Accuracy: 0.7170\n",
      "Training loss (for one batch) at step 40: 237.9184, Accuracy: 0.7523\n",
      "Training loss (for one batch) at step 50: 241.6323, Accuracy: 0.7753\n",
      "Training loss (for one batch) at step 60: 241.8824, Accuracy: 0.7886\n",
      "Training loss (for one batch) at step 70: 271.7332, Accuracy: 0.7710\n",
      "Training loss (for one batch) at step 80: 257.7793, Accuracy: 0.7514\n",
      "Training loss (for one batch) at step 90: 235.3580, Accuracy: 0.7492\n",
      "Training loss (for one batch) at step 100: 221.0728, Accuracy: 0.7570\n",
      "Training loss (for one batch) at step 110: 231.8747, Accuracy: 0.7594\n",
      "---- Training ----\n",
      "Training loss: 91.7881\n",
      "Training acc over epoch: 0.7585\n",
      "---- Validation ----\n",
      "Validation loss: 47.3052\n",
      "Validation acc: 0.7002\n",
      "Time taken: 17.89s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 262.8831, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 258.3414, Accuracy: 0.5774\n",
      "Training loss (for one batch) at step 20: 225.3281, Accuracy: 0.6536\n",
      "Training loss (for one batch) at step 30: 227.2661, Accuracy: 0.7160\n",
      "Training loss (for one batch) at step 40: 221.7478, Accuracy: 0.7510\n",
      "Training loss (for one batch) at step 50: 221.2964, Accuracy: 0.7750\n",
      "Training loss (for one batch) at step 60: 253.2220, Accuracy: 0.7879\n",
      "Training loss (for one batch) at step 70: 256.9545, Accuracy: 0.7713\n",
      "Training loss (for one batch) at step 80: 265.6693, Accuracy: 0.7497\n",
      "Training loss (for one batch) at step 90: 224.4458, Accuracy: 0.7462\n",
      "Training loss (for one batch) at step 100: 242.9426, Accuracy: 0.7553\n",
      "Training loss (for one batch) at step 110: 254.4086, Accuracy: 0.7562\n",
      "---- Training ----\n",
      "Training loss: 102.9155\n",
      "Training acc over epoch: 0.7549\n",
      "---- Validation ----\n",
      "Validation loss: 37.7478\n",
      "Validation acc: 0.6886\n",
      "Time taken: 18.07s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 282.6396, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 269.0221, Accuracy: 0.5874\n",
      "Training loss (for one batch) at step 20: 233.6792, Accuracy: 0.6533\n",
      "Training loss (for one batch) at step 30: 250.1725, Accuracy: 0.7160\n",
      "Training loss (for one batch) at step 40: 224.6614, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 50: 228.7979, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 60: 230.6396, Accuracy: 0.7909\n",
      "Training loss (for one batch) at step 70: 263.3504, Accuracy: 0.7748\n",
      "Training loss (for one batch) at step 80: 263.0672, Accuracy: 0.7564\n",
      "Training loss (for one batch) at step 90: 241.7846, Accuracy: 0.7516\n",
      "Training loss (for one batch) at step 100: 243.6910, Accuracy: 0.7596\n",
      "Training loss (for one batch) at step 110: 244.2185, Accuracy: 0.7618\n",
      "---- Training ----\n",
      "Training loss: 75.8892\n",
      "Training acc over epoch: 0.7595\n",
      "---- Validation ----\n",
      "Validation loss: 68.9330\n",
      "Validation acc: 0.7020\n",
      "Time taken: 17.89s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 265.2086, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 252.6167, Accuracy: 0.5881\n",
      "Training loss (for one batch) at step 20: 253.0890, Accuracy: 0.6536\n",
      "Training loss (for one batch) at step 30: 230.6691, Accuracy: 0.7160\n",
      "Training loss (for one batch) at step 40: 224.0461, Accuracy: 0.7540\n",
      "Training loss (for one batch) at step 50: 230.7354, Accuracy: 0.7779\n",
      "Training loss (for one batch) at step 60: 239.5332, Accuracy: 0.7923\n",
      "Training loss (for one batch) at step 70: 283.5790, Accuracy: 0.7740\n",
      "Training loss (for one batch) at step 80: 262.3965, Accuracy: 0.7541\n",
      "Training loss (for one batch) at step 90: 229.3955, Accuracy: 0.7509\n",
      "Training loss (for one batch) at step 100: 235.3092, Accuracy: 0.7591\n",
      "Training loss (for one batch) at step 110: 226.8613, Accuracy: 0.7612\n",
      "---- Training ----\n",
      "Training loss: 100.4589\n",
      "Training acc over epoch: 0.7591\n",
      "---- Validation ----\n",
      "Validation loss: 39.3747\n",
      "Validation acc: 0.7020\n",
      "Time taken: 17.87s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 266.6211, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 251.2736, Accuracy: 0.6051\n",
      "Training loss (for one batch) at step 20: 240.4341, Accuracy: 0.6559\n",
      "Training loss (for one batch) at step 30: 222.6817, Accuracy: 0.7182\n",
      "Training loss (for one batch) at step 40: 220.3866, Accuracy: 0.7540\n",
      "Training loss (for one batch) at step 50: 227.5781, Accuracy: 0.7780\n",
      "Training loss (for one batch) at step 60: 235.5286, Accuracy: 0.7906\n",
      "Training loss (for one batch) at step 70: 265.6701, Accuracy: 0.7740\n",
      "Training loss (for one batch) at step 80: 259.7739, Accuracy: 0.7532\n",
      "Training loss (for one batch) at step 90: 242.4371, Accuracy: 0.7501\n",
      "Training loss (for one batch) at step 100: 224.4774, Accuracy: 0.7581\n",
      "Training loss (for one batch) at step 110: 247.0674, Accuracy: 0.7608\n",
      "---- Training ----\n",
      "Training loss: 74.5702\n",
      "Training acc over epoch: 0.7591\n",
      "---- Validation ----\n",
      "Validation loss: 46.0403\n",
      "Validation acc: 0.6757\n",
      "Time taken: 17.87s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 268.0834, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 265.4402, Accuracy: 0.5909\n",
      "Training loss (for one batch) at step 20: 233.5982, Accuracy: 0.6611\n",
      "Training loss (for one batch) at step 30: 260.3870, Accuracy: 0.7182\n",
      "Training loss (for one batch) at step 40: 218.7077, Accuracy: 0.7559\n",
      "Training loss (for one batch) at step 50: 246.2291, Accuracy: 0.7768\n",
      "Training loss (for one batch) at step 60: 219.5397, Accuracy: 0.7907\n",
      "Training loss (for one batch) at step 70: 268.9104, Accuracy: 0.7745\n",
      "Training loss (for one batch) at step 80: 283.4183, Accuracy: 0.7535\n",
      "Training loss (for one batch) at step 90: 236.0189, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 100: 247.6425, Accuracy: 0.7573\n",
      "Training loss (for one batch) at step 110: 272.0240, Accuracy: 0.7588\n",
      "---- Training ----\n",
      "Training loss: 75.3226\n",
      "Training acc over epoch: 0.7575\n",
      "---- Validation ----\n",
      "Validation loss: 45.3526\n",
      "Validation acc: 0.6977\n",
      "Time taken: 17.88s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 262.2217, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 264.0406, Accuracy: 0.5838\n",
      "Training loss (for one batch) at step 20: 240.9380, Accuracy: 0.6626\n",
      "Training loss (for one batch) at step 30: 218.0447, Accuracy: 0.7185\n",
      "Training loss (for one batch) at step 40: 224.5430, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 50: 227.6314, Accuracy: 0.7786\n",
      "Training loss (for one batch) at step 60: 222.7316, Accuracy: 0.7918\n",
      "Training loss (for one batch) at step 70: 259.8675, Accuracy: 0.7753\n",
      "Training loss (for one batch) at step 80: 273.5318, Accuracy: 0.7536\n",
      "Training loss (for one batch) at step 90: 234.5644, Accuracy: 0.7522\n",
      "Training loss (for one batch) at step 100: 223.2193, Accuracy: 0.7594\n",
      "Training loss (for one batch) at step 110: 262.0630, Accuracy: 0.7616\n",
      "---- Training ----\n",
      "Training loss: 90.4268\n",
      "Training acc over epoch: 0.7604\n",
      "---- Validation ----\n",
      "Validation loss: 51.8862\n",
      "Validation acc: 0.6913\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 271.9465, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 260.0037, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 20: 232.8374, Accuracy: 0.6667\n",
      "Training loss (for one batch) at step 30: 229.6485, Accuracy: 0.7210\n",
      "Training loss (for one batch) at step 40: 225.7232, Accuracy: 0.7548\n",
      "Training loss (for one batch) at step 50: 230.1101, Accuracy: 0.7780\n",
      "Training loss (for one batch) at step 60: 237.9876, Accuracy: 0.7907\n",
      "Training loss (for one batch) at step 70: 239.2544, Accuracy: 0.7751\n",
      "Training loss (for one batch) at step 80: 250.0678, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 90: 237.3150, Accuracy: 0.7507\n",
      "Training loss (for one batch) at step 100: 239.0261, Accuracy: 0.7589\n",
      "Training loss (for one batch) at step 110: 233.1837, Accuracy: 0.7606\n",
      "---- Training ----\n",
      "Training loss: 79.2605\n",
      "Training acc over epoch: 0.7579\n",
      "---- Validation ----\n",
      "Validation loss: 67.2475\n",
      "Validation acc: 0.7047\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 279.4760, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 272.7727, Accuracy: 0.5810\n",
      "Training loss (for one batch) at step 20: 243.8172, Accuracy: 0.6503\n",
      "Training loss (for one batch) at step 30: 215.2382, Accuracy: 0.7152\n",
      "Training loss (for one batch) at step 40: 233.9721, Accuracy: 0.7525\n",
      "Training loss (for one batch) at step 50: 224.3721, Accuracy: 0.7797\n",
      "Training loss (for one batch) at step 60: 243.4397, Accuracy: 0.7884\n",
      "Training loss (for one batch) at step 70: 265.1184, Accuracy: 0.7716\n",
      "Training loss (for one batch) at step 80: 257.3752, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 90: 234.2309, Accuracy: 0.7512\n",
      "Training loss (for one batch) at step 100: 240.3908, Accuracy: 0.7579\n",
      "Training loss (for one batch) at step 110: 238.5938, Accuracy: 0.7597\n",
      "---- Training ----\n",
      "Training loss: 85.9209\n",
      "Training acc over epoch: 0.7575\n",
      "---- Validation ----\n",
      "Validation loss: 58.0007\n",
      "Validation acc: 0.6897\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 274.6122, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 319.0196, Accuracy: 0.6058\n",
      "Training loss (for one batch) at step 20: 241.2033, Accuracy: 0.6656\n",
      "Training loss (for one batch) at step 30: 230.0308, Accuracy: 0.7182\n",
      "Training loss (for one batch) at step 40: 233.1615, Accuracy: 0.7544\n",
      "Training loss (for one batch) at step 50: 231.0479, Accuracy: 0.7774\n",
      "Training loss (for one batch) at step 60: 237.1804, Accuracy: 0.7920\n",
      "Training loss (for one batch) at step 70: 248.4392, Accuracy: 0.7750\n",
      "Training loss (for one batch) at step 80: 243.2501, Accuracy: 0.7558\n",
      "Training loss (for one batch) at step 90: 226.6284, Accuracy: 0.7533\n",
      "Training loss (for one batch) at step 100: 234.9059, Accuracy: 0.7604\n",
      "Training loss (for one batch) at step 110: 232.7316, Accuracy: 0.7623\n",
      "---- Training ----\n",
      "Training loss: 81.0420\n",
      "Training acc over epoch: 0.7595\n",
      "---- Validation ----\n",
      "Validation loss: 36.9711\n",
      "Validation acc: 0.7198\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 300.2325, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 269.9193, Accuracy: 0.5980\n",
      "Training loss (for one batch) at step 20: 235.7854, Accuracy: 0.6581\n",
      "Training loss (for one batch) at step 30: 214.0842, Accuracy: 0.7240\n",
      "Training loss (for one batch) at step 40: 242.6162, Accuracy: 0.7597\n",
      "Training loss (for one batch) at step 50: 221.2401, Accuracy: 0.7799\n",
      "Training loss (for one batch) at step 60: 234.6888, Accuracy: 0.7906\n",
      "Training loss (for one batch) at step 70: 261.9821, Accuracy: 0.7754\n",
      "Training loss (for one batch) at step 80: 256.6243, Accuracy: 0.7548\n",
      "Training loss (for one batch) at step 90: 237.4068, Accuracy: 0.7511\n",
      "Training loss (for one batch) at step 100: 222.1934, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 110: 251.1324, Accuracy: 0.7603\n",
      "---- Training ----\n",
      "Training loss: 87.5055\n",
      "Training acc over epoch: 0.7588\n",
      "---- Validation ----\n",
      "Validation loss: 46.7568\n",
      "Validation acc: 0.6969\n",
      "Time taken: 17.89s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 262.5441, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 252.4584, Accuracy: 0.5817\n",
      "Training loss (for one batch) at step 20: 238.4692, Accuracy: 0.6589\n",
      "Training loss (for one batch) at step 30: 208.9058, Accuracy: 0.7152\n",
      "Training loss (for one batch) at step 40: 235.7703, Accuracy: 0.7511\n",
      "Training loss (for one batch) at step 50: 224.3781, Accuracy: 0.7767\n",
      "Training loss (for one batch) at step 60: 228.1419, Accuracy: 0.7919\n",
      "Training loss (for one batch) at step 70: 239.1679, Accuracy: 0.7743\n",
      "Training loss (for one batch) at step 80: 260.6178, Accuracy: 0.7537\n",
      "Training loss (for one batch) at step 90: 236.0240, Accuracy: 0.7510\n",
      "Training loss (for one batch) at step 100: 234.8109, Accuracy: 0.7589\n",
      "Training loss (for one batch) at step 110: 252.3489, Accuracy: 0.7609\n",
      "---- Training ----\n",
      "Training loss: 82.1008\n",
      "Training acc over epoch: 0.7582\n",
      "---- Validation ----\n",
      "Validation loss: 41.7522\n",
      "Validation acc: 0.7023\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 273.3339, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 238.7514, Accuracy: 0.5817\n",
      "Training loss (for one batch) at step 20: 244.2103, Accuracy: 0.6566\n",
      "Training loss (for one batch) at step 30: 227.5961, Accuracy: 0.7182\n",
      "Training loss (for one batch) at step 40: 218.0817, Accuracy: 0.7546\n",
      "Training loss (for one batch) at step 50: 215.8135, Accuracy: 0.7786\n",
      "Training loss (for one batch) at step 60: 240.2670, Accuracy: 0.7924\n",
      "Training loss (for one batch) at step 70: 262.0395, Accuracy: 0.7776\n",
      "Training loss (for one batch) at step 80: 272.3711, Accuracy: 0.7598\n",
      "Training loss (for one batch) at step 90: 230.5665, Accuracy: 0.7549\n",
      "Training loss (for one batch) at step 100: 217.7750, Accuracy: 0.7610\n",
      "Training loss (for one batch) at step 110: 257.0195, Accuracy: 0.7624\n",
      "---- Training ----\n",
      "Training loss: 72.8575\n",
      "Training acc over epoch: 0.7604\n",
      "---- Validation ----\n",
      "Validation loss: 60.8189\n",
      "Validation acc: 0.6886\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 272.2483, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 261.7660, Accuracy: 0.5838\n",
      "Training loss (for one batch) at step 20: 238.9180, Accuracy: 0.6581\n",
      "Training loss (for one batch) at step 30: 227.7195, Accuracy: 0.7190\n",
      "Training loss (for one batch) at step 40: 227.4873, Accuracy: 0.7530\n",
      "Training loss (for one batch) at step 50: 220.1305, Accuracy: 0.7765\n",
      "Training loss (for one batch) at step 60: 239.2099, Accuracy: 0.7914\n",
      "Training loss (for one batch) at step 70: 256.9613, Accuracy: 0.7751\n",
      "Training loss (for one batch) at step 80: 236.3484, Accuracy: 0.7546\n",
      "Training loss (for one batch) at step 90: 229.5671, Accuracy: 0.7503\n",
      "Training loss (for one batch) at step 100: 239.2509, Accuracy: 0.7591\n",
      "Training loss (for one batch) at step 110: 237.5845, Accuracy: 0.7596\n",
      "---- Training ----\n",
      "Training loss: 75.4170\n",
      "Training acc over epoch: 0.7575\n",
      "---- Validation ----\n",
      "Validation loss: 33.7707\n",
      "Validation acc: 0.6991\n",
      "Time taken: 17.84s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 272.5869, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 241.8777, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 20: 228.0482, Accuracy: 0.6544\n",
      "Training loss (for one batch) at step 30: 239.5578, Accuracy: 0.7155\n",
      "Training loss (for one batch) at step 40: 225.9340, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 50: 230.9684, Accuracy: 0.7759\n",
      "Training loss (for one batch) at step 60: 258.4934, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 70: 249.7996, Accuracy: 0.7759\n",
      "Training loss (for one batch) at step 80: 252.6153, Accuracy: 0.7553\n",
      "Training loss (for one batch) at step 90: 230.6973, Accuracy: 0.7503\n",
      "Training loss (for one batch) at step 100: 231.9182, Accuracy: 0.7584\n",
      "Training loss (for one batch) at step 110: 247.5692, Accuracy: 0.7609\n",
      "---- Training ----\n",
      "Training loss: 96.7240\n",
      "Training acc over epoch: 0.7586\n",
      "---- Validation ----\n",
      "Validation loss: 62.8586\n",
      "Validation acc: 0.6969\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 268.7425, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 254.1510, Accuracy: 0.5895\n",
      "Training loss (for one batch) at step 20: 222.3377, Accuracy: 0.6529\n",
      "Training loss (for one batch) at step 30: 234.2851, Accuracy: 0.7162\n",
      "Training loss (for one batch) at step 40: 227.3602, Accuracy: 0.7544\n",
      "Training loss (for one batch) at step 50: 245.0499, Accuracy: 0.7782\n",
      "Training loss (for one batch) at step 60: 239.2595, Accuracy: 0.7907\n",
      "Training loss (for one batch) at step 70: 246.4738, Accuracy: 0.7767\n",
      "Training loss (for one batch) at step 80: 230.6737, Accuracy: 0.7576\n",
      "Training loss (for one batch) at step 90: 242.7492, Accuracy: 0.7509\n",
      "Training loss (for one batch) at step 100: 227.2276, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 110: 240.3668, Accuracy: 0.7587\n",
      "---- Training ----\n",
      "Training loss: 71.1092\n",
      "Training acc over epoch: 0.7581\n",
      "---- Validation ----\n",
      "Validation loss: 58.6404\n",
      "Validation acc: 0.6943\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 274.3424, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 255.5092, Accuracy: 0.5838\n",
      "Training loss (for one batch) at step 20: 233.2235, Accuracy: 0.6499\n",
      "Training loss (for one batch) at step 30: 226.0354, Accuracy: 0.7140\n",
      "Training loss (for one batch) at step 40: 240.5397, Accuracy: 0.7513\n",
      "Training loss (for one batch) at step 50: 220.8706, Accuracy: 0.7783\n",
      "Training loss (for one batch) at step 60: 235.2012, Accuracy: 0.7903\n",
      "Training loss (for one batch) at step 70: 246.4036, Accuracy: 0.7755\n",
      "Training loss (for one batch) at step 80: 253.3441, Accuracy: 0.7551\n",
      "Training loss (for one batch) at step 90: 229.0077, Accuracy: 0.7531\n",
      "Training loss (for one batch) at step 100: 223.3699, Accuracy: 0.7584\n",
      "Training loss (for one batch) at step 110: 231.1827, Accuracy: 0.7601\n",
      "---- Training ----\n",
      "Training loss: 82.7270\n",
      "Training acc over epoch: 0.7593\n",
      "---- Validation ----\n",
      "Validation loss: 40.4293\n",
      "Validation acc: 0.6924\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 277.7928, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 243.2879, Accuracy: 0.5810\n",
      "Training loss (for one batch) at step 20: 227.1537, Accuracy: 0.6518\n",
      "Training loss (for one batch) at step 30: 226.9726, Accuracy: 0.7135\n",
      "Training loss (for one batch) at step 40: 227.1484, Accuracy: 0.7510\n",
      "Training loss (for one batch) at step 50: 213.7053, Accuracy: 0.7776\n",
      "Training loss (for one batch) at step 60: 228.5203, Accuracy: 0.7912\n",
      "Training loss (for one batch) at step 70: 261.1671, Accuracy: 0.7753\n",
      "Training loss (for one batch) at step 80: 278.0566, Accuracy: 0.7549\n",
      "Training loss (for one batch) at step 90: 238.6668, Accuracy: 0.7502\n",
      "Training loss (for one batch) at step 100: 235.0415, Accuracy: 0.7584\n",
      "Training loss (for one batch) at step 110: 235.1027, Accuracy: 0.7612\n",
      "---- Training ----\n",
      "Training loss: 84.7445\n",
      "Training acc over epoch: 0.7595\n",
      "---- Validation ----\n",
      "Validation loss: 74.0621\n",
      "Validation acc: 0.7047\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 252.2701, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 268.1134, Accuracy: 0.5717\n",
      "Training loss (for one batch) at step 20: 221.7837, Accuracy: 0.6503\n",
      "Training loss (for one batch) at step 30: 218.7423, Accuracy: 0.7172\n",
      "Training loss (for one batch) at step 40: 221.4905, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 50: 233.4268, Accuracy: 0.7765\n",
      "Training loss (for one batch) at step 60: 236.6011, Accuracy: 0.7906\n",
      "Training loss (for one batch) at step 70: 258.8539, Accuracy: 0.7754\n",
      "Training loss (for one batch) at step 80: 242.9945, Accuracy: 0.7564\n",
      "Training loss (for one batch) at step 90: 245.9769, Accuracy: 0.7512\n",
      "Training loss (for one batch) at step 100: 223.8934, Accuracy: 0.7597\n",
      "Training loss (for one batch) at step 110: 222.2722, Accuracy: 0.7615\n",
      "---- Training ----\n",
      "Training loss: 94.9070\n",
      "Training acc over epoch: 0.7584\n",
      "---- Validation ----\n",
      "Validation loss: 47.7880\n",
      "Validation acc: 0.7023\n",
      "Time taken: 17.82s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 270.1138, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 271.2200, Accuracy: 0.5888\n",
      "Training loss (for one batch) at step 20: 243.6989, Accuracy: 0.6540\n",
      "Training loss (for one batch) at step 30: 228.7185, Accuracy: 0.7170\n",
      "Training loss (for one batch) at step 40: 230.1500, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 50: 211.6460, Accuracy: 0.7794\n",
      "Training loss (for one batch) at step 60: 225.4965, Accuracy: 0.7905\n",
      "Training loss (for one batch) at step 70: 248.5353, Accuracy: 0.7756\n",
      "Training loss (for one batch) at step 80: 267.2137, Accuracy: 0.7560\n",
      "Training loss (for one batch) at step 90: 218.4576, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 100: 219.3329, Accuracy: 0.7606\n",
      "Training loss (for one batch) at step 110: 232.6518, Accuracy: 0.7617\n",
      "---- Training ----\n",
      "Training loss: 80.9425\n",
      "Training acc over epoch: 0.7601\n",
      "---- Validation ----\n",
      "Validation loss: 32.1413\n",
      "Validation acc: 0.6964\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 248.7448, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 248.3168, Accuracy: 0.5845\n",
      "Training loss (for one batch) at step 20: 231.1703, Accuracy: 0.6548\n",
      "Training loss (for one batch) at step 30: 235.7837, Accuracy: 0.7193\n",
      "Training loss (for one batch) at step 40: 221.6597, Accuracy: 0.7544\n",
      "Training loss (for one batch) at step 50: 226.3116, Accuracy: 0.7774\n",
      "Training loss (for one batch) at step 60: 221.0506, Accuracy: 0.7912\n",
      "Training loss (for one batch) at step 70: 253.0853, Accuracy: 0.7767\n",
      "Training loss (for one batch) at step 80: 246.9280, Accuracy: 0.7562\n",
      "Training loss (for one batch) at step 90: 232.1941, Accuracy: 0.7527\n",
      "Training loss (for one batch) at step 100: 236.3333, Accuracy: 0.7604\n",
      "Training loss (for one batch) at step 110: 251.0790, Accuracy: 0.7618\n",
      "---- Training ----\n",
      "Training loss: 73.9147\n",
      "Training acc over epoch: 0.7601\n",
      "---- Validation ----\n",
      "Validation loss: 42.7194\n",
      "Validation acc: 0.6975\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 278.5327, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 252.3167, Accuracy: 0.5845\n",
      "Training loss (for one batch) at step 20: 238.9108, Accuracy: 0.6555\n",
      "Training loss (for one batch) at step 30: 226.4360, Accuracy: 0.7172\n",
      "Training loss (for one batch) at step 40: 230.5855, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 50: 231.6814, Accuracy: 0.7756\n",
      "Training loss (for one batch) at step 60: 222.0892, Accuracy: 0.7897\n",
      "Training loss (for one batch) at step 70: 253.7525, Accuracy: 0.7727\n",
      "Training loss (for one batch) at step 80: 251.9666, Accuracy: 0.7534\n",
      "Training loss (for one batch) at step 90: 229.4259, Accuracy: 0.7525\n",
      "Training loss (for one batch) at step 100: 231.6050, Accuracy: 0.7590\n",
      "Training loss (for one batch) at step 110: 260.9701, Accuracy: 0.7611\n",
      "---- Training ----\n",
      "Training loss: 76.8712\n",
      "Training acc over epoch: 0.7593\n",
      "---- Validation ----\n",
      "Validation loss: 37.9107\n",
      "Validation acc: 0.7034\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 289.9431, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 244.8470, Accuracy: 0.5795\n",
      "Training loss (for one batch) at step 20: 236.6278, Accuracy: 0.6503\n",
      "Training loss (for one batch) at step 30: 214.3251, Accuracy: 0.7132\n",
      "Training loss (for one batch) at step 40: 212.1476, Accuracy: 0.7490\n",
      "Training loss (for one batch) at step 50: 214.3102, Accuracy: 0.7750\n",
      "Training loss (for one batch) at step 60: 221.4281, Accuracy: 0.7894\n",
      "Training loss (for one batch) at step 70: 263.3816, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 80: 245.5026, Accuracy: 0.7530\n",
      "Training loss (for one batch) at step 90: 225.2590, Accuracy: 0.7490\n",
      "Training loss (for one batch) at step 100: 229.3589, Accuracy: 0.7567\n",
      "Training loss (for one batch) at step 110: 231.4541, Accuracy: 0.7587\n",
      "---- Training ----\n",
      "Training loss: 84.3790\n",
      "Training acc over epoch: 0.7570\n",
      "---- Validation ----\n",
      "Validation loss: 51.0982\n",
      "Validation acc: 0.6983\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 263.9451, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 254.3312, Accuracy: 0.5852\n",
      "Training loss (for one batch) at step 20: 234.8118, Accuracy: 0.6458\n",
      "Training loss (for one batch) at step 30: 215.9653, Accuracy: 0.7097\n",
      "Training loss (for one batch) at step 40: 222.0720, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 50: 209.4871, Accuracy: 0.7736\n",
      "Training loss (for one batch) at step 60: 215.2887, Accuracy: 0.7886\n",
      "Training loss (for one batch) at step 70: 250.8924, Accuracy: 0.7720\n",
      "Training loss (for one batch) at step 80: 234.8123, Accuracy: 0.7540\n",
      "Training loss (for one batch) at step 90: 239.9583, Accuracy: 0.7487\n",
      "Training loss (for one batch) at step 100: 218.4479, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 110: 243.8101, Accuracy: 0.7591\n",
      "---- Training ----\n",
      "Training loss: 72.9147\n",
      "Training acc over epoch: 0.7575\n",
      "---- Validation ----\n",
      "Validation loss: 66.9580\n",
      "Validation acc: 0.7037\n",
      "Time taken: 17.85s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 242.2914, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 242.0688, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 20: 227.0282, Accuracy: 0.6540\n",
      "Training loss (for one batch) at step 30: 247.8153, Accuracy: 0.7122\n",
      "Training loss (for one batch) at step 40: 224.1383, Accuracy: 0.7508\n",
      "Training loss (for one batch) at step 50: 216.5876, Accuracy: 0.7753\n",
      "Training loss (for one batch) at step 60: 231.5108, Accuracy: 0.7883\n",
      "Training loss (for one batch) at step 70: 268.9601, Accuracy: 0.7716\n",
      "Training loss (for one batch) at step 80: 279.1425, Accuracy: 0.7514\n",
      "Training loss (for one batch) at step 90: 220.3303, Accuracy: 0.7496\n",
      "Training loss (for one batch) at step 100: 237.0688, Accuracy: 0.7568\n",
      "Training loss (for one batch) at step 110: 237.2882, Accuracy: 0.7589\n",
      "---- Training ----\n",
      "Training loss: 70.7808\n",
      "Training acc over epoch: 0.7568\n",
      "---- Validation ----\n",
      "Validation loss: 63.1729\n",
      "Validation acc: 0.6924\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 254.1340, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 263.2478, Accuracy: 0.5930\n",
      "Training loss (for one batch) at step 20: 224.7098, Accuracy: 0.6533\n",
      "Training loss (for one batch) at step 30: 229.0142, Accuracy: 0.7167\n",
      "Training loss (for one batch) at step 40: 207.1361, Accuracy: 0.7530\n",
      "Training loss (for one batch) at step 50: 235.2151, Accuracy: 0.7757\n",
      "Training loss (for one batch) at step 60: 237.1951, Accuracy: 0.7892\n",
      "Training loss (for one batch) at step 70: 250.2535, Accuracy: 0.7741\n",
      "Training loss (for one batch) at step 80: 238.6145, Accuracy: 0.7564\n",
      "Training loss (for one batch) at step 90: 220.3558, Accuracy: 0.7509\n",
      "Training loss (for one batch) at step 100: 222.8042, Accuracy: 0.7584\n",
      "Training loss (for one batch) at step 110: 239.1663, Accuracy: 0.7608\n",
      "---- Training ----\n",
      "Training loss: 66.2586\n",
      "Training acc over epoch: 0.7588\n",
      "---- Validation ----\n",
      "Validation loss: 43.1759\n",
      "Validation acc: 0.6910\n",
      "Time taken: 18.08s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 253.4519, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 234.3602, Accuracy: 0.5831\n",
      "Training loss (for one batch) at step 20: 220.2620, Accuracy: 0.6555\n",
      "Training loss (for one batch) at step 30: 224.7528, Accuracy: 0.7170\n",
      "Training loss (for one batch) at step 40: 226.2796, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 50: 213.6249, Accuracy: 0.7797\n",
      "Training loss (for one batch) at step 60: 226.4140, Accuracy: 0.7934\n",
      "Training loss (for one batch) at step 70: 253.7236, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 80: 253.5408, Accuracy: 0.7549\n",
      "Training loss (for one batch) at step 90: 225.2536, Accuracy: 0.7502\n",
      "Training loss (for one batch) at step 100: 229.5262, Accuracy: 0.7574\n",
      "Training loss (for one batch) at step 110: 228.1804, Accuracy: 0.7585\n",
      "---- Training ----\n",
      "Training loss: 80.6085\n",
      "Training acc over epoch: 0.7572\n",
      "---- Validation ----\n",
      "Validation loss: 47.8486\n",
      "Validation acc: 0.6838\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 258.0446, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 258.8449, Accuracy: 0.5803\n",
      "Training loss (for one batch) at step 20: 215.2397, Accuracy: 0.6533\n",
      "Training loss (for one batch) at step 30: 226.5889, Accuracy: 0.7203\n",
      "Training loss (for one batch) at step 40: 212.8034, Accuracy: 0.7565\n",
      "Training loss (for one batch) at step 50: 223.1800, Accuracy: 0.7788\n",
      "Training loss (for one batch) at step 60: 223.1417, Accuracy: 0.7924\n",
      "Training loss (for one batch) at step 70: 226.6044, Accuracy: 0.7771\n",
      "Training loss (for one batch) at step 80: 256.7820, Accuracy: 0.7565\n",
      "Training loss (for one batch) at step 90: 213.1447, Accuracy: 0.7536\n",
      "Training loss (for one batch) at step 100: 243.7678, Accuracy: 0.7593\n",
      "Training loss (for one batch) at step 110: 250.3939, Accuracy: 0.7599\n",
      "---- Training ----\n",
      "Training loss: 95.7528\n",
      "Training acc over epoch: 0.7585\n",
      "---- Validation ----\n",
      "Validation loss: 42.6348\n",
      "Validation acc: 0.7050\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 262.6916, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 272.9091, Accuracy: 0.5803\n",
      "Training loss (for one batch) at step 20: 230.2245, Accuracy: 0.6600\n",
      "Training loss (for one batch) at step 30: 232.6577, Accuracy: 0.7165\n",
      "Training loss (for one batch) at step 40: 216.6836, Accuracy: 0.7536\n",
      "Training loss (for one batch) at step 50: 217.0861, Accuracy: 0.7777\n",
      "Training loss (for one batch) at step 60: 220.5606, Accuracy: 0.7907\n",
      "Training loss (for one batch) at step 70: 254.1327, Accuracy: 0.7749\n",
      "Training loss (for one batch) at step 80: 238.6496, Accuracy: 0.7558\n",
      "Training loss (for one batch) at step 90: 223.5963, Accuracy: 0.7527\n",
      "Training loss (for one batch) at step 100: 208.5614, Accuracy: 0.7599\n",
      "Training loss (for one batch) at step 110: 242.9626, Accuracy: 0.7616\n",
      "---- Training ----\n",
      "Training loss: 77.1590\n",
      "Training acc over epoch: 0.7587\n",
      "---- Validation ----\n",
      "Validation loss: 39.1862\n",
      "Validation acc: 0.6846\n",
      "Time taken: 18.07s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 265.2778, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 246.0358, Accuracy: 0.5689\n",
      "Training loss (for one batch) at step 20: 218.9103, Accuracy: 0.6473\n",
      "Training loss (for one batch) at step 30: 215.8332, Accuracy: 0.7112\n",
      "Training loss (for one batch) at step 40: 239.2715, Accuracy: 0.7494\n",
      "Training loss (for one batch) at step 50: 216.3142, Accuracy: 0.7760\n",
      "Training loss (for one batch) at step 60: 242.5580, Accuracy: 0.7892\n",
      "Training loss (for one batch) at step 70: 274.3314, Accuracy: 0.7724\n",
      "Training loss (for one batch) at step 80: 250.1472, Accuracy: 0.7527\n",
      "Training loss (for one batch) at step 90: 218.4138, Accuracy: 0.7491\n",
      "Training loss (for one batch) at step 100: 213.2018, Accuracy: 0.7573\n",
      "Training loss (for one batch) at step 110: 235.5386, Accuracy: 0.7599\n",
      "---- Training ----\n",
      "Training loss: 80.3686\n",
      "Training acc over epoch: 0.7576\n",
      "---- Validation ----\n",
      "Validation loss: 59.0796\n",
      "Validation acc: 0.6999\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 259.2634, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 259.9485, Accuracy: 0.5874\n",
      "Training loss (for one batch) at step 20: 215.9240, Accuracy: 0.6503\n",
      "Training loss (for one batch) at step 30: 220.9893, Accuracy: 0.7145\n",
      "Training loss (for one batch) at step 40: 221.1015, Accuracy: 0.7513\n",
      "Training loss (for one batch) at step 50: 220.4331, Accuracy: 0.7760\n",
      "Training loss (for one batch) at step 60: 227.3900, Accuracy: 0.7892\n",
      "Training loss (for one batch) at step 70: 235.1315, Accuracy: 0.7719\n",
      "Training loss (for one batch) at step 80: 246.8265, Accuracy: 0.7510\n",
      "Training loss (for one batch) at step 90: 224.9689, Accuracy: 0.7473\n",
      "Training loss (for one batch) at step 100: 219.2842, Accuracy: 0.7561\n",
      "Training loss (for one batch) at step 110: 234.2578, Accuracy: 0.7570\n",
      "---- Training ----\n",
      "Training loss: 77.9390\n",
      "Training acc over epoch: 0.7555\n",
      "---- Validation ----\n",
      "Validation loss: 44.2982\n",
      "Validation acc: 0.7157\n",
      "Time taken: 17.96s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.7087\n",
      "Validation AUC: 0.7106\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 628.1870, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 577.8608, Accuracy: 0.4901\n",
      "Training loss (for one batch) at step 20: 537.4642, Accuracy: 0.5045\n",
      "Training loss (for one batch) at step 30: 508.4905, Accuracy: 0.5093\n",
      "Training loss (for one batch) at step 40: 492.2657, Accuracy: 0.5053\n",
      "Training loss (for one batch) at step 50: 496.1004, Accuracy: 0.5051\n",
      "Training loss (for one batch) at step 60: 482.5640, Accuracy: 0.5069\n",
      "Training loss (for one batch) at step 70: 497.0506, Accuracy: 0.5076\n",
      "Training loss (for one batch) at step 80: 475.4393, Accuracy: 0.5081\n",
      "Training loss (for one batch) at step 90: 478.4634, Accuracy: 0.5064\n",
      "Training loss (for one batch) at step 100: 479.2246, Accuracy: 0.5052\n",
      "Training loss (for one batch) at step 110: 466.7251, Accuracy: 0.5047\n",
      "---- Training ----\n",
      "Training loss: 143.9710\n",
      "Training acc over epoch: 0.5054\n",
      "---- Validation ----\n",
      "Validation loss: 34.4623\n",
      "Validation acc: 0.5145\n",
      "Time taken: 19.37s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 466.4599, Accuracy: 0.4922\n",
      "Training loss (for one batch) at step 10: 464.3213, Accuracy: 0.5092\n",
      "Training loss (for one batch) at step 20: 456.6103, Accuracy: 0.5167\n",
      "Training loss (for one batch) at step 30: 458.6460, Accuracy: 0.5111\n",
      "Training loss (for one batch) at step 40: 453.6134, Accuracy: 0.5034\n",
      "Training loss (for one batch) at step 50: 448.8364, Accuracy: 0.5041\n",
      "Training loss (for one batch) at step 60: 450.8824, Accuracy: 0.5076\n",
      "Training loss (for one batch) at step 70: 455.6190, Accuracy: 0.5072\n",
      "Training loss (for one batch) at step 80: 450.3237, Accuracy: 0.5104\n",
      "Training loss (for one batch) at step 90: 448.0472, Accuracy: 0.5082\n",
      "Training loss (for one batch) at step 100: 446.5055, Accuracy: 0.5053\n",
      "Training loss (for one batch) at step 110: 447.6706, Accuracy: 0.5064\n",
      "---- Training ----\n",
      "Training loss: 141.0688\n",
      "Training acc over epoch: 0.5077\n",
      "---- Validation ----\n",
      "Validation loss: 35.0643\n",
      "Validation acc: 0.4987\n",
      "Time taken: 20.21s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 446.3248, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 448.5602, Accuracy: 0.5497\n",
      "Training loss (for one batch) at step 20: 442.5760, Accuracy: 0.5450\n",
      "Training loss (for one batch) at step 30: 444.9119, Accuracy: 0.5396\n",
      "Training loss (for one batch) at step 40: 442.2939, Accuracy: 0.5333\n",
      "Training loss (for one batch) at step 50: 445.9219, Accuracy: 0.5351\n",
      "Training loss (for one batch) at step 60: 446.7229, Accuracy: 0.5342\n",
      "Training loss (for one batch) at step 70: 445.1721, Accuracy: 0.5380\n",
      "Training loss (for one batch) at step 80: 450.9785, Accuracy: 0.5367\n",
      "Training loss (for one batch) at step 90: 443.9023, Accuracy: 0.5357\n",
      "Training loss (for one batch) at step 100: 446.4006, Accuracy: 0.5336\n",
      "Training loss (for one batch) at step 110: 446.1420, Accuracy: 0.5352\n",
      "---- Training ----\n",
      "Training loss: 140.0317\n",
      "Training acc over epoch: 0.5343\n",
      "---- Validation ----\n",
      "Validation loss: 34.6721\n",
      "Validation acc: 0.5441\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 444.1993, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 441.6421, Accuracy: 0.5327\n",
      "Training loss (for one batch) at step 20: 441.5656, Accuracy: 0.5450\n",
      "Training loss (for one batch) at step 30: 442.4161, Accuracy: 0.5474\n",
      "Training loss (for one batch) at step 40: 443.3288, Accuracy: 0.5440\n",
      "Training loss (for one batch) at step 50: 442.2935, Accuracy: 0.5453\n",
      "Training loss (for one batch) at step 60: 441.7515, Accuracy: 0.5512\n",
      "Training loss (for one batch) at step 70: 441.6042, Accuracy: 0.5548\n",
      "Training loss (for one batch) at step 80: 442.4065, Accuracy: 0.5545\n",
      "Training loss (for one batch) at step 90: 441.3673, Accuracy: 0.5514\n",
      "Training loss (for one batch) at step 100: 446.1498, Accuracy: 0.5505\n",
      "Training loss (for one batch) at step 110: 444.1264, Accuracy: 0.5505\n",
      "---- Training ----\n",
      "Training loss: 137.6899\n",
      "Training acc over epoch: 0.5514\n",
      "---- Validation ----\n",
      "Validation loss: 35.1643\n",
      "Validation acc: 0.5594\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 439.7766, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 441.2037, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 20: 441.7169, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 30: 441.9386, Accuracy: 0.5628\n",
      "Training loss (for one batch) at step 40: 437.7101, Accuracy: 0.5655\n",
      "Training loss (for one batch) at step 50: 443.7916, Accuracy: 0.5692\n",
      "Training loss (for one batch) at step 60: 441.8290, Accuracy: 0.5749\n",
      "Training loss (for one batch) at step 70: 440.5731, Accuracy: 0.5748\n",
      "Training loss (for one batch) at step 80: 440.9348, Accuracy: 0.5723\n",
      "Training loss (for one batch) at step 90: 445.1097, Accuracy: 0.5683\n",
      "Training loss (for one batch) at step 100: 440.6802, Accuracy: 0.5640\n",
      "Training loss (for one batch) at step 110: 441.8204, Accuracy: 0.5640\n",
      "---- Training ----\n",
      "Training loss: 138.0804\n",
      "Training acc over epoch: 0.5651\n",
      "---- Validation ----\n",
      "Validation loss: 34.1495\n",
      "Validation acc: 0.5443\n",
      "Time taken: 17.88s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 443.0033, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 442.2748, Accuracy: 0.5476\n",
      "Training loss (for one batch) at step 20: 442.0699, Accuracy: 0.5424\n",
      "Training loss (for one batch) at step 30: 443.9391, Accuracy: 0.5474\n",
      "Training loss (for one batch) at step 40: 443.0994, Accuracy: 0.5484\n",
      "Training loss (for one batch) at step 50: 442.5434, Accuracy: 0.5602\n",
      "Training loss (for one batch) at step 60: 440.9104, Accuracy: 0.5662\n",
      "Training loss (for one batch) at step 70: 437.8589, Accuracy: 0.5690\n",
      "Training loss (for one batch) at step 80: 441.9034, Accuracy: 0.5673\n",
      "Training loss (for one batch) at step 90: 440.2565, Accuracy: 0.5601\n",
      "Training loss (for one batch) at step 100: 435.1013, Accuracy: 0.5595\n",
      "Training loss (for one batch) at step 110: 441.0374, Accuracy: 0.5615\n",
      "---- Training ----\n",
      "Training loss: 136.3261\n",
      "Training acc over epoch: 0.5622\n",
      "---- Validation ----\n",
      "Validation loss: 34.4675\n",
      "Validation acc: 0.5645\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 440.7213, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 440.7647, Accuracy: 0.5554\n",
      "Training loss (for one batch) at step 20: 439.8607, Accuracy: 0.5346\n",
      "Training loss (for one batch) at step 30: 436.2892, Accuracy: 0.5358\n",
      "Training loss (for one batch) at step 40: 442.9476, Accuracy: 0.5347\n",
      "Training loss (for one batch) at step 50: 435.8693, Accuracy: 0.5447\n",
      "Training loss (for one batch) at step 60: 440.1483, Accuracy: 0.5538\n",
      "Training loss (for one batch) at step 70: 437.0695, Accuracy: 0.5597\n",
      "Training loss (for one batch) at step 80: 438.6964, Accuracy: 0.5575\n",
      "Training loss (for one batch) at step 90: 440.8797, Accuracy: 0.5526\n",
      "Training loss (for one batch) at step 100: 435.0607, Accuracy: 0.5542\n",
      "Training loss (for one batch) at step 110: 438.8647, Accuracy: 0.5572\n",
      "---- Training ----\n",
      "Training loss: 137.9112\n",
      "Training acc over epoch: 0.5568\n",
      "---- Validation ----\n",
      "Validation loss: 34.8611\n",
      "Validation acc: 0.5567\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 439.3365, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 438.9624, Accuracy: 0.5433\n",
      "Training loss (for one batch) at step 20: 439.0934, Accuracy: 0.5272\n",
      "Training loss (for one batch) at step 30: 434.0570, Accuracy: 0.5307\n",
      "Training loss (for one batch) at step 40: 441.4143, Accuracy: 0.5419\n",
      "Training loss (for one batch) at step 50: 431.4881, Accuracy: 0.5486\n",
      "Training loss (for one batch) at step 60: 440.8983, Accuracy: 0.5575\n",
      "Training loss (for one batch) at step 70: 434.0753, Accuracy: 0.5632\n",
      "Training loss (for one batch) at step 80: 437.7971, Accuracy: 0.5618\n",
      "Training loss (for one batch) at step 90: 438.9747, Accuracy: 0.5576\n",
      "Training loss (for one batch) at step 100: 437.1793, Accuracy: 0.5586\n",
      "Training loss (for one batch) at step 110: 430.0854, Accuracy: 0.5598\n",
      "---- Training ----\n",
      "Training loss: 139.1108\n",
      "Training acc over epoch: 0.5604\n",
      "---- Validation ----\n",
      "Validation loss: 35.7773\n",
      "Validation acc: 0.5637\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 438.1644, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 442.2109, Accuracy: 0.5526\n",
      "Training loss (for one batch) at step 20: 438.4295, Accuracy: 0.5454\n",
      "Training loss (for one batch) at step 30: 436.4567, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 40: 435.5112, Accuracy: 0.5566\n",
      "Training loss (for one batch) at step 50: 432.7837, Accuracy: 0.5616\n",
      "Training loss (for one batch) at step 60: 436.1439, Accuracy: 0.5666\n",
      "Training loss (for one batch) at step 70: 430.3001, Accuracy: 0.5688\n",
      "Training loss (for one batch) at step 80: 437.4237, Accuracy: 0.5664\n",
      "Training loss (for one batch) at step 90: 433.9362, Accuracy: 0.5655\n",
      "Training loss (for one batch) at step 100: 432.9941, Accuracy: 0.5637\n",
      "Training loss (for one batch) at step 110: 439.6811, Accuracy: 0.5652\n",
      "---- Training ----\n",
      "Training loss: 134.6786\n",
      "Training acc over epoch: 0.5637\n",
      "---- Validation ----\n",
      "Validation loss: 33.3766\n",
      "Validation acc: 0.5661\n",
      "Time taken: 18.32s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 440.4602, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 432.4933, Accuracy: 0.5604\n",
      "Training loss (for one batch) at step 20: 439.2072, Accuracy: 0.5435\n",
      "Training loss (for one batch) at step 30: 427.6151, Accuracy: 0.5507\n",
      "Training loss (for one batch) at step 40: 431.0605, Accuracy: 0.5570\n",
      "Training loss (for one batch) at step 50: 426.4264, Accuracy: 0.5656\n",
      "Training loss (for one batch) at step 60: 429.3774, Accuracy: 0.5757\n",
      "Training loss (for one batch) at step 70: 436.4327, Accuracy: 0.5756\n",
      "Training loss (for one batch) at step 80: 437.2789, Accuracy: 0.5723\n",
      "Training loss (for one batch) at step 90: 434.9770, Accuracy: 0.5680\n",
      "Training loss (for one batch) at step 100: 431.3277, Accuracy: 0.5675\n",
      "Training loss (for one batch) at step 110: 435.0571, Accuracy: 0.5695\n",
      "---- Training ----\n",
      "Training loss: 133.5861\n",
      "Training acc over epoch: 0.5688\n",
      "---- Validation ----\n",
      "Validation loss: 34.0331\n",
      "Validation acc: 0.5768\n",
      "Time taken: 17.86s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 435.3380, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 441.4569, Accuracy: 0.5639\n",
      "Training loss (for one batch) at step 20: 433.6198, Accuracy: 0.5573\n",
      "Training loss (for one batch) at step 30: 435.6158, Accuracy: 0.5615\n",
      "Training loss (for one batch) at step 40: 421.1612, Accuracy: 0.5720\n",
      "Training loss (for one batch) at step 50: 424.7437, Accuracy: 0.5797\n",
      "Training loss (for one batch) at step 60: 432.7677, Accuracy: 0.5868\n",
      "Training loss (for one batch) at step 70: 429.5600, Accuracy: 0.5865\n",
      "Training loss (for one batch) at step 80: 433.9315, Accuracy: 0.5854\n",
      "Training loss (for one batch) at step 90: 432.0373, Accuracy: 0.5822\n",
      "Training loss (for one batch) at step 100: 430.8538, Accuracy: 0.5788\n",
      "Training loss (for one batch) at step 110: 429.4142, Accuracy: 0.5801\n",
      "---- Training ----\n",
      "Training loss: 135.6126\n",
      "Training acc over epoch: 0.5805\n",
      "---- Validation ----\n",
      "Validation loss: 34.5484\n",
      "Validation acc: 0.5712\n",
      "Time taken: 17.89s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 438.9341, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 431.4460, Accuracy: 0.5568\n",
      "Training loss (for one batch) at step 20: 432.5616, Accuracy: 0.5569\n",
      "Training loss (for one batch) at step 30: 425.9463, Accuracy: 0.5660\n",
      "Training loss (for one batch) at step 40: 416.2158, Accuracy: 0.5776\n",
      "Training loss (for one batch) at step 50: 425.0339, Accuracy: 0.5872\n",
      "Training loss (for one batch) at step 60: 431.1123, Accuracy: 0.5941\n",
      "Training loss (for one batch) at step 70: 430.2853, Accuracy: 0.5975\n",
      "Training loss (for one batch) at step 80: 427.5608, Accuracy: 0.5931\n",
      "Training loss (for one batch) at step 90: 431.3695, Accuracy: 0.5886\n",
      "Training loss (for one batch) at step 100: 429.8353, Accuracy: 0.5870\n",
      "Training loss (for one batch) at step 110: 434.9936, Accuracy: 0.5857\n",
      "---- Training ----\n",
      "Training loss: 138.9640\n",
      "Training acc over epoch: 0.5851\n",
      "---- Validation ----\n",
      "Validation loss: 34.9525\n",
      "Validation acc: 0.5758\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 441.5058, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 427.6474, Accuracy: 0.5838\n",
      "Training loss (for one batch) at step 20: 435.4870, Accuracy: 0.5755\n",
      "Training loss (for one batch) at step 30: 424.9563, Accuracy: 0.5814\n",
      "Training loss (for one batch) at step 40: 411.0350, Accuracy: 0.5913\n",
      "Training loss (for one batch) at step 50: 412.1657, Accuracy: 0.5987\n",
      "Training loss (for one batch) at step 60: 422.5612, Accuracy: 0.6040\n",
      "Training loss (for one batch) at step 70: 431.5033, Accuracy: 0.6057\n",
      "Training loss (for one batch) at step 80: 423.3547, Accuracy: 0.6014\n",
      "Training loss (for one batch) at step 90: 427.0395, Accuracy: 0.5976\n",
      "Training loss (for one batch) at step 100: 425.6946, Accuracy: 0.5978\n",
      "Training loss (for one batch) at step 110: 430.5810, Accuracy: 0.5980\n",
      "---- Training ----\n",
      "Training loss: 134.6302\n",
      "Training acc over epoch: 0.5979\n",
      "---- Validation ----\n",
      "Validation loss: 37.9500\n",
      "Validation acc: 0.5760\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 432.7773, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 427.3686, Accuracy: 0.5945\n",
      "Training loss (for one batch) at step 20: 430.5325, Accuracy: 0.5800\n",
      "Training loss (for one batch) at step 30: 426.1495, Accuracy: 0.5912\n",
      "Training loss (for one batch) at step 40: 421.8648, Accuracy: 0.6021\n",
      "Training loss (for one batch) at step 50: 410.1848, Accuracy: 0.6146\n",
      "Training loss (for one batch) at step 60: 413.7523, Accuracy: 0.6215\n",
      "Training loss (for one batch) at step 70: 432.2207, Accuracy: 0.6257\n",
      "Training loss (for one batch) at step 80: 433.8507, Accuracy: 0.6189\n",
      "Training loss (for one batch) at step 90: 408.2644, Accuracy: 0.6146\n",
      "Training loss (for one batch) at step 100: 410.2666, Accuracy: 0.6114\n",
      "Training loss (for one batch) at step 110: 426.2766, Accuracy: 0.6098\n",
      "---- Training ----\n",
      "Training loss: 135.7319\n",
      "Training acc over epoch: 0.6093\n",
      "---- Validation ----\n",
      "Validation loss: 35.1087\n",
      "Validation acc: 0.5852\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 436.7058, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 418.2944, Accuracy: 0.5717\n",
      "Training loss (for one batch) at step 20: 418.0847, Accuracy: 0.5815\n",
      "Training loss (for one batch) at step 30: 410.1020, Accuracy: 0.5925\n",
      "Training loss (for one batch) at step 40: 408.5846, Accuracy: 0.6004\n",
      "Training loss (for one batch) at step 50: 401.1089, Accuracy: 0.6163\n",
      "Training loss (for one batch) at step 60: 394.0983, Accuracy: 0.6269\n",
      "Training loss (for one batch) at step 70: 418.2474, Accuracy: 0.6263\n",
      "Training loss (for one batch) at step 80: 411.4647, Accuracy: 0.6214\n",
      "Training loss (for one batch) at step 90: 418.4925, Accuracy: 0.6168\n",
      "Training loss (for one batch) at step 100: 419.1735, Accuracy: 0.6133\n",
      "Training loss (for one batch) at step 110: 414.2538, Accuracy: 0.6143\n",
      "---- Training ----\n",
      "Training loss: 133.4931\n",
      "Training acc over epoch: 0.6134\n",
      "---- Validation ----\n",
      "Validation loss: 34.9141\n",
      "Validation acc: 0.5913\n",
      "Time taken: 20.15s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 417.8931, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 414.6938, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 20: 420.9630, Accuracy: 0.5889\n",
      "Training loss (for one batch) at step 30: 401.0386, Accuracy: 0.5968\n",
      "Training loss (for one batch) at step 40: 408.7202, Accuracy: 0.6084\n",
      "Training loss (for one batch) at step 50: 385.6964, Accuracy: 0.6206\n",
      "Training loss (for one batch) at step 60: 397.0320, Accuracy: 0.6277\n",
      "Training loss (for one batch) at step 70: 417.6255, Accuracy: 0.6305\n",
      "Training loss (for one batch) at step 80: 411.7776, Accuracy: 0.6222\n",
      "Training loss (for one batch) at step 90: 396.8036, Accuracy: 0.6187\n",
      "Training loss (for one batch) at step 100: 389.6045, Accuracy: 0.6192\n",
      "Training loss (for one batch) at step 110: 410.6795, Accuracy: 0.6198\n",
      "---- Training ----\n",
      "Training loss: 130.9907\n",
      "Training acc over epoch: 0.6206\n",
      "---- Validation ----\n",
      "Validation loss: 34.1113\n",
      "Validation acc: 0.6048\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 424.3797, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 399.0134, Accuracy: 0.5923\n",
      "Training loss (for one batch) at step 20: 398.3903, Accuracy: 0.6031\n",
      "Training loss (for one batch) at step 30: 399.7502, Accuracy: 0.6144\n",
      "Training loss (for one batch) at step 40: 373.0673, Accuracy: 0.6244\n",
      "Training loss (for one batch) at step 50: 381.9762, Accuracy: 0.6313\n",
      "Training loss (for one batch) at step 60: 382.5156, Accuracy: 0.6419\n",
      "Training loss (for one batch) at step 70: 419.1259, Accuracy: 0.6379\n",
      "Training loss (for one batch) at step 80: 406.8192, Accuracy: 0.6312\n",
      "Training loss (for one batch) at step 90: 401.5322, Accuracy: 0.6270\n",
      "Training loss (for one batch) at step 100: 393.5050, Accuracy: 0.6293\n",
      "Training loss (for one batch) at step 110: 407.8337, Accuracy: 0.6294\n",
      "---- Training ----\n",
      "Training loss: 120.2335\n",
      "Training acc over epoch: 0.6288\n",
      "---- Validation ----\n",
      "Validation loss: 35.6376\n",
      "Validation acc: 0.5870\n",
      "Time taken: 17.87s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 432.1748, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 407.5310, Accuracy: 0.5661\n",
      "Training loss (for one batch) at step 20: 386.8508, Accuracy: 0.5964\n",
      "Training loss (for one batch) at step 30: 382.0101, Accuracy: 0.6144\n",
      "Training loss (for one batch) at step 40: 383.3846, Accuracy: 0.6265\n",
      "Training loss (for one batch) at step 50: 368.9102, Accuracy: 0.6368\n",
      "Training loss (for one batch) at step 60: 383.8207, Accuracy: 0.6441\n",
      "Training loss (for one batch) at step 70: 384.9659, Accuracy: 0.6421\n",
      "Training loss (for one batch) at step 80: 402.0955, Accuracy: 0.6376\n",
      "Training loss (for one batch) at step 90: 376.3472, Accuracy: 0.6359\n",
      "Training loss (for one batch) at step 100: 381.4378, Accuracy: 0.6352\n",
      "Training loss (for one batch) at step 110: 385.3307, Accuracy: 0.6349\n",
      "---- Training ----\n",
      "Training loss: 124.0684\n",
      "Training acc over epoch: 0.6339\n",
      "---- Validation ----\n",
      "Validation loss: 36.5200\n",
      "Validation acc: 0.5857\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 405.7257, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 409.8321, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 20: 365.0169, Accuracy: 0.5952\n",
      "Training loss (for one batch) at step 30: 388.4402, Accuracy: 0.6154\n",
      "Training loss (for one batch) at step 40: 370.6560, Accuracy: 0.6332\n",
      "Training loss (for one batch) at step 50: 356.5350, Accuracy: 0.6440\n",
      "Training loss (for one batch) at step 60: 367.8469, Accuracy: 0.6509\n",
      "Training loss (for one batch) at step 70: 381.7640, Accuracy: 0.6500\n",
      "Training loss (for one batch) at step 80: 394.3351, Accuracy: 0.6396\n",
      "Training loss (for one batch) at step 90: 375.1654, Accuracy: 0.6376\n",
      "Training loss (for one batch) at step 100: 388.6411, Accuracy: 0.6393\n",
      "Training loss (for one batch) at step 110: 373.3148, Accuracy: 0.6401\n",
      "---- Training ----\n",
      "Training loss: 118.7533\n",
      "Training acc over epoch: 0.6390\n",
      "---- Validation ----\n",
      "Validation loss: 36.0564\n",
      "Validation acc: 0.5983\n",
      "Time taken: 17.89s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 394.5051, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 372.6908, Accuracy: 0.6115\n",
      "Training loss (for one batch) at step 20: 367.3974, Accuracy: 0.6265\n",
      "Training loss (for one batch) at step 30: 373.4812, Accuracy: 0.6341\n",
      "Training loss (for one batch) at step 40: 344.2231, Accuracy: 0.6465\n",
      "Training loss (for one batch) at step 50: 343.4323, Accuracy: 0.6592\n",
      "Training loss (for one batch) at step 60: 369.3537, Accuracy: 0.6619\n",
      "Training loss (for one batch) at step 70: 364.5643, Accuracy: 0.6556\n",
      "Training loss (for one batch) at step 80: 388.6346, Accuracy: 0.6463\n",
      "Training loss (for one batch) at step 90: 374.7586, Accuracy: 0.6413\n",
      "Training loss (for one batch) at step 100: 354.7070, Accuracy: 0.6428\n",
      "Training loss (for one batch) at step 110: 378.7749, Accuracy: 0.6438\n",
      "---- Training ----\n",
      "Training loss: 113.0133\n",
      "Training acc over epoch: 0.6430\n",
      "---- Validation ----\n",
      "Validation loss: 36.3741\n",
      "Validation acc: 0.5747\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 381.9713, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 368.5039, Accuracy: 0.5916\n",
      "Training loss (for one batch) at step 20: 352.0216, Accuracy: 0.6150\n",
      "Training loss (for one batch) at step 30: 367.1533, Accuracy: 0.6305\n",
      "Training loss (for one batch) at step 40: 346.5673, Accuracy: 0.6463\n",
      "Training loss (for one batch) at step 50: 341.3000, Accuracy: 0.6589\n",
      "Training loss (for one batch) at step 60: 344.7874, Accuracy: 0.6656\n",
      "Training loss (for one batch) at step 70: 392.3948, Accuracy: 0.6593\n",
      "Training loss (for one batch) at step 80: 363.7740, Accuracy: 0.6482\n",
      "Training loss (for one batch) at step 90: 367.2984, Accuracy: 0.6453\n",
      "Training loss (for one batch) at step 100: 349.1697, Accuracy: 0.6455\n",
      "Training loss (for one batch) at step 110: 368.3315, Accuracy: 0.6465\n",
      "---- Training ----\n",
      "Training loss: 110.9219\n",
      "Training acc over epoch: 0.6458\n",
      "---- Validation ----\n",
      "Validation loss: 34.7398\n",
      "Validation acc: 0.5924\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 389.2701, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 376.2021, Accuracy: 0.5852\n",
      "Training loss (for one batch) at step 20: 358.6066, Accuracy: 0.6060\n",
      "Training loss (for one batch) at step 30: 340.5065, Accuracy: 0.6313\n",
      "Training loss (for one batch) at step 40: 355.7206, Accuracy: 0.6481\n",
      "Training loss (for one batch) at step 50: 327.8270, Accuracy: 0.6601\n",
      "Training loss (for one batch) at step 60: 356.4276, Accuracy: 0.6656\n",
      "Training loss (for one batch) at step 70: 356.1145, Accuracy: 0.6614\n",
      "Training loss (for one batch) at step 80: 356.0192, Accuracy: 0.6521\n",
      "Training loss (for one batch) at step 90: 340.1744, Accuracy: 0.6478\n",
      "Training loss (for one batch) at step 100: 346.4801, Accuracy: 0.6509\n",
      "Training loss (for one batch) at step 110: 380.9513, Accuracy: 0.6501\n",
      "---- Training ----\n",
      "Training loss: 102.6103\n",
      "Training acc over epoch: 0.6482\n",
      "---- Validation ----\n",
      "Validation loss: 41.8750\n",
      "Validation acc: 0.5884\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 367.3115, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 355.2830, Accuracy: 0.5717\n",
      "Training loss (for one batch) at step 20: 334.3114, Accuracy: 0.6105\n",
      "Training loss (for one batch) at step 30: 345.2882, Accuracy: 0.6366\n",
      "Training loss (for one batch) at step 40: 335.2076, Accuracy: 0.6505\n",
      "Training loss (for one batch) at step 50: 347.8957, Accuracy: 0.6604\n",
      "Training loss (for one batch) at step 60: 339.1358, Accuracy: 0.6638\n",
      "Training loss (for one batch) at step 70: 361.9059, Accuracy: 0.6560\n",
      "Training loss (for one batch) at step 80: 352.8586, Accuracy: 0.6469\n",
      "Training loss (for one batch) at step 90: 341.7073, Accuracy: 0.6445\n",
      "Training loss (for one batch) at step 100: 329.1260, Accuracy: 0.6457\n",
      "Training loss (for one batch) at step 110: 361.7400, Accuracy: 0.6465\n",
      "---- Training ----\n",
      "Training loss: 113.1460\n",
      "Training acc over epoch: 0.6449\n",
      "---- Validation ----\n",
      "Validation loss: 47.3738\n",
      "Validation acc: 0.5905\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 374.5835, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 354.3071, Accuracy: 0.5675\n",
      "Training loss (for one batch) at step 20: 339.5202, Accuracy: 0.6068\n",
      "Training loss (for one batch) at step 30: 311.7954, Accuracy: 0.6361\n",
      "Training loss (for one batch) at step 40: 339.2971, Accuracy: 0.6486\n",
      "Training loss (for one batch) at step 50: 307.7183, Accuracy: 0.6627\n",
      "Training loss (for one batch) at step 60: 320.3602, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 70: 357.5004, Accuracy: 0.6668\n",
      "Training loss (for one batch) at step 80: 355.6286, Accuracy: 0.6542\n",
      "Training loss (for one batch) at step 90: 333.6205, Accuracy: 0.6492\n",
      "Training loss (for one batch) at step 100: 321.8767, Accuracy: 0.6506\n",
      "Training loss (for one batch) at step 110: 351.6501, Accuracy: 0.6522\n",
      "---- Training ----\n",
      "Training loss: 100.8342\n",
      "Training acc over epoch: 0.6506\n",
      "---- Validation ----\n",
      "Validation loss: 45.8950\n",
      "Validation acc: 0.5897\n",
      "Time taken: 20.09s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 378.9994, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 360.1591, Accuracy: 0.5994\n",
      "Training loss (for one batch) at step 20: 326.5523, Accuracy: 0.6269\n",
      "Training loss (for one batch) at step 30: 309.0720, Accuracy: 0.6497\n",
      "Training loss (for one batch) at step 40: 325.2601, Accuracy: 0.6623\n",
      "Training loss (for one batch) at step 50: 298.4947, Accuracy: 0.6725\n",
      "Training loss (for one batch) at step 60: 324.2376, Accuracy: 0.6770\n",
      "Training loss (for one batch) at step 70: 342.4360, Accuracy: 0.6693\n",
      "Training loss (for one batch) at step 80: 330.1297, Accuracy: 0.6584\n",
      "Training loss (for one batch) at step 90: 319.5028, Accuracy: 0.6546\n",
      "Training loss (for one batch) at step 100: 320.5261, Accuracy: 0.6559\n",
      "Training loss (for one batch) at step 110: 310.9526, Accuracy: 0.6555\n",
      "---- Training ----\n",
      "Training loss: 105.3297\n",
      "Training acc over epoch: 0.6531\n",
      "---- Validation ----\n",
      "Validation loss: 37.5717\n",
      "Validation acc: 0.5803\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 359.2805, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 352.7004, Accuracy: 0.5526\n",
      "Training loss (for one batch) at step 20: 330.7069, Accuracy: 0.6008\n",
      "Training loss (for one batch) at step 30: 296.2103, Accuracy: 0.6348\n",
      "Training loss (for one batch) at step 40: 306.3202, Accuracy: 0.6568\n",
      "Training loss (for one batch) at step 50: 310.1866, Accuracy: 0.6685\n",
      "Training loss (for one batch) at step 60: 313.7814, Accuracy: 0.6732\n",
      "Training loss (for one batch) at step 70: 349.5662, Accuracy: 0.6660\n",
      "Training loss (for one batch) at step 80: 334.3433, Accuracy: 0.6545\n",
      "Training loss (for one batch) at step 90: 300.6586, Accuracy: 0.6508\n",
      "Training loss (for one batch) at step 100: 316.3907, Accuracy: 0.6507\n",
      "Training loss (for one batch) at step 110: 314.3828, Accuracy: 0.6517\n",
      "---- Training ----\n",
      "Training loss: 103.6488\n",
      "Training acc over epoch: 0.6511\n",
      "---- Validation ----\n",
      "Validation loss: 44.5484\n",
      "Validation acc: 0.5844\n",
      "Time taken: 17.89s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 340.9019, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 337.0539, Accuracy: 0.5952\n",
      "Training loss (for one batch) at step 20: 303.6373, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 30: 314.8798, Accuracy: 0.6389\n",
      "Training loss (for one batch) at step 40: 313.0762, Accuracy: 0.6526\n",
      "Training loss (for one batch) at step 50: 302.0883, Accuracy: 0.6657\n",
      "Training loss (for one batch) at step 60: 317.5835, Accuracy: 0.6716\n",
      "Training loss (for one batch) at step 70: 313.1437, Accuracy: 0.6630\n",
      "Training loss (for one batch) at step 80: 344.7317, Accuracy: 0.6518\n",
      "Training loss (for one batch) at step 90: 326.6614, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 100: 286.9857, Accuracy: 0.6497\n",
      "Training loss (for one batch) at step 110: 318.9048, Accuracy: 0.6513\n",
      "---- Training ----\n",
      "Training loss: 102.3847\n",
      "Training acc over epoch: 0.6487\n",
      "---- Validation ----\n",
      "Validation loss: 38.5577\n",
      "Validation acc: 0.6029\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 356.3612, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 332.8571, Accuracy: 0.5923\n",
      "Training loss (for one batch) at step 20: 305.9193, Accuracy: 0.6283\n",
      "Training loss (for one batch) at step 30: 304.4016, Accuracy: 0.6469\n",
      "Training loss (for one batch) at step 40: 300.7417, Accuracy: 0.6612\n",
      "Training loss (for one batch) at step 50: 282.9402, Accuracy: 0.6699\n",
      "Training loss (for one batch) at step 60: 317.4777, Accuracy: 0.6766\n",
      "Training loss (for one batch) at step 70: 307.0362, Accuracy: 0.6670\n",
      "Training loss (for one batch) at step 80: 354.4908, Accuracy: 0.6555\n",
      "Training loss (for one batch) at step 90: 313.0175, Accuracy: 0.6544\n",
      "Training loss (for one batch) at step 100: 306.3872, Accuracy: 0.6546\n",
      "Training loss (for one batch) at step 110: 319.1206, Accuracy: 0.6549\n",
      "---- Training ----\n",
      "Training loss: 110.3137\n",
      "Training acc over epoch: 0.6530\n",
      "---- Validation ----\n",
      "Validation loss: 35.3861\n",
      "Validation acc: 0.5938\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 331.6142, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 336.0341, Accuracy: 0.5717\n",
      "Training loss (for one batch) at step 20: 329.5503, Accuracy: 0.6179\n",
      "Training loss (for one batch) at step 30: 288.9838, Accuracy: 0.6396\n",
      "Training loss (for one batch) at step 40: 276.4866, Accuracy: 0.6591\n",
      "Training loss (for one batch) at step 50: 282.5168, Accuracy: 0.6736\n",
      "Training loss (for one batch) at step 60: 310.2916, Accuracy: 0.6787\n",
      "Training loss (for one batch) at step 70: 308.8233, Accuracy: 0.6679\n",
      "Training loss (for one batch) at step 80: 348.0940, Accuracy: 0.6596\n",
      "Training loss (for one batch) at step 90: 323.1835, Accuracy: 0.6554\n",
      "Training loss (for one batch) at step 100: 296.2795, Accuracy: 0.6556\n",
      "Training loss (for one batch) at step 110: 330.3079, Accuracy: 0.6541\n",
      "---- Training ----\n",
      "Training loss: 106.3317\n",
      "Training acc over epoch: 0.6535\n",
      "---- Validation ----\n",
      "Validation loss: 35.4570\n",
      "Validation acc: 0.5854\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 329.1107, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 320.2037, Accuracy: 0.5795\n",
      "Training loss (for one batch) at step 20: 301.5822, Accuracy: 0.6179\n",
      "Training loss (for one batch) at step 30: 299.0065, Accuracy: 0.6442\n",
      "Training loss (for one batch) at step 40: 282.2064, Accuracy: 0.6608\n",
      "Training loss (for one batch) at step 50: 273.2988, Accuracy: 0.6754\n",
      "Training loss (for one batch) at step 60: 318.8894, Accuracy: 0.6806\n",
      "Training loss (for one batch) at step 70: 312.7968, Accuracy: 0.6695\n",
      "Training loss (for one batch) at step 80: 326.9074, Accuracy: 0.6559\n",
      "Training loss (for one batch) at step 90: 321.9690, Accuracy: 0.6523\n",
      "Training loss (for one batch) at step 100: 287.3504, Accuracy: 0.6504\n",
      "Training loss (for one batch) at step 110: 304.8257, Accuracy: 0.6515\n",
      "---- Training ----\n",
      "Training loss: 99.6877\n",
      "Training acc over epoch: 0.6508\n",
      "---- Validation ----\n",
      "Validation loss: 43.7830\n",
      "Validation acc: 0.5868\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 334.0962, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 324.5098, Accuracy: 0.5803\n",
      "Training loss (for one batch) at step 20: 330.0878, Accuracy: 0.6161\n",
      "Training loss (for one batch) at step 30: 274.4952, Accuracy: 0.6404\n",
      "Training loss (for one batch) at step 40: 289.4896, Accuracy: 0.6561\n",
      "Training loss (for one batch) at step 50: 279.4696, Accuracy: 0.6691\n",
      "Training loss (for one batch) at step 60: 323.1820, Accuracy: 0.6733\n",
      "Training loss (for one batch) at step 70: 301.8530, Accuracy: 0.6648\n",
      "Training loss (for one batch) at step 80: 324.9962, Accuracy: 0.6526\n",
      "Training loss (for one batch) at step 90: 289.9052, Accuracy: 0.6514\n",
      "Training loss (for one batch) at step 100: 310.2262, Accuracy: 0.6506\n",
      "Training loss (for one batch) at step 110: 298.7610, Accuracy: 0.6497\n",
      "---- Training ----\n",
      "Training loss: 109.2277\n",
      "Training acc over epoch: 0.6511\n",
      "---- Validation ----\n",
      "Validation loss: 49.6987\n",
      "Validation acc: 0.5879\n",
      "Time taken: 17.86s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 318.0496, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 303.0093, Accuracy: 0.5760\n",
      "Training loss (for one batch) at step 20: 285.6823, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 30: 275.6923, Accuracy: 0.6462\n",
      "Training loss (for one batch) at step 40: 291.2900, Accuracy: 0.6625\n",
      "Training loss (for one batch) at step 50: 286.8369, Accuracy: 0.6725\n",
      "Training loss (for one batch) at step 60: 306.9911, Accuracy: 0.6778\n",
      "Training loss (for one batch) at step 70: 322.2430, Accuracy: 0.6699\n",
      "Training loss (for one batch) at step 80: 291.8680, Accuracy: 0.6571\n",
      "Training loss (for one batch) at step 90: 321.0573, Accuracy: 0.6544\n",
      "Training loss (for one batch) at step 100: 290.4649, Accuracy: 0.6542\n",
      "Training loss (for one batch) at step 110: 316.4491, Accuracy: 0.6544\n",
      "---- Training ----\n",
      "Training loss: 119.4018\n",
      "Training acc over epoch: 0.6537\n",
      "---- Validation ----\n",
      "Validation loss: 44.8416\n",
      "Validation acc: 0.6048\n",
      "Time taken: 18.09s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 333.3936, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 301.8637, Accuracy: 0.5739\n",
      "Training loss (for one batch) at step 20: 303.0220, Accuracy: 0.6142\n",
      "Training loss (for one batch) at step 30: 276.2456, Accuracy: 0.6434\n",
      "Training loss (for one batch) at step 40: 272.9957, Accuracy: 0.6599\n",
      "Training loss (for one batch) at step 50: 267.2443, Accuracy: 0.6745\n",
      "Training loss (for one batch) at step 60: 291.7216, Accuracy: 0.6802\n",
      "Training loss (for one batch) at step 70: 335.8726, Accuracy: 0.6721\n",
      "Training loss (for one batch) at step 80: 299.7965, Accuracy: 0.6608\n",
      "Training loss (for one batch) at step 90: 283.5033, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 100: 299.3972, Accuracy: 0.6583\n",
      "Training loss (for one batch) at step 110: 299.7338, Accuracy: 0.6570\n",
      "---- Training ----\n",
      "Training loss: 99.5421\n",
      "Training acc over epoch: 0.6562\n",
      "---- Validation ----\n",
      "Validation loss: 71.8624\n",
      "Validation acc: 0.5924\n",
      "Time taken: 17.87s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 321.1589, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 295.5305, Accuracy: 0.5682\n",
      "Training loss (for one batch) at step 20: 289.1712, Accuracy: 0.6127\n",
      "Training loss (for one batch) at step 30: 295.6407, Accuracy: 0.6419\n",
      "Training loss (for one batch) at step 40: 282.6563, Accuracy: 0.6603\n",
      "Training loss (for one batch) at step 50: 274.6554, Accuracy: 0.6713\n",
      "Training loss (for one batch) at step 60: 267.9097, Accuracy: 0.6778\n",
      "Training loss (for one batch) at step 70: 314.1976, Accuracy: 0.6700\n",
      "Training loss (for one batch) at step 80: 292.5848, Accuracy: 0.6590\n",
      "Training loss (for one batch) at step 90: 298.5366, Accuracy: 0.6536\n",
      "Training loss (for one batch) at step 100: 272.1710, Accuracy: 0.6536\n",
      "Training loss (for one batch) at step 110: 279.2485, Accuracy: 0.6533\n",
      "---- Training ----\n",
      "Training loss: 99.3616\n",
      "Training acc over epoch: 0.6531\n",
      "---- Validation ----\n",
      "Validation loss: 48.0679\n",
      "Validation acc: 0.5793\n",
      "Time taken: 17.86s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 323.4357, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 302.5333, Accuracy: 0.5661\n",
      "Training loss (for one batch) at step 20: 303.3537, Accuracy: 0.6034\n",
      "Training loss (for one batch) at step 30: 273.1078, Accuracy: 0.6409\n",
      "Training loss (for one batch) at step 40: 284.5963, Accuracy: 0.6593\n",
      "Training loss (for one batch) at step 50: 264.5876, Accuracy: 0.6702\n",
      "Training loss (for one batch) at step 60: 278.9760, Accuracy: 0.6766\n",
      "Training loss (for one batch) at step 70: 289.1167, Accuracy: 0.6666\n",
      "Training loss (for one batch) at step 80: 303.1095, Accuracy: 0.6541\n",
      "Training loss (for one batch) at step 90: 276.5027, Accuracy: 0.6511\n",
      "Training loss (for one batch) at step 100: 290.6142, Accuracy: 0.6532\n",
      "Training loss (for one batch) at step 110: 286.7179, Accuracy: 0.6517\n",
      "---- Training ----\n",
      "Training loss: 93.5352\n",
      "Training acc over epoch: 0.6513\n",
      "---- Validation ----\n",
      "Validation loss: 63.3189\n",
      "Validation acc: 0.5870\n",
      "Time taken: 18.26s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 305.8948, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 321.1884, Accuracy: 0.5689\n",
      "Training loss (for one batch) at step 20: 290.4461, Accuracy: 0.6194\n",
      "Training loss (for one batch) at step 30: 270.3950, Accuracy: 0.6401\n",
      "Training loss (for one batch) at step 40: 261.8747, Accuracy: 0.6566\n",
      "Training loss (for one batch) at step 50: 260.7418, Accuracy: 0.6693\n",
      "Training loss (for one batch) at step 60: 280.7905, Accuracy: 0.6758\n",
      "Training loss (for one batch) at step 70: 318.5679, Accuracy: 0.6634\n",
      "Training loss (for one batch) at step 80: 306.6097, Accuracy: 0.6527\n",
      "Training loss (for one batch) at step 90: 291.6307, Accuracy: 0.6502\n",
      "Training loss (for one batch) at step 100: 274.0699, Accuracy: 0.6523\n",
      "Training loss (for one batch) at step 110: 296.9680, Accuracy: 0.6524\n",
      "---- Training ----\n",
      "Training loss: 98.8502\n",
      "Training acc over epoch: 0.6511\n",
      "---- Validation ----\n",
      "Validation loss: 30.6400\n",
      "Validation acc: 0.5854\n",
      "Time taken: 17.86s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 303.2798, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 296.5055, Accuracy: 0.5795\n",
      "Training loss (for one batch) at step 20: 312.2772, Accuracy: 0.6124\n",
      "Training loss (for one batch) at step 30: 279.2664, Accuracy: 0.6416\n",
      "Training loss (for one batch) at step 40: 257.4033, Accuracy: 0.6603\n",
      "Training loss (for one batch) at step 50: 269.8328, Accuracy: 0.6734\n",
      "Training loss (for one batch) at step 60: 277.1338, Accuracy: 0.6778\n",
      "Training loss (for one batch) at step 70: 298.7157, Accuracy: 0.6677\n",
      "Training loss (for one batch) at step 80: 302.8435, Accuracy: 0.6535\n",
      "Training loss (for one batch) at step 90: 258.8105, Accuracy: 0.6521\n",
      "Training loss (for one batch) at step 100: 271.5027, Accuracy: 0.6542\n",
      "Training loss (for one batch) at step 110: 311.4978, Accuracy: 0.6536\n",
      "---- Training ----\n",
      "Training loss: 113.4062\n",
      "Training acc over epoch: 0.6530\n",
      "---- Validation ----\n",
      "Validation loss: 42.6630\n",
      "Validation acc: 0.5690\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 308.5569, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 294.1388, Accuracy: 0.5810\n",
      "Training loss (for one batch) at step 20: 290.7315, Accuracy: 0.6157\n",
      "Training loss (for one batch) at step 30: 264.9530, Accuracy: 0.6507\n",
      "Training loss (for one batch) at step 40: 256.6738, Accuracy: 0.6667\n",
      "Training loss (for one batch) at step 50: 253.3004, Accuracy: 0.6751\n",
      "Training loss (for one batch) at step 60: 279.8448, Accuracy: 0.6793\n",
      "Training loss (for one batch) at step 70: 273.2346, Accuracy: 0.6701\n",
      "Training loss (for one batch) at step 80: 286.4281, Accuracy: 0.6577\n",
      "Training loss (for one batch) at step 90: 275.1525, Accuracy: 0.6519\n",
      "Training loss (for one batch) at step 100: 296.2335, Accuracy: 0.6541\n",
      "Training loss (for one batch) at step 110: 295.4338, Accuracy: 0.6541\n",
      "---- Training ----\n",
      "Training loss: 75.6519\n",
      "Training acc over epoch: 0.6521\n",
      "---- Validation ----\n",
      "Validation loss: 36.9272\n",
      "Validation acc: 0.5946\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 286.1876, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 300.1251, Accuracy: 0.5661\n",
      "Training loss (for one batch) at step 20: 292.4024, Accuracy: 0.6105\n",
      "Training loss (for one batch) at step 30: 274.9877, Accuracy: 0.6389\n",
      "Training loss (for one batch) at step 40: 282.3567, Accuracy: 0.6585\n",
      "Training loss (for one batch) at step 50: 265.9470, Accuracy: 0.6697\n",
      "Training loss (for one batch) at step 60: 292.5741, Accuracy: 0.6753\n",
      "Training loss (for one batch) at step 70: 287.2102, Accuracy: 0.6670\n",
      "Training loss (for one batch) at step 80: 306.1161, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 90: 262.6681, Accuracy: 0.6519\n",
      "Training loss (for one batch) at step 100: 267.8895, Accuracy: 0.6527\n",
      "Training loss (for one batch) at step 110: 281.6214, Accuracy: 0.6529\n",
      "---- Training ----\n",
      "Training loss: 98.5921\n",
      "Training acc over epoch: 0.6531\n",
      "---- Validation ----\n",
      "Validation loss: 44.8644\n",
      "Validation acc: 0.5962\n",
      "Time taken: 18.18s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 323.9028, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 269.2177, Accuracy: 0.5909\n",
      "Training loss (for one batch) at step 20: 270.5538, Accuracy: 0.6190\n",
      "Training loss (for one batch) at step 30: 261.4434, Accuracy: 0.6424\n",
      "Training loss (for one batch) at step 40: 256.0008, Accuracy: 0.6606\n",
      "Training loss (for one batch) at step 50: 251.5883, Accuracy: 0.6740\n",
      "Training loss (for one batch) at step 60: 270.8985, Accuracy: 0.6787\n",
      "Training loss (for one batch) at step 70: 284.0463, Accuracy: 0.6670\n",
      "Training loss (for one batch) at step 80: 285.4615, Accuracy: 0.6568\n",
      "Training loss (for one batch) at step 90: 278.0134, Accuracy: 0.6533\n",
      "Training loss (for one batch) at step 100: 261.9995, Accuracy: 0.6542\n",
      "Training loss (for one batch) at step 110: 277.6722, Accuracy: 0.6534\n",
      "---- Training ----\n",
      "Training loss: 90.5945\n",
      "Training acc over epoch: 0.6521\n",
      "---- Validation ----\n",
      "Validation loss: 43.6888\n",
      "Validation acc: 0.5975\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 292.6461, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 299.3305, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 20: 261.8094, Accuracy: 0.6194\n",
      "Training loss (for one batch) at step 30: 262.7071, Accuracy: 0.6522\n",
      "Training loss (for one batch) at step 40: 262.1750, Accuracy: 0.6639\n",
      "Training loss (for one batch) at step 50: 263.8907, Accuracy: 0.6759\n",
      "Training loss (for one batch) at step 60: 288.7807, Accuracy: 0.6825\n",
      "Training loss (for one batch) at step 70: 315.0316, Accuracy: 0.6710\n",
      "Training loss (for one batch) at step 80: 301.4610, Accuracy: 0.6589\n",
      "Training loss (for one batch) at step 90: 276.6912, Accuracy: 0.6525\n",
      "Training loss (for one batch) at step 100: 255.9552, Accuracy: 0.6545\n",
      "Training loss (for one batch) at step 110: 285.6925, Accuracy: 0.6543\n",
      "---- Training ----\n",
      "Training loss: 84.5213\n",
      "Training acc over epoch: 0.6535\n",
      "---- Validation ----\n",
      "Validation loss: 35.2980\n",
      "Validation acc: 0.5862\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 288.9172, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 278.1173, Accuracy: 0.5739\n",
      "Training loss (for one batch) at step 20: 288.3658, Accuracy: 0.6127\n",
      "Training loss (for one batch) at step 30: 283.0881, Accuracy: 0.6434\n",
      "Training loss (for one batch) at step 40: 255.0504, Accuracy: 0.6633\n",
      "Training loss (for one batch) at step 50: 267.0674, Accuracy: 0.6729\n",
      "Training loss (for one batch) at step 60: 262.6263, Accuracy: 0.6774\n",
      "Training loss (for one batch) at step 70: 278.6886, Accuracy: 0.6658\n",
      "Training loss (for one batch) at step 80: 283.8359, Accuracy: 0.6535\n",
      "Training loss (for one batch) at step 90: 264.8967, Accuracy: 0.6515\n",
      "Training loss (for one batch) at step 100: 271.0923, Accuracy: 0.6514\n",
      "Training loss (for one batch) at step 110: 280.7600, Accuracy: 0.6521\n",
      "---- Training ----\n",
      "Training loss: 96.7723\n",
      "Training acc over epoch: 0.6507\n",
      "---- Validation ----\n",
      "Validation loss: 44.8515\n",
      "Validation acc: 0.5857\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 301.2118, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 294.7950, Accuracy: 0.5987\n",
      "Training loss (for one batch) at step 20: 269.1839, Accuracy: 0.6287\n",
      "Training loss (for one batch) at step 30: 252.9680, Accuracy: 0.6487\n",
      "Training loss (for one batch) at step 40: 253.4110, Accuracy: 0.6646\n",
      "Training loss (for one batch) at step 50: 243.2454, Accuracy: 0.6739\n",
      "Training loss (for one batch) at step 60: 280.8030, Accuracy: 0.6794\n",
      "Training loss (for one batch) at step 70: 288.6779, Accuracy: 0.6704\n",
      "Training loss (for one batch) at step 80: 289.9951, Accuracy: 0.6574\n",
      "Training loss (for one batch) at step 90: 258.1890, Accuracy: 0.6532\n",
      "Training loss (for one batch) at step 100: 262.3350, Accuracy: 0.6542\n",
      "Training loss (for one batch) at step 110: 277.7794, Accuracy: 0.6558\n",
      "---- Training ----\n",
      "Training loss: 88.6379\n",
      "Training acc over epoch: 0.6530\n",
      "---- Validation ----\n",
      "Validation loss: 41.8784\n",
      "Validation acc: 0.5830\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 280.9371, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 274.4835, Accuracy: 0.5675\n",
      "Training loss (for one batch) at step 20: 277.5136, Accuracy: 0.6071\n",
      "Training loss (for one batch) at step 30: 262.1334, Accuracy: 0.6389\n",
      "Training loss (for one batch) at step 40: 263.2234, Accuracy: 0.6604\n",
      "Training loss (for one batch) at step 50: 250.3039, Accuracy: 0.6737\n",
      "Training loss (for one batch) at step 60: 272.7127, Accuracy: 0.6784\n",
      "Training loss (for one batch) at step 70: 278.4288, Accuracy: 0.6728\n",
      "Training loss (for one batch) at step 80: 286.4091, Accuracy: 0.6600\n",
      "Training loss (for one batch) at step 90: 261.9063, Accuracy: 0.6543\n",
      "Training loss (for one batch) at step 100: 264.9489, Accuracy: 0.6556\n",
      "Training loss (for one batch) at step 110: 303.7098, Accuracy: 0.6541\n",
      "---- Training ----\n",
      "Training loss: 86.5938\n",
      "Training acc over epoch: 0.6530\n",
      "---- Validation ----\n",
      "Validation loss: 34.2015\n",
      "Validation acc: 0.5913\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 293.2142, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 282.8893, Accuracy: 0.5774\n",
      "Training loss (for one batch) at step 20: 280.8311, Accuracy: 0.6220\n",
      "Training loss (for one batch) at step 30: 248.0750, Accuracy: 0.6452\n",
      "Training loss (for one batch) at step 40: 254.5661, Accuracy: 0.6622\n",
      "Training loss (for one batch) at step 50: 247.2529, Accuracy: 0.6731\n",
      "Training loss (for one batch) at step 60: 277.8994, Accuracy: 0.6784\n",
      "Training loss (for one batch) at step 70: 294.0296, Accuracy: 0.6707\n",
      "Training loss (for one batch) at step 80: 281.0348, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 90: 250.7836, Accuracy: 0.6526\n",
      "Training loss (for one batch) at step 100: 263.2867, Accuracy: 0.6541\n",
      "Training loss (for one batch) at step 110: 267.7567, Accuracy: 0.6548\n",
      "---- Training ----\n",
      "Training loss: 97.9863\n",
      "Training acc over epoch: 0.6542\n",
      "---- Validation ----\n",
      "Validation loss: 37.8733\n",
      "Validation acc: 0.5857\n",
      "Time taken: 17.83s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 281.4117, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 277.7998, Accuracy: 0.5923\n",
      "Training loss (for one batch) at step 20: 244.2928, Accuracy: 0.6231\n",
      "Training loss (for one batch) at step 30: 252.9267, Accuracy: 0.6517\n",
      "Training loss (for one batch) at step 40: 243.9418, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 50: 261.8668, Accuracy: 0.6763\n",
      "Training loss (for one batch) at step 60: 252.6709, Accuracy: 0.6825\n",
      "Training loss (for one batch) at step 70: 265.2324, Accuracy: 0.6710\n",
      "Training loss (for one batch) at step 80: 267.4080, Accuracy: 0.6579\n",
      "Training loss (for one batch) at step 90: 273.3869, Accuracy: 0.6534\n",
      "Training loss (for one batch) at step 100: 264.7794, Accuracy: 0.6536\n",
      "Training loss (for one batch) at step 110: 266.1484, Accuracy: 0.6541\n",
      "---- Training ----\n",
      "Training loss: 93.4525\n",
      "Training acc over epoch: 0.6535\n",
      "---- Validation ----\n",
      "Validation loss: 52.0854\n",
      "Validation acc: 0.5927\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 280.0638, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 301.4571, Accuracy: 0.5824\n",
      "Training loss (for one batch) at step 20: 262.5207, Accuracy: 0.6209\n",
      "Training loss (for one batch) at step 30: 252.1336, Accuracy: 0.6477\n",
      "Training loss (for one batch) at step 40: 252.2564, Accuracy: 0.6667\n",
      "Training loss (for one batch) at step 50: 253.7990, Accuracy: 0.6774\n",
      "Training loss (for one batch) at step 60: 264.3086, Accuracy: 0.6805\n",
      "Training loss (for one batch) at step 70: 265.7889, Accuracy: 0.6721\n",
      "Training loss (for one batch) at step 80: 295.6875, Accuracy: 0.6573\n",
      "Training loss (for one batch) at step 90: 245.6507, Accuracy: 0.6514\n",
      "Training loss (for one batch) at step 100: 266.5110, Accuracy: 0.6528\n",
      "Training loss (for one batch) at step 110: 287.1502, Accuracy: 0.6528\n",
      "---- Training ----\n",
      "Training loss: 90.5963\n",
      "Training acc over epoch: 0.6530\n",
      "---- Validation ----\n",
      "Validation loss: 42.7970\n",
      "Validation acc: 0.5846\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 299.4092, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 284.0931, Accuracy: 0.5717\n",
      "Training loss (for one batch) at step 20: 262.6109, Accuracy: 0.6127\n",
      "Training loss (for one batch) at step 30: 235.7944, Accuracy: 0.6424\n",
      "Training loss (for one batch) at step 40: 256.4831, Accuracy: 0.6604\n",
      "Training loss (for one batch) at step 50: 260.3675, Accuracy: 0.6743\n",
      "Training loss (for one batch) at step 60: 271.1924, Accuracy: 0.6771\n",
      "Training loss (for one batch) at step 70: 274.8900, Accuracy: 0.6671\n",
      "Training loss (for one batch) at step 80: 262.1844, Accuracy: 0.6545\n",
      "Training loss (for one batch) at step 90: 283.4739, Accuracy: 0.6510\n",
      "Training loss (for one batch) at step 100: 253.8214, Accuracy: 0.6532\n",
      "Training loss (for one batch) at step 110: 273.2395, Accuracy: 0.6548\n",
      "---- Training ----\n",
      "Training loss: 82.0632\n",
      "Training acc over epoch: 0.6538\n",
      "---- Validation ----\n",
      "Validation loss: 38.3831\n",
      "Validation acc: 0.5913\n",
      "Time taken: 18.10s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 303.5323, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 307.9463, Accuracy: 0.5760\n",
      "Training loss (for one batch) at step 20: 277.7474, Accuracy: 0.6045\n",
      "Training loss (for one batch) at step 30: 242.0243, Accuracy: 0.6457\n",
      "Training loss (for one batch) at step 40: 253.7197, Accuracy: 0.6606\n",
      "Training loss (for one batch) at step 50: 229.5925, Accuracy: 0.6734\n",
      "Training loss (for one batch) at step 60: 241.3210, Accuracy: 0.6785\n",
      "Training loss (for one batch) at step 70: 276.8783, Accuracy: 0.6709\n",
      "Training loss (for one batch) at step 80: 286.3932, Accuracy: 0.6556\n",
      "Training loss (for one batch) at step 90: 263.2148, Accuracy: 0.6513\n",
      "Training loss (for one batch) at step 100: 260.7127, Accuracy: 0.6520\n",
      "Training loss (for one batch) at step 110: 258.9585, Accuracy: 0.6517\n",
      "---- Training ----\n",
      "Training loss: 85.1662\n",
      "Training acc over epoch: 0.6513\n",
      "---- Validation ----\n",
      "Validation loss: 44.5992\n",
      "Validation acc: 0.5744\n",
      "Time taken: 18.15s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 285.3262, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 279.1499, Accuracy: 0.5930\n",
      "Training loss (for one batch) at step 20: 268.1969, Accuracy: 0.6217\n",
      "Training loss (for one batch) at step 30: 245.3230, Accuracy: 0.6416\n",
      "Training loss (for one batch) at step 40: 234.6359, Accuracy: 0.6644\n",
      "Training loss (for one batch) at step 50: 246.5569, Accuracy: 0.6739\n",
      "Training loss (for one batch) at step 60: 267.0849, Accuracy: 0.6787\n",
      "Training loss (for one batch) at step 70: 263.3580, Accuracy: 0.6697\n",
      "Training loss (for one batch) at step 80: 259.6794, Accuracy: 0.6575\n",
      "Training loss (for one batch) at step 90: 244.1562, Accuracy: 0.6516\n",
      "Training loss (for one batch) at step 100: 263.9000, Accuracy: 0.6529\n",
      "Training loss (for one batch) at step 110: 280.1306, Accuracy: 0.6546\n",
      "---- Training ----\n",
      "Training loss: 109.8089\n",
      "Training acc over epoch: 0.6531\n",
      "---- Validation ----\n",
      "Validation loss: 38.3095\n",
      "Validation acc: 0.5766\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 274.7241, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 287.0392, Accuracy: 0.5845\n",
      "Training loss (for one batch) at step 20: 253.5696, Accuracy: 0.6217\n",
      "Training loss (for one batch) at step 30: 259.3121, Accuracy: 0.6431\n",
      "Training loss (for one batch) at step 40: 252.2874, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 50: 255.0081, Accuracy: 0.6768\n",
      "Training loss (for one batch) at step 60: 254.9786, Accuracy: 0.6805\n",
      "Training loss (for one batch) at step 70: 266.2584, Accuracy: 0.6678\n",
      "Training loss (for one batch) at step 80: 276.1963, Accuracy: 0.6550\n",
      "Training loss (for one batch) at step 90: 253.8067, Accuracy: 0.6531\n",
      "Training loss (for one batch) at step 100: 259.2308, Accuracy: 0.6532\n",
      "Training loss (for one batch) at step 110: 256.2210, Accuracy: 0.6533\n",
      "---- Training ----\n",
      "Training loss: 83.4709\n",
      "Training acc over epoch: 0.6531\n",
      "---- Validation ----\n",
      "Validation loss: 41.4407\n",
      "Validation acc: 0.5809\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 276.8496, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 279.2214, Accuracy: 0.5909\n",
      "Training loss (for one batch) at step 20: 256.9338, Accuracy: 0.6176\n",
      "Training loss (for one batch) at step 30: 277.6102, Accuracy: 0.6411\n",
      "Training loss (for one batch) at step 40: 236.7123, Accuracy: 0.6595\n",
      "Training loss (for one batch) at step 50: 245.1638, Accuracy: 0.6737\n",
      "Training loss (for one batch) at step 60: 274.8551, Accuracy: 0.6790\n",
      "Training loss (for one batch) at step 70: 262.4815, Accuracy: 0.6707\n",
      "Training loss (for one batch) at step 80: 254.1805, Accuracy: 0.6575\n",
      "Training loss (for one batch) at step 90: 264.9944, Accuracy: 0.6538\n",
      "Training loss (for one batch) at step 100: 254.2361, Accuracy: 0.6554\n",
      "Training loss (for one batch) at step 110: 261.3394, Accuracy: 0.6548\n",
      "---- Training ----\n",
      "Training loss: 106.9864\n",
      "Training acc over epoch: 0.6543\n",
      "---- Validation ----\n",
      "Validation loss: 54.1749\n",
      "Validation acc: 0.5922\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 290.0242, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 260.6204, Accuracy: 0.5661\n",
      "Training loss (for one batch) at step 20: 235.9172, Accuracy: 0.6146\n",
      "Training loss (for one batch) at step 30: 229.6952, Accuracy: 0.6507\n",
      "Training loss (for one batch) at step 40: 238.7114, Accuracy: 0.6652\n",
      "Training loss (for one batch) at step 50: 257.0853, Accuracy: 0.6748\n",
      "Training loss (for one batch) at step 60: 244.6892, Accuracy: 0.6829\n",
      "Training loss (for one batch) at step 70: 258.0608, Accuracy: 0.6712\n",
      "Training loss (for one batch) at step 80: 270.0325, Accuracy: 0.6569\n",
      "Training loss (for one batch) at step 90: 247.6399, Accuracy: 0.6550\n",
      "Training loss (for one batch) at step 100: 265.1260, Accuracy: 0.6539\n",
      "Training loss (for one batch) at step 110: 248.6321, Accuracy: 0.6546\n",
      "---- Training ----\n",
      "Training loss: 92.7976\n",
      "Training acc over epoch: 0.6537\n",
      "---- Validation ----\n",
      "Validation loss: 39.5531\n",
      "Validation acc: 0.5956\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 297.5931, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 295.4754, Accuracy: 0.5888\n",
      "Training loss (for one batch) at step 20: 242.4959, Accuracy: 0.6127\n",
      "Training loss (for one batch) at step 30: 240.7216, Accuracy: 0.6436\n",
      "Training loss (for one batch) at step 40: 237.3467, Accuracy: 0.6582\n",
      "Training loss (for one batch) at step 50: 241.4091, Accuracy: 0.6716\n",
      "Training loss (for one batch) at step 60: 270.3277, Accuracy: 0.6776\n",
      "Training loss (for one batch) at step 70: 278.4347, Accuracy: 0.6691\n",
      "Training loss (for one batch) at step 80: 267.6148, Accuracy: 0.6551\n",
      "Training loss (for one batch) at step 90: 253.6608, Accuracy: 0.6513\n",
      "Training loss (for one batch) at step 100: 262.0695, Accuracy: 0.6530\n",
      "Training loss (for one batch) at step 110: 263.9398, Accuracy: 0.6525\n",
      "---- Training ----\n",
      "Training loss: 79.5760\n",
      "Training acc over epoch: 0.6518\n",
      "---- Validation ----\n",
      "Validation loss: 36.3287\n",
      "Validation acc: 0.5862\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 266.9199, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 290.1890, Accuracy: 0.5888\n",
      "Training loss (for one batch) at step 20: 257.7579, Accuracy: 0.6157\n",
      "Training loss (for one batch) at step 30: 226.6217, Accuracy: 0.6444\n",
      "Training loss (for one batch) at step 40: 235.1661, Accuracy: 0.6627\n",
      "Training loss (for one batch) at step 50: 242.1942, Accuracy: 0.6766\n",
      "Training loss (for one batch) at step 60: 252.4602, Accuracy: 0.6774\n",
      "Training loss (for one batch) at step 70: 278.0276, Accuracy: 0.6690\n",
      "Training loss (for one batch) at step 80: 271.3099, Accuracy: 0.6555\n",
      "Training loss (for one batch) at step 90: 269.5746, Accuracy: 0.6527\n",
      "Training loss (for one batch) at step 100: 265.1382, Accuracy: 0.6537\n",
      "Training loss (for one batch) at step 110: 249.4487, Accuracy: 0.6538\n",
      "---- Training ----\n",
      "Training loss: 80.5039\n",
      "Training acc over epoch: 0.6521\n",
      "---- Validation ----\n",
      "Validation loss: 49.3982\n",
      "Validation acc: 0.6193\n",
      "Time taken: 17.89s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 268.7733, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 261.5170, Accuracy: 0.5717\n",
      "Training loss (for one batch) at step 20: 243.5792, Accuracy: 0.6112\n",
      "Training loss (for one batch) at step 30: 227.5426, Accuracy: 0.6434\n",
      "Training loss (for one batch) at step 40: 228.8502, Accuracy: 0.6639\n",
      "Training loss (for one batch) at step 50: 233.1016, Accuracy: 0.6745\n",
      "Training loss (for one batch) at step 60: 247.2229, Accuracy: 0.6792\n",
      "Training loss (for one batch) at step 70: 271.6782, Accuracy: 0.6679\n",
      "Training loss (for one batch) at step 80: 278.5202, Accuracy: 0.6560\n",
      "Training loss (for one batch) at step 90: 251.4539, Accuracy: 0.6520\n",
      "Training loss (for one batch) at step 100: 242.0918, Accuracy: 0.6539\n",
      "Training loss (for one batch) at step 110: 260.8363, Accuracy: 0.6546\n",
      "---- Training ----\n",
      "Training loss: 89.2449\n",
      "Training acc over epoch: 0.6527\n",
      "---- Validation ----\n",
      "Validation loss: 55.5316\n",
      "Validation acc: 0.6037\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 282.7242, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 261.6183, Accuracy: 0.5795\n",
      "Training loss (for one batch) at step 20: 249.9231, Accuracy: 0.6176\n",
      "Training loss (for one batch) at step 30: 230.7437, Accuracy: 0.6457\n",
      "Training loss (for one batch) at step 40: 238.6943, Accuracy: 0.6614\n",
      "Training loss (for one batch) at step 50: 240.0853, Accuracy: 0.6745\n",
      "Training loss (for one batch) at step 60: 269.1199, Accuracy: 0.6798\n",
      "Training loss (for one batch) at step 70: 271.5067, Accuracy: 0.6698\n",
      "Training loss (for one batch) at step 80: 273.0872, Accuracy: 0.6559\n",
      "Training loss (for one batch) at step 90: 235.5290, Accuracy: 0.6522\n",
      "Training loss (for one batch) at step 100: 243.5658, Accuracy: 0.6535\n",
      "Training loss (for one batch) at step 110: 269.0124, Accuracy: 0.6505\n",
      "---- Training ----\n",
      "Training loss: 85.0013\n",
      "Training acc over epoch: 0.6509\n",
      "---- Validation ----\n",
      "Validation loss: 47.0765\n",
      "Validation acc: 0.5943\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 252.3539, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 271.0646, Accuracy: 0.5774\n",
      "Training loss (for one batch) at step 20: 259.0851, Accuracy: 0.6243\n",
      "Training loss (for one batch) at step 30: 262.4041, Accuracy: 0.6535\n",
      "Training loss (for one batch) at step 40: 254.0356, Accuracy: 0.6654\n",
      "Training loss (for one batch) at step 50: 251.1702, Accuracy: 0.6757\n",
      "Training loss (for one batch) at step 60: 267.4777, Accuracy: 0.6794\n",
      "Training loss (for one batch) at step 70: 261.8121, Accuracy: 0.6707\n",
      "Training loss (for one batch) at step 80: 285.4671, Accuracy: 0.6576\n",
      "Training loss (for one batch) at step 90: 243.7594, Accuracy: 0.6532\n",
      "Training loss (for one batch) at step 100: 252.6224, Accuracy: 0.6540\n",
      "Training loss (for one batch) at step 110: 246.4815, Accuracy: 0.6538\n",
      "---- Training ----\n",
      "Training loss: 89.5599\n",
      "Training acc over epoch: 0.6531\n",
      "---- Validation ----\n",
      "Validation loss: 36.8305\n",
      "Validation acc: 0.5771\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 272.1431, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 260.8422, Accuracy: 0.5724\n",
      "Training loss (for one batch) at step 20: 257.5235, Accuracy: 0.6097\n",
      "Training loss (for one batch) at step 30: 249.4802, Accuracy: 0.6482\n",
      "Training loss (for one batch) at step 40: 253.3458, Accuracy: 0.6597\n",
      "Training loss (for one batch) at step 50: 241.6102, Accuracy: 0.6711\n",
      "Training loss (for one batch) at step 60: 249.6766, Accuracy: 0.6807\n",
      "Training loss (for one batch) at step 70: 286.7354, Accuracy: 0.6692\n",
      "Training loss (for one batch) at step 80: 254.9017, Accuracy: 0.6553\n",
      "Training loss (for one batch) at step 90: 246.7939, Accuracy: 0.6514\n",
      "Training loss (for one batch) at step 100: 259.0685, Accuracy: 0.6534\n",
      "Training loss (for one batch) at step 110: 255.7795, Accuracy: 0.6531\n",
      "---- Training ----\n",
      "Training loss: 73.6754\n",
      "Training acc over epoch: 0.6526\n",
      "---- Validation ----\n",
      "Validation loss: 55.7954\n",
      "Validation acc: 0.5965\n",
      "Time taken: 17.86s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 253.0409, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 257.5796, Accuracy: 0.5724\n",
      "Training loss (for one batch) at step 20: 246.4167, Accuracy: 0.6209\n",
      "Training loss (for one batch) at step 30: 245.2145, Accuracy: 0.6489\n",
      "Training loss (for one batch) at step 40: 253.3479, Accuracy: 0.6648\n",
      "Training loss (for one batch) at step 50: 238.8361, Accuracy: 0.6759\n",
      "Training loss (for one batch) at step 60: 250.7975, Accuracy: 0.6821\n",
      "Training loss (for one batch) at step 70: 257.4299, Accuracy: 0.6711\n",
      "Training loss (for one batch) at step 80: 272.0074, Accuracy: 0.6553\n",
      "Training loss (for one batch) at step 90: 258.3165, Accuracy: 0.6525\n",
      "Training loss (for one batch) at step 100: 264.1952, Accuracy: 0.6538\n",
      "Training loss (for one batch) at step 110: 265.9254, Accuracy: 0.6541\n",
      "---- Training ----\n",
      "Training loss: 80.7092\n",
      "Training acc over epoch: 0.6529\n",
      "---- Validation ----\n",
      "Validation loss: 44.5775\n",
      "Validation acc: 0.5887\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 258.2811, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 248.8213, Accuracy: 0.5739\n",
      "Training loss (for one batch) at step 20: 255.9043, Accuracy: 0.6135\n",
      "Training loss (for one batch) at step 30: 240.5418, Accuracy: 0.6464\n",
      "Training loss (for one batch) at step 40: 237.7874, Accuracy: 0.6633\n",
      "Training loss (for one batch) at step 50: 238.6794, Accuracy: 0.6749\n",
      "Training loss (for one batch) at step 60: 260.5288, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 70: 246.8839, Accuracy: 0.6664\n",
      "Training loss (for one batch) at step 80: 268.9169, Accuracy: 0.6557\n",
      "Training loss (for one batch) at step 90: 242.6488, Accuracy: 0.6520\n",
      "Training loss (for one batch) at step 100: 262.2763, Accuracy: 0.6541\n",
      "Training loss (for one batch) at step 110: 250.7946, Accuracy: 0.6545\n",
      "---- Training ----\n",
      "Training loss: 80.3669\n",
      "Training acc over epoch: 0.6532\n",
      "---- Validation ----\n",
      "Validation loss: 35.9154\n",
      "Validation acc: 0.5873\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 282.9398, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 255.0950, Accuracy: 0.5824\n",
      "Training loss (for one batch) at step 20: 248.9560, Accuracy: 0.6179\n",
      "Training loss (for one batch) at step 30: 234.6720, Accuracy: 0.6474\n",
      "Training loss (for one batch) at step 40: 259.5930, Accuracy: 0.6673\n",
      "Training loss (for one batch) at step 50: 240.9655, Accuracy: 0.6777\n",
      "Training loss (for one batch) at step 60: 234.5612, Accuracy: 0.6798\n",
      "Training loss (for one batch) at step 70: 279.1177, Accuracy: 0.6689\n",
      "Training loss (for one batch) at step 80: 269.6659, Accuracy: 0.6536\n",
      "Training loss (for one batch) at step 90: 243.5779, Accuracy: 0.6490\n",
      "Training loss (for one batch) at step 100: 251.9160, Accuracy: 0.6503\n",
      "Training loss (for one batch) at step 110: 244.0710, Accuracy: 0.6527\n",
      "---- Training ----\n",
      "Training loss: 76.2153\n",
      "Training acc over epoch: 0.6508\n",
      "---- Validation ----\n",
      "Validation loss: 41.5906\n",
      "Validation acc: 0.5873\n",
      "Time taken: 18.05s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 255.7299, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 257.9482, Accuracy: 0.5980\n",
      "Training loss (for one batch) at step 20: 270.9781, Accuracy: 0.6190\n",
      "Training loss (for one batch) at step 30: 234.7220, Accuracy: 0.6436\n",
      "Training loss (for one batch) at step 40: 234.7707, Accuracy: 0.6688\n",
      "Training loss (for one batch) at step 50: 230.1727, Accuracy: 0.6759\n",
      "Training loss (for one batch) at step 60: 233.7155, Accuracy: 0.6798\n",
      "Training loss (for one batch) at step 70: 263.5070, Accuracy: 0.6679\n",
      "Training loss (for one batch) at step 80: 261.9835, Accuracy: 0.6542\n",
      "Training loss (for one batch) at step 90: 261.0803, Accuracy: 0.6503\n",
      "Training loss (for one batch) at step 100: 243.3568, Accuracy: 0.6514\n",
      "Training loss (for one batch) at step 110: 250.3029, Accuracy: 0.6517\n",
      "---- Training ----\n",
      "Training loss: 85.1645\n",
      "Training acc over epoch: 0.6507\n",
      "---- Validation ----\n",
      "Validation loss: 61.5147\n",
      "Validation acc: 0.5881\n",
      "Time taken: 17.84s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 255.1294, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 261.4053, Accuracy: 0.5746\n",
      "Training loss (for one batch) at step 20: 241.9703, Accuracy: 0.6146\n",
      "Training loss (for one batch) at step 30: 234.3903, Accuracy: 0.6434\n",
      "Training loss (for one batch) at step 40: 243.1137, Accuracy: 0.6599\n",
      "Training loss (for one batch) at step 50: 223.5401, Accuracy: 0.6759\n",
      "Training loss (for one batch) at step 60: 250.0962, Accuracy: 0.6793\n",
      "Training loss (for one batch) at step 70: 252.6700, Accuracy: 0.6684\n",
      "Training loss (for one batch) at step 80: 297.6573, Accuracy: 0.6551\n",
      "Training loss (for one batch) at step 90: 269.2272, Accuracy: 0.6525\n",
      "Training loss (for one batch) at step 100: 280.1990, Accuracy: 0.6535\n",
      "Training loss (for one batch) at step 110: 247.4498, Accuracy: 0.6527\n",
      "---- Training ----\n",
      "Training loss: 73.1692\n",
      "Training acc over epoch: 0.6521\n",
      "---- Validation ----\n",
      "Validation loss: 39.3410\n",
      "Validation acc: 0.5919\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 298.1981, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 247.5725, Accuracy: 0.5710\n",
      "Training loss (for one batch) at step 20: 243.2092, Accuracy: 0.6164\n",
      "Training loss (for one batch) at step 30: 240.3401, Accuracy: 0.6454\n",
      "Training loss (for one batch) at step 40: 256.5995, Accuracy: 0.6646\n",
      "Training loss (for one batch) at step 50: 240.7627, Accuracy: 0.6742\n",
      "Training loss (for one batch) at step 60: 256.8672, Accuracy: 0.6775\n",
      "Training loss (for one batch) at step 70: 260.0448, Accuracy: 0.6689\n",
      "Training loss (for one batch) at step 80: 247.4779, Accuracy: 0.6552\n",
      "Training loss (for one batch) at step 90: 266.5592, Accuracy: 0.6520\n",
      "Training loss (for one batch) at step 100: 254.9568, Accuracy: 0.6518\n",
      "Training loss (for one batch) at step 110: 244.6600, Accuracy: 0.6536\n",
      "---- Training ----\n",
      "Training loss: 81.6889\n",
      "Training acc over epoch: 0.6520\n",
      "---- Validation ----\n",
      "Validation loss: 47.5009\n",
      "Validation acc: 0.5903\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 277.0017, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 271.3339, Accuracy: 0.5774\n",
      "Training loss (for one batch) at step 20: 240.1888, Accuracy: 0.6116\n",
      "Training loss (for one batch) at step 30: 249.2205, Accuracy: 0.6386\n",
      "Training loss (for one batch) at step 40: 233.0329, Accuracy: 0.6608\n",
      "Training loss (for one batch) at step 50: 231.9207, Accuracy: 0.6729\n",
      "Training loss (for one batch) at step 60: 245.5608, Accuracy: 0.6774\n",
      "Training loss (for one batch) at step 70: 232.4810, Accuracy: 0.6690\n",
      "Training loss (for one batch) at step 80: 249.0367, Accuracy: 0.6544\n",
      "Training loss (for one batch) at step 90: 257.8192, Accuracy: 0.6497\n",
      "Training loss (for one batch) at step 100: 250.5922, Accuracy: 0.6518\n",
      "Training loss (for one batch) at step 110: 259.7403, Accuracy: 0.6521\n",
      "---- Training ----\n",
      "Training loss: 91.2633\n",
      "Training acc over epoch: 0.6505\n",
      "---- Validation ----\n",
      "Validation loss: 66.3864\n",
      "Validation acc: 0.5946\n",
      "Time taken: 17.87s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 244.1528, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 260.2925, Accuracy: 0.5902\n",
      "Training loss (for one batch) at step 20: 234.4200, Accuracy: 0.6124\n",
      "Training loss (for one batch) at step 30: 247.7160, Accuracy: 0.6452\n",
      "Training loss (for one batch) at step 40: 260.5408, Accuracy: 0.6656\n",
      "Training loss (for one batch) at step 50: 236.4694, Accuracy: 0.6742\n",
      "Training loss (for one batch) at step 60: 265.7978, Accuracy: 0.6808\n",
      "Training loss (for one batch) at step 70: 255.1185, Accuracy: 0.6708\n",
      "Training loss (for one batch) at step 80: 273.5211, Accuracy: 0.6572\n",
      "Training loss (for one batch) at step 90: 258.3308, Accuracy: 0.6534\n",
      "Training loss (for one batch) at step 100: 234.0458, Accuracy: 0.6550\n",
      "Training loss (for one batch) at step 110: 249.8376, Accuracy: 0.6558\n",
      "---- Training ----\n",
      "Training loss: 100.2331\n",
      "Training acc over epoch: 0.6543\n",
      "---- Validation ----\n",
      "Validation loss: 59.8592\n",
      "Validation acc: 0.5924\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 272.3704, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 252.5760, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 20: 235.8701, Accuracy: 0.6198\n",
      "Training loss (for one batch) at step 30: 255.8817, Accuracy: 0.6469\n",
      "Training loss (for one batch) at step 40: 241.3425, Accuracy: 0.6637\n",
      "Training loss (for one batch) at step 50: 238.0673, Accuracy: 0.6703\n",
      "Training loss (for one batch) at step 60: 256.6805, Accuracy: 0.6770\n",
      "Training loss (for one batch) at step 70: 238.8566, Accuracy: 0.6681\n",
      "Training loss (for one batch) at step 80: 263.9614, Accuracy: 0.6563\n",
      "Training loss (for one batch) at step 90: 245.2477, Accuracy: 0.6517\n",
      "Training loss (for one batch) at step 100: 231.3921, Accuracy: 0.6530\n",
      "Training loss (for one batch) at step 110: 238.7655, Accuracy: 0.6538\n",
      "---- Training ----\n",
      "Training loss: 89.0868\n",
      "Training acc over epoch: 0.6518\n",
      "---- Validation ----\n",
      "Validation loss: 55.0432\n",
      "Validation acc: 0.5930\n",
      "Time taken: 17.84s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 246.5576, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 260.5188, Accuracy: 0.5852\n",
      "Training loss (for one batch) at step 20: 243.3797, Accuracy: 0.6202\n",
      "Training loss (for one batch) at step 30: 231.4366, Accuracy: 0.6494\n",
      "Training loss (for one batch) at step 40: 233.8097, Accuracy: 0.6631\n",
      "Training loss (for one batch) at step 50: 240.7562, Accuracy: 0.6742\n",
      "Training loss (for one batch) at step 60: 242.4959, Accuracy: 0.6782\n",
      "Training loss (for one batch) at step 70: 263.9176, Accuracy: 0.6689\n",
      "Training loss (for one batch) at step 80: 243.6183, Accuracy: 0.6563\n",
      "Training loss (for one batch) at step 90: 238.5346, Accuracy: 0.6514\n",
      "Training loss (for one batch) at step 100: 233.6029, Accuracy: 0.6518\n",
      "Training loss (for one batch) at step 110: 254.5174, Accuracy: 0.6517\n",
      "---- Training ----\n",
      "Training loss: 76.6310\n",
      "Training acc over epoch: 0.6518\n",
      "---- Validation ----\n",
      "Validation loss: 48.1886\n",
      "Validation acc: 0.5983\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 267.0964, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 254.8773, Accuracy: 0.5845\n",
      "Training loss (for one batch) at step 20: 247.0587, Accuracy: 0.6127\n",
      "Training loss (for one batch) at step 30: 231.0380, Accuracy: 0.6477\n",
      "Training loss (for one batch) at step 40: 245.1096, Accuracy: 0.6648\n",
      "Training loss (for one batch) at step 50: 232.9711, Accuracy: 0.6752\n",
      "Training loss (for one batch) at step 60: 247.4306, Accuracy: 0.6819\n",
      "Training loss (for one batch) at step 70: 259.0135, Accuracy: 0.6725\n",
      "Training loss (for one batch) at step 80: 261.9495, Accuracy: 0.6569\n",
      "Training loss (for one batch) at step 90: 244.2093, Accuracy: 0.6530\n",
      "Training loss (for one batch) at step 100: 251.0114, Accuracy: 0.6520\n",
      "Training loss (for one batch) at step 110: 239.6543, Accuracy: 0.6528\n",
      "---- Training ----\n",
      "Training loss: 79.1336\n",
      "Training acc over epoch: 0.6520\n",
      "---- Validation ----\n",
      "Validation loss: 49.9487\n",
      "Validation acc: 0.5978\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 258.2671, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 264.1287, Accuracy: 0.5646\n",
      "Training loss (for one batch) at step 20: 229.4339, Accuracy: 0.6146\n",
      "Training loss (for one batch) at step 30: 230.7625, Accuracy: 0.6482\n",
      "Training loss (for one batch) at step 40: 240.4943, Accuracy: 0.6616\n",
      "Training loss (for one batch) at step 50: 240.5463, Accuracy: 0.6716\n",
      "Training loss (for one batch) at step 60: 240.3619, Accuracy: 0.6793\n",
      "Training loss (for one batch) at step 70: 233.4316, Accuracy: 0.6695\n",
      "Training loss (for one batch) at step 80: 258.8634, Accuracy: 0.6547\n",
      "Training loss (for one batch) at step 90: 241.2733, Accuracy: 0.6517\n",
      "Training loss (for one batch) at step 100: 247.0253, Accuracy: 0.6542\n",
      "Training loss (for one batch) at step 110: 236.7973, Accuracy: 0.6530\n",
      "---- Training ----\n",
      "Training loss: 70.9007\n",
      "Training acc over epoch: 0.6512\n",
      "---- Validation ----\n",
      "Validation loss: 41.2955\n",
      "Validation acc: 0.6021\n",
      "Time taken: 17.88s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 269.6402, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 238.1180, Accuracy: 0.5831\n",
      "Training loss (for one batch) at step 20: 233.6717, Accuracy: 0.6164\n",
      "Training loss (for one batch) at step 30: 234.9258, Accuracy: 0.6439\n",
      "Training loss (for one batch) at step 40: 242.3311, Accuracy: 0.6606\n",
      "Training loss (for one batch) at step 50: 240.1964, Accuracy: 0.6760\n",
      "Training loss (for one batch) at step 60: 230.7843, Accuracy: 0.6806\n",
      "Training loss (for one batch) at step 70: 259.0496, Accuracy: 0.6686\n",
      "Training loss (for one batch) at step 80: 245.7433, Accuracy: 0.6553\n",
      "Training loss (for one batch) at step 90: 245.9237, Accuracy: 0.6515\n",
      "Training loss (for one batch) at step 100: 258.8877, Accuracy: 0.6530\n",
      "Training loss (for one batch) at step 110: 246.5876, Accuracy: 0.6543\n",
      "---- Training ----\n",
      "Training loss: 85.0347\n",
      "Training acc over epoch: 0.6519\n",
      "---- Validation ----\n",
      "Validation loss: 42.9361\n",
      "Validation acc: 0.5916\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 242.9764, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 260.6759, Accuracy: 0.5739\n",
      "Training loss (for one batch) at step 20: 247.4688, Accuracy: 0.6153\n",
      "Training loss (for one batch) at step 30: 229.4512, Accuracy: 0.6442\n",
      "Training loss (for one batch) at step 40: 257.3839, Accuracy: 0.6608\n",
      "Training loss (for one batch) at step 50: 219.1167, Accuracy: 0.6736\n",
      "Training loss (for one batch) at step 60: 242.7213, Accuracy: 0.6776\n",
      "Training loss (for one batch) at step 70: 260.7449, Accuracy: 0.6663\n",
      "Training loss (for one batch) at step 80: 240.9790, Accuracy: 0.6538\n",
      "Training loss (for one batch) at step 90: 234.7525, Accuracy: 0.6503\n",
      "Training loss (for one batch) at step 100: 240.4095, Accuracy: 0.6518\n",
      "Training loss (for one batch) at step 110: 234.8338, Accuracy: 0.6510\n",
      "---- Training ----\n",
      "Training loss: 76.1027\n",
      "Training acc over epoch: 0.6511\n",
      "---- Validation ----\n",
      "Validation loss: 64.4599\n",
      "Validation acc: 0.6034\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 279.1739, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 243.8194, Accuracy: 0.5739\n",
      "Training loss (for one batch) at step 20: 241.2413, Accuracy: 0.6086\n",
      "Training loss (for one batch) at step 30: 227.9061, Accuracy: 0.6404\n",
      "Training loss (for one batch) at step 40: 239.2566, Accuracy: 0.6604\n",
      "Training loss (for one batch) at step 50: 237.9729, Accuracy: 0.6743\n",
      "Training loss (for one batch) at step 60: 248.2935, Accuracy: 0.6782\n",
      "Training loss (for one batch) at step 70: 246.4466, Accuracy: 0.6653\n",
      "Training loss (for one batch) at step 80: 258.1782, Accuracy: 0.6535\n",
      "Training loss (for one batch) at step 90: 254.0688, Accuracy: 0.6505\n",
      "Training loss (for one batch) at step 100: 248.3574, Accuracy: 0.6507\n",
      "Training loss (for one batch) at step 110: 239.0063, Accuracy: 0.6523\n",
      "---- Training ----\n",
      "Training loss: 82.8902\n",
      "Training acc over epoch: 0.6505\n",
      "---- Validation ----\n",
      "Validation loss: 33.0463\n",
      "Validation acc: 0.6206\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 274.0796, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 238.6404, Accuracy: 0.5866\n",
      "Training loss (for one batch) at step 20: 235.5728, Accuracy: 0.6116\n",
      "Training loss (for one batch) at step 30: 232.7845, Accuracy: 0.6459\n",
      "Training loss (for one batch) at step 40: 245.4474, Accuracy: 0.6631\n",
      "Training loss (for one batch) at step 50: 244.9578, Accuracy: 0.6749\n",
      "Training loss (for one batch) at step 60: 251.4067, Accuracy: 0.6788\n",
      "Training loss (for one batch) at step 70: 242.3391, Accuracy: 0.6673\n",
      "Training loss (for one batch) at step 80: 246.9427, Accuracy: 0.6540\n",
      "Training loss (for one batch) at step 90: 242.1848, Accuracy: 0.6508\n",
      "Training loss (for one batch) at step 100: 239.1483, Accuracy: 0.6528\n",
      "Training loss (for one batch) at step 110: 248.2353, Accuracy: 0.6537\n",
      "---- Training ----\n",
      "Training loss: 102.8411\n",
      "Training acc over epoch: 0.6521\n",
      "---- Validation ----\n",
      "Validation loss: 51.1828\n",
      "Validation acc: 0.6064\n",
      "Time taken: 17.88s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 242.4345, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 252.6557, Accuracy: 0.5611\n",
      "Training loss (for one batch) at step 20: 228.0116, Accuracy: 0.6209\n",
      "Training loss (for one batch) at step 30: 228.1220, Accuracy: 0.6396\n",
      "Training loss (for one batch) at step 40: 228.5793, Accuracy: 0.6593\n",
      "Training loss (for one batch) at step 50: 257.6227, Accuracy: 0.6697\n",
      "Training loss (for one batch) at step 60: 238.4561, Accuracy: 0.6769\n",
      "Training loss (for one batch) at step 70: 255.2731, Accuracy: 0.6662\n",
      "Training loss (for one batch) at step 80: 271.6817, Accuracy: 0.6536\n",
      "Training loss (for one batch) at step 90: 240.0482, Accuracy: 0.6502\n",
      "Training loss (for one batch) at step 100: 271.3182, Accuracy: 0.6502\n",
      "Training loss (for one batch) at step 110: 238.0546, Accuracy: 0.6502\n",
      "---- Training ----\n",
      "Training loss: 91.1779\n",
      "Training acc over epoch: 0.6495\n",
      "---- Validation ----\n",
      "Validation loss: 50.8576\n",
      "Validation acc: 0.5943\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 242.7908, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 239.0412, Accuracy: 0.5774\n",
      "Training loss (for one batch) at step 20: 233.5425, Accuracy: 0.6246\n",
      "Training loss (for one batch) at step 30: 256.8571, Accuracy: 0.6502\n",
      "Training loss (for one batch) at step 40: 234.0702, Accuracy: 0.6654\n",
      "Training loss (for one batch) at step 50: 229.5011, Accuracy: 0.6785\n",
      "Training loss (for one batch) at step 60: 234.2323, Accuracy: 0.6828\n",
      "Training loss (for one batch) at step 70: 255.4914, Accuracy: 0.6711\n",
      "Training loss (for one batch) at step 80: 272.4055, Accuracy: 0.6556\n",
      "Training loss (for one batch) at step 90: 234.5568, Accuracy: 0.6518\n",
      "Training loss (for one batch) at step 100: 247.5753, Accuracy: 0.6527\n",
      "Training loss (for one batch) at step 110: 258.8811, Accuracy: 0.6525\n",
      "---- Training ----\n",
      "Training loss: 89.3369\n",
      "Training acc over epoch: 0.6512\n",
      "---- Validation ----\n",
      "Validation loss: 50.0530\n",
      "Validation acc: 0.5795\n",
      "Time taken: 18.12s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 278.7600, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 239.0675, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 20: 242.2904, Accuracy: 0.6220\n",
      "Training loss (for one batch) at step 30: 218.9252, Accuracy: 0.6421\n",
      "Training loss (for one batch) at step 40: 224.8837, Accuracy: 0.6587\n",
      "Training loss (for one batch) at step 50: 224.6851, Accuracy: 0.6728\n",
      "Training loss (for one batch) at step 60: 232.1168, Accuracy: 0.6798\n",
      "Training loss (for one batch) at step 70: 236.1814, Accuracy: 0.6696\n",
      "Training loss (for one batch) at step 80: 247.9828, Accuracy: 0.6554\n",
      "Training loss (for one batch) at step 90: 237.7714, Accuracy: 0.6514\n",
      "Training loss (for one batch) at step 100: 222.3369, Accuracy: 0.6541\n",
      "Training loss (for one batch) at step 110: 240.5510, Accuracy: 0.6521\n",
      "---- Training ----\n",
      "Training loss: 71.0541\n",
      "Training acc over epoch: 0.6512\n",
      "---- Validation ----\n",
      "Validation loss: 60.4535\n",
      "Validation acc: 0.5760\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 272.9152, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 245.3697, Accuracy: 0.5795\n",
      "Training loss (for one batch) at step 20: 231.2524, Accuracy: 0.6027\n",
      "Training loss (for one batch) at step 30: 225.6600, Accuracy: 0.6376\n",
      "Training loss (for one batch) at step 40: 235.0020, Accuracy: 0.6603\n",
      "Training loss (for one batch) at step 50: 221.3810, Accuracy: 0.6731\n",
      "Training loss (for one batch) at step 60: 233.4787, Accuracy: 0.6764\n",
      "Training loss (for one batch) at step 70: 224.5435, Accuracy: 0.6680\n",
      "Training loss (for one batch) at step 80: 264.4460, Accuracy: 0.6535\n",
      "Training loss (for one batch) at step 90: 240.4956, Accuracy: 0.6491\n",
      "Training loss (for one batch) at step 100: 235.2361, Accuracy: 0.6509\n",
      "Training loss (for one batch) at step 110: 254.1259, Accuracy: 0.6515\n",
      "---- Training ----\n",
      "Training loss: 78.2117\n",
      "Training acc over epoch: 0.6507\n",
      "---- Validation ----\n",
      "Validation loss: 54.1768\n",
      "Validation acc: 0.5822\n",
      "Time taken: 18.05s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 254.2004, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 235.8540, Accuracy: 0.5689\n",
      "Training loss (for one batch) at step 20: 252.7218, Accuracy: 0.6228\n",
      "Training loss (for one batch) at step 30: 228.7083, Accuracy: 0.6469\n",
      "Training loss (for one batch) at step 40: 232.0327, Accuracy: 0.6622\n",
      "Training loss (for one batch) at step 50: 238.5034, Accuracy: 0.6778\n",
      "Training loss (for one batch) at step 60: 224.5322, Accuracy: 0.6807\n",
      "Training loss (for one batch) at step 70: 253.7633, Accuracy: 0.6687\n",
      "Training loss (for one batch) at step 80: 252.3410, Accuracy: 0.6533\n",
      "Training loss (for one batch) at step 90: 236.8753, Accuracy: 0.6505\n",
      "Training loss (for one batch) at step 100: 229.6756, Accuracy: 0.6532\n",
      "Training loss (for one batch) at step 110: 228.8549, Accuracy: 0.6536\n",
      "---- Training ----\n",
      "Training loss: 80.9674\n",
      "Training acc over epoch: 0.6515\n",
      "---- Validation ----\n",
      "Validation loss: 42.4122\n",
      "Validation acc: 0.5752\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 242.9184, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 257.6612, Accuracy: 0.5831\n",
      "Training loss (for one batch) at step 20: 224.8165, Accuracy: 0.6116\n",
      "Training loss (for one batch) at step 30: 230.7151, Accuracy: 0.6469\n",
      "Training loss (for one batch) at step 40: 236.7178, Accuracy: 0.6658\n",
      "Training loss (for one batch) at step 50: 227.2537, Accuracy: 0.6777\n",
      "Training loss (for one batch) at step 60: 243.2040, Accuracy: 0.6831\n",
      "Training loss (for one batch) at step 70: 244.9407, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 80: 253.3895, Accuracy: 0.6561\n",
      "Training loss (for one batch) at step 90: 240.2751, Accuracy: 0.6537\n",
      "Training loss (for one batch) at step 100: 239.0520, Accuracy: 0.6530\n",
      "Training loss (for one batch) at step 110: 242.1277, Accuracy: 0.6518\n",
      "---- Training ----\n",
      "Training loss: 71.7997\n",
      "Training acc over epoch: 0.6517\n",
      "---- Validation ----\n",
      "Validation loss: 58.9196\n",
      "Validation acc: 0.5873\n",
      "Time taken: 18.09s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 237.9353, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 275.0824, Accuracy: 0.5710\n",
      "Training loss (for one batch) at step 20: 250.2880, Accuracy: 0.6105\n",
      "Training loss (for one batch) at step 30: 227.0699, Accuracy: 0.6401\n",
      "Training loss (for one batch) at step 40: 244.5329, Accuracy: 0.6578\n",
      "Training loss (for one batch) at step 50: 228.4529, Accuracy: 0.6714\n",
      "Training loss (for one batch) at step 60: 234.5219, Accuracy: 0.6782\n",
      "Training loss (for one batch) at step 70: 255.0721, Accuracy: 0.6688\n",
      "Training loss (for one batch) at step 80: 240.4782, Accuracy: 0.6536\n",
      "Training loss (for one batch) at step 90: 244.5724, Accuracy: 0.6475\n",
      "Training loss (for one batch) at step 100: 230.8015, Accuracy: 0.6510\n",
      "Training loss (for one batch) at step 110: 236.9390, Accuracy: 0.6518\n",
      "---- Training ----\n",
      "Training loss: 73.0257\n",
      "Training acc over epoch: 0.6503\n",
      "---- Validation ----\n",
      "Validation loss: 64.7414\n",
      "Validation acc: 0.6102\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 235.5814, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 241.8520, Accuracy: 0.5952\n",
      "Training loss (for one batch) at step 20: 225.2803, Accuracy: 0.6243\n",
      "Training loss (for one batch) at step 30: 225.2791, Accuracy: 0.6499\n",
      "Training loss (for one batch) at step 40: 223.2675, Accuracy: 0.6637\n",
      "Training loss (for one batch) at step 50: 227.3910, Accuracy: 0.6759\n",
      "Training loss (for one batch) at step 60: 244.2219, Accuracy: 0.6793\n",
      "Training loss (for one batch) at step 70: 243.0249, Accuracy: 0.6702\n",
      "Training loss (for one batch) at step 80: 247.6624, Accuracy: 0.6578\n",
      "Training loss (for one batch) at step 90: 260.1221, Accuracy: 0.6528\n",
      "Training loss (for one batch) at step 100: 228.5775, Accuracy: 0.6545\n",
      "Training loss (for one batch) at step 110: 232.8516, Accuracy: 0.6534\n",
      "---- Training ----\n",
      "Training loss: 82.6258\n",
      "Training acc over epoch: 0.6521\n",
      "---- Validation ----\n",
      "Validation loss: 41.9937\n",
      "Validation acc: 0.5801\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 231.5632, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 237.7695, Accuracy: 0.5810\n",
      "Training loss (for one batch) at step 20: 234.1277, Accuracy: 0.6161\n",
      "Training loss (for one batch) at step 30: 229.3391, Accuracy: 0.6489\n",
      "Training loss (for one batch) at step 40: 232.7777, Accuracy: 0.6618\n",
      "Training loss (for one batch) at step 50: 242.2790, Accuracy: 0.6722\n",
      "Training loss (for one batch) at step 60: 212.1999, Accuracy: 0.6764\n",
      "Training loss (for one batch) at step 70: 251.5637, Accuracy: 0.6686\n",
      "Training loss (for one batch) at step 80: 241.8900, Accuracy: 0.6557\n",
      "Training loss (for one batch) at step 90: 230.8753, Accuracy: 0.6497\n",
      "Training loss (for one batch) at step 100: 244.4416, Accuracy: 0.6534\n",
      "Training loss (for one batch) at step 110: 253.0078, Accuracy: 0.6534\n",
      "---- Training ----\n",
      "Training loss: 81.4910\n",
      "Training acc over epoch: 0.6518\n",
      "---- Validation ----\n",
      "Validation loss: 51.9652\n",
      "Validation acc: 0.5970\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 241.9178, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 240.3254, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 20: 231.1637, Accuracy: 0.6127\n",
      "Training loss (for one batch) at step 30: 233.6120, Accuracy: 0.6381\n",
      "Training loss (for one batch) at step 40: 223.2681, Accuracy: 0.6612\n",
      "Training loss (for one batch) at step 50: 231.5812, Accuracy: 0.6748\n",
      "Training loss (for one batch) at step 60: 232.2780, Accuracy: 0.6811\n",
      "Training loss (for one batch) at step 70: 243.9317, Accuracy: 0.6675\n",
      "Training loss (for one batch) at step 80: 233.0919, Accuracy: 0.6544\n",
      "Training loss (for one batch) at step 90: 228.1618, Accuracy: 0.6506\n",
      "Training loss (for one batch) at step 100: 235.6662, Accuracy: 0.6535\n",
      "Training loss (for one batch) at step 110: 241.8931, Accuracy: 0.6537\n",
      "---- Training ----\n",
      "Training loss: 68.7707\n",
      "Training acc over epoch: 0.6524\n",
      "---- Validation ----\n",
      "Validation loss: 59.3067\n",
      "Validation acc: 0.5997\n",
      "Time taken: 18.09s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 242.4963, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 236.2808, Accuracy: 0.5881\n",
      "Training loss (for one batch) at step 20: 226.0055, Accuracy: 0.6198\n",
      "Training loss (for one batch) at step 30: 218.8055, Accuracy: 0.6439\n",
      "Training loss (for one batch) at step 40: 216.6235, Accuracy: 0.6633\n",
      "Training loss (for one batch) at step 50: 220.4172, Accuracy: 0.6762\n",
      "Training loss (for one batch) at step 60: 228.9606, Accuracy: 0.6808\n",
      "Training loss (for one batch) at step 70: 230.5923, Accuracy: 0.6693\n",
      "Training loss (for one batch) at step 80: 224.7933, Accuracy: 0.6570\n",
      "Training loss (for one batch) at step 90: 248.2608, Accuracy: 0.6502\n",
      "Training loss (for one batch) at step 100: 232.5334, Accuracy: 0.6510\n",
      "Training loss (for one batch) at step 110: 224.3345, Accuracy: 0.6522\n",
      "---- Training ----\n",
      "Training loss: 75.5251\n",
      "Training acc over epoch: 0.6499\n",
      "---- Validation ----\n",
      "Validation loss: 49.7586\n",
      "Validation acc: 0.6134\n",
      "Time taken: 18.09s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 232.4318, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 244.0279, Accuracy: 0.5788\n",
      "Training loss (for one batch) at step 20: 231.7044, Accuracy: 0.6179\n",
      "Training loss (for one batch) at step 30: 247.3944, Accuracy: 0.6452\n",
      "Training loss (for one batch) at step 40: 263.8481, Accuracy: 0.6614\n",
      "Training loss (for one batch) at step 50: 229.1549, Accuracy: 0.6737\n",
      "Training loss (for one batch) at step 60: 246.3973, Accuracy: 0.6802\n",
      "Training loss (for one batch) at step 70: 245.0818, Accuracy: 0.6688\n",
      "Training loss (for one batch) at step 80: 235.3096, Accuracy: 0.6563\n",
      "Training loss (for one batch) at step 90: 238.0780, Accuracy: 0.6515\n",
      "Training loss (for one batch) at step 100: 252.8051, Accuracy: 0.6528\n",
      "Training loss (for one batch) at step 110: 223.7012, Accuracy: 0.6526\n",
      "---- Training ----\n",
      "Training loss: 81.4162\n",
      "Training acc over epoch: 0.6515\n",
      "---- Validation ----\n",
      "Validation loss: 43.3913\n",
      "Validation acc: 0.6008\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 230.2423, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 244.5895, Accuracy: 0.5696\n",
      "Training loss (for one batch) at step 20: 249.0795, Accuracy: 0.6124\n",
      "Training loss (for one batch) at step 30: 218.9425, Accuracy: 0.6429\n",
      "Training loss (for one batch) at step 40: 227.3844, Accuracy: 0.6622\n",
      "Training loss (for one batch) at step 50: 226.6566, Accuracy: 0.6751\n",
      "Training loss (for one batch) at step 60: 231.7210, Accuracy: 0.6785\n",
      "Training loss (for one batch) at step 70: 248.2592, Accuracy: 0.6693\n",
      "Training loss (for one batch) at step 80: 234.4923, Accuracy: 0.6566\n",
      "Training loss (for one batch) at step 90: 225.1026, Accuracy: 0.6525\n",
      "Training loss (for one batch) at step 100: 233.0649, Accuracy: 0.6522\n",
      "Training loss (for one batch) at step 110: 234.5743, Accuracy: 0.6529\n",
      "---- Training ----\n",
      "Training loss: 81.0656\n",
      "Training acc over epoch: 0.6511\n",
      "---- Validation ----\n",
      "Validation loss: 36.9247\n",
      "Validation acc: 0.5911\n",
      "Time taken: 18.11s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 243.8320, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 256.0658, Accuracy: 0.5810\n",
      "Training loss (for one batch) at step 20: 227.0744, Accuracy: 0.6060\n",
      "Training loss (for one batch) at step 30: 228.3392, Accuracy: 0.6474\n",
      "Training loss (for one batch) at step 40: 221.1212, Accuracy: 0.6620\n",
      "Training loss (for one batch) at step 50: 219.2378, Accuracy: 0.6705\n",
      "Training loss (for one batch) at step 60: 231.4139, Accuracy: 0.6769\n",
      "Training loss (for one batch) at step 70: 257.0569, Accuracy: 0.6686\n",
      "Training loss (for one batch) at step 80: 231.3305, Accuracy: 0.6536\n",
      "Training loss (for one batch) at step 90: 232.6220, Accuracy: 0.6508\n",
      "Training loss (for one batch) at step 100: 233.8259, Accuracy: 0.6509\n",
      "Training loss (for one batch) at step 110: 231.0983, Accuracy: 0.6517\n",
      "---- Training ----\n",
      "Training loss: 69.1031\n",
      "Training acc over epoch: 0.6507\n",
      "---- Validation ----\n",
      "Validation loss: 38.0675\n",
      "Validation acc: 0.5943\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 232.0833, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 253.9434, Accuracy: 0.5710\n",
      "Training loss (for one batch) at step 20: 234.2501, Accuracy: 0.6187\n",
      "Training loss (for one batch) at step 30: 235.5773, Accuracy: 0.6426\n",
      "Training loss (for one batch) at step 40: 233.1588, Accuracy: 0.6612\n",
      "Training loss (for one batch) at step 50: 238.9660, Accuracy: 0.6720\n",
      "Training loss (for one batch) at step 60: 229.0391, Accuracy: 0.6771\n",
      "Training loss (for one batch) at step 70: 247.3391, Accuracy: 0.6662\n",
      "Training loss (for one batch) at step 80: 241.1591, Accuracy: 0.6523\n",
      "Training loss (for one batch) at step 90: 247.6463, Accuracy: 0.6505\n",
      "Training loss (for one batch) at step 100: 224.1703, Accuracy: 0.6512\n",
      "Training loss (for one batch) at step 110: 236.0742, Accuracy: 0.6518\n",
      "---- Training ----\n",
      "Training loss: 80.9311\n",
      "Training acc over epoch: 0.6500\n",
      "---- Validation ----\n",
      "Validation loss: 56.3015\n",
      "Validation acc: 0.5819\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 239.3773, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 234.0948, Accuracy: 0.5710\n",
      "Training loss (for one batch) at step 20: 230.0996, Accuracy: 0.6064\n",
      "Training loss (for one batch) at step 30: 214.9397, Accuracy: 0.6436\n",
      "Training loss (for one batch) at step 40: 238.4441, Accuracy: 0.6582\n",
      "Training loss (for one batch) at step 50: 230.8410, Accuracy: 0.6737\n",
      "Training loss (for one batch) at step 60: 218.4978, Accuracy: 0.6789\n",
      "Training loss (for one batch) at step 70: 231.5994, Accuracy: 0.6667\n",
      "Training loss (for one batch) at step 80: 232.4266, Accuracy: 0.6554\n",
      "Training loss (for one batch) at step 90: 237.4408, Accuracy: 0.6507\n",
      "Training loss (for one batch) at step 100: 219.3836, Accuracy: 0.6528\n",
      "Training loss (for one batch) at step 110: 224.2766, Accuracy: 0.6521\n",
      "---- Training ----\n",
      "Training loss: 78.3787\n",
      "Training acc over epoch: 0.6506\n",
      "---- Validation ----\n",
      "Validation loss: 75.2767\n",
      "Validation acc: 0.5930\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 257.1287, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 255.1447, Accuracy: 0.5724\n",
      "Training loss (for one batch) at step 20: 228.9999, Accuracy: 0.6153\n",
      "Training loss (for one batch) at step 30: 225.4704, Accuracy: 0.6436\n",
      "Training loss (for one batch) at step 40: 213.9142, Accuracy: 0.6622\n",
      "Training loss (for one batch) at step 50: 228.9197, Accuracy: 0.6746\n",
      "Training loss (for one batch) at step 60: 256.7167, Accuracy: 0.6753\n",
      "Training loss (for one batch) at step 70: 253.6789, Accuracy: 0.6646\n",
      "Training loss (for one batch) at step 80: 253.4564, Accuracy: 0.6502\n",
      "Training loss (for one batch) at step 90: 245.4766, Accuracy: 0.6477\n",
      "Training loss (for one batch) at step 100: 241.6799, Accuracy: 0.6489\n",
      "Training loss (for one batch) at step 110: 236.4835, Accuracy: 0.6494\n",
      "---- Training ----\n",
      "Training loss: 77.5004\n",
      "Training acc over epoch: 0.6484\n",
      "---- Validation ----\n",
      "Validation loss: 35.9253\n",
      "Validation acc: 0.5782\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 231.7794, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 239.5732, Accuracy: 0.5895\n",
      "Training loss (for one batch) at step 20: 230.6612, Accuracy: 0.6235\n",
      "Training loss (for one batch) at step 30: 241.7052, Accuracy: 0.6487\n",
      "Training loss (for one batch) at step 40: 229.6600, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 50: 215.2648, Accuracy: 0.6757\n",
      "Training loss (for one batch) at step 60: 235.4048, Accuracy: 0.6808\n",
      "Training loss (for one batch) at step 70: 227.7402, Accuracy: 0.6713\n",
      "Training loss (for one batch) at step 80: 242.1358, Accuracy: 0.6571\n",
      "Training loss (for one batch) at step 90: 241.1017, Accuracy: 0.6513\n",
      "Training loss (for one batch) at step 100: 226.5571, Accuracy: 0.6522\n",
      "Training loss (for one batch) at step 110: 273.3527, Accuracy: 0.6521\n",
      "---- Training ----\n",
      "Training loss: 75.2118\n",
      "Training acc over epoch: 0.6509\n",
      "---- Validation ----\n",
      "Validation loss: 42.5283\n",
      "Validation acc: 0.5876\n",
      "Time taken: 18.29s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 265.3506, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 240.2508, Accuracy: 0.5930\n",
      "Training loss (for one batch) at step 20: 219.9503, Accuracy: 0.6198\n",
      "Training loss (for one batch) at step 30: 235.9373, Accuracy: 0.6467\n",
      "Training loss (for one batch) at step 40: 234.1103, Accuracy: 0.6599\n",
      "Training loss (for one batch) at step 50: 244.5039, Accuracy: 0.6740\n",
      "Training loss (for one batch) at step 60: 240.6263, Accuracy: 0.6780\n",
      "Training loss (for one batch) at step 70: 244.1651, Accuracy: 0.6682\n",
      "Training loss (for one batch) at step 80: 247.0543, Accuracy: 0.6540\n",
      "Training loss (for one batch) at step 90: 230.3442, Accuracy: 0.6490\n",
      "Training loss (for one batch) at step 100: 241.4669, Accuracy: 0.6517\n",
      "Training loss (for one batch) at step 110: 221.8873, Accuracy: 0.6522\n",
      "---- Training ----\n",
      "Training loss: 67.1957\n",
      "Training acc over epoch: 0.6510\n",
      "---- Validation ----\n",
      "Validation loss: 48.1546\n",
      "Validation acc: 0.5798\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 234.3213, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 237.0756, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 20: 241.8902, Accuracy: 0.6150\n",
      "Training loss (for one batch) at step 30: 229.2225, Accuracy: 0.6439\n",
      "Training loss (for one batch) at step 40: 223.0510, Accuracy: 0.6583\n",
      "Training loss (for one batch) at step 50: 246.3423, Accuracy: 0.6725\n",
      "Training loss (for one batch) at step 60: 222.1353, Accuracy: 0.6798\n",
      "Training loss (for one batch) at step 70: 248.9805, Accuracy: 0.6690\n",
      "Training loss (for one batch) at step 80: 234.6429, Accuracy: 0.6557\n",
      "Training loss (for one batch) at step 90: 238.6157, Accuracy: 0.6510\n",
      "Training loss (for one batch) at step 100: 224.0835, Accuracy: 0.6525\n",
      "Training loss (for one batch) at step 110: 232.7740, Accuracy: 0.6529\n",
      "---- Training ----\n",
      "Training loss: 67.8074\n",
      "Training acc over epoch: 0.6515\n",
      "---- Validation ----\n",
      "Validation loss: 63.6925\n",
      "Validation acc: 0.5836\n",
      "Time taken: 18.31s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 250.7525, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 239.7552, Accuracy: 0.5866\n",
      "Training loss (for one batch) at step 20: 214.0142, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 30: 218.7071, Accuracy: 0.6394\n",
      "Training loss (for one batch) at step 40: 232.0542, Accuracy: 0.6620\n",
      "Training loss (for one batch) at step 50: 220.9940, Accuracy: 0.6736\n",
      "Training loss (for one batch) at step 60: 216.4175, Accuracy: 0.6782\n",
      "Training loss (for one batch) at step 70: 237.0237, Accuracy: 0.6646\n",
      "Training loss (for one batch) at step 80: 237.0685, Accuracy: 0.6517\n",
      "Training loss (for one batch) at step 90: 243.2685, Accuracy: 0.6486\n",
      "Training loss (for one batch) at step 100: 227.2843, Accuracy: 0.6491\n",
      "Training loss (for one batch) at step 110: 234.8793, Accuracy: 0.6511\n",
      "---- Training ----\n",
      "Training loss: 71.9905\n",
      "Training acc over epoch: 0.6495\n",
      "---- Validation ----\n",
      "Validation loss: 51.3166\n",
      "Validation acc: 0.5946\n",
      "Time taken: 18.05s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 243.5427, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 247.1314, Accuracy: 0.5788\n",
      "Training loss (for one batch) at step 20: 228.7842, Accuracy: 0.6168\n",
      "Training loss (for one batch) at step 30: 229.3374, Accuracy: 0.6454\n",
      "Training loss (for one batch) at step 40: 224.7600, Accuracy: 0.6601\n",
      "Training loss (for one batch) at step 50: 222.8730, Accuracy: 0.6710\n",
      "Training loss (for one batch) at step 60: 226.2656, Accuracy: 0.6792\n",
      "Training loss (for one batch) at step 70: 235.8185, Accuracy: 0.6676\n",
      "Training loss (for one batch) at step 80: 237.7973, Accuracy: 0.6543\n",
      "Training loss (for one batch) at step 90: 231.7927, Accuracy: 0.6502\n",
      "Training loss (for one batch) at step 100: 220.1977, Accuracy: 0.6515\n",
      "Training loss (for one batch) at step 110: 233.0367, Accuracy: 0.6518\n",
      "---- Training ----\n",
      "Training loss: 78.9362\n",
      "Training acc over epoch: 0.6499\n",
      "---- Validation ----\n",
      "Validation loss: 46.4663\n",
      "Validation acc: 0.5924\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 235.0519, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 235.1616, Accuracy: 0.5831\n",
      "Training loss (for one batch) at step 20: 219.1688, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 30: 223.8601, Accuracy: 0.6431\n",
      "Training loss (for one batch) at step 40: 210.4091, Accuracy: 0.6597\n",
      "Training loss (for one batch) at step 50: 214.8694, Accuracy: 0.6728\n",
      "Training loss (for one batch) at step 60: 217.7130, Accuracy: 0.6785\n",
      "Training loss (for one batch) at step 70: 219.0828, Accuracy: 0.6698\n",
      "Training loss (for one batch) at step 80: 229.0242, Accuracy: 0.6538\n",
      "Training loss (for one batch) at step 90: 244.1350, Accuracy: 0.6505\n",
      "Training loss (for one batch) at step 100: 219.2765, Accuracy: 0.6518\n",
      "Training loss (for one batch) at step 110: 236.9097, Accuracy: 0.6508\n",
      "---- Training ----\n",
      "Training loss: 72.8008\n",
      "Training acc over epoch: 0.6500\n",
      "---- Validation ----\n",
      "Validation loss: 86.7877\n",
      "Validation acc: 0.6072\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 234.1155, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 228.7999, Accuracy: 0.5845\n",
      "Training loss (for one batch) at step 20: 245.3920, Accuracy: 0.6168\n",
      "Training loss (for one batch) at step 30: 231.3711, Accuracy: 0.6434\n",
      "Training loss (for one batch) at step 40: 219.4553, Accuracy: 0.6601\n",
      "Training loss (for one batch) at step 50: 216.7565, Accuracy: 0.6716\n",
      "Training loss (for one batch) at step 60: 223.5493, Accuracy: 0.6789\n",
      "Training loss (for one batch) at step 70: 235.6664, Accuracy: 0.6649\n",
      "Training loss (for one batch) at step 80: 245.2245, Accuracy: 0.6515\n",
      "Training loss (for one batch) at step 90: 246.2270, Accuracy: 0.6469\n",
      "Training loss (for one batch) at step 100: 238.3219, Accuracy: 0.6494\n",
      "Training loss (for one batch) at step 110: 227.6961, Accuracy: 0.6496\n",
      "---- Training ----\n",
      "Training loss: 75.0297\n",
      "Training acc over epoch: 0.6494\n",
      "---- Validation ----\n",
      "Validation loss: 39.6588\n",
      "Validation acc: 0.5731\n",
      "Time taken: 18.07s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 249.2249, Accuracy: 0.4922\n",
      "Training loss (for one batch) at step 10: 237.5931, Accuracy: 0.5696\n",
      "Training loss (for one batch) at step 20: 219.6200, Accuracy: 0.6135\n",
      "Training loss (for one batch) at step 30: 224.0872, Accuracy: 0.6462\n",
      "Training loss (for one batch) at step 40: 224.7206, Accuracy: 0.6580\n",
      "Training loss (for one batch) at step 50: 227.5876, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 60: 233.4396, Accuracy: 0.6796\n",
      "Training loss (for one batch) at step 70: 243.7547, Accuracy: 0.6684\n",
      "Training loss (for one batch) at step 80: 260.0677, Accuracy: 0.6522\n",
      "Training loss (for one batch) at step 90: 228.4460, Accuracy: 0.6499\n",
      "Training loss (for one batch) at step 100: 227.6682, Accuracy: 0.6508\n",
      "Training loss (for one batch) at step 110: 221.5087, Accuracy: 0.6504\n",
      "---- Training ----\n",
      "Training loss: 112.0330\n",
      "Training acc over epoch: 0.6497\n",
      "---- Validation ----\n",
      "Validation loss: 41.3959\n",
      "Validation acc: 0.5846\n",
      "Time taken: 17.96s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.5868\n",
      "Validation AUC: 0.5900\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 692.2372, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 629.3203, Accuracy: 0.5668\n",
      "Training loss (for one batch) at step 20: 588.2375, Accuracy: 0.5413\n",
      "Training loss (for one batch) at step 30: 532.3591, Accuracy: 0.5315\n",
      "Training loss (for one batch) at step 40: 524.3278, Accuracy: 0.5276\n",
      "Training loss (for one batch) at step 50: 517.4301, Accuracy: 0.5306\n",
      "Training loss (for one batch) at step 60: 527.5547, Accuracy: 0.5336\n",
      "Training loss (for one batch) at step 70: 479.6271, Accuracy: 0.5336\n",
      "Training loss (for one batch) at step 80: 500.4022, Accuracy: 0.5308\n",
      "Training loss (for one batch) at step 90: 488.1693, Accuracy: 0.5267\n",
      "Training loss (for one batch) at step 100: 498.9133, Accuracy: 0.5253\n",
      "Training loss (for one batch) at step 110: 471.6760, Accuracy: 0.5258\n",
      "---- Training ----\n",
      "Training loss: 147.7240\n",
      "Training acc over epoch: 0.5263\n",
      "---- Validation ----\n",
      "Validation loss: 34.1617\n",
      "Validation acc: 0.5134\n",
      "Time taken: 21.95s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 460.1594, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 464.8267, Accuracy: 0.5490\n",
      "Training loss (for one batch) at step 20: 469.9790, Accuracy: 0.5502\n",
      "Training loss (for one batch) at step 30: 459.6346, Accuracy: 0.5481\n",
      "Training loss (for one batch) at step 40: 463.5560, Accuracy: 0.5404\n",
      "Training loss (for one batch) at step 50: 457.3235, Accuracy: 0.5365\n",
      "Training loss (for one batch) at step 60: 454.4033, Accuracy: 0.5345\n",
      "Training loss (for one batch) at step 70: 451.8601, Accuracy: 0.5328\n",
      "Training loss (for one batch) at step 80: 450.3338, Accuracy: 0.5333\n",
      "Training loss (for one batch) at step 90: 454.6089, Accuracy: 0.5338\n",
      "Training loss (for one batch) at step 100: 452.3087, Accuracy: 0.5350\n",
      "Training loss (for one batch) at step 110: 454.5349, Accuracy: 0.5334\n",
      "---- Training ----\n",
      "Training loss: 141.7345\n",
      "Training acc over epoch: 0.5320\n",
      "---- Validation ----\n",
      "Validation loss: 35.0976\n",
      "Validation acc: 0.5116\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 454.8994, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 448.6319, Accuracy: 0.5405\n",
      "Training loss (for one batch) at step 20: 448.3515, Accuracy: 0.5387\n",
      "Training loss (for one batch) at step 30: 446.3850, Accuracy: 0.5456\n",
      "Training loss (for one batch) at step 40: 447.3971, Accuracy: 0.5394\n",
      "Training loss (for one batch) at step 50: 448.1672, Accuracy: 0.5351\n",
      "Training loss (for one batch) at step 60: 448.2753, Accuracy: 0.5328\n",
      "Training loss (for one batch) at step 70: 447.3571, Accuracy: 0.5354\n",
      "Training loss (for one batch) at step 80: 446.9546, Accuracy: 0.5413\n",
      "Training loss (for one batch) at step 90: 445.9252, Accuracy: 0.5427\n",
      "Training loss (for one batch) at step 100: 445.2571, Accuracy: 0.5430\n",
      "Training loss (for one batch) at step 110: 447.4921, Accuracy: 0.5425\n",
      "---- Training ----\n",
      "Training loss: 138.6040\n",
      "Training acc over epoch: 0.5426\n",
      "---- Validation ----\n",
      "Validation loss: 34.8654\n",
      "Validation acc: 0.5793\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 441.3638, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 442.2029, Accuracy: 0.5845\n",
      "Training loss (for one batch) at step 20: 442.0331, Accuracy: 0.5707\n",
      "Training loss (for one batch) at step 30: 446.9914, Accuracy: 0.5706\n",
      "Training loss (for one batch) at step 40: 445.0059, Accuracy: 0.5659\n",
      "Training loss (for one batch) at step 50: 445.2677, Accuracy: 0.5630\n",
      "Training loss (for one batch) at step 60: 444.2343, Accuracy: 0.5578\n",
      "Training loss (for one batch) at step 70: 442.4562, Accuracy: 0.5534\n",
      "Training loss (for one batch) at step 80: 443.2440, Accuracy: 0.5514\n",
      "Training loss (for one batch) at step 90: 445.5055, Accuracy: 0.5540\n",
      "Training loss (for one batch) at step 100: 445.4245, Accuracy: 0.5558\n",
      "Training loss (for one batch) at step 110: 444.3546, Accuracy: 0.5564\n",
      "---- Training ----\n",
      "Training loss: 138.3053\n",
      "Training acc over epoch: 0.5558\n",
      "---- Validation ----\n",
      "Validation loss: 34.9860\n",
      "Validation acc: 0.5948\n",
      "Time taken: 17.89s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 441.9662, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 443.5487, Accuracy: 0.5604\n",
      "Training loss (for one batch) at step 20: 443.7578, Accuracy: 0.5662\n",
      "Training loss (for one batch) at step 30: 442.6957, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 40: 446.7538, Accuracy: 0.5785\n",
      "Training loss (for one batch) at step 50: 444.4367, Accuracy: 0.5764\n",
      "Training loss (for one batch) at step 60: 443.0047, Accuracy: 0.5753\n",
      "Training loss (for one batch) at step 70: 443.0898, Accuracy: 0.5749\n",
      "Training loss (for one batch) at step 80: 443.1789, Accuracy: 0.5761\n",
      "Training loss (for one batch) at step 90: 446.2892, Accuracy: 0.5793\n",
      "Training loss (for one batch) at step 100: 441.0338, Accuracy: 0.5810\n",
      "Training loss (for one batch) at step 110: 443.1243, Accuracy: 0.5813\n",
      "---- Training ----\n",
      "Training loss: 140.1808\n",
      "Training acc over epoch: 0.5819\n",
      "---- Validation ----\n",
      "Validation loss: 34.7361\n",
      "Validation acc: 0.6454\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 439.6204, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 441.4240, Accuracy: 0.5945\n",
      "Training loss (for one batch) at step 20: 441.4602, Accuracy: 0.5934\n",
      "Training loss (for one batch) at step 30: 439.5074, Accuracy: 0.5917\n",
      "Training loss (for one batch) at step 40: 441.2822, Accuracy: 0.5888\n",
      "Training loss (for one batch) at step 50: 444.2673, Accuracy: 0.5885\n",
      "Training loss (for one batch) at step 60: 440.5714, Accuracy: 0.5962\n",
      "Training loss (for one batch) at step 70: 442.7759, Accuracy: 0.5924\n",
      "Training loss (for one batch) at step 80: 442.5189, Accuracy: 0.5936\n",
      "Training loss (for one batch) at step 90: 438.7433, Accuracy: 0.5929\n",
      "Training loss (for one batch) at step 100: 442.6514, Accuracy: 0.5902\n",
      "Training loss (for one batch) at step 110: 440.0082, Accuracy: 0.5903\n",
      "---- Training ----\n",
      "Training loss: 136.6616\n",
      "Training acc over epoch: 0.5909\n",
      "---- Validation ----\n",
      "Validation loss: 34.6434\n",
      "Validation acc: 0.6214\n",
      "Time taken: 17.89s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 441.7044, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 440.2776, Accuracy: 0.6236\n",
      "Training loss (for one batch) at step 20: 443.6724, Accuracy: 0.6235\n",
      "Training loss (for one batch) at step 30: 440.1846, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 40: 443.1853, Accuracy: 0.6122\n",
      "Training loss (for one batch) at step 50: 441.2039, Accuracy: 0.6164\n",
      "Training loss (for one batch) at step 60: 446.2140, Accuracy: 0.6174\n",
      "Training loss (for one batch) at step 70: 444.7308, Accuracy: 0.6199\n",
      "Training loss (for one batch) at step 80: 441.7709, Accuracy: 0.6180\n",
      "Training loss (for one batch) at step 90: 441.1949, Accuracy: 0.6132\n",
      "Training loss (for one batch) at step 100: 440.1833, Accuracy: 0.6116\n",
      "Training loss (for one batch) at step 110: 442.6760, Accuracy: 0.6108\n",
      "---- Training ----\n",
      "Training loss: 138.2280\n",
      "Training acc over epoch: 0.6119\n",
      "---- Validation ----\n",
      "Validation loss: 35.0654\n",
      "Validation acc: 0.6467\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 442.0750, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 438.2885, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 20: 436.9774, Accuracy: 0.6451\n",
      "Training loss (for one batch) at step 30: 443.9117, Accuracy: 0.6384\n",
      "Training loss (for one batch) at step 40: 439.4393, Accuracy: 0.6324\n",
      "Training loss (for one batch) at step 50: 442.6127, Accuracy: 0.6334\n",
      "Training loss (for one batch) at step 60: 437.3954, Accuracy: 0.6370\n",
      "Training loss (for one batch) at step 70: 443.2059, Accuracy: 0.6366\n",
      "Training loss (for one batch) at step 80: 443.9896, Accuracy: 0.6341\n",
      "Training loss (for one batch) at step 90: 439.1381, Accuracy: 0.6271\n",
      "Training loss (for one batch) at step 100: 441.9373, Accuracy: 0.6262\n",
      "Training loss (for one batch) at step 110: 444.2843, Accuracy: 0.6283\n",
      "---- Training ----\n",
      "Training loss: 137.8913\n",
      "Training acc over epoch: 0.6284\n",
      "---- Validation ----\n",
      "Validation loss: 35.5008\n",
      "Validation acc: 0.6711\n",
      "Time taken: 18.13s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 436.3718, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 437.7457, Accuracy: 0.6506\n",
      "Training loss (for one batch) at step 20: 438.7973, Accuracy: 0.6488\n",
      "Training loss (for one batch) at step 30: 432.9433, Accuracy: 0.6550\n",
      "Training loss (for one batch) at step 40: 437.7031, Accuracy: 0.6479\n",
      "Training loss (for one batch) at step 50: 437.3767, Accuracy: 0.6504\n",
      "Training loss (for one batch) at step 60: 436.4517, Accuracy: 0.6530\n",
      "Training loss (for one batch) at step 70: 438.4777, Accuracy: 0.6510\n",
      "Training loss (for one batch) at step 80: 442.6673, Accuracy: 0.6495\n",
      "Training loss (for one batch) at step 90: 442.3087, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 100: 441.3548, Accuracy: 0.6481\n",
      "Training loss (for one batch) at step 110: 436.6527, Accuracy: 0.6480\n",
      "---- Training ----\n",
      "Training loss: 137.9894\n",
      "Training acc over epoch: 0.6489\n",
      "---- Validation ----\n",
      "Validation loss: 33.1679\n",
      "Validation acc: 0.6625\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 440.9578, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 435.9663, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 20: 431.9534, Accuracy: 0.6544\n",
      "Training loss (for one batch) at step 30: 437.7212, Accuracy: 0.6605\n",
      "Training loss (for one batch) at step 40: 427.4406, Accuracy: 0.6623\n",
      "Training loss (for one batch) at step 50: 426.8018, Accuracy: 0.6664\n",
      "Training loss (for one batch) at step 60: 432.9064, Accuracy: 0.6701\n",
      "Training loss (for one batch) at step 70: 440.4372, Accuracy: 0.6686\n",
      "Training loss (for one batch) at step 80: 443.7068, Accuracy: 0.6619\n",
      "Training loss (for one batch) at step 90: 441.4383, Accuracy: 0.6578\n",
      "Training loss (for one batch) at step 100: 438.7250, Accuracy: 0.6542\n",
      "Training loss (for one batch) at step 110: 437.0156, Accuracy: 0.6559\n",
      "---- Training ----\n",
      "Training loss: 135.4635\n",
      "Training acc over epoch: 0.6576\n",
      "---- Validation ----\n",
      "Validation loss: 34.0665\n",
      "Validation acc: 0.6787\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 438.6287, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 436.9815, Accuracy: 0.6705\n",
      "Training loss (for one batch) at step 20: 433.7361, Accuracy: 0.6581\n",
      "Training loss (for one batch) at step 30: 436.9378, Accuracy: 0.6653\n",
      "Training loss (for one batch) at step 40: 421.7866, Accuracy: 0.6690\n",
      "Training loss (for one batch) at step 50: 426.2303, Accuracy: 0.6756\n",
      "Training loss (for one batch) at step 60: 440.5065, Accuracy: 0.6760\n",
      "Training loss (for one batch) at step 70: 447.4354, Accuracy: 0.6735\n",
      "Training loss (for one batch) at step 80: 442.3758, Accuracy: 0.6651\n",
      "Training loss (for one batch) at step 90: 433.1724, Accuracy: 0.6579\n",
      "Training loss (for one batch) at step 100: 440.7479, Accuracy: 0.6550\n",
      "Training loss (for one batch) at step 110: 431.7319, Accuracy: 0.6565\n",
      "---- Training ----\n",
      "Training loss: 137.1471\n",
      "Training acc over epoch: 0.6566\n",
      "---- Validation ----\n",
      "Validation loss: 35.5539\n",
      "Validation acc: 0.6365\n",
      "Time taken: 17.87s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 445.5368, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 438.1251, Accuracy: 0.6513\n",
      "Training loss (for one batch) at step 20: 431.2164, Accuracy: 0.6696\n",
      "Training loss (for one batch) at step 30: 427.7392, Accuracy: 0.6802\n",
      "Training loss (for one batch) at step 40: 416.8300, Accuracy: 0.6877\n",
      "Training loss (for one batch) at step 50: 426.7101, Accuracy: 0.6976\n",
      "Training loss (for one batch) at step 60: 417.8635, Accuracy: 0.7013\n",
      "Training loss (for one batch) at step 70: 440.9813, Accuracy: 0.6995\n",
      "Training loss (for one batch) at step 80: 438.1920, Accuracy: 0.6893\n",
      "Training loss (for one batch) at step 90: 435.3137, Accuracy: 0.6834\n",
      "Training loss (for one batch) at step 100: 423.1144, Accuracy: 0.6814\n",
      "Training loss (for one batch) at step 110: 430.0967, Accuracy: 0.6825\n",
      "---- Training ----\n",
      "Training loss: 138.3954\n",
      "Training acc over epoch: 0.6810\n",
      "---- Validation ----\n",
      "Validation loss: 35.1128\n",
      "Validation acc: 0.6779\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 441.8367, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 434.4316, Accuracy: 0.6747\n",
      "Training loss (for one batch) at step 20: 431.1359, Accuracy: 0.6548\n",
      "Training loss (for one batch) at step 30: 418.1306, Accuracy: 0.6774\n",
      "Training loss (for one batch) at step 40: 405.9174, Accuracy: 0.6905\n",
      "Training loss (for one batch) at step 50: 410.4055, Accuracy: 0.7001\n",
      "Training loss (for one batch) at step 60: 421.1710, Accuracy: 0.7071\n",
      "Training loss (for one batch) at step 70: 435.1063, Accuracy: 0.7050\n",
      "Training loss (for one batch) at step 80: 433.5488, Accuracy: 0.6943\n",
      "Training loss (for one batch) at step 90: 425.3975, Accuracy: 0.6872\n",
      "Training loss (for one batch) at step 100: 420.4686, Accuracy: 0.6867\n",
      "Training loss (for one batch) at step 110: 433.3024, Accuracy: 0.6868\n",
      "---- Training ----\n",
      "Training loss: 135.5243\n",
      "Training acc over epoch: 0.6865\n",
      "---- Validation ----\n",
      "Validation loss: 35.4255\n",
      "Validation acc: 0.6730\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 442.1287, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 436.7832, Accuracy: 0.6761\n",
      "Training loss (for one batch) at step 20: 436.7749, Accuracy: 0.6633\n",
      "Training loss (for one batch) at step 30: 418.0747, Accuracy: 0.6784\n",
      "Training loss (for one batch) at step 40: 410.8462, Accuracy: 0.6919\n",
      "Training loss (for one batch) at step 50: 387.3212, Accuracy: 0.7077\n",
      "Training loss (for one batch) at step 60: 413.3690, Accuracy: 0.7204\n",
      "Training loss (for one batch) at step 70: 426.4448, Accuracy: 0.7162\n",
      "Training loss (for one batch) at step 80: 443.7380, Accuracy: 0.7055\n",
      "Training loss (for one batch) at step 90: 428.3311, Accuracy: 0.6969\n",
      "Training loss (for one batch) at step 100: 420.4285, Accuracy: 0.6972\n",
      "Training loss (for one batch) at step 110: 431.9447, Accuracy: 0.7006\n",
      "---- Training ----\n",
      "Training loss: 135.5530\n",
      "Training acc over epoch: 0.6999\n",
      "---- Validation ----\n",
      "Validation loss: 37.0839\n",
      "Validation acc: 0.6811\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 431.0866, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 434.9524, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 20: 419.1757, Accuracy: 0.6670\n",
      "Training loss (for one batch) at step 30: 404.3944, Accuracy: 0.6822\n",
      "Training loss (for one batch) at step 40: 396.2963, Accuracy: 0.6955\n",
      "Training loss (for one batch) at step 50: 390.3620, Accuracy: 0.7128\n",
      "Training loss (for one batch) at step 60: 400.4985, Accuracy: 0.7235\n",
      "Training loss (for one batch) at step 70: 435.2480, Accuracy: 0.7158\n",
      "Training loss (for one batch) at step 80: 427.3885, Accuracy: 0.7045\n",
      "Training loss (for one batch) at step 90: 425.0285, Accuracy: 0.6990\n",
      "Training loss (for one batch) at step 100: 404.8973, Accuracy: 0.6997\n",
      "Training loss (for one batch) at step 110: 414.0807, Accuracy: 0.7012\n",
      "---- Training ----\n",
      "Training loss: 125.4350\n",
      "Training acc over epoch: 0.7020\n",
      "---- Validation ----\n",
      "Validation loss: 36.3109\n",
      "Validation acc: 0.6889\n",
      "Time taken: 18.06s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 430.3589, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 425.7791, Accuracy: 0.6847\n",
      "Training loss (for one batch) at step 20: 408.4288, Accuracy: 0.6842\n",
      "Training loss (for one batch) at step 30: 390.9414, Accuracy: 0.7069\n",
      "Training loss (for one batch) at step 40: 379.9684, Accuracy: 0.7214\n",
      "Training loss (for one batch) at step 50: 374.2673, Accuracy: 0.7391\n",
      "Training loss (for one batch) at step 60: 374.8150, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 70: 400.3895, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 80: 399.7528, Accuracy: 0.7290\n",
      "Training loss (for one batch) at step 90: 407.9078, Accuracy: 0.7214\n",
      "Training loss (for one batch) at step 100: 404.3852, Accuracy: 0.7232\n",
      "Training loss (for one batch) at step 110: 407.8317, Accuracy: 0.7227\n",
      "---- Training ----\n",
      "Training loss: 128.2478\n",
      "Training acc over epoch: 0.7229\n",
      "---- Validation ----\n",
      "Validation loss: 34.4013\n",
      "Validation acc: 0.6623\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 431.1120, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 419.6718, Accuracy: 0.6435\n",
      "Training loss (for one batch) at step 20: 409.0756, Accuracy: 0.6540\n",
      "Training loss (for one batch) at step 30: 388.9974, Accuracy: 0.6830\n",
      "Training loss (for one batch) at step 40: 378.5869, Accuracy: 0.7088\n",
      "Training loss (for one batch) at step 50: 372.6918, Accuracy: 0.7276\n",
      "Training loss (for one batch) at step 60: 366.8766, Accuracy: 0.7413\n",
      "Training loss (for one batch) at step 70: 402.7677, Accuracy: 0.7375\n",
      "Training loss (for one batch) at step 80: 394.8496, Accuracy: 0.7237\n",
      "Training loss (for one batch) at step 90: 416.2131, Accuracy: 0.7197\n",
      "Training loss (for one batch) at step 100: 381.2920, Accuracy: 0.7250\n",
      "Training loss (for one batch) at step 110: 402.0119, Accuracy: 0.7263\n",
      "---- Training ----\n",
      "Training loss: 125.1290\n",
      "Training acc over epoch: 0.7253\n",
      "---- Validation ----\n",
      "Validation loss: 38.6779\n",
      "Validation acc: 0.6695\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 405.3442, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 415.4332, Accuracy: 0.6570\n",
      "Training loss (for one batch) at step 20: 401.2601, Accuracy: 0.6756\n",
      "Training loss (for one batch) at step 30: 369.6352, Accuracy: 0.6935\n",
      "Training loss (for one batch) at step 40: 360.6297, Accuracy: 0.7237\n",
      "Training loss (for one batch) at step 50: 352.6577, Accuracy: 0.7442\n",
      "Training loss (for one batch) at step 60: 370.9838, Accuracy: 0.7581\n",
      "Training loss (for one batch) at step 70: 385.3976, Accuracy: 0.7483\n",
      "Training loss (for one batch) at step 80: 409.2672, Accuracy: 0.7358\n",
      "Training loss (for one batch) at step 90: 380.3855, Accuracy: 0.7293\n",
      "Training loss (for one batch) at step 100: 369.7073, Accuracy: 0.7328\n",
      "Training loss (for one batch) at step 110: 398.7142, Accuracy: 0.7333\n",
      "---- Training ----\n",
      "Training loss: 118.8933\n",
      "Training acc over epoch: 0.7333\n",
      "---- Validation ----\n",
      "Validation loss: 34.1171\n",
      "Validation acc: 0.6757\n",
      "Time taken: 17.87s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 407.9331, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 407.5810, Accuracy: 0.6193\n",
      "Training loss (for one batch) at step 20: 382.2175, Accuracy: 0.6477\n",
      "Training loss (for one batch) at step 30: 374.8969, Accuracy: 0.6900\n",
      "Training loss (for one batch) at step 40: 340.3825, Accuracy: 0.7216\n",
      "Training loss (for one batch) at step 50: 348.3809, Accuracy: 0.7448\n",
      "Training loss (for one batch) at step 60: 380.0644, Accuracy: 0.7558\n",
      "Training loss (for one batch) at step 70: 374.2118, Accuracy: 0.7463\n",
      "Training loss (for one batch) at step 80: 390.9466, Accuracy: 0.7331\n",
      "Training loss (for one batch) at step 90: 366.3542, Accuracy: 0.7299\n",
      "Training loss (for one batch) at step 100: 353.6360, Accuracy: 0.7350\n",
      "Training loss (for one batch) at step 110: 358.5533, Accuracy: 0.7369\n",
      "---- Training ----\n",
      "Training loss: 118.3796\n",
      "Training acc over epoch: 0.7383\n",
      "---- Validation ----\n",
      "Validation loss: 36.3395\n",
      "Validation acc: 0.6660\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 397.0857, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 385.1332, Accuracy: 0.6286\n",
      "Training loss (for one batch) at step 20: 364.7839, Accuracy: 0.6708\n",
      "Training loss (for one batch) at step 30: 338.5626, Accuracy: 0.7102\n",
      "Training loss (for one batch) at step 40: 340.5415, Accuracy: 0.7365\n",
      "Training loss (for one batch) at step 50: 338.0676, Accuracy: 0.7570\n",
      "Training loss (for one batch) at step 60: 340.8555, Accuracy: 0.7665\n",
      "Training loss (for one batch) at step 70: 397.3455, Accuracy: 0.7580\n",
      "Training loss (for one batch) at step 80: 371.3619, Accuracy: 0.7406\n",
      "Training loss (for one batch) at step 90: 350.8167, Accuracy: 0.7387\n",
      "Training loss (for one batch) at step 100: 369.5149, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 110: 356.2634, Accuracy: 0.7451\n",
      "---- Training ----\n",
      "Training loss: 118.9635\n",
      "Training acc over epoch: 0.7440\n",
      "---- Validation ----\n",
      "Validation loss: 39.3593\n",
      "Validation acc: 0.6808\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 399.3278, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 373.4709, Accuracy: 0.6030\n",
      "Training loss (for one batch) at step 20: 360.8881, Accuracy: 0.6514\n",
      "Training loss (for one batch) at step 30: 341.0418, Accuracy: 0.6978\n",
      "Training loss (for one batch) at step 40: 324.6935, Accuracy: 0.7308\n",
      "Training loss (for one batch) at step 50: 328.0966, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 60: 346.3972, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 70: 344.6882, Accuracy: 0.7555\n",
      "Training loss (for one batch) at step 80: 367.5922, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 90: 335.9604, Accuracy: 0.7368\n",
      "Training loss (for one batch) at step 100: 353.9123, Accuracy: 0.7423\n",
      "Training loss (for one batch) at step 110: 337.7187, Accuracy: 0.7443\n",
      "---- Training ----\n",
      "Training loss: 122.3390\n",
      "Training acc over epoch: 0.7435\n",
      "---- Validation ----\n",
      "Validation loss: 36.4775\n",
      "Validation acc: 0.6709\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 366.6400, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 365.1463, Accuracy: 0.6136\n",
      "Training loss (for one batch) at step 20: 350.6750, Accuracy: 0.6663\n",
      "Training loss (for one batch) at step 30: 312.2445, Accuracy: 0.7167\n",
      "Training loss (for one batch) at step 40: 334.8623, Accuracy: 0.7473\n",
      "Training loss (for one batch) at step 50: 307.7447, Accuracy: 0.7652\n",
      "Training loss (for one batch) at step 60: 313.8477, Accuracy: 0.7764\n",
      "Training loss (for one batch) at step 70: 367.0262, Accuracy: 0.7617\n",
      "Training loss (for one batch) at step 80: 363.0908, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 90: 337.6353, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 100: 343.6721, Accuracy: 0.7478\n",
      "Training loss (for one batch) at step 110: 345.7273, Accuracy: 0.7501\n",
      "---- Training ----\n",
      "Training loss: 117.3137\n",
      "Training acc over epoch: 0.7472\n",
      "---- Validation ----\n",
      "Validation loss: 57.3530\n",
      "Validation acc: 0.6488\n",
      "Time taken: 18.06s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 367.8392, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 356.4215, Accuracy: 0.6186\n",
      "Training loss (for one batch) at step 20: 344.3852, Accuracy: 0.6700\n",
      "Training loss (for one batch) at step 30: 317.1662, Accuracy: 0.7175\n",
      "Training loss (for one batch) at step 40: 316.1327, Accuracy: 0.7492\n",
      "Training loss (for one batch) at step 50: 308.5941, Accuracy: 0.7684\n",
      "Training loss (for one batch) at step 60: 341.3044, Accuracy: 0.7792\n",
      "Training loss (for one batch) at step 70: 362.2895, Accuracy: 0.7643\n",
      "Training loss (for one batch) at step 80: 350.8054, Accuracy: 0.7473\n",
      "Training loss (for one batch) at step 90: 337.2499, Accuracy: 0.7449\n",
      "Training loss (for one batch) at step 100: 321.9732, Accuracy: 0.7523\n",
      "Training loss (for one batch) at step 110: 332.7426, Accuracy: 0.7548\n",
      "---- Training ----\n",
      "Training loss: 104.3933\n",
      "Training acc over epoch: 0.7537\n",
      "---- Validation ----\n",
      "Validation loss: 63.1845\n",
      "Validation acc: 0.6784\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 371.0676, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 364.4709, Accuracy: 0.5895\n",
      "Training loss (for one batch) at step 20: 320.0297, Accuracy: 0.6507\n",
      "Training loss (for one batch) at step 30: 302.7369, Accuracy: 0.7006\n",
      "Training loss (for one batch) at step 40: 320.0262, Accuracy: 0.7359\n",
      "Training loss (for one batch) at step 50: 293.5242, Accuracy: 0.7613\n",
      "Training loss (for one batch) at step 60: 327.8328, Accuracy: 0.7728\n",
      "Training loss (for one batch) at step 70: 342.6443, Accuracy: 0.7591\n",
      "Training loss (for one batch) at step 80: 343.3393, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 90: 320.0817, Accuracy: 0.7366\n",
      "Training loss (for one batch) at step 100: 302.1065, Accuracy: 0.7441\n",
      "Training loss (for one batch) at step 110: 340.5784, Accuracy: 0.7465\n",
      "---- Training ----\n",
      "Training loss: 95.6415\n",
      "Training acc over epoch: 0.7460\n",
      "---- Validation ----\n",
      "Validation loss: 34.7964\n",
      "Validation acc: 0.6642\n",
      "Time taken: 18.13s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 354.2746, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 363.9590, Accuracy: 0.5952\n",
      "Training loss (for one batch) at step 20: 318.3161, Accuracy: 0.6544\n",
      "Training loss (for one batch) at step 30: 311.3268, Accuracy: 0.7102\n",
      "Training loss (for one batch) at step 40: 297.9540, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 50: 304.7133, Accuracy: 0.7664\n",
      "Training loss (for one batch) at step 60: 304.2874, Accuracy: 0.7768\n",
      "Training loss (for one batch) at step 70: 344.4873, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 80: 336.8351, Accuracy: 0.7467\n",
      "Training loss (for one batch) at step 90: 305.7737, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 100: 317.1991, Accuracy: 0.7505\n",
      "Training loss (for one batch) at step 110: 326.1124, Accuracy: 0.7525\n",
      "---- Training ----\n",
      "Training loss: 120.8961\n",
      "Training acc over epoch: 0.7502\n",
      "---- Validation ----\n",
      "Validation loss: 47.5493\n",
      "Validation acc: 0.6679\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 384.0333, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 358.5962, Accuracy: 0.6108\n",
      "Training loss (for one batch) at step 20: 317.5981, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 30: 316.6119, Accuracy: 0.7157\n",
      "Training loss (for one batch) at step 40: 288.5963, Accuracy: 0.7502\n",
      "Training loss (for one batch) at step 50: 295.8659, Accuracy: 0.7702\n",
      "Training loss (for one batch) at step 60: 308.2462, Accuracy: 0.7807\n",
      "Training loss (for one batch) at step 70: 313.0693, Accuracy: 0.7631\n",
      "Training loss (for one batch) at step 80: 346.3152, Accuracy: 0.7465\n",
      "Training loss (for one batch) at step 90: 327.1916, Accuracy: 0.7426\n",
      "Training loss (for one batch) at step 100: 303.6709, Accuracy: 0.7494\n",
      "Training loss (for one batch) at step 110: 324.1072, Accuracy: 0.7505\n",
      "---- Training ----\n",
      "Training loss: 101.2569\n",
      "Training acc over epoch: 0.7497\n",
      "---- Validation ----\n",
      "Validation loss: 47.0087\n",
      "Validation acc: 0.6760\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 360.3472, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 341.1299, Accuracy: 0.6001\n",
      "Training loss (for one batch) at step 20: 304.8474, Accuracy: 0.6559\n",
      "Training loss (for one batch) at step 30: 294.6622, Accuracy: 0.7077\n",
      "Training loss (for one batch) at step 40: 300.8683, Accuracy: 0.7416\n",
      "Training loss (for one batch) at step 50: 280.0540, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 60: 288.5199, Accuracy: 0.7802\n",
      "Training loss (for one batch) at step 70: 335.9851, Accuracy: 0.7638\n",
      "Training loss (for one batch) at step 80: 329.1558, Accuracy: 0.7449\n",
      "Training loss (for one batch) at step 90: 325.5900, Accuracy: 0.7404\n",
      "Training loss (for one batch) at step 100: 314.4396, Accuracy: 0.7489\n",
      "Training loss (for one batch) at step 110: 306.6868, Accuracy: 0.7522\n",
      "---- Training ----\n",
      "Training loss: 96.8944\n",
      "Training acc over epoch: 0.7513\n",
      "---- Validation ----\n",
      "Validation loss: 40.6934\n",
      "Validation acc: 0.6835\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 351.7259, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 351.6078, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 20: 307.4140, Accuracy: 0.6548\n",
      "Training loss (for one batch) at step 30: 293.6403, Accuracy: 0.7112\n",
      "Training loss (for one batch) at step 40: 290.4218, Accuracy: 0.7449\n",
      "Training loss (for one batch) at step 50: 278.1960, Accuracy: 0.7707\n",
      "Training loss (for one batch) at step 60: 315.4445, Accuracy: 0.7825\n",
      "Training loss (for one batch) at step 70: 337.8663, Accuracy: 0.7653\n",
      "Training loss (for one batch) at step 80: 336.9537, Accuracy: 0.7459\n",
      "Training loss (for one batch) at step 90: 289.9381, Accuracy: 0.7423\n",
      "Training loss (for one batch) at step 100: 294.7337, Accuracy: 0.7511\n",
      "Training loss (for one batch) at step 110: 318.1161, Accuracy: 0.7538\n",
      "---- Training ----\n",
      "Training loss: 99.2184\n",
      "Training acc over epoch: 0.7522\n",
      "---- Validation ----\n",
      "Validation loss: 48.2576\n",
      "Validation acc: 0.6835\n",
      "Time taken: 17.87s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 343.6399, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 343.3289, Accuracy: 0.5902\n",
      "Training loss (for one batch) at step 20: 301.3759, Accuracy: 0.6518\n",
      "Training loss (for one batch) at step 30: 294.9097, Accuracy: 0.7067\n",
      "Training loss (for one batch) at step 40: 293.4408, Accuracy: 0.7418\n",
      "Training loss (for one batch) at step 50: 290.3499, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 60: 308.6227, Accuracy: 0.7779\n",
      "Training loss (for one batch) at step 70: 315.0466, Accuracy: 0.7632\n",
      "Training loss (for one batch) at step 80: 321.5359, Accuracy: 0.7432\n",
      "Training loss (for one batch) at step 90: 305.7351, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 100: 290.0588, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 110: 294.2543, Accuracy: 0.7514\n",
      "---- Training ----\n",
      "Training loss: 103.9787\n",
      "Training acc over epoch: 0.7499\n",
      "---- Validation ----\n",
      "Validation loss: 56.9961\n",
      "Validation acc: 0.6857\n",
      "Time taken: 18.17s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 330.9542, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 324.8957, Accuracy: 0.5902\n",
      "Training loss (for one batch) at step 20: 284.0889, Accuracy: 0.6559\n",
      "Training loss (for one batch) at step 30: 276.8462, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 40: 276.0493, Accuracy: 0.7487\n",
      "Training loss (for one batch) at step 50: 277.7474, Accuracy: 0.7753\n",
      "Training loss (for one batch) at step 60: 298.5588, Accuracy: 0.7860\n",
      "Training loss (for one batch) at step 70: 332.1266, Accuracy: 0.7713\n",
      "Training loss (for one batch) at step 80: 343.1696, Accuracy: 0.7497\n",
      "Training loss (for one batch) at step 90: 290.5514, Accuracy: 0.7464\n",
      "Training loss (for one batch) at step 100: 285.4358, Accuracy: 0.7539\n",
      "Training loss (for one batch) at step 110: 300.0741, Accuracy: 0.7574\n",
      "---- Training ----\n",
      "Training loss: 100.9197\n",
      "Training acc over epoch: 0.7558\n",
      "---- Validation ----\n",
      "Validation loss: 37.3987\n",
      "Validation acc: 0.6846\n",
      "Time taken: 17.87s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 339.5067, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 356.7918, Accuracy: 0.5895\n",
      "Training loss (for one batch) at step 20: 289.7653, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 30: 288.0471, Accuracy: 0.7119\n",
      "Training loss (for one batch) at step 40: 286.5417, Accuracy: 0.7504\n",
      "Training loss (for one batch) at step 50: 265.3559, Accuracy: 0.7718\n",
      "Training loss (for one batch) at step 60: 280.4338, Accuracy: 0.7814\n",
      "Training loss (for one batch) at step 70: 321.3217, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 80: 319.7850, Accuracy: 0.7452\n",
      "Training loss (for one batch) at step 90: 308.3985, Accuracy: 0.7418\n",
      "Training loss (for one batch) at step 100: 281.4547, Accuracy: 0.7514\n",
      "Training loss (for one batch) at step 110: 289.8668, Accuracy: 0.7547\n",
      "---- Training ----\n",
      "Training loss: 90.1446\n",
      "Training acc over epoch: 0.7531\n",
      "---- Validation ----\n",
      "Validation loss: 34.9474\n",
      "Validation acc: 0.6894\n",
      "Time taken: 18.97s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 335.8048, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 311.3835, Accuracy: 0.5959\n",
      "Training loss (for one batch) at step 20: 310.5974, Accuracy: 0.6503\n",
      "Training loss (for one batch) at step 30: 288.2051, Accuracy: 0.7094\n",
      "Training loss (for one batch) at step 40: 272.9575, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 50: 262.1305, Accuracy: 0.7707\n",
      "Training loss (for one batch) at step 60: 293.8712, Accuracy: 0.7802\n",
      "Training loss (for one batch) at step 70: 324.8546, Accuracy: 0.7641\n",
      "Training loss (for one batch) at step 80: 321.5088, Accuracy: 0.7440\n",
      "Training loss (for one batch) at step 90: 283.8788, Accuracy: 0.7426\n",
      "Training loss (for one batch) at step 100: 285.1596, Accuracy: 0.7509\n",
      "Training loss (for one batch) at step 110: 300.9249, Accuracy: 0.7540\n",
      "---- Training ----\n",
      "Training loss: 99.1606\n",
      "Training acc over epoch: 0.7524\n",
      "---- Validation ----\n",
      "Validation loss: 44.5966\n",
      "Validation acc: 0.6918\n",
      "Time taken: 17.87s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 321.7371, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 340.7378, Accuracy: 0.5689\n",
      "Training loss (for one batch) at step 20: 273.6017, Accuracy: 0.6451\n",
      "Training loss (for one batch) at step 30: 284.7622, Accuracy: 0.7084\n",
      "Training loss (for one batch) at step 40: 260.2434, Accuracy: 0.7462\n",
      "Training loss (for one batch) at step 50: 259.9793, Accuracy: 0.7705\n",
      "Training loss (for one batch) at step 60: 292.2480, Accuracy: 0.7824\n",
      "Training loss (for one batch) at step 70: 311.8612, Accuracy: 0.7682\n",
      "Training loss (for one batch) at step 80: 311.2929, Accuracy: 0.7447\n",
      "Training loss (for one batch) at step 90: 302.9452, Accuracy: 0.7430\n",
      "Training loss (for one batch) at step 100: 274.0590, Accuracy: 0.7519\n",
      "Training loss (for one batch) at step 110: 322.0513, Accuracy: 0.7549\n",
      "---- Training ----\n",
      "Training loss: 91.9054\n",
      "Training acc over epoch: 0.7526\n",
      "---- Validation ----\n",
      "Validation loss: 39.1602\n",
      "Validation acc: 0.6832\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 313.5342, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 295.2904, Accuracy: 0.5682\n",
      "Training loss (for one batch) at step 20: 278.0033, Accuracy: 0.6414\n",
      "Training loss (for one batch) at step 30: 269.2048, Accuracy: 0.7001\n",
      "Training loss (for one batch) at step 40: 276.3874, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 50: 259.4239, Accuracy: 0.7667\n",
      "Training loss (for one batch) at step 60: 286.7691, Accuracy: 0.7800\n",
      "Training loss (for one batch) at step 70: 333.8614, Accuracy: 0.7653\n",
      "Training loss (for one batch) at step 80: 302.8433, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 90: 293.7863, Accuracy: 0.7400\n",
      "Training loss (for one batch) at step 100: 301.8071, Accuracy: 0.7495\n",
      "Training loss (for one batch) at step 110: 316.0723, Accuracy: 0.7526\n",
      "---- Training ----\n",
      "Training loss: 92.7424\n",
      "Training acc over epoch: 0.7521\n",
      "---- Validation ----\n",
      "Validation loss: 53.8507\n",
      "Validation acc: 0.6867\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 305.9916, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 320.3267, Accuracy: 0.5945\n",
      "Training loss (for one batch) at step 20: 265.1862, Accuracy: 0.6425\n",
      "Training loss (for one batch) at step 30: 276.4211, Accuracy: 0.7016\n",
      "Training loss (for one batch) at step 40: 248.8019, Accuracy: 0.7431\n",
      "Training loss (for one batch) at step 50: 271.1521, Accuracy: 0.7669\n",
      "Training loss (for one batch) at step 60: 274.6895, Accuracy: 0.7793\n",
      "Training loss (for one batch) at step 70: 308.3846, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 80: 301.8860, Accuracy: 0.7434\n",
      "Training loss (for one batch) at step 90: 280.3414, Accuracy: 0.7410\n",
      "Training loss (for one batch) at step 100: 260.4685, Accuracy: 0.7489\n",
      "Training loss (for one batch) at step 110: 272.0427, Accuracy: 0.7501\n",
      "---- Training ----\n",
      "Training loss: 102.6763\n",
      "Training acc over epoch: 0.7497\n",
      "---- Validation ----\n",
      "Validation loss: 38.8864\n",
      "Validation acc: 0.6902\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 326.9730, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 296.5537, Accuracy: 0.5866\n",
      "Training loss (for one batch) at step 20: 273.2584, Accuracy: 0.6469\n",
      "Training loss (for one batch) at step 30: 276.8761, Accuracy: 0.7122\n",
      "Training loss (for one batch) at step 40: 273.5573, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 50: 292.4361, Accuracy: 0.7741\n",
      "Training loss (for one batch) at step 60: 276.8401, Accuracy: 0.7866\n",
      "Training loss (for one batch) at step 70: 318.1495, Accuracy: 0.7688\n",
      "Training loss (for one batch) at step 80: 314.7014, Accuracy: 0.7463\n",
      "Training loss (for one batch) at step 90: 285.9695, Accuracy: 0.7413\n",
      "Training loss (for one batch) at step 100: 272.0451, Accuracy: 0.7499\n",
      "Training loss (for one batch) at step 110: 309.8525, Accuracy: 0.7542\n",
      "---- Training ----\n",
      "Training loss: 94.6945\n",
      "Training acc over epoch: 0.7520\n",
      "---- Validation ----\n",
      "Validation loss: 47.8257\n",
      "Validation acc: 0.6846\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 321.2102, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 303.0094, Accuracy: 0.5824\n",
      "Training loss (for one batch) at step 20: 278.7804, Accuracy: 0.6577\n",
      "Training loss (for one batch) at step 30: 264.7432, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 40: 270.6721, Accuracy: 0.7458\n",
      "Training loss (for one batch) at step 50: 259.0583, Accuracy: 0.7730\n",
      "Training loss (for one batch) at step 60: 292.0326, Accuracy: 0.7832\n",
      "Training loss (for one batch) at step 70: 310.1689, Accuracy: 0.7677\n",
      "Training loss (for one batch) at step 80: 306.5671, Accuracy: 0.7466\n",
      "Training loss (for one batch) at step 90: 291.1544, Accuracy: 0.7434\n",
      "Training loss (for one batch) at step 100: 272.6955, Accuracy: 0.7523\n",
      "Training loss (for one batch) at step 110: 286.4030, Accuracy: 0.7557\n",
      "---- Training ----\n",
      "Training loss: 88.8186\n",
      "Training acc over epoch: 0.7539\n",
      "---- Validation ----\n",
      "Validation loss: 40.1157\n",
      "Validation acc: 0.6706\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 322.5974, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 325.8479, Accuracy: 0.5696\n",
      "Training loss (for one batch) at step 20: 287.5281, Accuracy: 0.6417\n",
      "Training loss (for one batch) at step 30: 267.2469, Accuracy: 0.7051\n",
      "Training loss (for one batch) at step 40: 273.1989, Accuracy: 0.7420\n",
      "Training loss (for one batch) at step 50: 252.7736, Accuracy: 0.7684\n",
      "Training loss (for one batch) at step 60: 287.3854, Accuracy: 0.7787\n",
      "Training loss (for one batch) at step 70: 296.2529, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 80: 306.0047, Accuracy: 0.7432\n",
      "Training loss (for one batch) at step 90: 275.4554, Accuracy: 0.7419\n",
      "Training loss (for one batch) at step 100: 254.8141, Accuracy: 0.7504\n",
      "Training loss (for one batch) at step 110: 284.3304, Accuracy: 0.7530\n",
      "---- Training ----\n",
      "Training loss: 93.3107\n",
      "Training acc over epoch: 0.7519\n",
      "---- Validation ----\n",
      "Validation loss: 51.0690\n",
      "Validation acc: 0.6902\n",
      "Time taken: 18.10s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 309.2356, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 307.4421, Accuracy: 0.5817\n",
      "Training loss (for one batch) at step 20: 260.9594, Accuracy: 0.6440\n",
      "Training loss (for one batch) at step 30: 266.3889, Accuracy: 0.7039\n",
      "Training loss (for one batch) at step 40: 266.3452, Accuracy: 0.7435\n",
      "Training loss (for one batch) at step 50: 259.7541, Accuracy: 0.7695\n",
      "Training loss (for one batch) at step 60: 266.4562, Accuracy: 0.7802\n",
      "Training loss (for one batch) at step 70: 300.3594, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 80: 301.6249, Accuracy: 0.7434\n",
      "Training loss (for one batch) at step 90: 261.3012, Accuracy: 0.7408\n",
      "Training loss (for one batch) at step 100: 269.6723, Accuracy: 0.7505\n",
      "Training loss (for one batch) at step 110: 287.2393, Accuracy: 0.7539\n",
      "---- Training ----\n",
      "Training loss: 88.4535\n",
      "Training acc over epoch: 0.7526\n",
      "---- Validation ----\n",
      "Validation loss: 61.8429\n",
      "Validation acc: 0.6781\n",
      "Time taken: 17.86s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 315.1004, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 301.0143, Accuracy: 0.5653\n",
      "Training loss (for one batch) at step 20: 267.2310, Accuracy: 0.6425\n",
      "Training loss (for one batch) at step 30: 252.5565, Accuracy: 0.7089\n",
      "Training loss (for one batch) at step 40: 259.5677, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 50: 256.9098, Accuracy: 0.7687\n",
      "Training loss (for one batch) at step 60: 286.8139, Accuracy: 0.7798\n",
      "Training loss (for one batch) at step 70: 291.5237, Accuracy: 0.7625\n",
      "Training loss (for one batch) at step 80: 296.5173, Accuracy: 0.7432\n",
      "Training loss (for one batch) at step 90: 264.1085, Accuracy: 0.7395\n",
      "Training loss (for one batch) at step 100: 252.0912, Accuracy: 0.7489\n",
      "Training loss (for one batch) at step 110: 293.1951, Accuracy: 0.7530\n",
      "---- Training ----\n",
      "Training loss: 91.3674\n",
      "Training acc over epoch: 0.7515\n",
      "---- Validation ----\n",
      "Validation loss: 48.6828\n",
      "Validation acc: 0.6881\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 309.1342, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 307.1086, Accuracy: 0.5689\n",
      "Training loss (for one batch) at step 20: 277.7608, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 30: 259.1913, Accuracy: 0.7049\n",
      "Training loss (for one batch) at step 40: 252.9995, Accuracy: 0.7426\n",
      "Training loss (for one batch) at step 50: 243.5369, Accuracy: 0.7708\n",
      "Training loss (for one batch) at step 60: 272.9463, Accuracy: 0.7797\n",
      "Training loss (for one batch) at step 70: 285.0278, Accuracy: 0.7653\n",
      "Training loss (for one batch) at step 80: 296.3716, Accuracy: 0.7432\n",
      "Training loss (for one batch) at step 90: 265.2463, Accuracy: 0.7410\n",
      "Training loss (for one batch) at step 100: 268.7877, Accuracy: 0.7508\n",
      "Training loss (for one batch) at step 110: 289.9764, Accuracy: 0.7546\n",
      "---- Training ----\n",
      "Training loss: 91.8430\n",
      "Training acc over epoch: 0.7536\n",
      "---- Validation ----\n",
      "Validation loss: 29.0696\n",
      "Validation acc: 0.6848\n",
      "Time taken: 20.15s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 292.1124, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 288.1246, Accuracy: 0.5831\n",
      "Training loss (for one batch) at step 20: 275.4603, Accuracy: 0.6533\n",
      "Training loss (for one batch) at step 30: 255.0229, Accuracy: 0.7099\n",
      "Training loss (for one batch) at step 40: 254.9725, Accuracy: 0.7487\n",
      "Training loss (for one batch) at step 50: 243.9966, Accuracy: 0.7721\n",
      "Training loss (for one batch) at step 60: 271.6664, Accuracy: 0.7833\n",
      "Training loss (for one batch) at step 70: 295.3871, Accuracy: 0.7676\n",
      "Training loss (for one batch) at step 80: 289.3998, Accuracy: 0.7477\n",
      "Training loss (for one batch) at step 90: 262.5110, Accuracy: 0.7438\n",
      "Training loss (for one batch) at step 100: 259.9586, Accuracy: 0.7541\n",
      "Training loss (for one batch) at step 110: 293.8235, Accuracy: 0.7557\n",
      "---- Training ----\n",
      "Training loss: 93.8789\n",
      "Training acc over epoch: 0.7541\n",
      "---- Validation ----\n",
      "Validation loss: 52.3650\n",
      "Validation acc: 0.6964\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 298.2418, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 306.1037, Accuracy: 0.5881\n",
      "Training loss (for one batch) at step 20: 266.0426, Accuracy: 0.6429\n",
      "Training loss (for one batch) at step 30: 265.1455, Accuracy: 0.7067\n",
      "Training loss (for one batch) at step 40: 254.4884, Accuracy: 0.7464\n",
      "Training loss (for one batch) at step 50: 260.1609, Accuracy: 0.7698\n",
      "Training loss (for one batch) at step 60: 275.1042, Accuracy: 0.7821\n",
      "Training loss (for one batch) at step 70: 288.1314, Accuracy: 0.7654\n",
      "Training loss (for one batch) at step 80: 291.4612, Accuracy: 0.7447\n",
      "Training loss (for one batch) at step 90: 285.4561, Accuracy: 0.7423\n",
      "Training loss (for one batch) at step 100: 255.5527, Accuracy: 0.7505\n",
      "Training loss (for one batch) at step 110: 277.5510, Accuracy: 0.7550\n",
      "---- Training ----\n",
      "Training loss: 92.5623\n",
      "Training acc over epoch: 0.7528\n",
      "---- Validation ----\n",
      "Validation loss: 51.4592\n",
      "Validation acc: 0.6916\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 303.5561, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 290.7802, Accuracy: 0.5831\n",
      "Training loss (for one batch) at step 20: 261.5414, Accuracy: 0.6536\n",
      "Training loss (for one batch) at step 30: 261.1336, Accuracy: 0.7094\n",
      "Training loss (for one batch) at step 40: 266.2856, Accuracy: 0.7464\n",
      "Training loss (for one batch) at step 50: 245.1182, Accuracy: 0.7675\n",
      "Training loss (for one batch) at step 60: 267.6225, Accuracy: 0.7821\n",
      "Training loss (for one batch) at step 70: 304.5023, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 80: 286.5903, Accuracy: 0.7434\n",
      "Training loss (for one batch) at step 90: 255.8746, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 100: 268.8694, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 110: 272.7072, Accuracy: 0.7525\n",
      "---- Training ----\n",
      "Training loss: 88.0665\n",
      "Training acc over epoch: 0.7521\n",
      "---- Validation ----\n",
      "Validation loss: 63.2444\n",
      "Validation acc: 0.6787\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 303.2073, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 288.2010, Accuracy: 0.5724\n",
      "Training loss (for one batch) at step 20: 291.4400, Accuracy: 0.6443\n",
      "Training loss (for one batch) at step 30: 253.2323, Accuracy: 0.7087\n",
      "Training loss (for one batch) at step 40: 254.0655, Accuracy: 0.7460\n",
      "Training loss (for one batch) at step 50: 249.7472, Accuracy: 0.7713\n",
      "Training loss (for one batch) at step 60: 267.1090, Accuracy: 0.7846\n",
      "Training loss (for one batch) at step 70: 315.8990, Accuracy: 0.7690\n",
      "Training loss (for one batch) at step 80: 297.5909, Accuracy: 0.7461\n",
      "Training loss (for one batch) at step 90: 258.2743, Accuracy: 0.7418\n",
      "Training loss (for one batch) at step 100: 257.3586, Accuracy: 0.7505\n",
      "Training loss (for one batch) at step 110: 273.2990, Accuracy: 0.7537\n",
      "---- Training ----\n",
      "Training loss: 86.2491\n",
      "Training acc over epoch: 0.7534\n",
      "---- Validation ----\n",
      "Validation loss: 41.7650\n",
      "Validation acc: 0.6805\n",
      "Time taken: 18.13s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 311.9707, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 294.4708, Accuracy: 0.5902\n",
      "Training loss (for one batch) at step 20: 258.3618, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 30: 261.7690, Accuracy: 0.7112\n",
      "Training loss (for one batch) at step 40: 258.8742, Accuracy: 0.7483\n",
      "Training loss (for one batch) at step 50: 243.6477, Accuracy: 0.7762\n",
      "Training loss (for one batch) at step 60: 260.7996, Accuracy: 0.7852\n",
      "Training loss (for one batch) at step 70: 291.2302, Accuracy: 0.7711\n",
      "Training loss (for one batch) at step 80: 278.1610, Accuracy: 0.7490\n",
      "Training loss (for one batch) at step 90: 254.5446, Accuracy: 0.7448\n",
      "Training loss (for one batch) at step 100: 268.7514, Accuracy: 0.7530\n",
      "Training loss (for one batch) at step 110: 263.2912, Accuracy: 0.7556\n",
      "---- Training ----\n",
      "Training loss: 104.0283\n",
      "Training acc over epoch: 0.7548\n",
      "---- Validation ----\n",
      "Validation loss: 48.5543\n",
      "Validation acc: 0.6846\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 286.1066, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 307.0196, Accuracy: 0.5739\n",
      "Training loss (for one batch) at step 20: 270.3983, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 30: 242.0815, Accuracy: 0.7046\n",
      "Training loss (for one batch) at step 40: 258.3876, Accuracy: 0.7445\n",
      "Training loss (for one batch) at step 50: 268.6342, Accuracy: 0.7688\n",
      "Training loss (for one batch) at step 60: 269.6800, Accuracy: 0.7814\n",
      "Training loss (for one batch) at step 70: 278.8935, Accuracy: 0.7636\n",
      "Training loss (for one batch) at step 80: 272.3387, Accuracy: 0.7425\n",
      "Training loss (for one batch) at step 90: 251.3063, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 100: 262.0321, Accuracy: 0.7496\n",
      "Training loss (for one batch) at step 110: 266.9121, Accuracy: 0.7532\n",
      "---- Training ----\n",
      "Training loss: 82.4872\n",
      "Training acc over epoch: 0.7519\n",
      "---- Validation ----\n",
      "Validation loss: 37.8843\n",
      "Validation acc: 0.6698\n",
      "Time taken: 20.15s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 318.7179, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 295.8231, Accuracy: 0.5732\n",
      "Training loss (for one batch) at step 20: 264.4923, Accuracy: 0.6391\n",
      "Training loss (for one batch) at step 30: 241.2436, Accuracy: 0.7056\n",
      "Training loss (for one batch) at step 40: 252.1792, Accuracy: 0.7443\n",
      "Training loss (for one batch) at step 50: 249.7117, Accuracy: 0.7702\n",
      "Training loss (for one batch) at step 60: 245.1655, Accuracy: 0.7805\n",
      "Training loss (for one batch) at step 70: 295.9662, Accuracy: 0.7663\n",
      "Training loss (for one batch) at step 80: 284.5500, Accuracy: 0.7456\n",
      "Training loss (for one batch) at step 90: 254.9358, Accuracy: 0.7421\n",
      "Training loss (for one batch) at step 100: 268.1136, Accuracy: 0.7517\n",
      "Training loss (for one batch) at step 110: 271.8149, Accuracy: 0.7530\n",
      "---- Training ----\n",
      "Training loss: 94.6612\n",
      "Training acc over epoch: 0.7511\n",
      "---- Validation ----\n",
      "Validation loss: 48.1898\n",
      "Validation acc: 0.6921\n",
      "Time taken: 20.18s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 291.8656, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 299.5651, Accuracy: 0.5774\n",
      "Training loss (for one batch) at step 20: 265.8411, Accuracy: 0.6488\n",
      "Training loss (for one batch) at step 30: 244.3957, Accuracy: 0.7059\n",
      "Training loss (for one batch) at step 40: 252.9305, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 50: 243.6702, Accuracy: 0.7710\n",
      "Training loss (for one batch) at step 60: 269.6207, Accuracy: 0.7836\n",
      "Training loss (for one batch) at step 70: 274.5694, Accuracy: 0.7683\n",
      "Training loss (for one batch) at step 80: 281.8234, Accuracy: 0.7486\n",
      "Training loss (for one batch) at step 90: 248.0198, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 100: 253.1427, Accuracy: 0.7540\n",
      "Training loss (for one batch) at step 110: 269.8032, Accuracy: 0.7553\n",
      "---- Training ----\n",
      "Training loss: 84.3449\n",
      "Training acc over epoch: 0.7542\n",
      "---- Validation ----\n",
      "Validation loss: 43.5348\n",
      "Validation acc: 0.6870\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 275.8540, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 286.0791, Accuracy: 0.5852\n",
      "Training loss (for one batch) at step 20: 264.3366, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 30: 255.4260, Accuracy: 0.7140\n",
      "Training loss (for one batch) at step 40: 252.0306, Accuracy: 0.7460\n",
      "Training loss (for one batch) at step 50: 228.0073, Accuracy: 0.7728\n",
      "Training loss (for one batch) at step 60: 255.5514, Accuracy: 0.7841\n",
      "Training loss (for one batch) at step 70: 265.3147, Accuracy: 0.7676\n",
      "Training loss (for one batch) at step 80: 298.7796, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 90: 251.2533, Accuracy: 0.7436\n",
      "Training loss (for one batch) at step 100: 259.4526, Accuracy: 0.7532\n",
      "Training loss (for one batch) at step 110: 242.8898, Accuracy: 0.7556\n",
      "---- Training ----\n",
      "Training loss: 106.2932\n",
      "Training acc over epoch: 0.7545\n",
      "---- Validation ----\n",
      "Validation loss: 47.6574\n",
      "Validation acc: 0.6808\n",
      "Time taken: 18.11s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 296.4763, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 279.8867, Accuracy: 0.5696\n",
      "Training loss (for one batch) at step 20: 248.1858, Accuracy: 0.6436\n",
      "Training loss (for one batch) at step 30: 248.0906, Accuracy: 0.7072\n",
      "Training loss (for one batch) at step 40: 252.5828, Accuracy: 0.7470\n",
      "Training loss (for one batch) at step 50: 258.5574, Accuracy: 0.7724\n",
      "Training loss (for one batch) at step 60: 251.5279, Accuracy: 0.7845\n",
      "Training loss (for one batch) at step 70: 283.3451, Accuracy: 0.7669\n",
      "Training loss (for one batch) at step 80: 292.9364, Accuracy: 0.7455\n",
      "Training loss (for one batch) at step 90: 259.3268, Accuracy: 0.7421\n",
      "Training loss (for one batch) at step 100: 277.2114, Accuracy: 0.7516\n",
      "Training loss (for one batch) at step 110: 280.5405, Accuracy: 0.7552\n",
      "---- Training ----\n",
      "Training loss: 75.1960\n",
      "Training acc over epoch: 0.7531\n",
      "---- Validation ----\n",
      "Validation loss: 46.0709\n",
      "Validation acc: 0.6875\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 292.6954, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 305.9209, Accuracy: 0.5661\n",
      "Training loss (for one batch) at step 20: 250.8261, Accuracy: 0.6443\n",
      "Training loss (for one batch) at step 30: 238.2249, Accuracy: 0.7077\n",
      "Training loss (for one batch) at step 40: 246.6198, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 50: 237.6261, Accuracy: 0.7719\n",
      "Training loss (for one batch) at step 60: 279.9138, Accuracy: 0.7820\n",
      "Training loss (for one batch) at step 70: 264.1167, Accuracy: 0.7643\n",
      "Training loss (for one batch) at step 80: 294.2728, Accuracy: 0.7430\n",
      "Training loss (for one batch) at step 90: 258.8918, Accuracy: 0.7415\n",
      "Training loss (for one batch) at step 100: 244.3948, Accuracy: 0.7495\n",
      "Training loss (for one batch) at step 110: 268.0591, Accuracy: 0.7521\n",
      "---- Training ----\n",
      "Training loss: 89.7947\n",
      "Training acc over epoch: 0.7520\n",
      "---- Validation ----\n",
      "Validation loss: 43.5906\n",
      "Validation acc: 0.6862\n",
      "Time taken: 18.06s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 285.3184, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 290.8401, Accuracy: 0.5753\n",
      "Training loss (for one batch) at step 20: 257.7625, Accuracy: 0.6507\n",
      "Training loss (for one batch) at step 30: 251.3963, Accuracy: 0.7114\n",
      "Training loss (for one batch) at step 40: 244.0853, Accuracy: 0.7492\n",
      "Training loss (for one batch) at step 50: 255.1637, Accuracy: 0.7724\n",
      "Training loss (for one batch) at step 60: 264.8268, Accuracy: 0.7843\n",
      "Training loss (for one batch) at step 70: 276.5256, Accuracy: 0.7688\n",
      "Training loss (for one batch) at step 80: 293.0006, Accuracy: 0.7475\n",
      "Training loss (for one batch) at step 90: 267.2887, Accuracy: 0.7449\n",
      "Training loss (for one batch) at step 100: 251.8950, Accuracy: 0.7533\n",
      "Training loss (for one batch) at step 110: 261.9333, Accuracy: 0.7572\n",
      "---- Training ----\n",
      "Training loss: 90.1176\n",
      "Training acc over epoch: 0.7554\n",
      "---- Validation ----\n",
      "Validation loss: 38.8490\n",
      "Validation acc: 0.6749\n",
      "Time taken: 18.25s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 294.8590, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 282.5128, Accuracy: 0.5739\n",
      "Training loss (for one batch) at step 20: 256.3990, Accuracy: 0.6432\n",
      "Training loss (for one batch) at step 30: 265.2086, Accuracy: 0.7072\n",
      "Training loss (for one batch) at step 40: 252.4813, Accuracy: 0.7443\n",
      "Training loss (for one batch) at step 50: 241.2083, Accuracy: 0.7714\n",
      "Training loss (for one batch) at step 60: 241.3764, Accuracy: 0.7837\n",
      "Training loss (for one batch) at step 70: 292.0717, Accuracy: 0.7674\n",
      "Training loss (for one batch) at step 80: 263.1153, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 90: 261.5758, Accuracy: 0.7394\n",
      "Training loss (for one batch) at step 100: 252.2175, Accuracy: 0.7483\n",
      "Training loss (for one batch) at step 110: 271.7549, Accuracy: 0.7534\n",
      "---- Training ----\n",
      "Training loss: 96.2797\n",
      "Training acc over epoch: 0.7515\n",
      "---- Validation ----\n",
      "Validation loss: 39.6067\n",
      "Validation acc: 0.6679\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 272.3804, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 270.3220, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 20: 241.8806, Accuracy: 0.6503\n",
      "Training loss (for one batch) at step 30: 255.3753, Accuracy: 0.7099\n",
      "Training loss (for one batch) at step 40: 240.6684, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 50: 229.2567, Accuracy: 0.7750\n",
      "Training loss (for one batch) at step 60: 237.0624, Accuracy: 0.7852\n",
      "Training loss (for one batch) at step 70: 264.0368, Accuracy: 0.7673\n",
      "Training loss (for one batch) at step 80: 280.9257, Accuracy: 0.7463\n",
      "Training loss (for one batch) at step 90: 248.5320, Accuracy: 0.7434\n",
      "Training loss (for one batch) at step 100: 258.2820, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 110: 263.0544, Accuracy: 0.7556\n",
      "---- Training ----\n",
      "Training loss: 96.1712\n",
      "Training acc over epoch: 0.7538\n",
      "---- Validation ----\n",
      "Validation loss: 41.7997\n",
      "Validation acc: 0.6832\n",
      "Time taken: 18.07s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 281.9781, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 295.8252, Accuracy: 0.5689\n",
      "Training loss (for one batch) at step 20: 236.1505, Accuracy: 0.6466\n",
      "Training loss (for one batch) at step 30: 257.2809, Accuracy: 0.7119\n",
      "Training loss (for one batch) at step 40: 243.6441, Accuracy: 0.7492\n",
      "Training loss (for one batch) at step 50: 237.4799, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 60: 253.2893, Accuracy: 0.7853\n",
      "Training loss (for one batch) at step 70: 269.8445, Accuracy: 0.7685\n",
      "Training loss (for one batch) at step 80: 259.9178, Accuracy: 0.7480\n",
      "Training loss (for one batch) at step 90: 260.0956, Accuracy: 0.7444\n",
      "Training loss (for one batch) at step 100: 245.8248, Accuracy: 0.7506\n",
      "Training loss (for one batch) at step 110: 243.0645, Accuracy: 0.7538\n",
      "---- Training ----\n",
      "Training loss: 87.2979\n",
      "Training acc over epoch: 0.7529\n",
      "---- Validation ----\n",
      "Validation loss: 36.6325\n",
      "Validation acc: 0.6894\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 292.7011, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 293.8039, Accuracy: 0.5717\n",
      "Training loss (for one batch) at step 20: 262.4551, Accuracy: 0.6417\n",
      "Training loss (for one batch) at step 30: 264.9696, Accuracy: 0.7089\n",
      "Training loss (for one batch) at step 40: 247.0707, Accuracy: 0.7445\n",
      "Training loss (for one batch) at step 50: 238.3034, Accuracy: 0.7722\n",
      "Training loss (for one batch) at step 60: 256.1967, Accuracy: 0.7839\n",
      "Training loss (for one batch) at step 70: 271.6044, Accuracy: 0.7667\n",
      "Training loss (for one batch) at step 80: 286.7337, Accuracy: 0.7445\n",
      "Training loss (for one batch) at step 90: 246.6202, Accuracy: 0.7414\n",
      "Training loss (for one batch) at step 100: 237.6171, Accuracy: 0.7499\n",
      "Training loss (for one batch) at step 110: 258.3184, Accuracy: 0.7524\n",
      "---- Training ----\n",
      "Training loss: 82.4234\n",
      "Training acc over epoch: 0.7514\n",
      "---- Validation ----\n",
      "Validation loss: 57.1799\n",
      "Validation acc: 0.6905\n",
      "Time taken: 18.08s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 289.6060, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 289.8215, Accuracy: 0.5852\n",
      "Training loss (for one batch) at step 20: 265.9227, Accuracy: 0.6492\n",
      "Training loss (for one batch) at step 30: 256.4795, Accuracy: 0.7064\n",
      "Training loss (for one batch) at step 40: 250.1785, Accuracy: 0.7439\n",
      "Training loss (for one batch) at step 50: 237.5112, Accuracy: 0.7695\n",
      "Training loss (for one batch) at step 60: 254.1775, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 70: 289.0231, Accuracy: 0.7654\n",
      "Training loss (for one batch) at step 80: 282.8371, Accuracy: 0.7430\n",
      "Training loss (for one batch) at step 90: 247.2781, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 100: 249.1195, Accuracy: 0.7507\n",
      "Training loss (for one batch) at step 110: 254.8586, Accuracy: 0.7537\n",
      "---- Training ----\n",
      "Training loss: 87.8924\n",
      "Training acc over epoch: 0.7517\n",
      "---- Validation ----\n",
      "Validation loss: 52.1396\n",
      "Validation acc: 0.6803\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 271.2059, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 290.1142, Accuracy: 0.5874\n",
      "Training loss (for one batch) at step 20: 229.0631, Accuracy: 0.6499\n",
      "Training loss (for one batch) at step 30: 246.0297, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 40: 230.8208, Accuracy: 0.7506\n",
      "Training loss (for one batch) at step 50: 229.6215, Accuracy: 0.7745\n",
      "Training loss (for one batch) at step 60: 257.3148, Accuracy: 0.7868\n",
      "Training loss (for one batch) at step 70: 271.0960, Accuracy: 0.7687\n",
      "Training loss (for one batch) at step 80: 269.5436, Accuracy: 0.7464\n",
      "Training loss (for one batch) at step 90: 262.7297, Accuracy: 0.7444\n",
      "Training loss (for one batch) at step 100: 244.3502, Accuracy: 0.7530\n",
      "Training loss (for one batch) at step 110: 253.3061, Accuracy: 0.7557\n",
      "---- Training ----\n",
      "Training loss: 83.8629\n",
      "Training acc over epoch: 0.7538\n",
      "---- Validation ----\n",
      "Validation loss: 38.4003\n",
      "Validation acc: 0.6873\n",
      "Time taken: 18.09s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 277.6784, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 271.0359, Accuracy: 0.5618\n",
      "Training loss (for one batch) at step 20: 241.6980, Accuracy: 0.6436\n",
      "Training loss (for one batch) at step 30: 239.6777, Accuracy: 0.7092\n",
      "Training loss (for one batch) at step 40: 231.6338, Accuracy: 0.7466\n",
      "Training loss (for one batch) at step 50: 240.3159, Accuracy: 0.7745\n",
      "Training loss (for one batch) at step 60: 238.4653, Accuracy: 0.7820\n",
      "Training loss (for one batch) at step 70: 250.7823, Accuracy: 0.7667\n",
      "Training loss (for one batch) at step 80: 273.6826, Accuracy: 0.7482\n",
      "Training loss (for one batch) at step 90: 250.2019, Accuracy: 0.7448\n",
      "Training loss (for one batch) at step 100: 246.6413, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 110: 242.4868, Accuracy: 0.7565\n",
      "---- Training ----\n",
      "Training loss: 88.3542\n",
      "Training acc over epoch: 0.7537\n",
      "---- Validation ----\n",
      "Validation loss: 44.0546\n",
      "Validation acc: 0.6943\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 274.5208, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 258.6224, Accuracy: 0.5724\n",
      "Training loss (for one batch) at step 20: 257.4951, Accuracy: 0.6499\n",
      "Training loss (for one batch) at step 30: 240.7588, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 40: 242.3551, Accuracy: 0.7487\n",
      "Training loss (for one batch) at step 50: 260.4602, Accuracy: 0.7733\n",
      "Training loss (for one batch) at step 60: 246.8006, Accuracy: 0.7850\n",
      "Training loss (for one batch) at step 70: 281.1507, Accuracy: 0.7680\n",
      "Training loss (for one batch) at step 80: 277.8629, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 90: 268.8578, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 100: 262.6530, Accuracy: 0.7503\n",
      "Training loss (for one batch) at step 110: 264.3566, Accuracy: 0.7537\n",
      "---- Training ----\n",
      "Training loss: 84.4266\n",
      "Training acc over epoch: 0.7534\n",
      "---- Validation ----\n",
      "Validation loss: 34.6425\n",
      "Validation acc: 0.6741\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 282.9391, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 294.5201, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 20: 258.6774, Accuracy: 0.6488\n",
      "Training loss (for one batch) at step 30: 242.9079, Accuracy: 0.7092\n",
      "Training loss (for one batch) at step 40: 270.0440, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 50: 234.8689, Accuracy: 0.7747\n",
      "Training loss (for one batch) at step 60: 248.8719, Accuracy: 0.7855\n",
      "Training loss (for one batch) at step 70: 286.8362, Accuracy: 0.7722\n",
      "Training loss (for one batch) at step 80: 267.8105, Accuracy: 0.7472\n",
      "Training loss (for one batch) at step 90: 243.0033, Accuracy: 0.7431\n",
      "Training loss (for one batch) at step 100: 238.6955, Accuracy: 0.7512\n",
      "Training loss (for one batch) at step 110: 257.0558, Accuracy: 0.7545\n",
      "---- Training ----\n",
      "Training loss: 80.0951\n",
      "Training acc over epoch: 0.7529\n",
      "---- Validation ----\n",
      "Validation loss: 47.2829\n",
      "Validation acc: 0.6762\n",
      "Time taken: 18.06s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 289.5291, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 269.5096, Accuracy: 0.5795\n",
      "Training loss (for one batch) at step 20: 239.8607, Accuracy: 0.6529\n",
      "Training loss (for one batch) at step 30: 242.6119, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 40: 236.2285, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 50: 229.4053, Accuracy: 0.7754\n",
      "Training loss (for one batch) at step 60: 241.0434, Accuracy: 0.7862\n",
      "Training loss (for one batch) at step 70: 275.6693, Accuracy: 0.7696\n",
      "Training loss (for one batch) at step 80: 267.4075, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 90: 250.5743, Accuracy: 0.7431\n",
      "Training loss (for one batch) at step 100: 261.4777, Accuracy: 0.7531\n",
      "Training loss (for one batch) at step 110: 269.3910, Accuracy: 0.7563\n",
      "---- Training ----\n",
      "Training loss: 93.6730\n",
      "Training acc over epoch: 0.7534\n",
      "---- Validation ----\n",
      "Validation loss: 49.8160\n",
      "Validation acc: 0.6832\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 273.5137, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 270.5216, Accuracy: 0.5668\n",
      "Training loss (for one batch) at step 20: 258.9851, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 30: 234.2454, Accuracy: 0.7074\n",
      "Training loss (for one batch) at step 40: 247.9426, Accuracy: 0.7399\n",
      "Training loss (for one batch) at step 50: 250.1216, Accuracy: 0.7690\n",
      "Training loss (for one batch) at step 60: 258.6316, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 70: 257.2798, Accuracy: 0.7645\n",
      "Training loss (for one batch) at step 80: 274.8579, Accuracy: 0.7434\n",
      "Training loss (for one batch) at step 90: 240.0321, Accuracy: 0.7402\n",
      "Training loss (for one batch) at step 100: 244.5859, Accuracy: 0.7488\n",
      "Training loss (for one batch) at step 110: 250.9269, Accuracy: 0.7506\n",
      "---- Training ----\n",
      "Training loss: 73.2473\n",
      "Training acc over epoch: 0.7503\n",
      "---- Validation ----\n",
      "Validation loss: 45.0156\n",
      "Validation acc: 0.6644\n",
      "Time taken: 18.26s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 285.7040, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 284.3771, Accuracy: 0.5661\n",
      "Training loss (for one batch) at step 20: 257.7299, Accuracy: 0.6365\n",
      "Training loss (for one batch) at step 30: 232.7981, Accuracy: 0.7056\n",
      "Training loss (for one batch) at step 40: 251.2592, Accuracy: 0.7479\n",
      "Training loss (for one batch) at step 50: 236.9903, Accuracy: 0.7722\n",
      "Training loss (for one batch) at step 60: 243.7575, Accuracy: 0.7836\n",
      "Training loss (for one batch) at step 70: 278.6577, Accuracy: 0.7657\n",
      "Training loss (for one batch) at step 80: 262.3354, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 90: 246.9536, Accuracy: 0.7406\n",
      "Training loss (for one batch) at step 100: 268.5255, Accuracy: 0.7505\n",
      "Training loss (for one batch) at step 110: 257.3100, Accuracy: 0.7530\n",
      "---- Training ----\n",
      "Training loss: 81.0534\n",
      "Training acc over epoch: 0.7518\n",
      "---- Validation ----\n",
      "Validation loss: 40.6616\n",
      "Validation acc: 0.6848\n",
      "Time taken: 18.29s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 280.2324, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 281.9583, Accuracy: 0.5561\n",
      "Training loss (for one batch) at step 20: 259.1187, Accuracy: 0.6332\n",
      "Training loss (for one batch) at step 30: 256.9736, Accuracy: 0.7064\n",
      "Training loss (for one batch) at step 40: 243.9167, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 50: 231.6550, Accuracy: 0.7695\n",
      "Training loss (for one batch) at step 60: 249.9000, Accuracy: 0.7827\n",
      "Training loss (for one batch) at step 70: 256.3768, Accuracy: 0.7666\n",
      "Training loss (for one batch) at step 80: 286.9989, Accuracy: 0.7435\n",
      "Training loss (for one batch) at step 90: 243.9614, Accuracy: 0.7399\n",
      "Training loss (for one batch) at step 100: 237.3542, Accuracy: 0.7495\n",
      "Training loss (for one batch) at step 110: 259.0072, Accuracy: 0.7522\n",
      "---- Training ----\n",
      "Training loss: 97.9572\n",
      "Training acc over epoch: 0.7511\n",
      "---- Validation ----\n",
      "Validation loss: 30.6309\n",
      "Validation acc: 0.6870\n",
      "Time taken: 18.35s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 266.5866, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 282.4071, Accuracy: 0.5696\n",
      "Training loss (for one batch) at step 20: 253.5198, Accuracy: 0.6410\n",
      "Training loss (for one batch) at step 30: 236.1077, Accuracy: 0.7009\n",
      "Training loss (for one batch) at step 40: 234.7477, Accuracy: 0.7445\n",
      "Training loss (for one batch) at step 50: 232.3484, Accuracy: 0.7698\n",
      "Training loss (for one batch) at step 60: 251.6914, Accuracy: 0.7810\n",
      "Training loss (for one batch) at step 70: 257.1034, Accuracy: 0.7632\n",
      "Training loss (for one batch) at step 80: 257.6553, Accuracy: 0.7420\n",
      "Training loss (for one batch) at step 90: 247.1646, Accuracy: 0.7399\n",
      "Training loss (for one batch) at step 100: 236.9469, Accuracy: 0.7479\n",
      "Training loss (for one batch) at step 110: 261.9867, Accuracy: 0.7528\n",
      "---- Training ----\n",
      "Training loss: 83.2613\n",
      "Training acc over epoch: 0.7514\n",
      "---- Validation ----\n",
      "Validation loss: 64.6714\n",
      "Validation acc: 0.6787\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 277.9763, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 285.2493, Accuracy: 0.5696\n",
      "Training loss (for one batch) at step 20: 237.2056, Accuracy: 0.6458\n",
      "Training loss (for one batch) at step 30: 237.4888, Accuracy: 0.7054\n",
      "Training loss (for one batch) at step 40: 253.8357, Accuracy: 0.7443\n",
      "Training loss (for one batch) at step 50: 234.2663, Accuracy: 0.7688\n",
      "Training loss (for one batch) at step 60: 255.5839, Accuracy: 0.7810\n",
      "Training loss (for one batch) at step 70: 270.2190, Accuracy: 0.7673\n",
      "Training loss (for one batch) at step 80: 267.2106, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 90: 247.0082, Accuracy: 0.7406\n",
      "Training loss (for one batch) at step 100: 264.8109, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 110: 263.7733, Accuracy: 0.7509\n",
      "---- Training ----\n",
      "Training loss: 83.1848\n",
      "Training acc over epoch: 0.7503\n",
      "---- Validation ----\n",
      "Validation loss: 43.8852\n",
      "Validation acc: 0.6886\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 305.5328, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 261.5835, Accuracy: 0.5710\n",
      "Training loss (for one batch) at step 20: 235.5499, Accuracy: 0.6380\n",
      "Training loss (for one batch) at step 30: 232.1507, Accuracy: 0.7036\n",
      "Training loss (for one batch) at step 40: 240.5515, Accuracy: 0.7445\n",
      "Training loss (for one batch) at step 50: 234.9764, Accuracy: 0.7704\n",
      "Training loss (for one batch) at step 60: 246.8214, Accuracy: 0.7800\n",
      "Training loss (for one batch) at step 70: 258.0835, Accuracy: 0.7640\n",
      "Training loss (for one batch) at step 80: 277.9656, Accuracy: 0.7441\n",
      "Training loss (for one batch) at step 90: 254.5565, Accuracy: 0.7406\n",
      "Training loss (for one batch) at step 100: 238.0807, Accuracy: 0.7508\n",
      "Training loss (for one batch) at step 110: 269.1965, Accuracy: 0.7534\n",
      "---- Training ----\n",
      "Training loss: 71.3805\n",
      "Training acc over epoch: 0.7523\n",
      "---- Validation ----\n",
      "Validation loss: 57.5870\n",
      "Validation acc: 0.7028\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 281.8882, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 264.2402, Accuracy: 0.5803\n",
      "Training loss (for one batch) at step 20: 236.6698, Accuracy: 0.6388\n",
      "Training loss (for one batch) at step 30: 239.8972, Accuracy: 0.7082\n",
      "Training loss (for one batch) at step 40: 258.6153, Accuracy: 0.7473\n",
      "Training loss (for one batch) at step 50: 226.3090, Accuracy: 0.7708\n",
      "Training loss (for one batch) at step 60: 238.0558, Accuracy: 0.7847\n",
      "Training loss (for one batch) at step 70: 269.9848, Accuracy: 0.7662\n",
      "Training loss (for one batch) at step 80: 281.4561, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 90: 250.1616, Accuracy: 0.7428\n",
      "Training loss (for one batch) at step 100: 231.1445, Accuracy: 0.7506\n",
      "Training loss (for one batch) at step 110: 253.8347, Accuracy: 0.7534\n",
      "---- Training ----\n",
      "Training loss: 93.6770\n",
      "Training acc over epoch: 0.7524\n",
      "---- Validation ----\n",
      "Validation loss: 52.9139\n",
      "Validation acc: 0.6548\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 282.9589, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 263.7368, Accuracy: 0.5604\n",
      "Training loss (for one batch) at step 20: 246.0974, Accuracy: 0.6469\n",
      "Training loss (for one batch) at step 30: 239.8541, Accuracy: 0.7074\n",
      "Training loss (for one batch) at step 40: 231.7590, Accuracy: 0.7462\n",
      "Training loss (for one batch) at step 50: 231.5443, Accuracy: 0.7725\n",
      "Training loss (for one batch) at step 60: 264.3431, Accuracy: 0.7859\n",
      "Training loss (for one batch) at step 70: 269.7641, Accuracy: 0.7679\n",
      "Training loss (for one batch) at step 80: 270.1465, Accuracy: 0.7452\n",
      "Training loss (for one batch) at step 90: 241.4514, Accuracy: 0.7430\n",
      "Training loss (for one batch) at step 100: 240.0736, Accuracy: 0.7516\n",
      "Training loss (for one batch) at step 110: 260.4574, Accuracy: 0.7542\n",
      "---- Training ----\n",
      "Training loss: 80.9015\n",
      "Training acc over epoch: 0.7532\n",
      "---- Validation ----\n",
      "Validation loss: 50.3600\n",
      "Validation acc: 0.6744\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 269.9782, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 276.4023, Accuracy: 0.5874\n",
      "Training loss (for one batch) at step 20: 247.3168, Accuracy: 0.6432\n",
      "Training loss (for one batch) at step 30: 237.2460, Accuracy: 0.7092\n",
      "Training loss (for one batch) at step 40: 237.5886, Accuracy: 0.7460\n",
      "Training loss (for one batch) at step 50: 234.3322, Accuracy: 0.7731\n",
      "Training loss (for one batch) at step 60: 238.7274, Accuracy: 0.7857\n",
      "Training loss (for one batch) at step 70: 271.8220, Accuracy: 0.7679\n",
      "Training loss (for one batch) at step 80: 258.6615, Accuracy: 0.7461\n",
      "Training loss (for one batch) at step 90: 238.1706, Accuracy: 0.7416\n",
      "Training loss (for one batch) at step 100: 250.4616, Accuracy: 0.7503\n",
      "Training loss (for one batch) at step 110: 264.0770, Accuracy: 0.7538\n",
      "---- Training ----\n",
      "Training loss: 92.5973\n",
      "Training acc over epoch: 0.7527\n",
      "---- Validation ----\n",
      "Validation loss: 58.2796\n",
      "Validation acc: 0.6859\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 268.1683, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 271.3543, Accuracy: 0.5732\n",
      "Training loss (for one batch) at step 20: 255.2247, Accuracy: 0.6488\n",
      "Training loss (for one batch) at step 30: 233.2715, Accuracy: 0.7046\n",
      "Training loss (for one batch) at step 40: 238.7582, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 50: 232.3395, Accuracy: 0.7691\n",
      "Training loss (for one batch) at step 60: 239.5772, Accuracy: 0.7824\n",
      "Training loss (for one batch) at step 70: 262.8638, Accuracy: 0.7634\n",
      "Training loss (for one batch) at step 80: 264.7163, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 90: 231.9768, Accuracy: 0.7395\n",
      "Training loss (for one batch) at step 100: 249.1737, Accuracy: 0.7479\n",
      "Training loss (for one batch) at step 110: 251.8377, Accuracy: 0.7515\n",
      "---- Training ----\n",
      "Training loss: 85.9151\n",
      "Training acc over epoch: 0.7498\n",
      "---- Validation ----\n",
      "Validation loss: 52.9454\n",
      "Validation acc: 0.6905\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 266.6063, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 274.3832, Accuracy: 0.5817\n",
      "Training loss (for one batch) at step 20: 235.2119, Accuracy: 0.6469\n",
      "Training loss (for one batch) at step 30: 244.6703, Accuracy: 0.7097\n",
      "Training loss (for one batch) at step 40: 241.9034, Accuracy: 0.7498\n",
      "Training loss (for one batch) at step 50: 235.0913, Accuracy: 0.7744\n",
      "Training loss (for one batch) at step 60: 233.0982, Accuracy: 0.7865\n",
      "Training loss (for one batch) at step 70: 265.3086, Accuracy: 0.7685\n",
      "Training loss (for one batch) at step 80: 274.6660, Accuracy: 0.7461\n",
      "Training loss (for one batch) at step 90: 243.3406, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 100: 242.3081, Accuracy: 0.7505\n",
      "Training loss (for one batch) at step 110: 248.7887, Accuracy: 0.7542\n",
      "---- Training ----\n",
      "Training loss: 89.2569\n",
      "Training acc over epoch: 0.7525\n",
      "---- Validation ----\n",
      "Validation loss: 51.5049\n",
      "Validation acc: 0.6714\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 266.2909, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 267.5260, Accuracy: 0.5803\n",
      "Training loss (for one batch) at step 20: 238.1492, Accuracy: 0.6477\n",
      "Training loss (for one batch) at step 30: 240.5927, Accuracy: 0.7011\n",
      "Training loss (for one batch) at step 40: 243.9310, Accuracy: 0.7424\n",
      "Training loss (for one batch) at step 50: 226.5954, Accuracy: 0.7711\n",
      "Training loss (for one batch) at step 60: 242.0330, Accuracy: 0.7832\n",
      "Training loss (for one batch) at step 70: 274.2744, Accuracy: 0.7651\n",
      "Training loss (for one batch) at step 80: 272.2681, Accuracy: 0.7431\n",
      "Training loss (for one batch) at step 90: 239.7728, Accuracy: 0.7404\n",
      "Training loss (for one batch) at step 100: 259.4854, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 110: 244.2930, Accuracy: 0.7525\n",
      "---- Training ----\n",
      "Training loss: 78.7920\n",
      "Training acc over epoch: 0.7515\n",
      "---- Validation ----\n",
      "Validation loss: 43.7443\n",
      "Validation acc: 0.6784\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 270.2016, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 263.9892, Accuracy: 0.5675\n",
      "Training loss (for one batch) at step 20: 235.0255, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 30: 244.9354, Accuracy: 0.7069\n",
      "Training loss (for one batch) at step 40: 232.3224, Accuracy: 0.7485\n",
      "Training loss (for one batch) at step 50: 238.3659, Accuracy: 0.7707\n",
      "Training loss (for one batch) at step 60: 256.1680, Accuracy: 0.7811\n",
      "Training loss (for one batch) at step 70: 250.8387, Accuracy: 0.7654\n",
      "Training loss (for one batch) at step 80: 253.8054, Accuracy: 0.7420\n",
      "Training loss (for one batch) at step 90: 231.7763, Accuracy: 0.7388\n",
      "Training loss (for one batch) at step 100: 259.7908, Accuracy: 0.7489\n",
      "Training loss (for one batch) at step 110: 250.3556, Accuracy: 0.7522\n",
      "---- Training ----\n",
      "Training loss: 106.7945\n",
      "Training acc over epoch: 0.7512\n",
      "---- Validation ----\n",
      "Validation loss: 40.8390\n",
      "Validation acc: 0.6881\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 261.8412, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 256.4662, Accuracy: 0.5568\n",
      "Training loss (for one batch) at step 20: 233.7386, Accuracy: 0.6388\n",
      "Training loss (for one batch) at step 30: 234.0890, Accuracy: 0.7049\n",
      "Training loss (for one batch) at step 40: 251.0219, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 50: 253.8449, Accuracy: 0.7701\n",
      "Training loss (for one batch) at step 60: 230.0174, Accuracy: 0.7829\n",
      "Training loss (for one batch) at step 70: 266.1517, Accuracy: 0.7631\n",
      "Training loss (for one batch) at step 80: 260.1739, Accuracy: 0.7427\n",
      "Training loss (for one batch) at step 90: 233.4729, Accuracy: 0.7408\n",
      "Training loss (for one batch) at step 100: 251.5179, Accuracy: 0.7494\n",
      "Training loss (for one batch) at step 110: 263.6029, Accuracy: 0.7520\n",
      "---- Training ----\n",
      "Training loss: 95.2785\n",
      "Training acc over epoch: 0.7502\n",
      "---- Validation ----\n",
      "Validation loss: 45.0738\n",
      "Validation acc: 0.6599\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 288.8808, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 267.6816, Accuracy: 0.5753\n",
      "Training loss (for one batch) at step 20: 237.4921, Accuracy: 0.6410\n",
      "Training loss (for one batch) at step 30: 254.8395, Accuracy: 0.7046\n",
      "Training loss (for one batch) at step 40: 227.0840, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 50: 228.0793, Accuracy: 0.7711\n",
      "Training loss (for one batch) at step 60: 231.8677, Accuracy: 0.7810\n",
      "Training loss (for one batch) at step 70: 273.3857, Accuracy: 0.7644\n",
      "Training loss (for one batch) at step 80: 277.2014, Accuracy: 0.7421\n",
      "Training loss (for one batch) at step 90: 251.9909, Accuracy: 0.7394\n",
      "Training loss (for one batch) at step 100: 233.3773, Accuracy: 0.7475\n",
      "Training loss (for one batch) at step 110: 253.9426, Accuracy: 0.7511\n",
      "---- Training ----\n",
      "Training loss: 77.7345\n",
      "Training acc over epoch: 0.7500\n",
      "---- Validation ----\n",
      "Validation loss: 46.8540\n",
      "Validation acc: 0.6913\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 262.4913, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 264.6266, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 20: 236.4033, Accuracy: 0.6391\n",
      "Training loss (for one batch) at step 30: 251.0391, Accuracy: 0.7021\n",
      "Training loss (for one batch) at step 40: 240.7834, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 50: 221.0473, Accuracy: 0.7688\n",
      "Training loss (for one batch) at step 60: 240.0382, Accuracy: 0.7804\n",
      "Training loss (for one batch) at step 70: 254.3001, Accuracy: 0.7643\n",
      "Training loss (for one batch) at step 80: 263.2372, Accuracy: 0.7418\n",
      "Training loss (for one batch) at step 90: 252.9392, Accuracy: 0.7390\n",
      "Training loss (for one batch) at step 100: 235.0116, Accuracy: 0.7488\n",
      "Training loss (for one batch) at step 110: 244.4584, Accuracy: 0.7523\n",
      "---- Training ----\n",
      "Training loss: 81.0862\n",
      "Training acc over epoch: 0.7507\n",
      "---- Validation ----\n",
      "Validation loss: 65.9905\n",
      "Validation acc: 0.6908\n",
      "Time taken: 17.85s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 280.7370, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 261.9054, Accuracy: 0.5710\n",
      "Training loss (for one batch) at step 20: 230.0224, Accuracy: 0.6443\n",
      "Training loss (for one batch) at step 30: 238.5446, Accuracy: 0.7074\n",
      "Training loss (for one batch) at step 40: 242.6161, Accuracy: 0.7498\n",
      "Training loss (for one batch) at step 50: 256.2491, Accuracy: 0.7724\n",
      "Training loss (for one batch) at step 60: 228.6768, Accuracy: 0.7824\n",
      "Training loss (for one batch) at step 70: 273.8102, Accuracy: 0.7660\n",
      "Training loss (for one batch) at step 80: 280.8506, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 90: 246.5563, Accuracy: 0.7426\n",
      "Training loss (for one batch) at step 100: 235.2251, Accuracy: 0.7508\n",
      "Training loss (for one batch) at step 110: 237.3989, Accuracy: 0.7548\n",
      "---- Training ----\n",
      "Training loss: 84.7433\n",
      "Training acc over epoch: 0.7529\n",
      "---- Validation ----\n",
      "Validation loss: 55.7656\n",
      "Validation acc: 0.6781\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 275.8177, Accuracy: 0.4844\n",
      "Training loss (for one batch) at step 10: 254.6475, Accuracy: 0.5653\n",
      "Training loss (for one batch) at step 20: 227.0351, Accuracy: 0.6347\n",
      "Training loss (for one batch) at step 30: 230.0283, Accuracy: 0.7019\n",
      "Training loss (for one batch) at step 40: 236.3755, Accuracy: 0.7426\n",
      "Training loss (for one batch) at step 50: 244.4335, Accuracy: 0.7699\n",
      "Training loss (for one batch) at step 60: 245.3456, Accuracy: 0.7798\n",
      "Training loss (for one batch) at step 70: 253.0637, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 80: 252.6804, Accuracy: 0.7416\n",
      "Training loss (for one batch) at step 90: 237.1740, Accuracy: 0.7404\n",
      "Training loss (for one batch) at step 100: 239.0287, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 110: 248.5214, Accuracy: 0.7509\n",
      "---- Training ----\n",
      "Training loss: 93.5103\n",
      "Training acc over epoch: 0.7492\n",
      "---- Validation ----\n",
      "Validation loss: 42.7103\n",
      "Validation acc: 0.6843\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 253.1191, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 299.8200, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 20: 227.0674, Accuracy: 0.6373\n",
      "Training loss (for one batch) at step 30: 232.2827, Accuracy: 0.7056\n",
      "Training loss (for one batch) at step 40: 242.7383, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 50: 238.8939, Accuracy: 0.7662\n",
      "Training loss (for one batch) at step 60: 230.7010, Accuracy: 0.7791\n",
      "Training loss (for one batch) at step 70: 257.0126, Accuracy: 0.7654\n",
      "Training loss (for one batch) at step 80: 247.9656, Accuracy: 0.7426\n",
      "Training loss (for one batch) at step 90: 228.5566, Accuracy: 0.7376\n",
      "Training loss (for one batch) at step 100: 234.1712, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 110: 248.1876, Accuracy: 0.7508\n",
      "---- Training ----\n",
      "Training loss: 78.3012\n",
      "Training acc over epoch: 0.7490\n",
      "---- Validation ----\n",
      "Validation loss: 43.2127\n",
      "Validation acc: 0.6846\n",
      "Time taken: 18.54s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 263.6394, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 259.8876, Accuracy: 0.5803\n",
      "Training loss (for one batch) at step 20: 245.3285, Accuracy: 0.6432\n",
      "Training loss (for one batch) at step 30: 215.8494, Accuracy: 0.7061\n",
      "Training loss (for one batch) at step 40: 238.1650, Accuracy: 0.7447\n",
      "Training loss (for one batch) at step 50: 219.8789, Accuracy: 0.7705\n",
      "Training loss (for one batch) at step 60: 243.5259, Accuracy: 0.7804\n",
      "Training loss (for one batch) at step 70: 231.7722, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 80: 263.3242, Accuracy: 0.7424\n",
      "Training loss (for one batch) at step 90: 244.2909, Accuracy: 0.7369\n",
      "Training loss (for one batch) at step 100: 227.7969, Accuracy: 0.7471\n",
      "Training loss (for one batch) at step 110: 236.4673, Accuracy: 0.7522\n",
      "---- Training ----\n",
      "Training loss: 69.0122\n",
      "Training acc over epoch: 0.7507\n",
      "---- Validation ----\n",
      "Validation loss: 57.1447\n",
      "Validation acc: 0.6980\n",
      "Time taken: 18.12s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 248.6935, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 263.5038, Accuracy: 0.5675\n",
      "Training loss (for one batch) at step 20: 246.6304, Accuracy: 0.6362\n",
      "Training loss (for one batch) at step 30: 231.7315, Accuracy: 0.7014\n",
      "Training loss (for one batch) at step 40: 219.2528, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 50: 231.3701, Accuracy: 0.7687\n",
      "Training loss (for one batch) at step 60: 237.9186, Accuracy: 0.7823\n",
      "Training loss (for one batch) at step 70: 261.0137, Accuracy: 0.7638\n",
      "Training loss (for one batch) at step 80: 263.8990, Accuracy: 0.7418\n",
      "Training loss (for one batch) at step 90: 239.8991, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 100: 239.5640, Accuracy: 0.7477\n",
      "Training loss (for one batch) at step 110: 229.3709, Accuracy: 0.7512\n",
      "---- Training ----\n",
      "Training loss: 87.1933\n",
      "Training acc over epoch: 0.7491\n",
      "---- Validation ----\n",
      "Validation loss: 47.0238\n",
      "Validation acc: 0.6768\n",
      "Time taken: 19.31s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 253.8740, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 267.6816, Accuracy: 0.5795\n",
      "Training loss (for one batch) at step 20: 232.5161, Accuracy: 0.6414\n",
      "Training loss (for one batch) at step 30: 229.5599, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 40: 240.7731, Accuracy: 0.7428\n",
      "Training loss (for one batch) at step 50: 225.0026, Accuracy: 0.7699\n",
      "Training loss (for one batch) at step 60: 244.2647, Accuracy: 0.7820\n",
      "Training loss (for one batch) at step 70: 255.5805, Accuracy: 0.7634\n",
      "Training loss (for one batch) at step 80: 259.0647, Accuracy: 0.7434\n",
      "Training loss (for one batch) at step 90: 252.9565, Accuracy: 0.7388\n",
      "Training loss (for one batch) at step 100: 235.7305, Accuracy: 0.7482\n",
      "Training loss (for one batch) at step 110: 245.0048, Accuracy: 0.7513\n",
      "---- Training ----\n",
      "Training loss: 77.7661\n",
      "Training acc over epoch: 0.7500\n",
      "---- Validation ----\n",
      "Validation loss: 64.6856\n",
      "Validation acc: 0.6873\n",
      "Time taken: 18.23s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 267.3684, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 255.5134, Accuracy: 0.5810\n",
      "Training loss (for one batch) at step 20: 222.9569, Accuracy: 0.6469\n",
      "Training loss (for one batch) at step 30: 230.5467, Accuracy: 0.7094\n",
      "Training loss (for one batch) at step 40: 237.0742, Accuracy: 0.7471\n",
      "Training loss (for one batch) at step 50: 216.9023, Accuracy: 0.7718\n",
      "Training loss (for one batch) at step 60: 228.9534, Accuracy: 0.7827\n",
      "Training loss (for one batch) at step 70: 256.4173, Accuracy: 0.7660\n",
      "Training loss (for one batch) at step 80: 246.2979, Accuracy: 0.7442\n",
      "Training loss (for one batch) at step 90: 241.4138, Accuracy: 0.7417\n",
      "Training loss (for one batch) at step 100: 224.7846, Accuracy: 0.7497\n",
      "Training loss (for one batch) at step 110: 228.4442, Accuracy: 0.7531\n",
      "---- Training ----\n",
      "Training loss: 98.7813\n",
      "Training acc over epoch: 0.7509\n",
      "---- Validation ----\n",
      "Validation loss: 44.8377\n",
      "Validation acc: 0.6768\n",
      "Time taken: 17.88s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 284.1865, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 268.0931, Accuracy: 0.5689\n",
      "Training loss (for one batch) at step 20: 222.3753, Accuracy: 0.6362\n",
      "Training loss (for one batch) at step 30: 240.4117, Accuracy: 0.7029\n",
      "Training loss (for one batch) at step 40: 233.6180, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 50: 220.5345, Accuracy: 0.7693\n",
      "Training loss (for one batch) at step 60: 227.8554, Accuracy: 0.7793\n",
      "Training loss (for one batch) at step 70: 263.4552, Accuracy: 0.7639\n",
      "Training loss (for one batch) at step 80: 279.8845, Accuracy: 0.7431\n",
      "Training loss (for one batch) at step 90: 247.3198, Accuracy: 0.7381\n",
      "Training loss (for one batch) at step 100: 239.5128, Accuracy: 0.7486\n",
      "Training loss (for one batch) at step 110: 246.7611, Accuracy: 0.7525\n",
      "---- Training ----\n",
      "Training loss: 76.7632\n",
      "Training acc over epoch: 0.7502\n",
      "---- Validation ----\n",
      "Validation loss: 47.3214\n",
      "Validation acc: 0.6830\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 272.8518, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 246.9445, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 20: 239.8775, Accuracy: 0.6451\n",
      "Training loss (for one batch) at step 30: 219.9579, Accuracy: 0.7092\n",
      "Training loss (for one batch) at step 40: 231.6770, Accuracy: 0.7487\n",
      "Training loss (for one batch) at step 50: 209.7805, Accuracy: 0.7737\n",
      "Training loss (for one batch) at step 60: 242.7595, Accuracy: 0.7857\n",
      "Training loss (for one batch) at step 70: 269.2014, Accuracy: 0.7672\n",
      "Training loss (for one batch) at step 80: 250.6184, Accuracy: 0.7438\n",
      "Training loss (for one batch) at step 90: 243.1991, Accuracy: 0.7404\n",
      "Training loss (for one batch) at step 100: 228.8402, Accuracy: 0.7502\n",
      "Training loss (for one batch) at step 110: 231.4557, Accuracy: 0.7527\n",
      "---- Training ----\n",
      "Training loss: 77.6589\n",
      "Training acc over epoch: 0.7511\n",
      "---- Validation ----\n",
      "Validation loss: 72.8189\n",
      "Validation acc: 0.6891\n",
      "Time taken: 18.14s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 268.3831, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 259.3122, Accuracy: 0.5668\n",
      "Training loss (for one batch) at step 20: 242.7069, Accuracy: 0.6369\n",
      "Training loss (for one batch) at step 30: 233.5979, Accuracy: 0.7072\n",
      "Training loss (for one batch) at step 40: 216.6647, Accuracy: 0.7449\n",
      "Training loss (for one batch) at step 50: 223.4700, Accuracy: 0.7695\n",
      "Training loss (for one batch) at step 60: 232.6603, Accuracy: 0.7828\n",
      "Training loss (for one batch) at step 70: 271.6123, Accuracy: 0.7650\n",
      "Training loss (for one batch) at step 80: 255.8042, Accuracy: 0.7421\n",
      "Training loss (for one batch) at step 90: 220.9167, Accuracy: 0.7391\n",
      "Training loss (for one batch) at step 100: 223.0725, Accuracy: 0.7471\n",
      "Training loss (for one batch) at step 110: 255.8508, Accuracy: 0.7513\n",
      "---- Training ----\n",
      "Training loss: 77.3001\n",
      "Training acc over epoch: 0.7505\n",
      "---- Validation ----\n",
      "Validation loss: 45.6272\n",
      "Validation acc: 0.6937\n",
      "Time taken: 18.39s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 285.2689, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 253.1427, Accuracy: 0.5604\n",
      "Training loss (for one batch) at step 20: 245.6353, Accuracy: 0.6272\n",
      "Training loss (for one batch) at step 30: 241.1472, Accuracy: 0.7004\n",
      "Training loss (for one batch) at step 40: 237.4726, Accuracy: 0.7388\n",
      "Training loss (for one batch) at step 50: 237.9296, Accuracy: 0.7669\n",
      "Training loss (for one batch) at step 60: 237.6436, Accuracy: 0.7804\n",
      "Training loss (for one batch) at step 70: 267.1802, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 80: 268.5766, Accuracy: 0.7419\n",
      "Training loss (for one batch) at step 90: 228.1339, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 100: 238.8829, Accuracy: 0.7486\n",
      "Training loss (for one batch) at step 110: 260.0995, Accuracy: 0.7516\n",
      "---- Training ----\n",
      "Training loss: 84.9939\n",
      "Training acc over epoch: 0.7489\n",
      "---- Validation ----\n",
      "Validation loss: 41.7291\n",
      "Validation acc: 0.6701\n",
      "Time taken: 18.39s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 258.5084, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 251.0758, Accuracy: 0.5597\n",
      "Training loss (for one batch) at step 20: 216.9294, Accuracy: 0.6354\n",
      "Training loss (for one batch) at step 30: 230.9922, Accuracy: 0.6971\n",
      "Training loss (for one batch) at step 40: 228.0248, Accuracy: 0.7393\n",
      "Training loss (for one batch) at step 50: 219.1325, Accuracy: 0.7672\n",
      "Training loss (for one batch) at step 60: 229.8521, Accuracy: 0.7774\n",
      "Training loss (for one batch) at step 70: 239.5411, Accuracy: 0.7631\n",
      "Training loss (for one batch) at step 80: 253.6780, Accuracy: 0.7410\n",
      "Training loss (for one batch) at step 90: 232.8525, Accuracy: 0.7377\n",
      "Training loss (for one batch) at step 100: 238.0186, Accuracy: 0.7476\n",
      "Training loss (for one batch) at step 110: 248.8753, Accuracy: 0.7510\n",
      "---- Training ----\n",
      "Training loss: 67.8893\n",
      "Training acc over epoch: 0.7495\n",
      "---- Validation ----\n",
      "Validation loss: 39.4075\n",
      "Validation acc: 0.6760\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 265.8998, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 258.6397, Accuracy: 0.5540\n",
      "Training loss (for one batch) at step 20: 224.9800, Accuracy: 0.6358\n",
      "Training loss (for one batch) at step 30: 226.0139, Accuracy: 0.7014\n",
      "Training loss (for one batch) at step 40: 221.2214, Accuracy: 0.7401\n",
      "Training loss (for one batch) at step 50: 223.0233, Accuracy: 0.7684\n",
      "Training loss (for one batch) at step 60: 238.6866, Accuracy: 0.7789\n",
      "Training loss (for one batch) at step 70: 260.8820, Accuracy: 0.7588\n",
      "Training loss (for one batch) at step 80: 247.1411, Accuracy: 0.7358\n",
      "Training loss (for one batch) at step 90: 253.3687, Accuracy: 0.7329\n",
      "Training loss (for one batch) at step 100: 240.5483, Accuracy: 0.7416\n",
      "Training loss (for one batch) at step 110: 250.1686, Accuracy: 0.7459\n",
      "---- Training ----\n",
      "Training loss: 86.3916\n",
      "Training acc over epoch: 0.7444\n",
      "---- Validation ----\n",
      "Validation loss: 37.7120\n",
      "Validation acc: 0.6685\n",
      "Time taken: 18.95s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 277.9665, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 268.2477, Accuracy: 0.5582\n",
      "Training loss (for one batch) at step 20: 231.6399, Accuracy: 0.6425\n",
      "Training loss (for one batch) at step 30: 246.6655, Accuracy: 0.7061\n",
      "Training loss (for one batch) at step 40: 223.2585, Accuracy: 0.7441\n",
      "Training loss (for one batch) at step 50: 229.9475, Accuracy: 0.7704\n",
      "Training loss (for one batch) at step 60: 228.9133, Accuracy: 0.7811\n",
      "Training loss (for one batch) at step 70: 249.4320, Accuracy: 0.7662\n",
      "Training loss (for one batch) at step 80: 240.2623, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 90: 229.7948, Accuracy: 0.7390\n",
      "Training loss (for one batch) at step 100: 238.5377, Accuracy: 0.7492\n",
      "Training loss (for one batch) at step 110: 263.4787, Accuracy: 0.7529\n",
      "---- Training ----\n",
      "Training loss: 108.7935\n",
      "Training acc over epoch: 0.7505\n",
      "---- Validation ----\n",
      "Validation loss: 60.4143\n",
      "Validation acc: 0.6550\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 268.9793, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 271.7577, Accuracy: 0.5639\n",
      "Training loss (for one batch) at step 20: 223.5944, Accuracy: 0.6358\n",
      "Training loss (for one batch) at step 30: 234.0095, Accuracy: 0.6983\n",
      "Training loss (for one batch) at step 40: 237.1031, Accuracy: 0.7363\n",
      "Training loss (for one batch) at step 50: 210.7478, Accuracy: 0.7647\n",
      "Training loss (for one batch) at step 60: 227.6127, Accuracy: 0.7792\n",
      "Training loss (for one batch) at step 70: 258.7567, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 80: 247.4065, Accuracy: 0.7406\n",
      "Training loss (for one batch) at step 90: 231.7847, Accuracy: 0.7363\n",
      "Training loss (for one batch) at step 100: 224.8416, Accuracy: 0.7451\n",
      "Training loss (for one batch) at step 110: 252.2682, Accuracy: 0.7498\n",
      "---- Training ----\n",
      "Training loss: 81.6951\n",
      "Training acc over epoch: 0.7488\n",
      "---- Validation ----\n",
      "Validation loss: 45.4951\n",
      "Validation acc: 0.6814\n",
      "Time taken: 18.22s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 272.0015, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 256.7687, Accuracy: 0.5632\n",
      "Training loss (for one batch) at step 20: 237.2350, Accuracy: 0.6440\n",
      "Training loss (for one batch) at step 30: 239.8828, Accuracy: 0.7054\n",
      "Training loss (for one batch) at step 40: 229.4085, Accuracy: 0.7410\n",
      "Training loss (for one batch) at step 50: 226.2465, Accuracy: 0.7688\n",
      "Training loss (for one batch) at step 60: 228.0478, Accuracy: 0.7825\n",
      "Training loss (for one batch) at step 70: 244.5128, Accuracy: 0.7628\n",
      "Training loss (for one batch) at step 80: 277.3108, Accuracy: 0.7400\n",
      "Training loss (for one batch) at step 90: 254.0993, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 100: 229.2430, Accuracy: 0.7482\n",
      "Training loss (for one batch) at step 110: 247.9909, Accuracy: 0.7504\n",
      "---- Training ----\n",
      "Training loss: 71.9251\n",
      "Training acc over epoch: 0.7494\n",
      "---- Validation ----\n",
      "Validation loss: 35.8351\n",
      "Validation acc: 0.6738\n",
      "Time taken: 18.14s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 257.5529, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 270.1526, Accuracy: 0.5533\n",
      "Training loss (for one batch) at step 20: 262.2442, Accuracy: 0.6324\n",
      "Training loss (for one batch) at step 30: 229.5402, Accuracy: 0.7044\n",
      "Training loss (for one batch) at step 40: 243.4657, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 50: 229.8205, Accuracy: 0.7704\n",
      "Training loss (for one batch) at step 60: 242.9983, Accuracy: 0.7815\n",
      "Training loss (for one batch) at step 70: 243.0703, Accuracy: 0.7614\n",
      "Training loss (for one batch) at step 80: 250.2299, Accuracy: 0.7416\n",
      "Training loss (for one batch) at step 90: 235.2129, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 100: 230.0975, Accuracy: 0.7485\n",
      "Training loss (for one batch) at step 110: 241.7280, Accuracy: 0.7510\n",
      "---- Training ----\n",
      "Training loss: 85.5268\n",
      "Training acc over epoch: 0.7497\n",
      "---- Validation ----\n",
      "Validation loss: 38.9702\n",
      "Validation acc: 0.6631\n",
      "Time taken: 18.07s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 274.5608, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 281.7451, Accuracy: 0.5554\n",
      "Training loss (for one batch) at step 20: 229.0792, Accuracy: 0.6350\n",
      "Training loss (for one batch) at step 30: 216.0181, Accuracy: 0.6948\n",
      "Training loss (for one batch) at step 40: 218.8522, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 50: 239.6607, Accuracy: 0.7641\n",
      "Training loss (for one batch) at step 60: 259.2473, Accuracy: 0.7788\n",
      "Training loss (for one batch) at step 70: 244.3625, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 80: 240.6316, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 90: 230.8526, Accuracy: 0.7363\n",
      "Training loss (for one batch) at step 100: 225.8715, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 110: 242.9305, Accuracy: 0.7504\n",
      "---- Training ----\n",
      "Training loss: 78.9118\n",
      "Training acc over epoch: 0.7483\n",
      "---- Validation ----\n",
      "Validation loss: 45.3090\n",
      "Validation acc: 0.6787\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 256.8313, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 251.0307, Accuracy: 0.5696\n",
      "Training loss (for one batch) at step 20: 241.7695, Accuracy: 0.6380\n",
      "Training loss (for one batch) at step 30: 234.8622, Accuracy: 0.7016\n",
      "Training loss (for one batch) at step 40: 221.0292, Accuracy: 0.7418\n",
      "Training loss (for one batch) at step 50: 217.8988, Accuracy: 0.7684\n",
      "Training loss (for one batch) at step 60: 255.3445, Accuracy: 0.7783\n",
      "Training loss (for one batch) at step 70: 240.1467, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 80: 269.3712, Accuracy: 0.7392\n",
      "Training loss (for one batch) at step 90: 218.6082, Accuracy: 0.7374\n",
      "Training loss (for one batch) at step 100: 257.1989, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 110: 236.6181, Accuracy: 0.7503\n",
      "---- Training ----\n",
      "Training loss: 82.5574\n",
      "Training acc over epoch: 0.7488\n",
      "---- Validation ----\n",
      "Validation loss: 47.0896\n",
      "Validation acc: 0.6585\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 255.8989, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 253.2754, Accuracy: 0.5788\n",
      "Training loss (for one batch) at step 20: 235.7179, Accuracy: 0.6481\n",
      "Training loss (for one batch) at step 30: 223.6371, Accuracy: 0.7069\n",
      "Training loss (for one batch) at step 40: 240.7935, Accuracy: 0.7449\n",
      "Training loss (for one batch) at step 50: 216.2142, Accuracy: 0.7713\n",
      "Training loss (for one batch) at step 60: 223.6295, Accuracy: 0.7829\n",
      "Training loss (for one batch) at step 70: 275.7086, Accuracy: 0.7657\n",
      "Training loss (for one batch) at step 80: 265.2923, Accuracy: 0.7440\n",
      "Training loss (for one batch) at step 90: 237.8022, Accuracy: 0.7398\n",
      "Training loss (for one batch) at step 100: 234.4408, Accuracy: 0.7484\n",
      "Training loss (for one batch) at step 110: 241.4341, Accuracy: 0.7524\n",
      "---- Training ----\n",
      "Training loss: 88.0654\n",
      "Training acc over epoch: 0.7509\n",
      "---- Validation ----\n",
      "Validation loss: 60.0660\n",
      "Validation acc: 0.6714\n",
      "Time taken: 18.08s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 250.3681, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 270.9955, Accuracy: 0.5682\n",
      "Training loss (for one batch) at step 20: 222.2355, Accuracy: 0.6436\n",
      "Training loss (for one batch) at step 30: 229.3490, Accuracy: 0.7021\n",
      "Training loss (for one batch) at step 40: 230.8940, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 50: 225.9936, Accuracy: 0.7695\n",
      "Training loss (for one batch) at step 60: 221.7106, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 70: 240.0467, Accuracy: 0.7638\n",
      "Training loss (for one batch) at step 80: 251.0170, Accuracy: 0.7419\n",
      "Training loss (for one batch) at step 90: 233.1709, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 100: 241.2796, Accuracy: 0.7470\n",
      "Training loss (for one batch) at step 110: 240.8816, Accuracy: 0.7513\n",
      "---- Training ----\n",
      "Training loss: 80.6079\n",
      "Training acc over epoch: 0.7494\n",
      "---- Validation ----\n",
      "Validation loss: 58.0976\n",
      "Validation acc: 0.6784\n",
      "Time taken: 18.03s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.6874\n",
      "Validation AUC: 0.6872\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 622.2669, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 561.4722, Accuracy: 0.4950\n",
      "Training loss (for one batch) at step 20: 528.0536, Accuracy: 0.4914\n",
      "Training loss (for one batch) at step 30: 535.8940, Accuracy: 0.4884\n",
      "Training loss (for one batch) at step 40: 541.7325, Accuracy: 0.4952\n",
      "Training loss (for one batch) at step 50: 487.1022, Accuracy: 0.5011\n",
      "Training loss (for one batch) at step 60: 520.9497, Accuracy: 0.4996\n",
      "Training loss (for one batch) at step 70: 491.9175, Accuracy: 0.5012\n",
      "Training loss (for one batch) at step 80: 483.0466, Accuracy: 0.5032\n",
      "Training loss (for one batch) at step 90: 477.5577, Accuracy: 0.5068\n",
      "Training loss (for one batch) at step 100: 470.7304, Accuracy: 0.5085\n",
      "Training loss (for one batch) at step 110: 469.1784, Accuracy: 0.5092\n",
      "---- Training ----\n",
      "Training loss: 146.8497\n",
      "Training acc over epoch: 0.5101\n",
      "---- Validation ----\n",
      "Validation loss: 35.7218\n",
      "Validation acc: 0.5134\n",
      "Time taken: 21.63s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 466.4556, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 453.4805, Accuracy: 0.4929\n",
      "Training loss (for one batch) at step 20: 459.3830, Accuracy: 0.4978\n",
      "Training loss (for one batch) at step 30: 464.7339, Accuracy: 0.5066\n",
      "Training loss (for one batch) at step 40: 453.9001, Accuracy: 0.5164\n",
      "Training loss (for one batch) at step 50: 453.7856, Accuracy: 0.5199\n",
      "Training loss (for one batch) at step 60: 450.2237, Accuracy: 0.5225\n",
      "Training loss (for one batch) at step 70: 450.9022, Accuracy: 0.5209\n",
      "Training loss (for one batch) at step 80: 458.6054, Accuracy: 0.5209\n",
      "Training loss (for one batch) at step 90: 449.1798, Accuracy: 0.5205\n",
      "Training loss (for one batch) at step 100: 447.7124, Accuracy: 0.5230\n",
      "Training loss (for one batch) at step 110: 445.8822, Accuracy: 0.5228\n",
      "---- Training ----\n",
      "Training loss: 139.0057\n",
      "Training acc over epoch: 0.5215\n",
      "---- Validation ----\n",
      "Validation loss: 35.3717\n",
      "Validation acc: 0.5091\n",
      "Time taken: 17.88s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 443.6427, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 447.2269, Accuracy: 0.5724\n",
      "Training loss (for one batch) at step 20: 446.2163, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 30: 445.7626, Accuracy: 0.5502\n",
      "Training loss (for one batch) at step 40: 446.1621, Accuracy: 0.5501\n",
      "Training loss (for one batch) at step 50: 443.8741, Accuracy: 0.5553\n",
      "Training loss (for one batch) at step 60: 445.4621, Accuracy: 0.5533\n",
      "Training loss (for one batch) at step 70: 442.5502, Accuracy: 0.5527\n",
      "Training loss (for one batch) at step 80: 444.2026, Accuracy: 0.5578\n",
      "Training loss (for one batch) at step 90: 445.0183, Accuracy: 0.5597\n",
      "Training loss (for one batch) at step 100: 446.6527, Accuracy: 0.5588\n",
      "Training loss (for one batch) at step 110: 446.2771, Accuracy: 0.5601\n",
      "---- Training ----\n",
      "Training loss: 139.0543\n",
      "Training acc over epoch: 0.5614\n",
      "---- Validation ----\n",
      "Validation loss: 34.7684\n",
      "Validation acc: 0.5290\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 443.8118, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 442.9848, Accuracy: 0.5788\n",
      "Training loss (for one batch) at step 20: 445.2722, Accuracy: 0.5740\n",
      "Training loss (for one batch) at step 30: 444.9957, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 40: 443.8264, Accuracy: 0.5772\n",
      "Training loss (for one batch) at step 50: 441.7967, Accuracy: 0.5712\n",
      "Training loss (for one batch) at step 60: 445.0774, Accuracy: 0.5731\n",
      "Training loss (for one batch) at step 70: 446.8464, Accuracy: 0.5761\n",
      "Training loss (for one batch) at step 80: 443.6478, Accuracy: 0.5799\n",
      "Training loss (for one batch) at step 90: 443.7974, Accuracy: 0.5794\n",
      "Training loss (for one batch) at step 100: 442.1656, Accuracy: 0.5801\n",
      "Training loss (for one batch) at step 110: 442.0905, Accuracy: 0.5819\n",
      "---- Training ----\n",
      "Training loss: 139.3304\n",
      "Training acc over epoch: 0.5817\n",
      "---- Validation ----\n",
      "Validation loss: 35.4941\n",
      "Validation acc: 0.5341\n",
      "Time taken: 17.88s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 441.8685, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 444.7723, Accuracy: 0.6101\n",
      "Training loss (for one batch) at step 20: 445.0933, Accuracy: 0.5993\n",
      "Training loss (for one batch) at step 30: 440.7254, Accuracy: 0.5975\n",
      "Training loss (for one batch) at step 40: 438.8468, Accuracy: 0.5995\n",
      "Training loss (for one batch) at step 50: 443.0085, Accuracy: 0.6028\n",
      "Training loss (for one batch) at step 60: 442.0638, Accuracy: 0.6009\n",
      "Training loss (for one batch) at step 70: 445.4142, Accuracy: 0.6032\n",
      "Training loss (for one batch) at step 80: 442.6907, Accuracy: 0.6060\n",
      "Training loss (for one batch) at step 90: 439.8965, Accuracy: 0.6053\n",
      "Training loss (for one batch) at step 100: 441.6287, Accuracy: 0.6029\n",
      "Training loss (for one batch) at step 110: 444.2947, Accuracy: 0.6044\n",
      "---- Training ----\n",
      "Training loss: 139.2064\n",
      "Training acc over epoch: 0.6051\n",
      "---- Validation ----\n",
      "Validation loss: 34.2886\n",
      "Validation acc: 0.5693\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 443.8697, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 444.0397, Accuracy: 0.5973\n",
      "Training loss (for one batch) at step 20: 442.5323, Accuracy: 0.5964\n",
      "Training loss (for one batch) at step 30: 434.7858, Accuracy: 0.5935\n",
      "Training loss (for one batch) at step 40: 442.3617, Accuracy: 0.6031\n",
      "Training loss (for one batch) at step 50: 442.2582, Accuracy: 0.6092\n",
      "Training loss (for one batch) at step 60: 445.8651, Accuracy: 0.6118\n",
      "Training loss (for one batch) at step 70: 443.5257, Accuracy: 0.6138\n",
      "Training loss (for one batch) at step 80: 443.6273, Accuracy: 0.6144\n",
      "Training loss (for one batch) at step 90: 440.6380, Accuracy: 0.6132\n",
      "Training loss (for one batch) at step 100: 438.7878, Accuracy: 0.6132\n",
      "Training loss (for one batch) at step 110: 439.3585, Accuracy: 0.6154\n",
      "---- Training ----\n",
      "Training loss: 137.9684\n",
      "Training acc over epoch: 0.6170\n",
      "---- Validation ----\n",
      "Validation loss: 35.7205\n",
      "Validation acc: 0.5696\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 446.0104, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 443.6006, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 20: 440.1813, Accuracy: 0.6399\n",
      "Training loss (for one batch) at step 30: 438.1235, Accuracy: 0.6373\n",
      "Training loss (for one batch) at step 40: 440.3923, Accuracy: 0.6343\n",
      "Training loss (for one batch) at step 50: 438.2834, Accuracy: 0.6386\n",
      "Training loss (for one batch) at step 60: 438.3455, Accuracy: 0.6428\n",
      "Training loss (for one batch) at step 70: 449.0996, Accuracy: 0.6436\n",
      "Training loss (for one batch) at step 80: 440.4531, Accuracy: 0.6431\n",
      "Training loss (for one batch) at step 90: 445.1683, Accuracy: 0.6390\n",
      "Training loss (for one batch) at step 100: 440.1758, Accuracy: 0.6353\n",
      "Training loss (for one batch) at step 110: 438.0978, Accuracy: 0.6325\n",
      "---- Training ----\n",
      "Training loss: 137.1507\n",
      "Training acc over epoch: 0.6332\n",
      "---- Validation ----\n",
      "Validation loss: 35.2604\n",
      "Validation acc: 0.6166\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 441.7360, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 443.8606, Accuracy: 0.6619\n",
      "Training loss (for one batch) at step 20: 437.9062, Accuracy: 0.6533\n",
      "Training loss (for one batch) at step 30: 433.0609, Accuracy: 0.6479\n",
      "Training loss (for one batch) at step 40: 428.2438, Accuracy: 0.6505\n",
      "Training loss (for one batch) at step 50: 427.1277, Accuracy: 0.6590\n",
      "Training loss (for one batch) at step 60: 436.5220, Accuracy: 0.6615\n",
      "Training loss (for one batch) at step 70: 443.0870, Accuracy: 0.6588\n",
      "Training loss (for one batch) at step 80: 442.1619, Accuracy: 0.6549\n",
      "Training loss (for one batch) at step 90: 441.9857, Accuracy: 0.6480\n",
      "Training loss (for one batch) at step 100: 437.4819, Accuracy: 0.6460\n",
      "Training loss (for one batch) at step 110: 441.1760, Accuracy: 0.6469\n",
      "---- Training ----\n",
      "Training loss: 136.6624\n",
      "Training acc over epoch: 0.6472\n",
      "---- Validation ----\n",
      "Validation loss: 36.1404\n",
      "Validation acc: 0.6139\n",
      "Time taken: 18.21s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 444.4111, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 444.1042, Accuracy: 0.6342\n",
      "Training loss (for one batch) at step 20: 436.9615, Accuracy: 0.6458\n",
      "Training loss (for one batch) at step 30: 436.1277, Accuracy: 0.6416\n",
      "Training loss (for one batch) at step 40: 428.9865, Accuracy: 0.6492\n",
      "Training loss (for one batch) at step 50: 422.1899, Accuracy: 0.6604\n",
      "Training loss (for one batch) at step 60: 428.8466, Accuracy: 0.6669\n",
      "Training loss (for one batch) at step 70: 445.4247, Accuracy: 0.6676\n",
      "Training loss (for one batch) at step 80: 439.9776, Accuracy: 0.6650\n",
      "Training loss (for one batch) at step 90: 436.9440, Accuracy: 0.6623\n",
      "Training loss (for one batch) at step 100: 439.2708, Accuracy: 0.6605\n",
      "Training loss (for one batch) at step 110: 432.7921, Accuracy: 0.6601\n",
      "---- Training ----\n",
      "Training loss: 137.3972\n",
      "Training acc over epoch: 0.6587\n",
      "---- Validation ----\n",
      "Validation loss: 32.9560\n",
      "Validation acc: 0.6354\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 445.3573, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 440.0907, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 20: 437.3073, Accuracy: 0.6700\n",
      "Training loss (for one batch) at step 30: 426.9117, Accuracy: 0.6744\n",
      "Training loss (for one batch) at step 40: 420.9442, Accuracy: 0.6808\n",
      "Training loss (for one batch) at step 50: 422.2489, Accuracy: 0.6895\n",
      "Training loss (for one batch) at step 60: 430.6955, Accuracy: 0.6965\n",
      "Training loss (for one batch) at step 70: 432.8156, Accuracy: 0.7013\n",
      "Training loss (for one batch) at step 80: 444.3119, Accuracy: 0.6941\n",
      "Training loss (for one batch) at step 90: 426.4147, Accuracy: 0.6882\n",
      "Training loss (for one batch) at step 100: 430.8802, Accuracy: 0.6848\n",
      "Training loss (for one batch) at step 110: 420.2753, Accuracy: 0.6834\n",
      "---- Training ----\n",
      "Training loss: 136.7331\n",
      "Training acc over epoch: 0.6822\n",
      "---- Validation ----\n",
      "Validation loss: 37.3240\n",
      "Validation acc: 0.6448\n",
      "Time taken: 18.09s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 439.5843, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 435.1863, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 20: 432.4036, Accuracy: 0.6815\n",
      "Training loss (for one batch) at step 30: 417.6440, Accuracy: 0.6860\n",
      "Training loss (for one batch) at step 40: 416.9761, Accuracy: 0.6867\n",
      "Training loss (for one batch) at step 50: 411.5804, Accuracy: 0.6968\n",
      "Training loss (for one batch) at step 60: 412.7788, Accuracy: 0.7041\n",
      "Training loss (for one batch) at step 70: 443.0320, Accuracy: 0.7051\n",
      "Training loss (for one batch) at step 80: 429.3727, Accuracy: 0.6997\n",
      "Training loss (for one batch) at step 90: 431.8399, Accuracy: 0.6942\n",
      "Training loss (for one batch) at step 100: 435.1754, Accuracy: 0.6938\n",
      "Training loss (for one batch) at step 110: 421.0166, Accuracy: 0.6926\n",
      "---- Training ----\n",
      "Training loss: 135.6005\n",
      "Training acc over epoch: 0.6919\n",
      "---- Validation ----\n",
      "Validation loss: 36.3976\n",
      "Validation acc: 0.6252\n",
      "Time taken: 18.05s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 443.9973, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 433.9955, Accuracy: 0.6683\n",
      "Training loss (for one batch) at step 20: 438.2970, Accuracy: 0.6693\n",
      "Training loss (for one batch) at step 30: 421.1324, Accuracy: 0.6764\n",
      "Training loss (for one batch) at step 40: 417.2131, Accuracy: 0.6885\n",
      "Training loss (for one batch) at step 50: 398.3400, Accuracy: 0.6999\n",
      "Training loss (for one batch) at step 60: 413.8906, Accuracy: 0.7094\n",
      "Training loss (for one batch) at step 70: 422.7786, Accuracy: 0.7126\n",
      "Training loss (for one batch) at step 80: 425.8911, Accuracy: 0.7055\n",
      "Training loss (for one batch) at step 90: 433.5760, Accuracy: 0.6994\n",
      "Training loss (for one batch) at step 100: 418.5057, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 110: 431.3806, Accuracy: 0.7025\n",
      "---- Training ----\n",
      "Training loss: 135.4282\n",
      "Training acc over epoch: 0.7006\n",
      "---- Validation ----\n",
      "Validation loss: 34.8368\n",
      "Validation acc: 0.6572\n",
      "Time taken: 18.06s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 444.2369, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 429.5376, Accuracy: 0.6811\n",
      "Training loss (for one batch) at step 20: 426.0813, Accuracy: 0.6808\n",
      "Training loss (for one batch) at step 30: 404.8362, Accuracy: 0.6930\n",
      "Training loss (for one batch) at step 40: 396.4829, Accuracy: 0.6997\n",
      "Training loss (for one batch) at step 50: 405.1613, Accuracy: 0.7152\n",
      "Training loss (for one batch) at step 60: 406.5719, Accuracy: 0.7239\n",
      "Training loss (for one batch) at step 70: 414.6150, Accuracy: 0.7230\n",
      "Training loss (for one batch) at step 80: 424.9079, Accuracy: 0.7153\n",
      "Training loss (for one batch) at step 90: 416.9803, Accuracy: 0.7108\n",
      "Training loss (for one batch) at step 100: 401.4677, Accuracy: 0.7096\n",
      "Training loss (for one batch) at step 110: 420.0059, Accuracy: 0.7090\n",
      "---- Training ----\n",
      "Training loss: 132.3360\n",
      "Training acc over epoch: 0.7094\n",
      "---- Validation ----\n",
      "Validation loss: 35.8569\n",
      "Validation acc: 0.6647\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 427.8888, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 436.5002, Accuracy: 0.6974\n",
      "Training loss (for one batch) at step 20: 416.0479, Accuracy: 0.6942\n",
      "Training loss (for one batch) at step 30: 399.6655, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 40: 398.2032, Accuracy: 0.7161\n",
      "Training loss (for one batch) at step 50: 375.3531, Accuracy: 0.7293\n",
      "Training loss (for one batch) at step 60: 402.3217, Accuracy: 0.7385\n",
      "Training loss (for one batch) at step 70: 413.7659, Accuracy: 0.7356\n",
      "Training loss (for one batch) at step 80: 410.5227, Accuracy: 0.7303\n",
      "Training loss (for one batch) at step 90: 405.3601, Accuracy: 0.7260\n",
      "Training loss (for one batch) at step 100: 423.5732, Accuracy: 0.7255\n",
      "Training loss (for one batch) at step 110: 418.0334, Accuracy: 0.7250\n",
      "---- Training ----\n",
      "Training loss: 124.5975\n",
      "Training acc over epoch: 0.7251\n",
      "---- Validation ----\n",
      "Validation loss: 34.2766\n",
      "Validation acc: 0.6647\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 427.5411, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 426.8557, Accuracy: 0.6591\n",
      "Training loss (for one batch) at step 20: 405.3310, Accuracy: 0.6708\n",
      "Training loss (for one batch) at step 30: 400.7419, Accuracy: 0.6948\n",
      "Training loss (for one batch) at step 40: 386.2453, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 50: 375.3791, Accuracy: 0.7278\n",
      "Training loss (for one batch) at step 60: 390.0082, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 70: 417.4593, Accuracy: 0.7310\n",
      "Training loss (for one batch) at step 80: 418.0985, Accuracy: 0.7210\n",
      "Training loss (for one batch) at step 90: 406.5982, Accuracy: 0.7186\n",
      "Training loss (for one batch) at step 100: 387.6945, Accuracy: 0.7198\n",
      "Training loss (for one batch) at step 110: 396.6931, Accuracy: 0.7230\n",
      "---- Training ----\n",
      "Training loss: 131.1413\n",
      "Training acc over epoch: 0.7229\n",
      "---- Validation ----\n",
      "Validation loss: 40.0039\n",
      "Validation acc: 0.6749\n",
      "Time taken: 18.18s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 434.7149, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 417.4479, Accuracy: 0.6697\n",
      "Training loss (for one batch) at step 20: 411.7744, Accuracy: 0.6823\n",
      "Training loss (for one batch) at step 30: 366.2019, Accuracy: 0.7041\n",
      "Training loss (for one batch) at step 40: 357.0267, Accuracy: 0.7226\n",
      "Training loss (for one batch) at step 50: 361.9371, Accuracy: 0.7371\n",
      "Training loss (for one batch) at step 60: 364.0786, Accuracy: 0.7441\n",
      "Training loss (for one batch) at step 70: 401.5206, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 80: 418.3494, Accuracy: 0.7319\n",
      "Training loss (for one batch) at step 90: 382.9163, Accuracy: 0.7302\n",
      "Training loss (for one batch) at step 100: 376.7108, Accuracy: 0.7338\n",
      "Training loss (for one batch) at step 110: 381.8908, Accuracy: 0.7342\n",
      "---- Training ----\n",
      "Training loss: 121.0774\n",
      "Training acc over epoch: 0.7326\n",
      "---- Validation ----\n",
      "Validation loss: 39.0138\n",
      "Validation acc: 0.6636\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 413.0759, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 408.0625, Accuracy: 0.6747\n",
      "Training loss (for one batch) at step 20: 374.7662, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 30: 371.7910, Accuracy: 0.6988\n",
      "Training loss (for one batch) at step 40: 368.1939, Accuracy: 0.7262\n",
      "Training loss (for one batch) at step 50: 347.7487, Accuracy: 0.7417\n",
      "Training loss (for one batch) at step 60: 357.0518, Accuracy: 0.7518\n",
      "Training loss (for one batch) at step 70: 391.4050, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 80: 395.1513, Accuracy: 0.7355\n",
      "Training loss (for one batch) at step 90: 390.6847, Accuracy: 0.7327\n",
      "Training loss (for one batch) at step 100: 369.3102, Accuracy: 0.7345\n",
      "Training loss (for one batch) at step 110: 381.7293, Accuracy: 0.7367\n",
      "---- Training ----\n",
      "Training loss: 121.4400\n",
      "Training acc over epoch: 0.7365\n",
      "---- Validation ----\n",
      "Validation loss: 33.9634\n",
      "Validation acc: 0.6762\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 398.5552, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 390.9481, Accuracy: 0.6683\n",
      "Training loss (for one batch) at step 20: 377.4194, Accuracy: 0.6845\n",
      "Training loss (for one batch) at step 30: 363.7164, Accuracy: 0.7112\n",
      "Training loss (for one batch) at step 40: 348.3148, Accuracy: 0.7348\n",
      "Training loss (for one batch) at step 50: 337.0863, Accuracy: 0.7508\n",
      "Training loss (for one batch) at step 60: 362.9095, Accuracy: 0.7572\n",
      "Training loss (for one batch) at step 70: 372.8216, Accuracy: 0.7503\n",
      "Training loss (for one batch) at step 80: 399.4478, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 90: 328.8803, Accuracy: 0.7339\n",
      "Training loss (for one batch) at step 100: 351.3838, Accuracy: 0.7359\n",
      "Training loss (for one batch) at step 110: 375.3453, Accuracy: 0.7372\n",
      "---- Training ----\n",
      "Training loss: 126.0488\n",
      "Training acc over epoch: 0.7372\n",
      "---- Validation ----\n",
      "Validation loss: 37.7001\n",
      "Validation acc: 0.6787\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 391.9958, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 375.0318, Accuracy: 0.6349\n",
      "Training loss (for one batch) at step 20: 356.6002, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 30: 346.3223, Accuracy: 0.7064\n",
      "Training loss (for one batch) at step 40: 327.3336, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 50: 348.0014, Accuracy: 0.7554\n",
      "Training loss (for one batch) at step 60: 361.6975, Accuracy: 0.7638\n",
      "Training loss (for one batch) at step 70: 387.6998, Accuracy: 0.7509\n",
      "Training loss (for one batch) at step 80: 371.9790, Accuracy: 0.7390\n",
      "Training loss (for one batch) at step 90: 362.6359, Accuracy: 0.7345\n",
      "Training loss (for one batch) at step 100: 340.8028, Accuracy: 0.7396\n",
      "Training loss (for one batch) at step 110: 345.4429, Accuracy: 0.7407\n",
      "---- Training ----\n",
      "Training loss: 123.6915\n",
      "Training acc over epoch: 0.7399\n",
      "---- Validation ----\n",
      "Validation loss: 38.3218\n",
      "Validation acc: 0.6948\n",
      "Time taken: 18.16s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 396.2286, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 366.3463, Accuracy: 0.6705\n",
      "Training loss (for one batch) at step 20: 349.6108, Accuracy: 0.7042\n",
      "Training loss (for one batch) at step 30: 338.9352, Accuracy: 0.7283\n",
      "Training loss (for one batch) at step 40: 330.3184, Accuracy: 0.7532\n",
      "Training loss (for one batch) at step 50: 320.6844, Accuracy: 0.7664\n",
      "Training loss (for one batch) at step 60: 353.6728, Accuracy: 0.7746\n",
      "Training loss (for one batch) at step 70: 363.4681, Accuracy: 0.7639\n",
      "Training loss (for one batch) at step 80: 362.2691, Accuracy: 0.7504\n",
      "Training loss (for one batch) at step 90: 350.2718, Accuracy: 0.7461\n",
      "Training loss (for one batch) at step 100: 343.0261, Accuracy: 0.7496\n",
      "Training loss (for one batch) at step 110: 348.5625, Accuracy: 0.7499\n",
      "---- Training ----\n",
      "Training loss: 112.6579\n",
      "Training acc over epoch: 0.7488\n",
      "---- Validation ----\n",
      "Validation loss: 41.8604\n",
      "Validation acc: 0.6733\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 379.6032, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 362.1596, Accuracy: 0.6477\n",
      "Training loss (for one batch) at step 20: 355.0685, Accuracy: 0.6771\n",
      "Training loss (for one batch) at step 30: 342.0655, Accuracy: 0.7092\n",
      "Training loss (for one batch) at step 40: 329.2155, Accuracy: 0.7359\n",
      "Training loss (for one batch) at step 50: 313.4719, Accuracy: 0.7552\n",
      "Training loss (for one batch) at step 60: 316.4061, Accuracy: 0.7626\n",
      "Training loss (for one batch) at step 70: 351.9025, Accuracy: 0.7533\n",
      "Training loss (for one batch) at step 80: 356.0408, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 90: 330.4958, Accuracy: 0.7375\n",
      "Training loss (for one batch) at step 100: 331.7699, Accuracy: 0.7428\n",
      "Training loss (for one batch) at step 110: 337.9285, Accuracy: 0.7444\n",
      "---- Training ----\n",
      "Training loss: 127.4153\n",
      "Training acc over epoch: 0.7428\n",
      "---- Validation ----\n",
      "Validation loss: 48.2289\n",
      "Validation acc: 0.6685\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 381.2830, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 352.2794, Accuracy: 0.6342\n",
      "Training loss (for one batch) at step 20: 337.6111, Accuracy: 0.6778\n",
      "Training loss (for one batch) at step 30: 314.5702, Accuracy: 0.7180\n",
      "Training loss (for one batch) at step 40: 312.8000, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 50: 307.9565, Accuracy: 0.7590\n",
      "Training loss (for one batch) at step 60: 314.1019, Accuracy: 0.7691\n",
      "Training loss (for one batch) at step 70: 329.7078, Accuracy: 0.7585\n",
      "Training loss (for one batch) at step 80: 343.9243, Accuracy: 0.7414\n",
      "Training loss (for one batch) at step 90: 312.6515, Accuracy: 0.7401\n",
      "Training loss (for one batch) at step 100: 326.5734, Accuracy: 0.7453\n",
      "Training loss (for one batch) at step 110: 350.4189, Accuracy: 0.7469\n",
      "---- Training ----\n",
      "Training loss: 121.0675\n",
      "Training acc over epoch: 0.7460\n",
      "---- Validation ----\n",
      "Validation loss: 39.3394\n",
      "Validation acc: 0.6736\n",
      "Time taken: 18.30s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 382.9434, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 346.5295, Accuracy: 0.6236\n",
      "Training loss (for one batch) at step 20: 326.1771, Accuracy: 0.6786\n",
      "Training loss (for one batch) at step 30: 313.0667, Accuracy: 0.7132\n",
      "Training loss (for one batch) at step 40: 320.2974, Accuracy: 0.7401\n",
      "Training loss (for one batch) at step 50: 315.1710, Accuracy: 0.7572\n",
      "Training loss (for one batch) at step 60: 336.6037, Accuracy: 0.7675\n",
      "Training loss (for one batch) at step 70: 355.1086, Accuracy: 0.7542\n",
      "Training loss (for one batch) at step 80: 353.4771, Accuracy: 0.7384\n",
      "Training loss (for one batch) at step 90: 316.6802, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 100: 307.1934, Accuracy: 0.7413\n",
      "Training loss (for one batch) at step 110: 342.6911, Accuracy: 0.7423\n",
      "---- Training ----\n",
      "Training loss: 110.3806\n",
      "Training acc over epoch: 0.7415\n",
      "---- Validation ----\n",
      "Validation loss: 44.3179\n",
      "Validation acc: 0.6548\n",
      "Time taken: 22.06s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 363.0699, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 349.7802, Accuracy: 0.6286\n",
      "Training loss (for one batch) at step 20: 309.3833, Accuracy: 0.6789\n",
      "Training loss (for one batch) at step 30: 310.6967, Accuracy: 0.7203\n",
      "Training loss (for one batch) at step 40: 302.7115, Accuracy: 0.7473\n",
      "Training loss (for one batch) at step 50: 304.3074, Accuracy: 0.7655\n",
      "Training loss (for one batch) at step 60: 308.0843, Accuracy: 0.7725\n",
      "Training loss (for one batch) at step 70: 346.8205, Accuracy: 0.7628\n",
      "Training loss (for one batch) at step 80: 328.9123, Accuracy: 0.7471\n",
      "Training loss (for one batch) at step 90: 323.0811, Accuracy: 0.7434\n",
      "Training loss (for one batch) at step 100: 316.8213, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 110: 330.2008, Accuracy: 0.7485\n",
      "---- Training ----\n",
      "Training loss: 106.1791\n",
      "Training acc over epoch: 0.7475\n",
      "---- Validation ----\n",
      "Validation loss: 56.8223\n",
      "Validation acc: 0.6835\n",
      "Time taken: 18.06s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 365.6505, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 343.6829, Accuracy: 0.6307\n",
      "Training loss (for one batch) at step 20: 315.3669, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 30: 300.9055, Accuracy: 0.7142\n",
      "Training loss (for one batch) at step 40: 301.5585, Accuracy: 0.7393\n",
      "Training loss (for one batch) at step 50: 292.5285, Accuracy: 0.7580\n",
      "Training loss (for one batch) at step 60: 316.0096, Accuracy: 0.7687\n",
      "Training loss (for one batch) at step 70: 338.2666, Accuracy: 0.7569\n",
      "Training loss (for one batch) at step 80: 333.9242, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 90: 295.1059, Accuracy: 0.7394\n",
      "Training loss (for one batch) at step 100: 319.0453, Accuracy: 0.7435\n",
      "Training loss (for one batch) at step 110: 307.6996, Accuracy: 0.7442\n",
      "---- Training ----\n",
      "Training loss: 121.3683\n",
      "Training acc over epoch: 0.7424\n",
      "---- Validation ----\n",
      "Validation loss: 39.8652\n",
      "Validation acc: 0.6695\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 367.0602, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 351.3435, Accuracy: 0.6392\n",
      "Training loss (for one batch) at step 20: 308.4982, Accuracy: 0.6827\n",
      "Training loss (for one batch) at step 30: 294.7334, Accuracy: 0.7205\n",
      "Training loss (for one batch) at step 40: 277.4283, Accuracy: 0.7494\n",
      "Training loss (for one batch) at step 50: 295.2478, Accuracy: 0.7653\n",
      "Training loss (for one batch) at step 60: 308.9087, Accuracy: 0.7751\n",
      "Training loss (for one batch) at step 70: 345.8604, Accuracy: 0.7625\n",
      "Training loss (for one batch) at step 80: 348.7504, Accuracy: 0.7460\n",
      "Training loss (for one batch) at step 90: 308.7892, Accuracy: 0.7424\n",
      "Training loss (for one batch) at step 100: 302.1332, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 110: 317.5966, Accuracy: 0.7494\n",
      "---- Training ----\n",
      "Training loss: 94.0928\n",
      "Training acc over epoch: 0.7481\n",
      "---- Validation ----\n",
      "Validation loss: 50.2587\n",
      "Validation acc: 0.6714\n",
      "Time taken: 18.09s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 348.1716, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 337.3399, Accuracy: 0.6385\n",
      "Training loss (for one batch) at step 20: 297.0096, Accuracy: 0.6778\n",
      "Training loss (for one batch) at step 30: 280.5264, Accuracy: 0.7215\n",
      "Training loss (for one batch) at step 40: 275.0102, Accuracy: 0.7494\n",
      "Training loss (for one batch) at step 50: 283.1341, Accuracy: 0.7658\n",
      "Training loss (for one batch) at step 60: 304.3671, Accuracy: 0.7739\n",
      "Training loss (for one batch) at step 70: 328.7706, Accuracy: 0.7622\n",
      "Training loss (for one batch) at step 80: 320.6406, Accuracy: 0.7461\n",
      "Training loss (for one batch) at step 90: 281.1112, Accuracy: 0.7421\n",
      "Training loss (for one batch) at step 100: 305.4147, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 110: 295.5683, Accuracy: 0.7504\n",
      "---- Training ----\n",
      "Training loss: 91.4541\n",
      "Training acc over epoch: 0.7485\n",
      "---- Validation ----\n",
      "Validation loss: 42.9990\n",
      "Validation acc: 0.6730\n",
      "Time taken: 19.15s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 344.3232, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 317.7004, Accuracy: 0.6243\n",
      "Training loss (for one batch) at step 20: 284.8406, Accuracy: 0.6760\n",
      "Training loss (for one batch) at step 30: 297.6755, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 40: 284.0202, Accuracy: 0.7479\n",
      "Training loss (for one batch) at step 50: 270.8030, Accuracy: 0.7675\n",
      "Training loss (for one batch) at step 60: 295.8968, Accuracy: 0.7777\n",
      "Training loss (for one batch) at step 70: 311.6685, Accuracy: 0.7641\n",
      "Training loss (for one batch) at step 80: 332.2378, Accuracy: 0.7469\n",
      "Training loss (for one batch) at step 90: 295.3809, Accuracy: 0.7413\n",
      "Training loss (for one batch) at step 100: 286.5959, Accuracy: 0.7469\n",
      "Training loss (for one batch) at step 110: 290.4404, Accuracy: 0.7495\n",
      "---- Training ----\n",
      "Training loss: 108.4066\n",
      "Training acc over epoch: 0.7481\n",
      "---- Validation ----\n",
      "Validation loss: 37.6066\n",
      "Validation acc: 0.6647\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 355.1213, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 322.1840, Accuracy: 0.6435\n",
      "Training loss (for one batch) at step 20: 302.3894, Accuracy: 0.6823\n",
      "Training loss (for one batch) at step 30: 295.9972, Accuracy: 0.7203\n",
      "Training loss (for one batch) at step 40: 282.9731, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 50: 270.3583, Accuracy: 0.7664\n",
      "Training loss (for one batch) at step 60: 281.7737, Accuracy: 0.7738\n",
      "Training loss (for one batch) at step 70: 319.5877, Accuracy: 0.7628\n",
      "Training loss (for one batch) at step 80: 320.2797, Accuracy: 0.7486\n",
      "Training loss (for one batch) at step 90: 286.4393, Accuracy: 0.7465\n",
      "Training loss (for one batch) at step 100: 279.5286, Accuracy: 0.7508\n",
      "Training loss (for one batch) at step 110: 321.0281, Accuracy: 0.7507\n",
      "---- Training ----\n",
      "Training loss: 89.2115\n",
      "Training acc over epoch: 0.7491\n",
      "---- Validation ----\n",
      "Validation loss: 42.5397\n",
      "Validation acc: 0.6824\n",
      "Time taken: 18.07s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 327.3959, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 341.1940, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 20: 281.5629, Accuracy: 0.6656\n",
      "Training loss (for one batch) at step 30: 278.5603, Accuracy: 0.7097\n",
      "Training loss (for one batch) at step 40: 262.5239, Accuracy: 0.7407\n",
      "Training loss (for one batch) at step 50: 288.2829, Accuracy: 0.7598\n",
      "Training loss (for one batch) at step 60: 295.4423, Accuracy: 0.7710\n",
      "Training loss (for one batch) at step 70: 295.3640, Accuracy: 0.7612\n",
      "Training loss (for one batch) at step 80: 326.8384, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 90: 278.8961, Accuracy: 0.7421\n",
      "Training loss (for one batch) at step 100: 278.5242, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 110: 278.8810, Accuracy: 0.7483\n",
      "---- Training ----\n",
      "Training loss: 105.5337\n",
      "Training acc over epoch: 0.7473\n",
      "---- Validation ----\n",
      "Validation loss: 45.8760\n",
      "Validation acc: 0.6714\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 345.4628, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 303.4167, Accuracy: 0.6357\n",
      "Training loss (for one batch) at step 20: 288.0175, Accuracy: 0.6793\n",
      "Training loss (for one batch) at step 30: 284.1520, Accuracy: 0.7215\n",
      "Training loss (for one batch) at step 40: 260.3086, Accuracy: 0.7504\n",
      "Training loss (for one batch) at step 50: 272.2043, Accuracy: 0.7707\n",
      "Training loss (for one batch) at step 60: 269.3495, Accuracy: 0.7804\n",
      "Training loss (for one batch) at step 70: 314.8723, Accuracy: 0.7689\n",
      "Training loss (for one batch) at step 80: 319.8026, Accuracy: 0.7516\n",
      "Training loss (for one batch) at step 90: 279.1678, Accuracy: 0.7467\n",
      "Training loss (for one batch) at step 100: 262.6502, Accuracy: 0.7509\n",
      "Training loss (for one batch) at step 110: 316.7964, Accuracy: 0.7515\n",
      "---- Training ----\n",
      "Training loss: 102.9878\n",
      "Training acc over epoch: 0.7499\n",
      "---- Validation ----\n",
      "Validation loss: 45.7204\n",
      "Validation acc: 0.6808\n",
      "Time taken: 18.05s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 324.8100, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 319.4552, Accuracy: 0.6179\n",
      "Training loss (for one batch) at step 20: 281.8781, Accuracy: 0.6689\n",
      "Training loss (for one batch) at step 30: 274.1172, Accuracy: 0.7119\n",
      "Training loss (for one batch) at step 40: 257.4885, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 50: 254.1735, Accuracy: 0.7612\n",
      "Training loss (for one batch) at step 60: 275.0390, Accuracy: 0.7720\n",
      "Training loss (for one batch) at step 70: 305.0929, Accuracy: 0.7610\n",
      "Training loss (for one batch) at step 80: 286.5297, Accuracy: 0.7446\n",
      "Training loss (for one batch) at step 90: 262.3650, Accuracy: 0.7407\n",
      "Training loss (for one batch) at step 100: 274.0694, Accuracy: 0.7464\n",
      "Training loss (for one batch) at step 110: 307.3490, Accuracy: 0.7480\n",
      "---- Training ----\n",
      "Training loss: 90.9416\n",
      "Training acc over epoch: 0.7459\n",
      "---- Validation ----\n",
      "Validation loss: 59.7065\n",
      "Validation acc: 0.6827\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 320.3214, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 313.4558, Accuracy: 0.6385\n",
      "Training loss (for one batch) at step 20: 287.5560, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 30: 246.1888, Accuracy: 0.7157\n",
      "Training loss (for one batch) at step 40: 273.8265, Accuracy: 0.7456\n",
      "Training loss (for one batch) at step 50: 277.7743, Accuracy: 0.7612\n",
      "Training loss (for one batch) at step 60: 273.9875, Accuracy: 0.7728\n",
      "Training loss (for one batch) at step 70: 313.5216, Accuracy: 0.7580\n",
      "Training loss (for one batch) at step 80: 299.2665, Accuracy: 0.7426\n",
      "Training loss (for one batch) at step 90: 293.1483, Accuracy: 0.7406\n",
      "Training loss (for one batch) at step 100: 284.4850, Accuracy: 0.7446\n",
      "Training loss (for one batch) at step 110: 293.2966, Accuracy: 0.7468\n",
      "---- Training ----\n",
      "Training loss: 97.2072\n",
      "Training acc over epoch: 0.7454\n",
      "---- Validation ----\n",
      "Validation loss: 59.1566\n",
      "Validation acc: 0.6636\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 326.9410, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 313.1092, Accuracy: 0.6101\n",
      "Training loss (for one batch) at step 20: 275.3866, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 30: 260.8353, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 40: 247.5262, Accuracy: 0.7439\n",
      "Training loss (for one batch) at step 50: 257.7657, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 60: 276.4124, Accuracy: 0.7692\n",
      "Training loss (for one batch) at step 70: 310.6094, Accuracy: 0.7584\n",
      "Training loss (for one batch) at step 80: 315.3497, Accuracy: 0.7404\n",
      "Training loss (for one batch) at step 90: 277.6323, Accuracy: 0.7388\n",
      "Training loss (for one batch) at step 100: 261.8902, Accuracy: 0.7435\n",
      "Training loss (for one batch) at step 110: 296.7791, Accuracy: 0.7439\n",
      "---- Training ----\n",
      "Training loss: 108.5267\n",
      "Training acc over epoch: 0.7431\n",
      "---- Validation ----\n",
      "Validation loss: 47.9239\n",
      "Validation acc: 0.6730\n",
      "Time taken: 18.15s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 333.2306, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 305.2219, Accuracy: 0.6278\n",
      "Training loss (for one batch) at step 20: 289.6600, Accuracy: 0.6741\n",
      "Training loss (for one batch) at step 30: 284.3565, Accuracy: 0.7172\n",
      "Training loss (for one batch) at step 40: 255.0177, Accuracy: 0.7490\n",
      "Training loss (for one batch) at step 50: 257.3261, Accuracy: 0.7685\n",
      "Training loss (for one batch) at step 60: 276.4181, Accuracy: 0.7768\n",
      "Training loss (for one batch) at step 70: 303.6222, Accuracy: 0.7620\n",
      "Training loss (for one batch) at step 80: 286.8323, Accuracy: 0.7442\n",
      "Training loss (for one batch) at step 90: 274.5964, Accuracy: 0.7414\n",
      "Training loss (for one batch) at step 100: 265.4318, Accuracy: 0.7488\n",
      "Training loss (for one batch) at step 110: 291.4506, Accuracy: 0.7489\n",
      "---- Training ----\n",
      "Training loss: 84.0111\n",
      "Training acc over epoch: 0.7479\n",
      "---- Validation ----\n",
      "Validation loss: 43.2813\n",
      "Validation acc: 0.6867\n",
      "Time taken: 17.89s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 347.1451, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 317.0049, Accuracy: 0.6321\n",
      "Training loss (for one batch) at step 20: 281.1761, Accuracy: 0.6771\n",
      "Training loss (for one batch) at step 30: 259.9440, Accuracy: 0.7193\n",
      "Training loss (for one batch) at step 40: 256.6461, Accuracy: 0.7490\n",
      "Training loss (for one batch) at step 50: 246.6274, Accuracy: 0.7681\n",
      "Training loss (for one batch) at step 60: 292.1862, Accuracy: 0.7766\n",
      "Training loss (for one batch) at step 70: 299.6749, Accuracy: 0.7627\n",
      "Training loss (for one batch) at step 80: 321.7395, Accuracy: 0.7490\n",
      "Training loss (for one batch) at step 90: 283.6235, Accuracy: 0.7464\n",
      "Training loss (for one batch) at step 100: 261.1339, Accuracy: 0.7513\n",
      "Training loss (for one batch) at step 110: 292.9162, Accuracy: 0.7513\n",
      "---- Training ----\n",
      "Training loss: 109.6398\n",
      "Training acc over epoch: 0.7495\n",
      "---- Validation ----\n",
      "Validation loss: 41.2041\n",
      "Validation acc: 0.6652\n",
      "Time taken: 18.09s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 313.5679, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 318.1212, Accuracy: 0.6072\n",
      "Training loss (for one batch) at step 20: 273.5901, Accuracy: 0.6663\n",
      "Training loss (for one batch) at step 30: 271.5497, Accuracy: 0.7167\n",
      "Training loss (for one batch) at step 40: 279.7375, Accuracy: 0.7456\n",
      "Training loss (for one batch) at step 50: 270.9735, Accuracy: 0.7641\n",
      "Training loss (for one batch) at step 60: 274.5761, Accuracy: 0.7720\n",
      "Training loss (for one batch) at step 70: 302.0040, Accuracy: 0.7597\n",
      "Training loss (for one batch) at step 80: 295.0303, Accuracy: 0.7417\n",
      "Training loss (for one batch) at step 90: 269.7681, Accuracy: 0.7388\n",
      "Training loss (for one batch) at step 100: 261.5955, Accuracy: 0.7469\n",
      "Training loss (for one batch) at step 110: 290.3378, Accuracy: 0.7478\n",
      "---- Training ----\n",
      "Training loss: 87.0471\n",
      "Training acc over epoch: 0.7470\n",
      "---- Validation ----\n",
      "Validation loss: 43.4297\n",
      "Validation acc: 0.6916\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 319.4133, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 303.8878, Accuracy: 0.6108\n",
      "Training loss (for one batch) at step 20: 302.2849, Accuracy: 0.6596\n",
      "Training loss (for one batch) at step 30: 258.0405, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 40: 270.0034, Accuracy: 0.7431\n",
      "Training loss (for one batch) at step 50: 243.8509, Accuracy: 0.7626\n",
      "Training loss (for one batch) at step 60: 274.3427, Accuracy: 0.7715\n",
      "Training loss (for one batch) at step 70: 278.5613, Accuracy: 0.7597\n",
      "Training loss (for one batch) at step 80: 291.6906, Accuracy: 0.7438\n",
      "Training loss (for one batch) at step 90: 262.0272, Accuracy: 0.7392\n",
      "Training loss (for one batch) at step 100: 265.3177, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 110: 285.9134, Accuracy: 0.7458\n",
      "---- Training ----\n",
      "Training loss: 95.3997\n",
      "Training acc over epoch: 0.7453\n",
      "---- Validation ----\n",
      "Validation loss: 48.2014\n",
      "Validation acc: 0.6961\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 315.4413, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 291.9494, Accuracy: 0.6200\n",
      "Training loss (for one batch) at step 20: 288.3348, Accuracy: 0.6763\n",
      "Training loss (for one batch) at step 30: 272.9827, Accuracy: 0.7167\n",
      "Training loss (for one batch) at step 40: 245.2007, Accuracy: 0.7473\n",
      "Training loss (for one batch) at step 50: 256.1577, Accuracy: 0.7669\n",
      "Training loss (for one batch) at step 60: 259.9919, Accuracy: 0.7748\n",
      "Training loss (for one batch) at step 70: 295.7283, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 80: 286.8253, Accuracy: 0.7439\n",
      "Training loss (for one batch) at step 90: 268.1996, Accuracy: 0.7418\n",
      "Training loss (for one batch) at step 100: 269.2812, Accuracy: 0.7488\n",
      "Training loss (for one batch) at step 110: 278.6790, Accuracy: 0.7488\n",
      "---- Training ----\n",
      "Training loss: 81.4212\n",
      "Training acc over epoch: 0.7480\n",
      "---- Validation ----\n",
      "Validation loss: 49.3132\n",
      "Validation acc: 0.6784\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 293.8711, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 306.9681, Accuracy: 0.6286\n",
      "Training loss (for one batch) at step 20: 268.4614, Accuracy: 0.6760\n",
      "Training loss (for one batch) at step 30: 279.1062, Accuracy: 0.7155\n",
      "Training loss (for one batch) at step 40: 240.0103, Accuracy: 0.7460\n",
      "Training loss (for one batch) at step 50: 268.9773, Accuracy: 0.7652\n",
      "Training loss (for one batch) at step 60: 275.3236, Accuracy: 0.7764\n",
      "Training loss (for one batch) at step 70: 295.5770, Accuracy: 0.7598\n",
      "Training loss (for one batch) at step 80: 266.7533, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 90: 279.1134, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 100: 261.4663, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 110: 278.9224, Accuracy: 0.7470\n",
      "---- Training ----\n",
      "Training loss: 91.7229\n",
      "Training acc over epoch: 0.7456\n",
      "---- Validation ----\n",
      "Validation loss: 49.7523\n",
      "Validation acc: 0.6741\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 288.5867, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 298.9413, Accuracy: 0.6101\n",
      "Training loss (for one batch) at step 20: 258.2885, Accuracy: 0.6704\n",
      "Training loss (for one batch) at step 30: 262.4574, Accuracy: 0.7170\n",
      "Training loss (for one batch) at step 40: 251.2978, Accuracy: 0.7477\n",
      "Training loss (for one batch) at step 50: 250.2873, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 60: 257.2415, Accuracy: 0.7741\n",
      "Training loss (for one batch) at step 70: 288.5809, Accuracy: 0.7628\n",
      "Training loss (for one batch) at step 80: 303.9633, Accuracy: 0.7451\n",
      "Training loss (for one batch) at step 90: 265.1990, Accuracy: 0.7418\n",
      "Training loss (for one batch) at step 100: 258.6463, Accuracy: 0.7485\n",
      "Training loss (for one batch) at step 110: 280.0043, Accuracy: 0.7494\n",
      "---- Training ----\n",
      "Training loss: 85.9989\n",
      "Training acc over epoch: 0.7471\n",
      "---- Validation ----\n",
      "Validation loss: 38.1164\n",
      "Validation acc: 0.6792\n",
      "Time taken: 18.13s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 318.7728, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 300.0548, Accuracy: 0.6072\n",
      "Training loss (for one batch) at step 20: 252.9110, Accuracy: 0.6674\n",
      "Training loss (for one batch) at step 30: 270.1402, Accuracy: 0.7172\n",
      "Training loss (for one batch) at step 40: 241.7951, Accuracy: 0.7452\n",
      "Training loss (for one batch) at step 50: 258.1812, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 60: 256.4045, Accuracy: 0.7724\n",
      "Training loss (for one batch) at step 70: 288.3002, Accuracy: 0.7577\n",
      "Training loss (for one batch) at step 80: 276.7272, Accuracy: 0.7423\n",
      "Training loss (for one batch) at step 90: 251.5516, Accuracy: 0.7381\n",
      "Training loss (for one batch) at step 100: 274.4013, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 110: 282.5587, Accuracy: 0.7452\n",
      "---- Training ----\n",
      "Training loss: 77.9597\n",
      "Training acc over epoch: 0.7436\n",
      "---- Validation ----\n",
      "Validation loss: 51.4149\n",
      "Validation acc: 0.6553\n",
      "Time taken: 17.88s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 308.3691, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 276.5511, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 20: 264.0558, Accuracy: 0.6689\n",
      "Training loss (for one batch) at step 30: 253.7417, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 40: 264.3697, Accuracy: 0.7471\n",
      "Training loss (for one batch) at step 50: 256.2588, Accuracy: 0.7655\n",
      "Training loss (for one batch) at step 60: 269.1187, Accuracy: 0.7738\n",
      "Training loss (for one batch) at step 70: 268.7826, Accuracy: 0.7623\n",
      "Training loss (for one batch) at step 80: 275.1287, Accuracy: 0.7440\n",
      "Training loss (for one batch) at step 90: 272.5808, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 100: 273.8389, Accuracy: 0.7484\n",
      "Training loss (for one batch) at step 110: 285.4709, Accuracy: 0.7491\n",
      "---- Training ----\n",
      "Training loss: 80.1919\n",
      "Training acc over epoch: 0.7474\n",
      "---- Validation ----\n",
      "Validation loss: 43.6542\n",
      "Validation acc: 0.6835\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 279.2620, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 286.3850, Accuracy: 0.6101\n",
      "Training loss (for one batch) at step 20: 254.0392, Accuracy: 0.6585\n",
      "Training loss (for one batch) at step 30: 258.5041, Accuracy: 0.7112\n",
      "Training loss (for one batch) at step 40: 249.7652, Accuracy: 0.7431\n",
      "Training loss (for one batch) at step 50: 252.2424, Accuracy: 0.7636\n",
      "Training loss (for one batch) at step 60: 272.4149, Accuracy: 0.7718\n",
      "Training loss (for one batch) at step 70: 272.8529, Accuracy: 0.7567\n",
      "Training loss (for one batch) at step 80: 261.7114, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 90: 268.5788, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 100: 242.2629, Accuracy: 0.7443\n",
      "Training loss (for one batch) at step 110: 279.9556, Accuracy: 0.7463\n",
      "---- Training ----\n",
      "Training loss: 86.3101\n",
      "Training acc over epoch: 0.7459\n",
      "---- Validation ----\n",
      "Validation loss: 40.0579\n",
      "Validation acc: 0.6800\n",
      "Time taken: 17.84s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 305.8181, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 293.7508, Accuracy: 0.6214\n",
      "Training loss (for one batch) at step 20: 270.7623, Accuracy: 0.6756\n",
      "Training loss (for one batch) at step 30: 242.4672, Accuracy: 0.7140\n",
      "Training loss (for one batch) at step 40: 248.8867, Accuracy: 0.7498\n",
      "Training loss (for one batch) at step 50: 228.6775, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 60: 255.6785, Accuracy: 0.7761\n",
      "Training loss (for one batch) at step 70: 276.9672, Accuracy: 0.7617\n",
      "Training loss (for one batch) at step 80: 302.6014, Accuracy: 0.7432\n",
      "Training loss (for one batch) at step 90: 250.8310, Accuracy: 0.7416\n",
      "Training loss (for one batch) at step 100: 265.3444, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 110: 265.2187, Accuracy: 0.7490\n",
      "---- Training ----\n",
      "Training loss: 90.5563\n",
      "Training acc over epoch: 0.7463\n",
      "---- Validation ----\n",
      "Validation loss: 52.5229\n",
      "Validation acc: 0.6808\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 276.0495, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 280.5557, Accuracy: 0.6122\n",
      "Training loss (for one batch) at step 20: 258.9369, Accuracy: 0.6637\n",
      "Training loss (for one batch) at step 30: 250.8977, Accuracy: 0.7157\n",
      "Training loss (for one batch) at step 40: 269.2348, Accuracy: 0.7477\n",
      "Training loss (for one batch) at step 50: 241.5005, Accuracy: 0.7650\n",
      "Training loss (for one batch) at step 60: 257.5462, Accuracy: 0.7751\n",
      "Training loss (for one batch) at step 70: 274.6201, Accuracy: 0.7624\n",
      "Training loss (for one batch) at step 80: 309.8224, Accuracy: 0.7430\n",
      "Training loss (for one batch) at step 90: 268.9219, Accuracy: 0.7396\n",
      "Training loss (for one batch) at step 100: 245.5551, Accuracy: 0.7445\n",
      "Training loss (for one batch) at step 110: 269.9989, Accuracy: 0.7473\n",
      "---- Training ----\n",
      "Training loss: 79.3438\n",
      "Training acc over epoch: 0.7448\n",
      "---- Validation ----\n",
      "Validation loss: 58.6346\n",
      "Validation acc: 0.6647\n",
      "Time taken: 17.85s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 283.2224, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 282.4097, Accuracy: 0.5994\n",
      "Training loss (for one batch) at step 20: 267.1656, Accuracy: 0.6626\n",
      "Training loss (for one batch) at step 30: 255.2837, Accuracy: 0.7147\n",
      "Training loss (for one batch) at step 40: 245.5673, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 50: 249.1914, Accuracy: 0.7630\n",
      "Training loss (for one batch) at step 60: 268.8223, Accuracy: 0.7736\n",
      "Training loss (for one batch) at step 70: 279.9244, Accuracy: 0.7602\n",
      "Training loss (for one batch) at step 80: 287.7878, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 90: 273.1002, Accuracy: 0.7391\n",
      "Training loss (for one batch) at step 100: 249.0216, Accuracy: 0.7444\n",
      "Training loss (for one batch) at step 110: 265.6768, Accuracy: 0.7456\n",
      "---- Training ----\n",
      "Training loss: 94.3394\n",
      "Training acc over epoch: 0.7440\n",
      "---- Validation ----\n",
      "Validation loss: 64.3216\n",
      "Validation acc: 0.6722\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 285.0377, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 299.7246, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 20: 250.0181, Accuracy: 0.6674\n",
      "Training loss (for one batch) at step 30: 249.3477, Accuracy: 0.7140\n",
      "Training loss (for one batch) at step 40: 237.9200, Accuracy: 0.7477\n",
      "Training loss (for one batch) at step 50: 238.5086, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 60: 254.1197, Accuracy: 0.7728\n",
      "Training loss (for one batch) at step 70: 273.1950, Accuracy: 0.7598\n",
      "Training loss (for one batch) at step 80: 266.9008, Accuracy: 0.7419\n",
      "Training loss (for one batch) at step 90: 259.3851, Accuracy: 0.7380\n",
      "Training loss (for one batch) at step 100: 262.0084, Accuracy: 0.7445\n",
      "Training loss (for one batch) at step 110: 283.6769, Accuracy: 0.7459\n",
      "---- Training ----\n",
      "Training loss: 81.6214\n",
      "Training acc over epoch: 0.7446\n",
      "---- Validation ----\n",
      "Validation loss: 36.9915\n",
      "Validation acc: 0.6824\n",
      "Time taken: 18.11s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 298.5476, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 284.2386, Accuracy: 0.6229\n",
      "Training loss (for one batch) at step 20: 261.6952, Accuracy: 0.6749\n",
      "Training loss (for one batch) at step 30: 236.6030, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 40: 233.2012, Accuracy: 0.7475\n",
      "Training loss (for one batch) at step 50: 253.0163, Accuracy: 0.7647\n",
      "Training loss (for one batch) at step 60: 252.1018, Accuracy: 0.7741\n",
      "Training loss (for one batch) at step 70: 259.2203, Accuracy: 0.7602\n",
      "Training loss (for one batch) at step 80: 262.2549, Accuracy: 0.7441\n",
      "Training loss (for one batch) at step 90: 245.2582, Accuracy: 0.7411\n",
      "Training loss (for one batch) at step 100: 246.1197, Accuracy: 0.7470\n",
      "Training loss (for one batch) at step 110: 271.4409, Accuracy: 0.7464\n",
      "---- Training ----\n",
      "Training loss: 80.8347\n",
      "Training acc over epoch: 0.7455\n",
      "---- Validation ----\n",
      "Validation loss: 43.1338\n",
      "Validation acc: 0.6827\n",
      "Time taken: 17.83s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 282.5980, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 305.3583, Accuracy: 0.6087\n",
      "Training loss (for one batch) at step 20: 245.9758, Accuracy: 0.6618\n",
      "Training loss (for one batch) at step 30: 234.9983, Accuracy: 0.7135\n",
      "Training loss (for one batch) at step 40: 244.8321, Accuracy: 0.7458\n",
      "Training loss (for one batch) at step 50: 237.9659, Accuracy: 0.7642\n",
      "Training loss (for one batch) at step 60: 264.4723, Accuracy: 0.7739\n",
      "Training loss (for one batch) at step 70: 276.3899, Accuracy: 0.7600\n",
      "Training loss (for one batch) at step 80: 275.9091, Accuracy: 0.7432\n",
      "Training loss (for one batch) at step 90: 242.7247, Accuracy: 0.7410\n",
      "Training loss (for one batch) at step 100: 245.1765, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 110: 254.6150, Accuracy: 0.7494\n",
      "---- Training ----\n",
      "Training loss: 95.5437\n",
      "Training acc over epoch: 0.7461\n",
      "---- Validation ----\n",
      "Validation loss: 39.9226\n",
      "Validation acc: 0.6838\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 280.8754, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 297.1535, Accuracy: 0.6136\n",
      "Training loss (for one batch) at step 20: 256.1009, Accuracy: 0.6708\n",
      "Training loss (for one batch) at step 30: 259.0640, Accuracy: 0.7155\n",
      "Training loss (for one batch) at step 40: 241.9323, Accuracy: 0.7466\n",
      "Training loss (for one batch) at step 50: 228.1085, Accuracy: 0.7662\n",
      "Training loss (for one batch) at step 60: 241.2084, Accuracy: 0.7715\n",
      "Training loss (for one batch) at step 70: 258.2596, Accuracy: 0.7611\n",
      "Training loss (for one batch) at step 80: 272.0926, Accuracy: 0.7427\n",
      "Training loss (for one batch) at step 90: 238.6092, Accuracy: 0.7394\n",
      "Training loss (for one batch) at step 100: 258.4149, Accuracy: 0.7446\n",
      "Training loss (for one batch) at step 110: 272.1898, Accuracy: 0.7463\n",
      "---- Training ----\n",
      "Training loss: 85.6044\n",
      "Training acc over epoch: 0.7452\n",
      "---- Validation ----\n",
      "Validation loss: 41.7627\n",
      "Validation acc: 0.6685\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 293.6476, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 286.5743, Accuracy: 0.6136\n",
      "Training loss (for one batch) at step 20: 244.3196, Accuracy: 0.6674\n",
      "Training loss (for one batch) at step 30: 243.6743, Accuracy: 0.7140\n",
      "Training loss (for one batch) at step 40: 240.6738, Accuracy: 0.7443\n",
      "Training loss (for one batch) at step 50: 239.0317, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 60: 267.2683, Accuracy: 0.7737\n",
      "Training loss (for one batch) at step 70: 257.6988, Accuracy: 0.7600\n",
      "Training loss (for one batch) at step 80: 253.8170, Accuracy: 0.7435\n",
      "Training loss (for one batch) at step 90: 250.5148, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 100: 243.6061, Accuracy: 0.7464\n",
      "Training loss (for one batch) at step 110: 265.4409, Accuracy: 0.7475\n",
      "---- Training ----\n",
      "Training loss: 89.6534\n",
      "Training acc over epoch: 0.7451\n",
      "---- Validation ----\n",
      "Validation loss: 67.7241\n",
      "Validation acc: 0.6746\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 284.0148, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 265.2729, Accuracy: 0.6087\n",
      "Training loss (for one batch) at step 20: 231.1553, Accuracy: 0.6693\n",
      "Training loss (for one batch) at step 30: 245.7278, Accuracy: 0.7097\n",
      "Training loss (for one batch) at step 40: 242.5028, Accuracy: 0.7443\n",
      "Training loss (for one batch) at step 50: 243.5156, Accuracy: 0.7618\n",
      "Training loss (for one batch) at step 60: 240.1842, Accuracy: 0.7732\n",
      "Training loss (for one batch) at step 70: 296.4315, Accuracy: 0.7588\n",
      "Training loss (for one batch) at step 80: 264.1062, Accuracy: 0.7401\n",
      "Training loss (for one batch) at step 90: 249.5695, Accuracy: 0.7367\n",
      "Training loss (for one batch) at step 100: 238.4122, Accuracy: 0.7431\n",
      "Training loss (for one batch) at step 110: 251.0902, Accuracy: 0.7456\n",
      "---- Training ----\n",
      "Training loss: 88.6488\n",
      "Training acc over epoch: 0.7427\n",
      "---- Validation ----\n",
      "Validation loss: 67.8188\n",
      "Validation acc: 0.6905\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 270.2759, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 274.8857, Accuracy: 0.6030\n",
      "Training loss (for one batch) at step 20: 259.8233, Accuracy: 0.6644\n",
      "Training loss (for one batch) at step 30: 234.5736, Accuracy: 0.7152\n",
      "Training loss (for one batch) at step 40: 225.1933, Accuracy: 0.7428\n",
      "Training loss (for one batch) at step 50: 232.8061, Accuracy: 0.7619\n",
      "Training loss (for one batch) at step 60: 268.2522, Accuracy: 0.7739\n",
      "Training loss (for one batch) at step 70: 282.1373, Accuracy: 0.7596\n",
      "Training loss (for one batch) at step 80: 257.6904, Accuracy: 0.7424\n",
      "Training loss (for one batch) at step 90: 242.0899, Accuracy: 0.7373\n",
      "Training loss (for one batch) at step 100: 243.0022, Accuracy: 0.7430\n",
      "Training loss (for one batch) at step 110: 271.2116, Accuracy: 0.7443\n",
      "---- Training ----\n",
      "Training loss: 81.9263\n",
      "Training acc over epoch: 0.7431\n",
      "---- Validation ----\n",
      "Validation loss: 41.3097\n",
      "Validation acc: 0.6784\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 272.8006, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 276.9128, Accuracy: 0.6101\n",
      "Training loss (for one batch) at step 20: 247.0847, Accuracy: 0.6611\n",
      "Training loss (for one batch) at step 30: 232.7641, Accuracy: 0.7092\n",
      "Training loss (for one batch) at step 40: 231.3754, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 50: 233.0462, Accuracy: 0.7609\n",
      "Training loss (for one batch) at step 60: 259.8182, Accuracy: 0.7716\n",
      "Training loss (for one batch) at step 70: 262.1524, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 80: 279.9560, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 90: 236.6187, Accuracy: 0.7380\n",
      "Training loss (for one batch) at step 100: 252.1437, Accuracy: 0.7439\n",
      "Training loss (for one batch) at step 110: 242.1704, Accuracy: 0.7442\n",
      "---- Training ----\n",
      "Training loss: 110.1518\n",
      "Training acc over epoch: 0.7423\n",
      "---- Validation ----\n",
      "Validation loss: 54.9546\n",
      "Validation acc: 0.6733\n",
      "Time taken: 18.11s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 271.5544, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 292.3783, Accuracy: 0.6243\n",
      "Training loss (for one batch) at step 20: 257.1517, Accuracy: 0.6730\n",
      "Training loss (for one batch) at step 30: 239.5394, Accuracy: 0.7182\n",
      "Training loss (for one batch) at step 40: 229.0091, Accuracy: 0.7483\n",
      "Training loss (for one batch) at step 50: 233.9058, Accuracy: 0.7704\n",
      "Training loss (for one batch) at step 60: 253.6824, Accuracy: 0.7751\n",
      "Training loss (for one batch) at step 70: 268.6964, Accuracy: 0.7623\n",
      "Training loss (for one batch) at step 80: 250.3219, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 90: 235.0247, Accuracy: 0.7404\n",
      "Training loss (for one batch) at step 100: 257.1340, Accuracy: 0.7458\n",
      "Training loss (for one batch) at step 110: 261.9589, Accuracy: 0.7468\n",
      "---- Training ----\n",
      "Training loss: 79.9250\n",
      "Training acc over epoch: 0.7462\n",
      "---- Validation ----\n",
      "Validation loss: 45.3601\n",
      "Validation acc: 0.6795\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 286.4899, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 278.8707, Accuracy: 0.6222\n",
      "Training loss (for one batch) at step 20: 243.6426, Accuracy: 0.6637\n",
      "Training loss (for one batch) at step 30: 249.6911, Accuracy: 0.7114\n",
      "Training loss (for one batch) at step 40: 223.6198, Accuracy: 0.7456\n",
      "Training loss (for one batch) at step 50: 235.6579, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 60: 244.5045, Accuracy: 0.7751\n",
      "Training loss (for one batch) at step 70: 249.9425, Accuracy: 0.7603\n",
      "Training loss (for one batch) at step 80: 249.7865, Accuracy: 0.7416\n",
      "Training loss (for one batch) at step 90: 263.9348, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 100: 265.0510, Accuracy: 0.7448\n",
      "Training loss (for one batch) at step 110: 244.9546, Accuracy: 0.7468\n",
      "---- Training ----\n",
      "Training loss: 94.3776\n",
      "Training acc over epoch: 0.7443\n",
      "---- Validation ----\n",
      "Validation loss: 53.6147\n",
      "Validation acc: 0.6757\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 287.8828, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 256.7720, Accuracy: 0.5973\n",
      "Training loss (for one batch) at step 20: 264.6618, Accuracy: 0.6696\n",
      "Training loss (for one batch) at step 30: 237.2806, Accuracy: 0.7167\n",
      "Training loss (for one batch) at step 40: 231.5323, Accuracy: 0.7466\n",
      "Training loss (for one batch) at step 50: 233.1923, Accuracy: 0.7667\n",
      "Training loss (for one batch) at step 60: 251.1807, Accuracy: 0.7765\n",
      "Training loss (for one batch) at step 70: 273.7502, Accuracy: 0.7642\n",
      "Training loss (for one batch) at step 80: 279.6855, Accuracy: 0.7419\n",
      "Training loss (for one batch) at step 90: 246.3965, Accuracy: 0.7400\n",
      "Training loss (for one batch) at step 100: 266.8534, Accuracy: 0.7465\n",
      "Training loss (for one batch) at step 110: 251.4015, Accuracy: 0.7468\n",
      "---- Training ----\n",
      "Training loss: 88.1411\n",
      "Training acc over epoch: 0.7455\n",
      "---- Validation ----\n",
      "Validation loss: 48.9120\n",
      "Validation acc: 0.6814\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 279.7262, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 279.8576, Accuracy: 0.6158\n",
      "Training loss (for one batch) at step 20: 229.9559, Accuracy: 0.6670\n",
      "Training loss (for one batch) at step 30: 233.7691, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 40: 248.4691, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 50: 231.1733, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 60: 248.9542, Accuracy: 0.7737\n",
      "Training loss (for one batch) at step 70: 265.4145, Accuracy: 0.7600\n",
      "Training loss (for one batch) at step 80: 255.0312, Accuracy: 0.7407\n",
      "Training loss (for one batch) at step 90: 247.7334, Accuracy: 0.7394\n",
      "Training loss (for one batch) at step 100: 244.1617, Accuracy: 0.7440\n",
      "Training loss (for one batch) at step 110: 259.8083, Accuracy: 0.7463\n",
      "---- Training ----\n",
      "Training loss: 72.4264\n",
      "Training acc over epoch: 0.7445\n",
      "---- Validation ----\n",
      "Validation loss: 50.2635\n",
      "Validation acc: 0.6765\n",
      "Time taken: 18.25s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 269.1333, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 286.8114, Accuracy: 0.6037\n",
      "Training loss (for one batch) at step 20: 252.9799, Accuracy: 0.6603\n",
      "Training loss (for one batch) at step 30: 255.8402, Accuracy: 0.7119\n",
      "Training loss (for one batch) at step 40: 228.5863, Accuracy: 0.7426\n",
      "Training loss (for one batch) at step 50: 240.2989, Accuracy: 0.7592\n",
      "Training loss (for one batch) at step 60: 253.4467, Accuracy: 0.7718\n",
      "Training loss (for one batch) at step 70: 257.6180, Accuracy: 0.7575\n",
      "Training loss (for one batch) at step 80: 264.7122, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 90: 247.8716, Accuracy: 0.7359\n",
      "Training loss (for one batch) at step 100: 228.9073, Accuracy: 0.7415\n",
      "Training loss (for one batch) at step 110: 249.5563, Accuracy: 0.7431\n",
      "---- Training ----\n",
      "Training loss: 80.4156\n",
      "Training acc over epoch: 0.7422\n",
      "---- Validation ----\n",
      "Validation loss: 48.2338\n",
      "Validation acc: 0.6873\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 269.1871, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 275.0594, Accuracy: 0.6193\n",
      "Training loss (for one batch) at step 20: 224.8504, Accuracy: 0.6704\n",
      "Training loss (for one batch) at step 30: 244.7278, Accuracy: 0.7119\n",
      "Training loss (for one batch) at step 40: 245.3834, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 50: 228.8722, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 60: 249.4995, Accuracy: 0.7720\n",
      "Training loss (for one batch) at step 70: 253.4500, Accuracy: 0.7576\n",
      "Training loss (for one batch) at step 80: 253.8632, Accuracy: 0.7416\n",
      "Training loss (for one batch) at step 90: 238.7113, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 100: 240.6584, Accuracy: 0.7436\n",
      "Training loss (for one batch) at step 110: 251.9498, Accuracy: 0.7451\n",
      "---- Training ----\n",
      "Training loss: 90.2980\n",
      "Training acc over epoch: 0.7438\n",
      "---- Validation ----\n",
      "Validation loss: 44.3473\n",
      "Validation acc: 0.6865\n",
      "Time taken: 20.20s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 296.1460, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 259.0347, Accuracy: 0.6072\n",
      "Training loss (for one batch) at step 20: 253.0844, Accuracy: 0.6693\n",
      "Training loss (for one batch) at step 30: 232.7128, Accuracy: 0.7147\n",
      "Training loss (for one batch) at step 40: 226.8877, Accuracy: 0.7439\n",
      "Training loss (for one batch) at step 50: 239.0051, Accuracy: 0.7642\n",
      "Training loss (for one batch) at step 60: 235.4836, Accuracy: 0.7729\n",
      "Training loss (for one batch) at step 70: 240.7518, Accuracy: 0.7591\n",
      "Training loss (for one batch) at step 80: 268.0852, Accuracy: 0.7420\n",
      "Training loss (for one batch) at step 90: 256.9172, Accuracy: 0.7366\n",
      "Training loss (for one batch) at step 100: 234.6254, Accuracy: 0.7426\n",
      "Training loss (for one batch) at step 110: 238.8073, Accuracy: 0.7443\n",
      "---- Training ----\n",
      "Training loss: 105.1966\n",
      "Training acc over epoch: 0.7426\n",
      "---- Validation ----\n",
      "Validation loss: 49.1908\n",
      "Validation acc: 0.6811\n",
      "Time taken: 18.44s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 268.6532, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 246.1545, Accuracy: 0.6115\n",
      "Training loss (for one batch) at step 20: 247.7192, Accuracy: 0.6611\n",
      "Training loss (for one batch) at step 30: 237.1282, Accuracy: 0.7056\n",
      "Training loss (for one batch) at step 40: 233.7714, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 50: 228.8410, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 60: 258.6327, Accuracy: 0.7691\n",
      "Training loss (for one batch) at step 70: 284.7955, Accuracy: 0.7540\n",
      "Training loss (for one batch) at step 80: 252.7402, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 90: 228.5652, Accuracy: 0.7369\n",
      "Training loss (for one batch) at step 100: 249.0603, Accuracy: 0.7420\n",
      "Training loss (for one batch) at step 110: 255.3240, Accuracy: 0.7423\n",
      "---- Training ----\n",
      "Training loss: 78.6513\n",
      "Training acc over epoch: 0.7415\n",
      "---- Validation ----\n",
      "Validation loss: 112.1748\n",
      "Validation acc: 0.6886\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 250.5755, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 261.7940, Accuracy: 0.6044\n",
      "Training loss (for one batch) at step 20: 220.3055, Accuracy: 0.6589\n",
      "Training loss (for one batch) at step 30: 226.6090, Accuracy: 0.7137\n",
      "Training loss (for one batch) at step 40: 240.4626, Accuracy: 0.7431\n",
      "Training loss (for one batch) at step 50: 237.6193, Accuracy: 0.7642\n",
      "Training loss (for one batch) at step 60: 237.0876, Accuracy: 0.7720\n",
      "Training loss (for one batch) at step 70: 253.0008, Accuracy: 0.7567\n",
      "Training loss (for one batch) at step 80: 250.6114, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 90: 257.0774, Accuracy: 0.7390\n",
      "Training loss (for one batch) at step 100: 249.3900, Accuracy: 0.7440\n",
      "Training loss (for one batch) at step 110: 243.9520, Accuracy: 0.7442\n",
      "---- Training ----\n",
      "Training loss: 86.6003\n",
      "Training acc over epoch: 0.7432\n",
      "---- Validation ----\n",
      "Validation loss: 52.7121\n",
      "Validation acc: 0.6854\n",
      "Time taken: 18.08s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 267.1815, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 262.3911, Accuracy: 0.6044\n",
      "Training loss (for one batch) at step 20: 231.4077, Accuracy: 0.6600\n",
      "Training loss (for one batch) at step 30: 239.1023, Accuracy: 0.7130\n",
      "Training loss (for one batch) at step 40: 229.1998, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 50: 250.1769, Accuracy: 0.7616\n",
      "Training loss (for one batch) at step 60: 238.1897, Accuracy: 0.7738\n",
      "Training loss (for one batch) at step 70: 262.2075, Accuracy: 0.7600\n",
      "Training loss (for one batch) at step 80: 268.4668, Accuracy: 0.7413\n",
      "Training loss (for one batch) at step 90: 234.5364, Accuracy: 0.7399\n",
      "Training loss (for one batch) at step 100: 233.0503, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 110: 248.0589, Accuracy: 0.7478\n",
      "---- Training ----\n",
      "Training loss: 96.2776\n",
      "Training acc over epoch: 0.7446\n",
      "---- Validation ----\n",
      "Validation loss: 73.0573\n",
      "Validation acc: 0.6776\n",
      "Time taken: 18.23s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 290.1025, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 259.0785, Accuracy: 0.5994\n",
      "Training loss (for one batch) at step 20: 245.2374, Accuracy: 0.6544\n",
      "Training loss (for one batch) at step 30: 240.3143, Accuracy: 0.7059\n",
      "Training loss (for one batch) at step 40: 227.8122, Accuracy: 0.7407\n",
      "Training loss (for one batch) at step 50: 229.1768, Accuracy: 0.7629\n",
      "Training loss (for one batch) at step 60: 254.3234, Accuracy: 0.7714\n",
      "Training loss (for one batch) at step 70: 249.1071, Accuracy: 0.7572\n",
      "Training loss (for one batch) at step 80: 259.0927, Accuracy: 0.7390\n",
      "Training loss (for one batch) at step 90: 237.9084, Accuracy: 0.7374\n",
      "Training loss (for one batch) at step 100: 261.7306, Accuracy: 0.7413\n",
      "Training loss (for one batch) at step 110: 275.1807, Accuracy: 0.7440\n",
      "---- Training ----\n",
      "Training loss: 83.8095\n",
      "Training acc over epoch: 0.7421\n",
      "---- Validation ----\n",
      "Validation loss: 45.6212\n",
      "Validation acc: 0.6601\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 263.9478, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 254.4526, Accuracy: 0.6087\n",
      "Training loss (for one batch) at step 20: 230.7762, Accuracy: 0.6693\n",
      "Training loss (for one batch) at step 30: 232.2607, Accuracy: 0.7130\n",
      "Training loss (for one batch) at step 40: 238.5723, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 50: 224.8005, Accuracy: 0.7638\n",
      "Training loss (for one batch) at step 60: 230.8952, Accuracy: 0.7733\n",
      "Training loss (for one batch) at step 70: 267.2583, Accuracy: 0.7596\n",
      "Training loss (for one batch) at step 80: 254.4332, Accuracy: 0.7400\n",
      "Training loss (for one batch) at step 90: 239.7879, Accuracy: 0.7365\n",
      "Training loss (for one batch) at step 100: 254.7668, Accuracy: 0.7430\n",
      "Training loss (for one batch) at step 110: 259.0052, Accuracy: 0.7438\n",
      "---- Training ----\n",
      "Training loss: 80.0617\n",
      "Training acc over epoch: 0.7425\n",
      "---- Validation ----\n",
      "Validation loss: 68.1071\n",
      "Validation acc: 0.6859\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 249.7191, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 278.3663, Accuracy: 0.6158\n",
      "Training loss (for one batch) at step 20: 244.8692, Accuracy: 0.6659\n",
      "Training loss (for one batch) at step 30: 234.2206, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 40: 241.0085, Accuracy: 0.7449\n",
      "Training loss (for one batch) at step 50: 229.1083, Accuracy: 0.7642\n",
      "Training loss (for one batch) at step 60: 237.9778, Accuracy: 0.7705\n",
      "Training loss (for one batch) at step 70: 268.3426, Accuracy: 0.7566\n",
      "Training loss (for one batch) at step 80: 258.6004, Accuracy: 0.7383\n",
      "Training loss (for one batch) at step 90: 228.1830, Accuracy: 0.7379\n",
      "Training loss (for one batch) at step 100: 233.3693, Accuracy: 0.7426\n",
      "Training loss (for one batch) at step 110: 244.3585, Accuracy: 0.7451\n",
      "---- Training ----\n",
      "Training loss: 89.6114\n",
      "Training acc over epoch: 0.7427\n",
      "---- Validation ----\n",
      "Validation loss: 59.9735\n",
      "Validation acc: 0.6865\n",
      "Time taken: 18.05s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 263.1379, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 270.4444, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 20: 238.4599, Accuracy: 0.6637\n",
      "Training loss (for one batch) at step 30: 231.5435, Accuracy: 0.7130\n",
      "Training loss (for one batch) at step 40: 225.3092, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 50: 227.9550, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 60: 231.5435, Accuracy: 0.7723\n",
      "Training loss (for one batch) at step 70: 233.4754, Accuracy: 0.7568\n",
      "Training loss (for one batch) at step 80: 259.2787, Accuracy: 0.7392\n",
      "Training loss (for one batch) at step 90: 235.9420, Accuracy: 0.7348\n",
      "Training loss (for one batch) at step 100: 236.7861, Accuracy: 0.7411\n",
      "Training loss (for one batch) at step 110: 236.2130, Accuracy: 0.7439\n",
      "---- Training ----\n",
      "Training loss: 85.5050\n",
      "Training acc over epoch: 0.7415\n",
      "---- Validation ----\n",
      "Validation loss: 54.0661\n",
      "Validation acc: 0.6940\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 273.3331, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 259.0810, Accuracy: 0.5994\n",
      "Training loss (for one batch) at step 20: 251.6436, Accuracy: 0.6592\n",
      "Training loss (for one batch) at step 30: 223.6059, Accuracy: 0.7089\n",
      "Training loss (for one batch) at step 40: 228.1005, Accuracy: 0.7401\n",
      "Training loss (for one batch) at step 50: 227.3928, Accuracy: 0.7601\n",
      "Training loss (for one batch) at step 60: 248.0456, Accuracy: 0.7704\n",
      "Training loss (for one batch) at step 70: 247.0108, Accuracy: 0.7555\n",
      "Training loss (for one batch) at step 80: 270.3450, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 90: 228.1080, Accuracy: 0.7340\n",
      "Training loss (for one batch) at step 100: 234.3047, Accuracy: 0.7418\n",
      "Training loss (for one batch) at step 110: 240.5578, Accuracy: 0.7433\n",
      "---- Training ----\n",
      "Training loss: 75.6284\n",
      "Training acc over epoch: 0.7415\n",
      "---- Validation ----\n",
      "Validation loss: 52.3871\n",
      "Validation acc: 0.6545\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 260.8030, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 255.1100, Accuracy: 0.6030\n",
      "Training loss (for one batch) at step 20: 228.1685, Accuracy: 0.6629\n",
      "Training loss (for one batch) at step 30: 218.4435, Accuracy: 0.7099\n",
      "Training loss (for one batch) at step 40: 231.6918, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 50: 227.6339, Accuracy: 0.7618\n",
      "Training loss (for one batch) at step 60: 248.2404, Accuracy: 0.7695\n",
      "Training loss (for one batch) at step 70: 253.2202, Accuracy: 0.7565\n",
      "Training loss (for one batch) at step 80: 235.5080, Accuracy: 0.7401\n",
      "Training loss (for one batch) at step 90: 245.1626, Accuracy: 0.7357\n",
      "Training loss (for one batch) at step 100: 243.0173, Accuracy: 0.7416\n",
      "Training loss (for one batch) at step 110: 240.4480, Accuracy: 0.7428\n",
      "---- Training ----\n",
      "Training loss: 73.7940\n",
      "Training acc over epoch: 0.7410\n",
      "---- Validation ----\n",
      "Validation loss: 38.8824\n",
      "Validation acc: 0.6830\n",
      "Time taken: 18.13s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 266.5525, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 260.7191, Accuracy: 0.6108\n",
      "Training loss (for one batch) at step 20: 232.8301, Accuracy: 0.6629\n",
      "Training loss (for one batch) at step 30: 234.9509, Accuracy: 0.7142\n",
      "Training loss (for one batch) at step 40: 220.4551, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 50: 227.6967, Accuracy: 0.7615\n",
      "Training loss (for one batch) at step 60: 240.3115, Accuracy: 0.7718\n",
      "Training loss (for one batch) at step 70: 233.7007, Accuracy: 0.7574\n",
      "Training loss (for one batch) at step 80: 258.5631, Accuracy: 0.7394\n",
      "Training loss (for one batch) at step 90: 228.6764, Accuracy: 0.7346\n",
      "Training loss (for one batch) at step 100: 244.5774, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 110: 253.6930, Accuracy: 0.7439\n",
      "---- Training ----\n",
      "Training loss: 75.6016\n",
      "Training acc over epoch: 0.7425\n",
      "---- Validation ----\n",
      "Validation loss: 91.0909\n",
      "Validation acc: 0.6835\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 278.6203, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 242.3911, Accuracy: 0.5952\n",
      "Training loss (for one batch) at step 20: 236.6766, Accuracy: 0.6536\n",
      "Training loss (for one batch) at step 30: 227.3900, Accuracy: 0.7049\n",
      "Training loss (for one batch) at step 40: 237.2906, Accuracy: 0.7395\n",
      "Training loss (for one batch) at step 50: 235.0031, Accuracy: 0.7592\n",
      "Training loss (for one batch) at step 60: 234.3109, Accuracy: 0.7710\n",
      "Training loss (for one batch) at step 70: 245.2411, Accuracy: 0.7577\n",
      "Training loss (for one batch) at step 80: 264.4943, Accuracy: 0.7393\n",
      "Training loss (for one batch) at step 90: 249.3548, Accuracy: 0.7357\n",
      "Training loss (for one batch) at step 100: 244.9277, Accuracy: 0.7423\n",
      "Training loss (for one batch) at step 110: 235.2316, Accuracy: 0.7434\n",
      "---- Training ----\n",
      "Training loss: 75.9586\n",
      "Training acc over epoch: 0.7418\n",
      "---- Validation ----\n",
      "Validation loss: 41.7981\n",
      "Validation acc: 0.6891\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 265.5269, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 259.8123, Accuracy: 0.6087\n",
      "Training loss (for one batch) at step 20: 234.7950, Accuracy: 0.6611\n",
      "Training loss (for one batch) at step 30: 233.9966, Accuracy: 0.7107\n",
      "Training loss (for one batch) at step 40: 230.9421, Accuracy: 0.7418\n",
      "Training loss (for one batch) at step 50: 232.4988, Accuracy: 0.7629\n",
      "Training loss (for one batch) at step 60: 220.1977, Accuracy: 0.7714\n",
      "Training loss (for one batch) at step 70: 245.2220, Accuracy: 0.7575\n",
      "Training loss (for one batch) at step 80: 260.2803, Accuracy: 0.7381\n",
      "Training loss (for one batch) at step 90: 233.3387, Accuracy: 0.7369\n",
      "Training loss (for one batch) at step 100: 215.1948, Accuracy: 0.7431\n",
      "Training loss (for one batch) at step 110: 240.8505, Accuracy: 0.7423\n",
      "---- Training ----\n",
      "Training loss: 86.8925\n",
      "Training acc over epoch: 0.7411\n",
      "---- Validation ----\n",
      "Validation loss: 61.2408\n",
      "Validation acc: 0.6787\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 260.5794, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 246.1535, Accuracy: 0.6087\n",
      "Training loss (for one batch) at step 20: 223.9620, Accuracy: 0.6693\n",
      "Training loss (for one batch) at step 30: 220.6835, Accuracy: 0.7132\n",
      "Training loss (for one batch) at step 40: 227.8748, Accuracy: 0.7445\n",
      "Training loss (for one batch) at step 50: 239.9971, Accuracy: 0.7644\n",
      "Training loss (for one batch) at step 60: 248.8307, Accuracy: 0.7732\n",
      "Training loss (for one batch) at step 70: 270.2602, Accuracy: 0.7599\n",
      "Training loss (for one batch) at step 80: 240.5300, Accuracy: 0.7417\n",
      "Training loss (for one batch) at step 90: 259.9839, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 100: 243.4430, Accuracy: 0.7428\n",
      "Training loss (for one batch) at step 110: 241.2263, Accuracy: 0.7450\n",
      "---- Training ----\n",
      "Training loss: 91.3722\n",
      "Training acc over epoch: 0.7431\n",
      "---- Validation ----\n",
      "Validation loss: 46.3048\n",
      "Validation acc: 0.6854\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 251.5540, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 268.5636, Accuracy: 0.6023\n",
      "Training loss (for one batch) at step 20: 225.3466, Accuracy: 0.6615\n",
      "Training loss (for one batch) at step 30: 230.5500, Accuracy: 0.7137\n",
      "Training loss (for one batch) at step 40: 220.8788, Accuracy: 0.7439\n",
      "Training loss (for one batch) at step 50: 227.8334, Accuracy: 0.7652\n",
      "Training loss (for one batch) at step 60: 254.4629, Accuracy: 0.7742\n",
      "Training loss (for one batch) at step 70: 254.6676, Accuracy: 0.7599\n",
      "Training loss (for one batch) at step 80: 232.6224, Accuracy: 0.7404\n",
      "Training loss (for one batch) at step 90: 247.7402, Accuracy: 0.7379\n",
      "Training loss (for one batch) at step 100: 243.7271, Accuracy: 0.7434\n",
      "Training loss (for one batch) at step 110: 242.9292, Accuracy: 0.7459\n",
      "---- Training ----\n",
      "Training loss: 99.5250\n",
      "Training acc over epoch: 0.7438\n",
      "---- Validation ----\n",
      "Validation loss: 52.5974\n",
      "Validation acc: 0.6636\n",
      "Time taken: 18.20s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 262.8383, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 263.7244, Accuracy: 0.5952\n",
      "Training loss (for one batch) at step 20: 243.8817, Accuracy: 0.6611\n",
      "Training loss (for one batch) at step 30: 226.3029, Accuracy: 0.7104\n",
      "Training loss (for one batch) at step 40: 207.9699, Accuracy: 0.7418\n",
      "Training loss (for one batch) at step 50: 232.7957, Accuracy: 0.7601\n",
      "Training loss (for one batch) at step 60: 257.2829, Accuracy: 0.7697\n",
      "Training loss (for one batch) at step 70: 283.8665, Accuracy: 0.7553\n",
      "Training loss (for one batch) at step 80: 255.6261, Accuracy: 0.7394\n",
      "Training loss (for one batch) at step 90: 239.4300, Accuracy: 0.7345\n",
      "Training loss (for one batch) at step 100: 241.1869, Accuracy: 0.7419\n",
      "Training loss (for one batch) at step 110: 250.7221, Accuracy: 0.7433\n",
      "---- Training ----\n",
      "Training loss: 83.3501\n",
      "Training acc over epoch: 0.7415\n",
      "---- Validation ----\n",
      "Validation loss: 65.6805\n",
      "Validation acc: 0.6905\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 255.3219, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 256.3280, Accuracy: 0.6051\n",
      "Training loss (for one batch) at step 20: 236.2064, Accuracy: 0.6574\n",
      "Training loss (for one batch) at step 30: 234.1516, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 40: 227.0279, Accuracy: 0.7449\n",
      "Training loss (for one batch) at step 50: 234.2047, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 60: 237.5468, Accuracy: 0.7748\n",
      "Training loss (for one batch) at step 70: 239.5010, Accuracy: 0.7576\n",
      "Training loss (for one batch) at step 80: 251.2280, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 90: 238.3073, Accuracy: 0.7379\n",
      "Training loss (for one batch) at step 100: 242.0683, Accuracy: 0.7432\n",
      "Training loss (for one batch) at step 110: 235.2926, Accuracy: 0.7463\n",
      "---- Training ----\n",
      "Training loss: 104.0387\n",
      "Training acc over epoch: 0.7429\n",
      "---- Validation ----\n",
      "Validation loss: 54.3950\n",
      "Validation acc: 0.6736\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 262.2149, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 292.3589, Accuracy: 0.6030\n",
      "Training loss (for one batch) at step 20: 237.4511, Accuracy: 0.6674\n",
      "Training loss (for one batch) at step 30: 217.8085, Accuracy: 0.7127\n",
      "Training loss (for one batch) at step 40: 244.4630, Accuracy: 0.7424\n",
      "Training loss (for one batch) at step 50: 217.9019, Accuracy: 0.7612\n",
      "Training loss (for one batch) at step 60: 235.4724, Accuracy: 0.7711\n",
      "Training loss (for one batch) at step 70: 249.8898, Accuracy: 0.7584\n",
      "Training loss (for one batch) at step 80: 259.6086, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 90: 235.1273, Accuracy: 0.7355\n",
      "Training loss (for one batch) at step 100: 234.1808, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 110: 256.3266, Accuracy: 0.7432\n",
      "---- Training ----\n",
      "Training loss: 78.5982\n",
      "Training acc over epoch: 0.7417\n",
      "---- Validation ----\n",
      "Validation loss: 48.3395\n",
      "Validation acc: 0.6795\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 245.7789, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 242.8088, Accuracy: 0.6087\n",
      "Training loss (for one batch) at step 20: 217.5181, Accuracy: 0.6611\n",
      "Training loss (for one batch) at step 30: 224.9778, Accuracy: 0.7056\n",
      "Training loss (for one batch) at step 40: 261.8863, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 50: 231.2257, Accuracy: 0.7601\n",
      "Training loss (for one batch) at step 60: 225.3240, Accuracy: 0.7706\n",
      "Training loss (for one batch) at step 70: 243.2290, Accuracy: 0.7567\n",
      "Training loss (for one batch) at step 80: 233.5953, Accuracy: 0.7373\n",
      "Training loss (for one batch) at step 90: 232.1151, Accuracy: 0.7346\n",
      "Training loss (for one batch) at step 100: 234.1140, Accuracy: 0.7407\n",
      "Training loss (for one batch) at step 110: 237.8412, Accuracy: 0.7422\n",
      "---- Training ----\n",
      "Training loss: 72.2707\n",
      "Training acc over epoch: 0.7409\n",
      "---- Validation ----\n",
      "Validation loss: 48.9840\n",
      "Validation acc: 0.6916\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 266.2768, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 260.7334, Accuracy: 0.5987\n",
      "Training loss (for one batch) at step 20: 223.9189, Accuracy: 0.6592\n",
      "Training loss (for one batch) at step 30: 216.1419, Accuracy: 0.7072\n",
      "Training loss (for one batch) at step 40: 219.2009, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 50: 222.2840, Accuracy: 0.7593\n",
      "Training loss (for one batch) at step 60: 228.4113, Accuracy: 0.7696\n",
      "Training loss (for one batch) at step 70: 245.7399, Accuracy: 0.7563\n",
      "Training loss (for one batch) at step 80: 254.8276, Accuracy: 0.7353\n",
      "Training loss (for one batch) at step 90: 241.9951, Accuracy: 0.7342\n",
      "Training loss (for one batch) at step 100: 214.2581, Accuracy: 0.7413\n",
      "Training loss (for one batch) at step 110: 237.8050, Accuracy: 0.7423\n",
      "---- Training ----\n",
      "Training loss: 78.5413\n",
      "Training acc over epoch: 0.7410\n",
      "---- Validation ----\n",
      "Validation loss: 51.0706\n",
      "Validation acc: 0.6827\n",
      "Time taken: 17.84s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 239.0377, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 249.2631, Accuracy: 0.6158\n",
      "Training loss (for one batch) at step 20: 233.0209, Accuracy: 0.6555\n",
      "Training loss (for one batch) at step 30: 241.4537, Accuracy: 0.7029\n",
      "Training loss (for one batch) at step 40: 234.5777, Accuracy: 0.7399\n",
      "Training loss (for one batch) at step 50: 239.2634, Accuracy: 0.7593\n",
      "Training loss (for one batch) at step 60: 232.3181, Accuracy: 0.7699\n",
      "Training loss (for one batch) at step 70: 240.8575, Accuracy: 0.7548\n",
      "Training loss (for one batch) at step 80: 242.9327, Accuracy: 0.7375\n",
      "Training loss (for one batch) at step 90: 228.4366, Accuracy: 0.7359\n",
      "Training loss (for one batch) at step 100: 234.7820, Accuracy: 0.7413\n",
      "Training loss (for one batch) at step 110: 239.0892, Accuracy: 0.7432\n",
      "---- Training ----\n",
      "Training loss: 70.8236\n",
      "Training acc over epoch: 0.7420\n",
      "---- Validation ----\n",
      "Validation loss: 48.7510\n",
      "Validation acc: 0.6814\n",
      "Time taken: 20.12s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 269.1319, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 262.3798, Accuracy: 0.6129\n",
      "Training loss (for one batch) at step 20: 226.6239, Accuracy: 0.6652\n",
      "Training loss (for one batch) at step 30: 216.8445, Accuracy: 0.7104\n",
      "Training loss (for one batch) at step 40: 235.2245, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 50: 228.3650, Accuracy: 0.7626\n",
      "Training loss (for one batch) at step 60: 241.8001, Accuracy: 0.7693\n",
      "Training loss (for one batch) at step 70: 254.2215, Accuracy: 0.7559\n",
      "Training loss (for one batch) at step 80: 234.9108, Accuracy: 0.7375\n",
      "Training loss (for one batch) at step 90: 232.5880, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 100: 230.1790, Accuracy: 0.7406\n",
      "Training loss (for one batch) at step 110: 250.1085, Accuracy: 0.7411\n",
      "---- Training ----\n",
      "Training loss: 78.8573\n",
      "Training acc over epoch: 0.7405\n",
      "---- Validation ----\n",
      "Validation loss: 53.7137\n",
      "Validation acc: 0.6937\n",
      "Time taken: 17.87s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 249.5445, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 242.6766, Accuracy: 0.5987\n",
      "Training loss (for one batch) at step 20: 247.1797, Accuracy: 0.6592\n",
      "Training loss (for one batch) at step 30: 240.4229, Accuracy: 0.7089\n",
      "Training loss (for one batch) at step 40: 217.3299, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 50: 216.0099, Accuracy: 0.7600\n",
      "Training loss (for one batch) at step 60: 227.6191, Accuracy: 0.7688\n",
      "Training loss (for one batch) at step 70: 255.8012, Accuracy: 0.7562\n",
      "Training loss (for one batch) at step 80: 242.6961, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 90: 232.5195, Accuracy: 0.7348\n",
      "Training loss (for one batch) at step 100: 241.5872, Accuracy: 0.7427\n",
      "Training loss (for one batch) at step 110: 233.7096, Accuracy: 0.7418\n",
      "---- Training ----\n",
      "Training loss: 91.5792\n",
      "Training acc over epoch: 0.7417\n",
      "---- Validation ----\n",
      "Validation loss: 47.9912\n",
      "Validation acc: 0.6862\n",
      "Time taken: 17.85s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 272.0071, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 229.8293, Accuracy: 0.6143\n",
      "Training loss (for one batch) at step 20: 239.3830, Accuracy: 0.6581\n",
      "Training loss (for one batch) at step 30: 233.2800, Accuracy: 0.7082\n",
      "Training loss (for one batch) at step 40: 215.5238, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 50: 231.4359, Accuracy: 0.7610\n",
      "Training loss (for one batch) at step 60: 235.8184, Accuracy: 0.7718\n",
      "Training loss (for one batch) at step 70: 236.0569, Accuracy: 0.7570\n",
      "Training loss (for one batch) at step 80: 251.4966, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 90: 232.6440, Accuracy: 0.7372\n",
      "Training loss (for one batch) at step 100: 227.5243, Accuracy: 0.7426\n",
      "Training loss (for one batch) at step 110: 226.4935, Accuracy: 0.7441\n",
      "---- Training ----\n",
      "Training loss: 85.3322\n",
      "Training acc over epoch: 0.7420\n",
      "---- Validation ----\n",
      "Validation loss: 60.7792\n",
      "Validation acc: 0.6771\n",
      "Time taken: 18.10s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 256.4183, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 228.1420, Accuracy: 0.5980\n",
      "Training loss (for one batch) at step 20: 243.4727, Accuracy: 0.6574\n",
      "Training loss (for one batch) at step 30: 219.8912, Accuracy: 0.7067\n",
      "Training loss (for one batch) at step 40: 218.4897, Accuracy: 0.7378\n",
      "Training loss (for one batch) at step 50: 215.9677, Accuracy: 0.7586\n",
      "Training loss (for one batch) at step 60: 220.8553, Accuracy: 0.7705\n",
      "Training loss (for one batch) at step 70: 248.7409, Accuracy: 0.7556\n",
      "Training loss (for one batch) at step 80: 235.3103, Accuracy: 0.7365\n",
      "Training loss (for one batch) at step 90: 233.6251, Accuracy: 0.7333\n",
      "Training loss (for one batch) at step 100: 228.2844, Accuracy: 0.7408\n",
      "Training loss (for one batch) at step 110: 247.9342, Accuracy: 0.7416\n",
      "---- Training ----\n",
      "Training loss: 76.0122\n",
      "Training acc over epoch: 0.7406\n",
      "---- Validation ----\n",
      "Validation loss: 59.6562\n",
      "Validation acc: 0.6953\n",
      "Time taken: 17.82s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 241.3866, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 223.3176, Accuracy: 0.6030\n",
      "Training loss (for one batch) at step 20: 240.0255, Accuracy: 0.6603\n",
      "Training loss (for one batch) at step 30: 239.3138, Accuracy: 0.7114\n",
      "Training loss (for one batch) at step 40: 214.2461, Accuracy: 0.7401\n",
      "Training loss (for one batch) at step 50: 224.3584, Accuracy: 0.7626\n",
      "Training loss (for one batch) at step 60: 225.1547, Accuracy: 0.7709\n",
      "Training loss (for one batch) at step 70: 237.0241, Accuracy: 0.7592\n",
      "Training loss (for one batch) at step 80: 248.9405, Accuracy: 0.7383\n",
      "Training loss (for one batch) at step 90: 209.0690, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 100: 224.7675, Accuracy: 0.7426\n",
      "Training loss (for one batch) at step 110: 236.9560, Accuracy: 0.7426\n",
      "---- Training ----\n",
      "Training loss: 76.8224\n",
      "Training acc over epoch: 0.7412\n",
      "---- Validation ----\n",
      "Validation loss: 36.3356\n",
      "Validation acc: 0.6918\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 250.2928, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 259.4821, Accuracy: 0.6108\n",
      "Training loss (for one batch) at step 20: 217.0701, Accuracy: 0.6659\n",
      "Training loss (for one batch) at step 30: 231.2939, Accuracy: 0.7160\n",
      "Training loss (for one batch) at step 40: 228.3885, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 50: 210.1025, Accuracy: 0.7630\n",
      "Training loss (for one batch) at step 60: 216.6631, Accuracy: 0.7733\n",
      "Training loss (for one batch) at step 70: 248.4501, Accuracy: 0.7581\n",
      "Training loss (for one batch) at step 80: 253.1861, Accuracy: 0.7420\n",
      "Training loss (for one batch) at step 90: 223.6010, Accuracy: 0.7377\n",
      "Training loss (for one batch) at step 100: 231.4553, Accuracy: 0.7427\n",
      "Training loss (for one batch) at step 110: 231.9374, Accuracy: 0.7444\n",
      "---- Training ----\n",
      "Training loss: 74.2950\n",
      "Training acc over epoch: 0.7429\n",
      "---- Validation ----\n",
      "Validation loss: 46.9003\n",
      "Validation acc: 0.6805\n",
      "Time taken: 20.10s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 251.4793, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 244.4364, Accuracy: 0.5973\n",
      "Training loss (for one batch) at step 20: 245.1211, Accuracy: 0.6570\n",
      "Training loss (for one batch) at step 30: 212.4341, Accuracy: 0.7079\n",
      "Training loss (for one batch) at step 40: 224.1396, Accuracy: 0.7399\n",
      "Training loss (for one batch) at step 50: 221.8642, Accuracy: 0.7607\n",
      "Training loss (for one batch) at step 60: 240.7890, Accuracy: 0.7700\n",
      "Training loss (for one batch) at step 70: 256.0298, Accuracy: 0.7575\n",
      "Training loss (for one batch) at step 80: 235.1262, Accuracy: 0.7376\n",
      "Training loss (for one batch) at step 90: 228.8177, Accuracy: 0.7348\n",
      "Training loss (for one batch) at step 100: 248.0870, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 110: 234.2974, Accuracy: 0.7435\n",
      "---- Training ----\n",
      "Training loss: 72.9071\n",
      "Training acc over epoch: 0.7414\n",
      "---- Validation ----\n",
      "Validation loss: 70.4276\n",
      "Validation acc: 0.6776\n",
      "Time taken: 17.88s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 253.4752, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 238.4944, Accuracy: 0.6087\n",
      "Training loss (for one batch) at step 20: 224.0539, Accuracy: 0.6626\n",
      "Training loss (for one batch) at step 30: 223.9651, Accuracy: 0.7130\n",
      "Training loss (for one batch) at step 40: 233.2090, Accuracy: 0.7401\n",
      "Training loss (for one batch) at step 50: 210.2322, Accuracy: 0.7597\n",
      "Training loss (for one batch) at step 60: 226.1047, Accuracy: 0.7701\n",
      "Training loss (for one batch) at step 70: 245.6198, Accuracy: 0.7563\n",
      "Training loss (for one batch) at step 80: 237.5801, Accuracy: 0.7393\n",
      "Training loss (for one batch) at step 90: 216.8672, Accuracy: 0.7373\n",
      "Training loss (for one batch) at step 100: 222.0286, Accuracy: 0.7428\n",
      "Training loss (for one batch) at step 110: 232.7691, Accuracy: 0.7439\n",
      "---- Training ----\n",
      "Training loss: 80.6009\n",
      "Training acc over epoch: 0.7417\n",
      "---- Validation ----\n",
      "Validation loss: 64.2485\n",
      "Validation acc: 0.6822\n",
      "Time taken: 17.84s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 257.0521, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 241.7893, Accuracy: 0.5916\n",
      "Training loss (for one batch) at step 20: 218.0775, Accuracy: 0.6548\n",
      "Training loss (for one batch) at step 30: 234.6414, Accuracy: 0.7051\n",
      "Training loss (for one batch) at step 40: 220.5977, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 50: 221.4637, Accuracy: 0.7574\n",
      "Training loss (for one batch) at step 60: 226.5795, Accuracy: 0.7692\n",
      "Training loss (for one batch) at step 70: 234.7757, Accuracy: 0.7556\n",
      "Training loss (for one batch) at step 80: 235.5887, Accuracy: 0.7368\n",
      "Training loss (for one batch) at step 90: 229.3039, Accuracy: 0.7330\n",
      "Training loss (for one batch) at step 100: 231.8986, Accuracy: 0.7393\n",
      "Training loss (for one batch) at step 110: 226.5671, Accuracy: 0.7411\n",
      "---- Training ----\n",
      "Training loss: 71.5087\n",
      "Training acc over epoch: 0.7399\n",
      "---- Validation ----\n",
      "Validation loss: 40.6947\n",
      "Validation acc: 0.6720\n",
      "Time taken: 17.82s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 236.3524, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 242.0503, Accuracy: 0.5909\n",
      "Training loss (for one batch) at step 20: 222.0622, Accuracy: 0.6533\n",
      "Training loss (for one batch) at step 30: 213.9861, Accuracy: 0.7114\n",
      "Training loss (for one batch) at step 40: 267.0541, Accuracy: 0.7424\n",
      "Training loss (for one batch) at step 50: 226.7300, Accuracy: 0.7619\n",
      "Training loss (for one batch) at step 60: 226.4096, Accuracy: 0.7716\n",
      "Training loss (for one batch) at step 70: 243.6885, Accuracy: 0.7595\n",
      "Training loss (for one batch) at step 80: 253.2790, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 90: 223.6474, Accuracy: 0.7368\n",
      "Training loss (for one batch) at step 100: 236.6618, Accuracy: 0.7424\n",
      "Training loss (for one batch) at step 110: 241.4580, Accuracy: 0.7434\n",
      "---- Training ----\n",
      "Training loss: 82.7155\n",
      "Training acc over epoch: 0.7411\n",
      "---- Validation ----\n",
      "Validation loss: 53.4749\n",
      "Validation acc: 0.6811\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 254.6520, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 248.8320, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 20: 226.6028, Accuracy: 0.6540\n",
      "Training loss (for one batch) at step 30: 220.1248, Accuracy: 0.7061\n",
      "Training loss (for one batch) at step 40: 242.0287, Accuracy: 0.7365\n",
      "Training loss (for one batch) at step 50: 246.3153, Accuracy: 0.7610\n",
      "Training loss (for one batch) at step 60: 229.6451, Accuracy: 0.7707\n",
      "Training loss (for one batch) at step 70: 215.9050, Accuracy: 0.7579\n",
      "Training loss (for one batch) at step 80: 239.0514, Accuracy: 0.7379\n",
      "Training loss (for one batch) at step 90: 218.6205, Accuracy: 0.7332\n",
      "Training loss (for one batch) at step 100: 218.0576, Accuracy: 0.7399\n",
      "Training loss (for one batch) at step 110: 243.0819, Accuracy: 0.7423\n",
      "---- Training ----\n",
      "Training loss: 78.5496\n",
      "Training acc over epoch: 0.7405\n",
      "---- Validation ----\n",
      "Validation loss: 62.7397\n",
      "Validation acc: 0.6822\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 270.6720, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 247.0096, Accuracy: 0.6037\n",
      "Training loss (for one batch) at step 20: 220.2370, Accuracy: 0.6696\n",
      "Training loss (for one batch) at step 30: 217.6232, Accuracy: 0.7157\n",
      "Training loss (for one batch) at step 40: 220.7322, Accuracy: 0.7462\n",
      "Training loss (for one batch) at step 50: 218.6429, Accuracy: 0.7623\n",
      "Training loss (for one batch) at step 60: 226.4648, Accuracy: 0.7720\n",
      "Training loss (for one batch) at step 70: 250.0716, Accuracy: 0.7586\n",
      "Training loss (for one batch) at step 80: 221.7675, Accuracy: 0.7415\n",
      "Training loss (for one batch) at step 90: 234.2945, Accuracy: 0.7376\n",
      "Training loss (for one batch) at step 100: 230.8929, Accuracy: 0.7430\n",
      "Training loss (for one batch) at step 110: 232.1929, Accuracy: 0.7448\n",
      "---- Training ----\n",
      "Training loss: 78.5924\n",
      "Training acc over epoch: 0.7424\n",
      "---- Validation ----\n",
      "Validation loss: 56.2698\n",
      "Validation acc: 0.6832\n",
      "Time taken: 17.88s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 257.5481, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 241.6817, Accuracy: 0.6051\n",
      "Training loss (for one batch) at step 20: 225.2948, Accuracy: 0.6633\n",
      "Training loss (for one batch) at step 30: 218.0437, Accuracy: 0.7094\n",
      "Training loss (for one batch) at step 40: 219.9794, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 50: 221.4783, Accuracy: 0.7590\n",
      "Training loss (for one batch) at step 60: 225.2181, Accuracy: 0.7702\n",
      "Training loss (for one batch) at step 70: 231.6244, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 80: 230.9804, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 90: 225.0786, Accuracy: 0.7348\n",
      "Training loss (for one batch) at step 100: 246.2325, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 110: 234.4615, Accuracy: 0.7432\n",
      "---- Training ----\n",
      "Training loss: 76.3629\n",
      "Training acc over epoch: 0.7415\n",
      "---- Validation ----\n",
      "Validation loss: 62.2543\n",
      "Validation acc: 0.6827\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 248.6278, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 244.1805, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 20: 227.1514, Accuracy: 0.6559\n",
      "Training loss (for one batch) at step 30: 233.6429, Accuracy: 0.7077\n",
      "Training loss (for one batch) at step 40: 217.3485, Accuracy: 0.7410\n",
      "Training loss (for one batch) at step 50: 242.7178, Accuracy: 0.7606\n",
      "Training loss (for one batch) at step 60: 226.8770, Accuracy: 0.7704\n",
      "Training loss (for one batch) at step 70: 250.9408, Accuracy: 0.7567\n",
      "Training loss (for one batch) at step 80: 244.2085, Accuracy: 0.7371\n",
      "Training loss (for one batch) at step 90: 248.0609, Accuracy: 0.7343\n",
      "Training loss (for one batch) at step 100: 223.3902, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 110: 226.9671, Accuracy: 0.7435\n",
      "---- Training ----\n",
      "Training loss: 77.5382\n",
      "Training acc over epoch: 0.7409\n",
      "---- Validation ----\n",
      "Validation loss: 37.7237\n",
      "Validation acc: 0.6741\n",
      "Time taken: 17.88s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 237.8315, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 250.8485, Accuracy: 0.5895\n",
      "Training loss (for one batch) at step 20: 268.1471, Accuracy: 0.6525\n",
      "Training loss (for one batch) at step 30: 224.5732, Accuracy: 0.7059\n",
      "Training loss (for one batch) at step 40: 221.7890, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 50: 233.6206, Accuracy: 0.7601\n",
      "Training loss (for one batch) at step 60: 227.7353, Accuracy: 0.7702\n",
      "Training loss (for one batch) at step 70: 244.1028, Accuracy: 0.7563\n",
      "Training loss (for one batch) at step 80: 248.2107, Accuracy: 0.7393\n",
      "Training loss (for one batch) at step 90: 231.8765, Accuracy: 0.7347\n",
      "Training loss (for one batch) at step 100: 224.0060, Accuracy: 0.7419\n",
      "Training loss (for one batch) at step 110: 231.1826, Accuracy: 0.7423\n",
      "---- Training ----\n",
      "Training loss: 79.9168\n",
      "Training acc over epoch: 0.7407\n",
      "---- Validation ----\n",
      "Validation loss: 41.0674\n",
      "Validation acc: 0.6666\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 243.7377, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 253.3269, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 20: 217.9893, Accuracy: 0.6589\n",
      "Training loss (for one batch) at step 30: 204.4582, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 40: 218.4191, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 50: 216.8265, Accuracy: 0.7626\n",
      "Training loss (for one batch) at step 60: 221.6444, Accuracy: 0.7714\n",
      "Training loss (for one batch) at step 70: 224.4673, Accuracy: 0.7587\n",
      "Training loss (for one batch) at step 80: 264.5077, Accuracy: 0.7381\n",
      "Training loss (for one batch) at step 90: 237.7380, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 100: 223.7788, Accuracy: 0.7413\n",
      "Training loss (for one batch) at step 110: 225.7257, Accuracy: 0.7426\n",
      "---- Training ----\n",
      "Training loss: 71.3933\n",
      "Training acc over epoch: 0.7409\n",
      "---- Validation ----\n",
      "Validation loss: 41.7475\n",
      "Validation acc: 0.6539\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 270.6964, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 244.0217, Accuracy: 0.5824\n",
      "Training loss (for one batch) at step 20: 207.3264, Accuracy: 0.6507\n",
      "Training loss (for one batch) at step 30: 224.9356, Accuracy: 0.7056\n",
      "Training loss (for one batch) at step 40: 214.1682, Accuracy: 0.7393\n",
      "Training loss (for one batch) at step 50: 221.9078, Accuracy: 0.7609\n",
      "Training loss (for one batch) at step 60: 227.5504, Accuracy: 0.7697\n",
      "Training loss (for one batch) at step 70: 239.2466, Accuracy: 0.7564\n",
      "Training loss (for one batch) at step 80: 262.5898, Accuracy: 0.7377\n",
      "Training loss (for one batch) at step 90: 222.3213, Accuracy: 0.7339\n",
      "Training loss (for one batch) at step 100: 233.9083, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 110: 237.7414, Accuracy: 0.7423\n",
      "---- Training ----\n",
      "Training loss: 77.5277\n",
      "Training acc over epoch: 0.7407\n",
      "---- Validation ----\n",
      "Validation loss: 49.9433\n",
      "Validation acc: 0.6636\n",
      "Time taken: 18.16s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 243.1594, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 232.3586, Accuracy: 0.5987\n",
      "Training loss (for one batch) at step 20: 228.5526, Accuracy: 0.6615\n",
      "Training loss (for one batch) at step 30: 227.1789, Accuracy: 0.7077\n",
      "Training loss (for one batch) at step 40: 232.3291, Accuracy: 0.7384\n",
      "Training loss (for one batch) at step 50: 222.5989, Accuracy: 0.7606\n",
      "Training loss (for one batch) at step 60: 224.3129, Accuracy: 0.7699\n",
      "Training loss (for one batch) at step 70: 239.1290, Accuracy: 0.7555\n",
      "Training loss (for one batch) at step 80: 236.4041, Accuracy: 0.7394\n",
      "Training loss (for one batch) at step 90: 215.2015, Accuracy: 0.7369\n",
      "Training loss (for one batch) at step 100: 219.7130, Accuracy: 0.7426\n",
      "Training loss (for one batch) at step 110: 229.8728, Accuracy: 0.7435\n",
      "---- Training ----\n",
      "Training loss: 82.1897\n",
      "Training acc over epoch: 0.7412\n",
      "---- Validation ----\n",
      "Validation loss: 62.2084\n",
      "Validation acc: 0.6862\n",
      "Time taken: 17.98s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.6786\n",
      "Validation AUC: 0.6810\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 595.0266, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 557.4879, Accuracy: 0.4943\n",
      "Training loss (for one batch) at step 20: 542.6710, Accuracy: 0.4851\n",
      "Training loss (for one batch) at step 30: 522.1675, Accuracy: 0.4932\n",
      "Training loss (for one batch) at step 40: 537.5087, Accuracy: 0.4981\n",
      "Training loss (for one batch) at step 50: 509.9998, Accuracy: 0.5026\n",
      "Training loss (for one batch) at step 60: 513.2633, Accuracy: 0.5085\n",
      "Training loss (for one batch) at step 70: 500.7504, Accuracy: 0.5133\n",
      "Training loss (for one batch) at step 80: 495.7657, Accuracy: 0.5131\n",
      "Training loss (for one batch) at step 90: 504.8313, Accuracy: 0.5121\n",
      "Training loss (for one batch) at step 100: 478.6949, Accuracy: 0.5128\n",
      "Training loss (for one batch) at step 110: 468.0410, Accuracy: 0.5136\n",
      "---- Training ----\n",
      "Training loss: 154.0699\n",
      "Training acc over epoch: 0.5145\n",
      "---- Validation ----\n",
      "Validation loss: 34.9472\n",
      "Validation acc: 0.4893\n",
      "Time taken: 19.49s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 486.3110, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 463.6232, Accuracy: 0.5064\n",
      "Training loss (for one batch) at step 20: 463.4338, Accuracy: 0.5071\n",
      "Training loss (for one batch) at step 30: 461.8126, Accuracy: 0.5141\n",
      "Training loss (for one batch) at step 40: 472.6227, Accuracy: 0.5225\n",
      "Training loss (for one batch) at step 50: 454.9458, Accuracy: 0.5247\n",
      "Training loss (for one batch) at step 60: 450.3025, Accuracy: 0.5236\n",
      "Training loss (for one batch) at step 70: 452.6360, Accuracy: 0.5256\n",
      "Training loss (for one batch) at step 80: 448.9085, Accuracy: 0.5246\n",
      "Training loss (for one batch) at step 90: 461.3895, Accuracy: 0.5211\n",
      "Training loss (for one batch) at step 100: 449.2736, Accuracy: 0.5229\n",
      "Training loss (for one batch) at step 110: 454.6813, Accuracy: 0.5245\n",
      "---- Training ----\n",
      "Training loss: 146.4045\n",
      "Training acc over epoch: 0.5252\n",
      "---- Validation ----\n",
      "Validation loss: 34.6511\n",
      "Validation acc: 0.5132\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 451.1314, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 458.1832, Accuracy: 0.5256\n",
      "Training loss (for one batch) at step 20: 451.4016, Accuracy: 0.5205\n",
      "Training loss (for one batch) at step 30: 451.3441, Accuracy: 0.5302\n",
      "Training loss (for one batch) at step 40: 449.0284, Accuracy: 0.5360\n",
      "Training loss (for one batch) at step 50: 449.8148, Accuracy: 0.5371\n",
      "Training loss (for one batch) at step 60: 455.2718, Accuracy: 0.5394\n",
      "Training loss (for one batch) at step 70: 443.4525, Accuracy: 0.5437\n",
      "Training loss (for one batch) at step 80: 447.9564, Accuracy: 0.5429\n",
      "Training loss (for one batch) at step 90: 447.1406, Accuracy: 0.5398\n",
      "Training loss (for one batch) at step 100: 450.0571, Accuracy: 0.5401\n",
      "Training loss (for one batch) at step 110: 444.6588, Accuracy: 0.5415\n",
      "---- Training ----\n",
      "Training loss: 138.8881\n",
      "Training acc over epoch: 0.5423\n",
      "---- Validation ----\n",
      "Validation loss: 34.8130\n",
      "Validation acc: 0.5674\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 447.3416, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 445.0283, Accuracy: 0.5121\n",
      "Training loss (for one batch) at step 20: 444.4727, Accuracy: 0.5223\n",
      "Training loss (for one batch) at step 30: 445.0549, Accuracy: 0.5403\n",
      "Training loss (for one batch) at step 40: 441.2180, Accuracy: 0.5532\n",
      "Training loss (for one batch) at step 50: 445.1253, Accuracy: 0.5591\n",
      "Training loss (for one batch) at step 60: 440.9454, Accuracy: 0.5621\n",
      "Training loss (for one batch) at step 70: 442.6100, Accuracy: 0.5656\n",
      "Training loss (for one batch) at step 80: 444.3257, Accuracy: 0.5624\n",
      "Training loss (for one batch) at step 90: 444.7472, Accuracy: 0.5578\n",
      "Training loss (for one batch) at step 100: 439.7696, Accuracy: 0.5610\n",
      "Training loss (for one batch) at step 110: 441.6659, Accuracy: 0.5665\n",
      "---- Training ----\n",
      "Training loss: 137.7144\n",
      "Training acc over epoch: 0.5682\n",
      "---- Validation ----\n",
      "Validation loss: 34.3668\n",
      "Validation acc: 0.5645\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 449.1453, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 442.0065, Accuracy: 0.5497\n",
      "Training loss (for one batch) at step 20: 442.0314, Accuracy: 0.5446\n",
      "Training loss (for one batch) at step 30: 441.3891, Accuracy: 0.5628\n",
      "Training loss (for one batch) at step 40: 439.1417, Accuracy: 0.5737\n",
      "Training loss (for one batch) at step 50: 440.5007, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 60: 442.1718, Accuracy: 0.5900\n",
      "Training loss (for one batch) at step 70: 444.7981, Accuracy: 0.5946\n",
      "Training loss (for one batch) at step 80: 442.2212, Accuracy: 0.5936\n",
      "Training loss (for one batch) at step 90: 442.2469, Accuracy: 0.5883\n",
      "Training loss (for one batch) at step 100: 441.1694, Accuracy: 0.5884\n",
      "Training loss (for one batch) at step 110: 440.1091, Accuracy: 0.5919\n",
      "---- Training ----\n",
      "Training loss: 138.9095\n",
      "Training acc over epoch: 0.5928\n",
      "---- Validation ----\n",
      "Validation loss: 34.7901\n",
      "Validation acc: 0.6042\n",
      "Time taken: 18.17s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 443.0906, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 444.1180, Accuracy: 0.6065\n",
      "Training loss (for one batch) at step 20: 440.3863, Accuracy: 0.6042\n",
      "Training loss (for one batch) at step 30: 437.3044, Accuracy: 0.6096\n",
      "Training loss (for one batch) at step 40: 440.1812, Accuracy: 0.6185\n",
      "Training loss (for one batch) at step 50: 438.3823, Accuracy: 0.6252\n",
      "Training loss (for one batch) at step 60: 436.6877, Accuracy: 0.6314\n",
      "Training loss (for one batch) at step 70: 442.1890, Accuracy: 0.6308\n",
      "Training loss (for one batch) at step 80: 443.4851, Accuracy: 0.6287\n",
      "Training loss (for one batch) at step 90: 442.4583, Accuracy: 0.6230\n",
      "Training loss (for one batch) at step 100: 439.6858, Accuracy: 0.6236\n",
      "Training loss (for one batch) at step 110: 437.9932, Accuracy: 0.6249\n",
      "---- Training ----\n",
      "Training loss: 134.4244\n",
      "Training acc over epoch: 0.6243\n",
      "---- Validation ----\n",
      "Validation loss: 34.4902\n",
      "Validation acc: 0.6314\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 441.4157, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 444.4284, Accuracy: 0.6165\n",
      "Training loss (for one batch) at step 20: 439.4653, Accuracy: 0.6127\n",
      "Training loss (for one batch) at step 30: 433.5857, Accuracy: 0.6300\n",
      "Training loss (for one batch) at step 40: 436.3735, Accuracy: 0.6313\n",
      "Training loss (for one batch) at step 50: 438.3936, Accuracy: 0.6403\n",
      "Training loss (for one batch) at step 60: 445.6154, Accuracy: 0.6451\n",
      "Training loss (for one batch) at step 70: 449.0858, Accuracy: 0.6478\n",
      "Training loss (for one batch) at step 80: 443.8469, Accuracy: 0.6412\n",
      "Training loss (for one batch) at step 90: 442.0595, Accuracy: 0.6330\n",
      "Training loss (for one batch) at step 100: 434.6537, Accuracy: 0.6356\n",
      "Training loss (for one batch) at step 110: 438.1154, Accuracy: 0.6354\n",
      "---- Training ----\n",
      "Training loss: 140.6958\n",
      "Training acc over epoch: 0.6372\n",
      "---- Validation ----\n",
      "Validation loss: 36.0183\n",
      "Validation acc: 0.6615\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 445.4565, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 444.7073, Accuracy: 0.6108\n",
      "Training loss (for one batch) at step 20: 441.0669, Accuracy: 0.6008\n",
      "Training loss (for one batch) at step 30: 433.6915, Accuracy: 0.6147\n",
      "Training loss (for one batch) at step 40: 426.9480, Accuracy: 0.6252\n",
      "Training loss (for one batch) at step 50: 430.2247, Accuracy: 0.6350\n",
      "Training loss (for one batch) at step 60: 429.9646, Accuracy: 0.6473\n",
      "Training loss (for one batch) at step 70: 439.1899, Accuracy: 0.6493\n",
      "Training loss (for one batch) at step 80: 445.9654, Accuracy: 0.6423\n",
      "Training loss (for one batch) at step 90: 439.9220, Accuracy: 0.6366\n",
      "Training loss (for one batch) at step 100: 438.9436, Accuracy: 0.6379\n",
      "Training loss (for one batch) at step 110: 436.1638, Accuracy: 0.6387\n",
      "---- Training ----\n",
      "Training loss: 138.1879\n",
      "Training acc over epoch: 0.6391\n",
      "---- Validation ----\n",
      "Validation loss: 35.4630\n",
      "Validation acc: 0.6480\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 447.3041, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 439.1458, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 20: 440.1237, Accuracy: 0.6112\n",
      "Training loss (for one batch) at step 30: 434.6516, Accuracy: 0.6308\n",
      "Training loss (for one batch) at step 40: 423.6478, Accuracy: 0.6414\n",
      "Training loss (for one batch) at step 50: 427.7898, Accuracy: 0.6530\n",
      "Training loss (for one batch) at step 60: 434.6975, Accuracy: 0.6623\n",
      "Training loss (for one batch) at step 70: 437.9155, Accuracy: 0.6674\n",
      "Training loss (for one batch) at step 80: 435.9323, Accuracy: 0.6603\n",
      "Training loss (for one batch) at step 90: 432.2097, Accuracy: 0.6540\n",
      "Training loss (for one batch) at step 100: 429.0818, Accuracy: 0.6551\n",
      "Training loss (for one batch) at step 110: 439.1040, Accuracy: 0.6579\n",
      "---- Training ----\n",
      "Training loss: 134.5103\n",
      "Training acc over epoch: 0.6602\n",
      "---- Validation ----\n",
      "Validation loss: 35.4028\n",
      "Validation acc: 0.6529\n",
      "Time taken: 18.14s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 442.7559, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 438.2720, Accuracy: 0.6428\n",
      "Training loss (for one batch) at step 20: 433.9321, Accuracy: 0.6391\n",
      "Training loss (for one batch) at step 30: 424.6459, Accuracy: 0.6510\n",
      "Training loss (for one batch) at step 40: 421.3739, Accuracy: 0.6570\n",
      "Training loss (for one batch) at step 50: 422.7516, Accuracy: 0.6679\n",
      "Training loss (for one batch) at step 60: 421.1169, Accuracy: 0.6814\n",
      "Training loss (for one batch) at step 70: 437.3787, Accuracy: 0.6843\n",
      "Training loss (for one batch) at step 80: 429.5675, Accuracy: 0.6782\n",
      "Training loss (for one batch) at step 90: 433.2197, Accuracy: 0.6753\n",
      "Training loss (for one batch) at step 100: 425.7841, Accuracy: 0.6768\n",
      "Training loss (for one batch) at step 110: 427.7459, Accuracy: 0.6767\n",
      "---- Training ----\n",
      "Training loss: 130.5862\n",
      "Training acc over epoch: 0.6775\n",
      "---- Validation ----\n",
      "Validation loss: 36.1118\n",
      "Validation acc: 0.6257\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 445.1122, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 449.4253, Accuracy: 0.6207\n",
      "Training loss (for one batch) at step 20: 435.1071, Accuracy: 0.6354\n",
      "Training loss (for one batch) at step 30: 410.2613, Accuracy: 0.6570\n",
      "Training loss (for one batch) at step 40: 428.6922, Accuracy: 0.6683\n",
      "Training loss (for one batch) at step 50: 401.0219, Accuracy: 0.6769\n",
      "Training loss (for one batch) at step 60: 406.9357, Accuracy: 0.6887\n",
      "Training loss (for one batch) at step 70: 434.9605, Accuracy: 0.6892\n",
      "Training loss (for one batch) at step 80: 446.4901, Accuracy: 0.6833\n",
      "Training loss (for one batch) at step 90: 430.6061, Accuracy: 0.6855\n",
      "Training loss (for one batch) at step 100: 412.2405, Accuracy: 0.6867\n",
      "Training loss (for one batch) at step 110: 434.6780, Accuracy: 0.6888\n",
      "---- Training ----\n",
      "Training loss: 137.7570\n",
      "Training acc over epoch: 0.6897\n",
      "---- Validation ----\n",
      "Validation loss: 36.5865\n",
      "Validation acc: 0.6515\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 439.7664, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 441.3108, Accuracy: 0.6349\n",
      "Training loss (for one batch) at step 20: 424.3983, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 30: 414.4519, Accuracy: 0.6636\n",
      "Training loss (for one batch) at step 40: 409.8748, Accuracy: 0.6814\n",
      "Training loss (for one batch) at step 50: 398.8557, Accuracy: 0.6955\n",
      "Training loss (for one batch) at step 60: 415.7055, Accuracy: 0.7068\n",
      "Training loss (for one batch) at step 70: 437.3189, Accuracy: 0.7047\n",
      "Training loss (for one batch) at step 80: 429.3831, Accuracy: 0.6963\n",
      "Training loss (for one batch) at step 90: 411.3061, Accuracy: 0.6939\n",
      "Training loss (for one batch) at step 100: 406.5013, Accuracy: 0.6958\n",
      "Training loss (for one batch) at step 110: 410.6960, Accuracy: 0.6964\n",
      "---- Training ----\n",
      "Training loss: 130.7722\n",
      "Training acc over epoch: 0.6977\n",
      "---- Validation ----\n",
      "Validation loss: 33.6064\n",
      "Validation acc: 0.7055\n",
      "Time taken: 18.18s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 433.0876, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 433.8535, Accuracy: 0.6541\n",
      "Training loss (for one batch) at step 20: 415.8107, Accuracy: 0.6611\n",
      "Training loss (for one batch) at step 30: 406.9964, Accuracy: 0.6840\n",
      "Training loss (for one batch) at step 40: 404.5946, Accuracy: 0.7027\n",
      "Training loss (for one batch) at step 50: 398.5646, Accuracy: 0.7168\n",
      "Training loss (for one batch) at step 60: 398.0163, Accuracy: 0.7275\n",
      "Training loss (for one batch) at step 70: 416.7963, Accuracy: 0.7257\n",
      "Training loss (for one batch) at step 80: 409.9407, Accuracy: 0.7172\n",
      "Training loss (for one batch) at step 90: 408.3878, Accuracy: 0.7135\n",
      "Training loss (for one batch) at step 100: 405.0038, Accuracy: 0.7151\n",
      "Training loss (for one batch) at step 110: 406.1310, Accuracy: 0.7172\n",
      "---- Training ----\n",
      "Training loss: 129.6620\n",
      "Training acc over epoch: 0.7180\n",
      "---- Validation ----\n",
      "Validation loss: 32.4626\n",
      "Validation acc: 0.6808\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 436.0079, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 425.9634, Accuracy: 0.6619\n",
      "Training loss (for one batch) at step 20: 400.5240, Accuracy: 0.6652\n",
      "Training loss (for one batch) at step 30: 391.6054, Accuracy: 0.6938\n",
      "Training loss (for one batch) at step 40: 383.4476, Accuracy: 0.7085\n",
      "Training loss (for one batch) at step 50: 370.5282, Accuracy: 0.7226\n",
      "Training loss (for one batch) at step 60: 396.4619, Accuracy: 0.7354\n",
      "Training loss (for one batch) at step 70: 391.2976, Accuracy: 0.7350\n",
      "Training loss (for one batch) at step 80: 409.9507, Accuracy: 0.7281\n",
      "Training loss (for one batch) at step 90: 377.4124, Accuracy: 0.7242\n",
      "Training loss (for one batch) at step 100: 382.3090, Accuracy: 0.7277\n",
      "Training loss (for one batch) at step 110: 398.9326, Accuracy: 0.7302\n",
      "---- Training ----\n",
      "Training loss: 131.0127\n",
      "Training acc over epoch: 0.7300\n",
      "---- Validation ----\n",
      "Validation loss: 41.6157\n",
      "Validation acc: 0.6760\n",
      "Time taken: 18.07s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 417.5515, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 401.5286, Accuracy: 0.6413\n",
      "Training loss (for one batch) at step 20: 392.8380, Accuracy: 0.6633\n",
      "Training loss (for one batch) at step 30: 387.6005, Accuracy: 0.6968\n",
      "Training loss (for one batch) at step 40: 375.6357, Accuracy: 0.7214\n",
      "Training loss (for one batch) at step 50: 362.0601, Accuracy: 0.7399\n",
      "Training loss (for one batch) at step 60: 365.2339, Accuracy: 0.7504\n",
      "Training loss (for one batch) at step 70: 390.9781, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 80: 397.3112, Accuracy: 0.7320\n",
      "Training loss (for one batch) at step 90: 365.6566, Accuracy: 0.7302\n",
      "Training loss (for one batch) at step 100: 367.4101, Accuracy: 0.7366\n",
      "Training loss (for one batch) at step 110: 394.5573, Accuracy: 0.7386\n",
      "---- Training ----\n",
      "Training loss: 121.9181\n",
      "Training acc over epoch: 0.7376\n",
      "---- Validation ----\n",
      "Validation loss: 41.5208\n",
      "Validation acc: 0.6805\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 427.5992, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 394.7079, Accuracy: 0.6598\n",
      "Training loss (for one batch) at step 20: 377.7032, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 30: 353.2469, Accuracy: 0.7064\n",
      "Training loss (for one batch) at step 40: 355.4357, Accuracy: 0.7309\n",
      "Training loss (for one batch) at step 50: 358.9080, Accuracy: 0.7443\n",
      "Training loss (for one batch) at step 60: 362.4816, Accuracy: 0.7549\n",
      "Training loss (for one batch) at step 70: 405.4352, Accuracy: 0.7486\n",
      "Training loss (for one batch) at step 80: 404.0085, Accuracy: 0.7349\n",
      "Training loss (for one batch) at step 90: 359.7005, Accuracy: 0.7319\n",
      "Training loss (for one batch) at step 100: 366.1823, Accuracy: 0.7381\n",
      "Training loss (for one batch) at step 110: 394.0892, Accuracy: 0.7404\n",
      "---- Training ----\n",
      "Training loss: 114.2357\n",
      "Training acc over epoch: 0.7390\n",
      "---- Validation ----\n",
      "Validation loss: 40.1581\n",
      "Validation acc: 0.6738\n",
      "Time taken: 18.21s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 403.8642, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 405.0336, Accuracy: 0.6470\n",
      "Training loss (for one batch) at step 20: 383.3261, Accuracy: 0.6618\n",
      "Training loss (for one batch) at step 30: 365.5342, Accuracy: 0.7061\n",
      "Training loss (for one batch) at step 40: 357.1719, Accuracy: 0.7300\n",
      "Training loss (for one batch) at step 50: 362.4783, Accuracy: 0.7475\n",
      "Training loss (for one batch) at step 60: 372.3400, Accuracy: 0.7563\n",
      "Training loss (for one batch) at step 70: 365.3234, Accuracy: 0.7526\n",
      "Training loss (for one batch) at step 80: 383.2708, Accuracy: 0.7390\n",
      "Training loss (for one batch) at step 90: 352.5895, Accuracy: 0.7372\n",
      "Training loss (for one batch) at step 100: 358.4975, Accuracy: 0.7417\n",
      "Training loss (for one batch) at step 110: 353.7937, Accuracy: 0.7451\n",
      "---- Training ----\n",
      "Training loss: 117.8450\n",
      "Training acc over epoch: 0.7442\n",
      "---- Validation ----\n",
      "Validation loss: 42.0055\n",
      "Validation acc: 0.6744\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 389.2054, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 383.4631, Accuracy: 0.6278\n",
      "Training loss (for one batch) at step 20: 367.3921, Accuracy: 0.6734\n",
      "Training loss (for one batch) at step 30: 326.7686, Accuracy: 0.7157\n",
      "Training loss (for one batch) at step 40: 336.1328, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 50: 328.4632, Accuracy: 0.7564\n",
      "Training loss (for one batch) at step 60: 348.7297, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 70: 378.1862, Accuracy: 0.7573\n",
      "Training loss (for one batch) at step 80: 358.8308, Accuracy: 0.7428\n",
      "Training loss (for one batch) at step 90: 379.6000, Accuracy: 0.7388\n",
      "Training loss (for one batch) at step 100: 340.7877, Accuracy: 0.7451\n",
      "Training loss (for one batch) at step 110: 364.6815, Accuracy: 0.7468\n",
      "---- Training ----\n",
      "Training loss: 119.4831\n",
      "Training acc over epoch: 0.7456\n",
      "---- Validation ----\n",
      "Validation loss: 40.0606\n",
      "Validation acc: 0.6781\n",
      "Time taken: 17.87s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 399.0494, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 372.0347, Accuracy: 0.6165\n",
      "Training loss (for one batch) at step 20: 348.3013, Accuracy: 0.6559\n",
      "Training loss (for one batch) at step 30: 339.7270, Accuracy: 0.7069\n",
      "Training loss (for one batch) at step 40: 323.7115, Accuracy: 0.7372\n",
      "Training loss (for one batch) at step 50: 315.3268, Accuracy: 0.7546\n",
      "Training loss (for one batch) at step 60: 342.4856, Accuracy: 0.7654\n",
      "Training loss (for one batch) at step 70: 352.1238, Accuracy: 0.7544\n",
      "Training loss (for one batch) at step 80: 379.5638, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 90: 348.3537, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 100: 328.8371, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 110: 358.6827, Accuracy: 0.7479\n",
      "---- Training ----\n",
      "Training loss: 108.0502\n",
      "Training acc over epoch: 0.7473\n",
      "---- Validation ----\n",
      "Validation loss: 34.5241\n",
      "Validation acc: 0.6980\n",
      "Time taken: 18.11s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 401.3943, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 358.9698, Accuracy: 0.6158\n",
      "Training loss (for one batch) at step 20: 328.0287, Accuracy: 0.6670\n",
      "Training loss (for one batch) at step 30: 331.3666, Accuracy: 0.7140\n",
      "Training loss (for one batch) at step 40: 330.5202, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 50: 314.5229, Accuracy: 0.7544\n",
      "Training loss (for one batch) at step 60: 320.6160, Accuracy: 0.7638\n",
      "Training loss (for one batch) at step 70: 361.2486, Accuracy: 0.7544\n",
      "Training loss (for one batch) at step 80: 354.3117, Accuracy: 0.7366\n",
      "Training loss (for one batch) at step 90: 327.7342, Accuracy: 0.7357\n",
      "Training loss (for one batch) at step 100: 317.9817, Accuracy: 0.7413\n",
      "Training loss (for one batch) at step 110: 333.7048, Accuracy: 0.7436\n",
      "---- Training ----\n",
      "Training loss: 111.8837\n",
      "Training acc over epoch: 0.7427\n",
      "---- Validation ----\n",
      "Validation loss: 42.7070\n",
      "Validation acc: 0.6620\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 377.7249, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 359.6540, Accuracy: 0.5994\n",
      "Training loss (for one batch) at step 20: 321.0365, Accuracy: 0.6555\n",
      "Training loss (for one batch) at step 30: 317.0337, Accuracy: 0.7026\n",
      "Training loss (for one batch) at step 40: 301.1501, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 50: 298.8767, Accuracy: 0.7575\n",
      "Training loss (for one batch) at step 60: 305.7414, Accuracy: 0.7659\n",
      "Training loss (for one batch) at step 70: 336.7613, Accuracy: 0.7539\n",
      "Training loss (for one batch) at step 80: 348.0907, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 90: 334.2995, Accuracy: 0.7369\n",
      "Training loss (for one batch) at step 100: 350.0880, Accuracy: 0.7441\n",
      "Training loss (for one batch) at step 110: 324.2639, Accuracy: 0.7458\n",
      "---- Training ----\n",
      "Training loss: 101.5801\n",
      "Training acc over epoch: 0.7454\n",
      "---- Validation ----\n",
      "Validation loss: 52.7245\n",
      "Validation acc: 0.6687\n",
      "Time taken: 17.88s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 351.6215, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 342.4439, Accuracy: 0.6129\n",
      "Training loss (for one batch) at step 20: 308.8188, Accuracy: 0.6596\n",
      "Training loss (for one batch) at step 30: 321.2831, Accuracy: 0.7049\n",
      "Training loss (for one batch) at step 40: 298.0224, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 50: 290.0285, Accuracy: 0.7537\n",
      "Training loss (for one batch) at step 60: 313.6405, Accuracy: 0.7654\n",
      "Training loss (for one batch) at step 70: 344.0431, Accuracy: 0.7524\n",
      "Training loss (for one batch) at step 80: 334.2320, Accuracy: 0.7353\n",
      "Training loss (for one batch) at step 90: 311.2206, Accuracy: 0.7316\n",
      "Training loss (for one batch) at step 100: 300.9692, Accuracy: 0.7392\n",
      "Training loss (for one batch) at step 110: 314.5274, Accuracy: 0.7420\n",
      "---- Training ----\n",
      "Training loss: 112.5374\n",
      "Training acc over epoch: 0.7413\n",
      "---- Validation ----\n",
      "Validation loss: 38.4401\n",
      "Validation acc: 0.6851\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 354.8536, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 353.3739, Accuracy: 0.5973\n",
      "Training loss (for one batch) at step 20: 305.3282, Accuracy: 0.6652\n",
      "Training loss (for one batch) at step 30: 324.0978, Accuracy: 0.7114\n",
      "Training loss (for one batch) at step 40: 287.4073, Accuracy: 0.7418\n",
      "Training loss (for one batch) at step 50: 293.5302, Accuracy: 0.7627\n",
      "Training loss (for one batch) at step 60: 318.9644, Accuracy: 0.7742\n",
      "Training loss (for one batch) at step 70: 353.3793, Accuracy: 0.7591\n",
      "Training loss (for one batch) at step 80: 353.3928, Accuracy: 0.7436\n",
      "Training loss (for one batch) at step 90: 297.2790, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 100: 297.5595, Accuracy: 0.7459\n",
      "Training loss (for one batch) at step 110: 320.1887, Accuracy: 0.7497\n",
      "---- Training ----\n",
      "Training loss: 102.7023\n",
      "Training acc over epoch: 0.7495\n",
      "---- Validation ----\n",
      "Validation loss: 50.3149\n",
      "Validation acc: 0.6859\n",
      "Time taken: 18.10s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 369.0059, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 355.3741, Accuracy: 0.5930\n",
      "Training loss (for one batch) at step 20: 307.9933, Accuracy: 0.6570\n",
      "Training loss (for one batch) at step 30: 318.3659, Accuracy: 0.7112\n",
      "Training loss (for one batch) at step 40: 293.8044, Accuracy: 0.7420\n",
      "Training loss (for one batch) at step 50: 285.2556, Accuracy: 0.7604\n",
      "Training loss (for one batch) at step 60: 311.0674, Accuracy: 0.7701\n",
      "Training loss (for one batch) at step 70: 339.2266, Accuracy: 0.7569\n",
      "Training loss (for one batch) at step 80: 338.3944, Accuracy: 0.7423\n",
      "Training loss (for one batch) at step 90: 316.5943, Accuracy: 0.7430\n",
      "Training loss (for one batch) at step 100: 286.5220, Accuracy: 0.7496\n",
      "Training loss (for one batch) at step 110: 297.1833, Accuracy: 0.7520\n",
      "---- Training ----\n",
      "Training loss: 104.5525\n",
      "Training acc over epoch: 0.7512\n",
      "---- Validation ----\n",
      "Validation loss: 37.4437\n",
      "Validation acc: 0.6781\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 350.3196, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 324.3670, Accuracy: 0.5909\n",
      "Training loss (for one batch) at step 20: 315.3382, Accuracy: 0.6466\n",
      "Training loss (for one batch) at step 30: 289.5399, Accuracy: 0.7061\n",
      "Training loss (for one batch) at step 40: 273.2314, Accuracy: 0.7401\n",
      "Training loss (for one batch) at step 50: 290.3312, Accuracy: 0.7580\n",
      "Training loss (for one batch) at step 60: 303.2867, Accuracy: 0.7692\n",
      "Training loss (for one batch) at step 70: 317.0849, Accuracy: 0.7537\n",
      "Training loss (for one batch) at step 80: 342.4837, Accuracy: 0.7374\n",
      "Training loss (for one batch) at step 90: 299.4896, Accuracy: 0.7354\n",
      "Training loss (for one batch) at step 100: 281.7437, Accuracy: 0.7448\n",
      "Training loss (for one batch) at step 110: 312.5017, Accuracy: 0.7469\n",
      "---- Training ----\n",
      "Training loss: 109.7396\n",
      "Training acc over epoch: 0.7466\n",
      "---- Validation ----\n",
      "Validation loss: 55.5954\n",
      "Validation acc: 0.6840\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 361.6344, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 334.2173, Accuracy: 0.5945\n",
      "Training loss (for one batch) at step 20: 298.3186, Accuracy: 0.6611\n",
      "Training loss (for one batch) at step 30: 297.0115, Accuracy: 0.7102\n",
      "Training loss (for one batch) at step 40: 285.3252, Accuracy: 0.7388\n",
      "Training loss (for one batch) at step 50: 291.9545, Accuracy: 0.7589\n",
      "Training loss (for one batch) at step 60: 306.8759, Accuracy: 0.7715\n",
      "Training loss (for one batch) at step 70: 316.0230, Accuracy: 0.7555\n",
      "Training loss (for one batch) at step 80: 319.2238, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 90: 305.5552, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 100: 280.1072, Accuracy: 0.7477\n",
      "Training loss (for one batch) at step 110: 293.9791, Accuracy: 0.7499\n",
      "---- Training ----\n",
      "Training loss: 96.5426\n",
      "Training acc over epoch: 0.7479\n",
      "---- Validation ----\n",
      "Validation loss: 44.4231\n",
      "Validation acc: 0.6835\n",
      "Time taken: 18.19s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 336.7120, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 339.6230, Accuracy: 0.5952\n",
      "Training loss (for one batch) at step 20: 315.7532, Accuracy: 0.6629\n",
      "Training loss (for one batch) at step 30: 287.0055, Accuracy: 0.7140\n",
      "Training loss (for one batch) at step 40: 279.8922, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 50: 285.1344, Accuracy: 0.7626\n",
      "Training loss (for one batch) at step 60: 286.9600, Accuracy: 0.7705\n",
      "Training loss (for one batch) at step 70: 332.4430, Accuracy: 0.7595\n",
      "Training loss (for one batch) at step 80: 313.6763, Accuracy: 0.7418\n",
      "Training loss (for one batch) at step 90: 293.2515, Accuracy: 0.7402\n",
      "Training loss (for one batch) at step 100: 300.6137, Accuracy: 0.7471\n",
      "Training loss (for one batch) at step 110: 299.1765, Accuracy: 0.7504\n",
      "---- Training ----\n",
      "Training loss: 92.2961\n",
      "Training acc over epoch: 0.7482\n",
      "---- Validation ----\n",
      "Validation loss: 52.5616\n",
      "Validation acc: 0.6862\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 323.6220, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 330.7833, Accuracy: 0.5852\n",
      "Training loss (for one batch) at step 20: 306.5285, Accuracy: 0.6458\n",
      "Training loss (for one batch) at step 30: 282.3552, Accuracy: 0.7029\n",
      "Training loss (for one batch) at step 40: 260.6841, Accuracy: 0.7338\n",
      "Training loss (for one batch) at step 50: 287.5732, Accuracy: 0.7561\n",
      "Training loss (for one batch) at step 60: 284.0165, Accuracy: 0.7661\n",
      "Training loss (for one batch) at step 70: 313.0776, Accuracy: 0.7530\n",
      "Training loss (for one batch) at step 80: 304.8362, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 90: 293.0602, Accuracy: 0.7363\n",
      "Training loss (for one batch) at step 100: 283.4762, Accuracy: 0.7442\n",
      "Training loss (for one batch) at step 110: 293.7262, Accuracy: 0.7470\n",
      "---- Training ----\n",
      "Training loss: 94.9307\n",
      "Training acc over epoch: 0.7468\n",
      "---- Validation ----\n",
      "Validation loss: 62.7275\n",
      "Validation acc: 0.6754\n",
      "Time taken: 18.13s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 318.8073, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 347.2283, Accuracy: 0.5952\n",
      "Training loss (for one batch) at step 20: 296.2579, Accuracy: 0.6540\n",
      "Training loss (for one batch) at step 30: 274.0833, Accuracy: 0.7061\n",
      "Training loss (for one batch) at step 40: 272.7715, Accuracy: 0.7372\n",
      "Training loss (for one batch) at step 50: 261.7347, Accuracy: 0.7592\n",
      "Training loss (for one batch) at step 60: 286.4746, Accuracy: 0.7687\n",
      "Training loss (for one batch) at step 70: 309.1528, Accuracy: 0.7536\n",
      "Training loss (for one batch) at step 80: 329.5674, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 90: 306.2005, Accuracy: 0.7340\n",
      "Training loss (for one batch) at step 100: 272.0835, Accuracy: 0.7428\n",
      "Training loss (for one batch) at step 110: 272.5132, Accuracy: 0.7442\n",
      "---- Training ----\n",
      "Training loss: 97.8245\n",
      "Training acc over epoch: 0.7427\n",
      "---- Validation ----\n",
      "Validation loss: 47.7531\n",
      "Validation acc: 0.6639\n",
      "Time taken: 17.89s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 317.7188, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 326.2556, Accuracy: 0.5916\n",
      "Training loss (for one batch) at step 20: 267.7712, Accuracy: 0.6603\n",
      "Training loss (for one batch) at step 30: 289.1389, Accuracy: 0.7092\n",
      "Training loss (for one batch) at step 40: 271.0745, Accuracy: 0.7414\n",
      "Training loss (for one batch) at step 50: 278.6005, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 60: 286.2173, Accuracy: 0.7722\n",
      "Training loss (for one batch) at step 70: 305.8275, Accuracy: 0.7574\n",
      "Training loss (for one batch) at step 80: 290.8164, Accuracy: 0.7392\n",
      "Training loss (for one batch) at step 90: 283.7154, Accuracy: 0.7383\n",
      "Training loss (for one batch) at step 100: 281.0726, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 110: 303.4716, Accuracy: 0.7482\n",
      "---- Training ----\n",
      "Training loss: 109.2938\n",
      "Training acc over epoch: 0.7472\n",
      "---- Validation ----\n",
      "Validation loss: 42.4423\n",
      "Validation acc: 0.6746\n",
      "Time taken: 18.08s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 314.0892, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 300.9348, Accuracy: 0.5895\n",
      "Training loss (for one batch) at step 20: 293.4192, Accuracy: 0.6458\n",
      "Training loss (for one batch) at step 30: 261.0364, Accuracy: 0.7046\n",
      "Training loss (for one batch) at step 40: 261.2520, Accuracy: 0.7321\n",
      "Training loss (for one batch) at step 50: 267.9358, Accuracy: 0.7555\n",
      "Training loss (for one batch) at step 60: 264.0300, Accuracy: 0.7693\n",
      "Training loss (for one batch) at step 70: 301.0337, Accuracy: 0.7524\n",
      "Training loss (for one batch) at step 80: 311.0490, Accuracy: 0.7363\n",
      "Training loss (for one batch) at step 90: 276.4229, Accuracy: 0.7323\n",
      "Training loss (for one batch) at step 100: 268.0654, Accuracy: 0.7430\n",
      "Training loss (for one batch) at step 110: 292.5641, Accuracy: 0.7459\n",
      "---- Training ----\n",
      "Training loss: 102.4963\n",
      "Training acc over epoch: 0.7449\n",
      "---- Validation ----\n",
      "Validation loss: 53.5209\n",
      "Validation acc: 0.6623\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 323.2650, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 312.4573, Accuracy: 0.5888\n",
      "Training loss (for one batch) at step 20: 293.6398, Accuracy: 0.6555\n",
      "Training loss (for one batch) at step 30: 269.5918, Accuracy: 0.7084\n",
      "Training loss (for one batch) at step 40: 259.1081, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 50: 259.2516, Accuracy: 0.7597\n",
      "Training loss (for one batch) at step 60: 273.4893, Accuracy: 0.7716\n",
      "Training loss (for one batch) at step 70: 333.5178, Accuracy: 0.7590\n",
      "Training loss (for one batch) at step 80: 322.8661, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 90: 273.0518, Accuracy: 0.7381\n",
      "Training loss (for one batch) at step 100: 282.7107, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 110: 284.8298, Accuracy: 0.7469\n",
      "---- Training ----\n",
      "Training loss: 91.7786\n",
      "Training acc over epoch: 0.7460\n",
      "---- Validation ----\n",
      "Validation loss: 47.1479\n",
      "Validation acc: 0.6760\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 308.2706, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 314.1206, Accuracy: 0.5838\n",
      "Training loss (for one batch) at step 20: 268.8436, Accuracy: 0.6429\n",
      "Training loss (for one batch) at step 30: 263.4297, Accuracy: 0.6983\n",
      "Training loss (for one batch) at step 40: 261.8694, Accuracy: 0.7340\n",
      "Training loss (for one batch) at step 50: 258.4740, Accuracy: 0.7570\n",
      "Training loss (for one batch) at step 60: 273.1410, Accuracy: 0.7704\n",
      "Training loss (for one batch) at step 70: 296.6752, Accuracy: 0.7561\n",
      "Training loss (for one batch) at step 80: 291.2309, Accuracy: 0.7378\n",
      "Training loss (for one batch) at step 90: 273.6768, Accuracy: 0.7359\n",
      "Training loss (for one batch) at step 100: 257.7693, Accuracy: 0.7444\n",
      "Training loss (for one batch) at step 110: 270.3364, Accuracy: 0.7471\n",
      "---- Training ----\n",
      "Training loss: 104.2577\n",
      "Training acc over epoch: 0.7458\n",
      "---- Validation ----\n",
      "Validation loss: 48.9564\n",
      "Validation acc: 0.6773\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 313.0634, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 304.9380, Accuracy: 0.5618\n",
      "Training loss (for one batch) at step 20: 277.1904, Accuracy: 0.6417\n",
      "Training loss (for one batch) at step 30: 250.7314, Accuracy: 0.7034\n",
      "Training loss (for one batch) at step 40: 271.4522, Accuracy: 0.7395\n",
      "Training loss (for one batch) at step 50: 266.6057, Accuracy: 0.7604\n",
      "Training loss (for one batch) at step 60: 278.3954, Accuracy: 0.7701\n",
      "Training loss (for one batch) at step 70: 288.2605, Accuracy: 0.7562\n",
      "Training loss (for one batch) at step 80: 301.8964, Accuracy: 0.7389\n",
      "Training loss (for one batch) at step 90: 279.9050, Accuracy: 0.7365\n",
      "Training loss (for one batch) at step 100: 267.7392, Accuracy: 0.7442\n",
      "Training loss (for one batch) at step 110: 271.3463, Accuracy: 0.7460\n",
      "---- Training ----\n",
      "Training loss: 95.5814\n",
      "Training acc over epoch: 0.7454\n",
      "---- Validation ----\n",
      "Validation loss: 43.4983\n",
      "Validation acc: 0.6787\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 336.6261, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 304.2574, Accuracy: 0.5945\n",
      "Training loss (for one batch) at step 20: 261.5326, Accuracy: 0.6522\n",
      "Training loss (for one batch) at step 30: 254.7664, Accuracy: 0.7001\n",
      "Training loss (for one batch) at step 40: 260.1371, Accuracy: 0.7355\n",
      "Training loss (for one batch) at step 50: 242.3527, Accuracy: 0.7569\n",
      "Training loss (for one batch) at step 60: 257.8802, Accuracy: 0.7706\n",
      "Training loss (for one batch) at step 70: 285.3277, Accuracy: 0.7558\n",
      "Training loss (for one batch) at step 80: 312.6486, Accuracy: 0.7366\n",
      "Training loss (for one batch) at step 90: 289.6877, Accuracy: 0.7324\n",
      "Training loss (for one batch) at step 100: 274.9932, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 110: 279.8267, Accuracy: 0.7437\n",
      "---- Training ----\n",
      "Training loss: 96.6224\n",
      "Training acc over epoch: 0.7417\n",
      "---- Validation ----\n",
      "Validation loss: 36.7058\n",
      "Validation acc: 0.6803\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 303.8800, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 285.1106, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 20: 259.1791, Accuracy: 0.6436\n",
      "Training loss (for one batch) at step 30: 274.2916, Accuracy: 0.6993\n",
      "Training loss (for one batch) at step 40: 261.4137, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 50: 249.7073, Accuracy: 0.7551\n",
      "Training loss (for one batch) at step 60: 262.5007, Accuracy: 0.7682\n",
      "Training loss (for one batch) at step 70: 302.7216, Accuracy: 0.7552\n",
      "Training loss (for one batch) at step 80: 311.6856, Accuracy: 0.7384\n",
      "Training loss (for one batch) at step 90: 283.4343, Accuracy: 0.7358\n",
      "Training loss (for one batch) at step 100: 266.5966, Accuracy: 0.7447\n",
      "Training loss (for one batch) at step 110: 275.4354, Accuracy: 0.7474\n",
      "---- Training ----\n",
      "Training loss: 100.9426\n",
      "Training acc over epoch: 0.7457\n",
      "---- Validation ----\n",
      "Validation loss: 29.3302\n",
      "Validation acc: 0.6730\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 323.3008, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 292.1708, Accuracy: 0.5817\n",
      "Training loss (for one batch) at step 20: 256.2303, Accuracy: 0.6492\n",
      "Training loss (for one batch) at step 30: 262.1586, Accuracy: 0.6978\n",
      "Training loss (for one batch) at step 40: 257.2828, Accuracy: 0.7309\n",
      "Training loss (for one batch) at step 50: 255.1626, Accuracy: 0.7541\n",
      "Training loss (for one batch) at step 60: 273.9167, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 70: 290.9081, Accuracy: 0.7534\n",
      "Training loss (for one batch) at step 80: 276.9671, Accuracy: 0.7349\n",
      "Training loss (for one batch) at step 90: 255.9822, Accuracy: 0.7321\n",
      "Training loss (for one batch) at step 100: 255.8515, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 110: 269.1393, Accuracy: 0.7437\n",
      "---- Training ----\n",
      "Training loss: 91.8129\n",
      "Training acc over epoch: 0.7426\n",
      "---- Validation ----\n",
      "Validation loss: 58.5404\n",
      "Validation acc: 0.6757\n",
      "Time taken: 18.14s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 322.1305, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 310.4363, Accuracy: 0.5732\n",
      "Training loss (for one batch) at step 20: 272.8647, Accuracy: 0.6477\n",
      "Training loss (for one batch) at step 30: 267.0618, Accuracy: 0.7039\n",
      "Training loss (for one batch) at step 40: 258.0116, Accuracy: 0.7395\n",
      "Training loss (for one batch) at step 50: 245.7933, Accuracy: 0.7601\n",
      "Training loss (for one batch) at step 60: 268.4530, Accuracy: 0.7700\n",
      "Training loss (for one batch) at step 70: 291.8896, Accuracy: 0.7556\n",
      "Training loss (for one batch) at step 80: 272.5962, Accuracy: 0.7391\n",
      "Training loss (for one batch) at step 90: 261.2371, Accuracy: 0.7371\n",
      "Training loss (for one batch) at step 100: 268.2643, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 110: 274.4591, Accuracy: 0.7473\n",
      "---- Training ----\n",
      "Training loss: 91.2660\n",
      "Training acc over epoch: 0.7459\n",
      "---- Validation ----\n",
      "Validation loss: 41.3522\n",
      "Validation acc: 0.6875\n",
      "Time taken: 18.11s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 297.9927, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 306.1005, Accuracy: 0.5866\n",
      "Training loss (for one batch) at step 20: 257.9218, Accuracy: 0.6496\n",
      "Training loss (for one batch) at step 30: 250.7020, Accuracy: 0.7082\n",
      "Training loss (for one batch) at step 40: 248.4548, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 50: 257.2954, Accuracy: 0.7619\n",
      "Training loss (for one batch) at step 60: 283.9276, Accuracy: 0.7705\n",
      "Training loss (for one batch) at step 70: 295.8660, Accuracy: 0.7558\n",
      "Training loss (for one batch) at step 80: 288.6748, Accuracy: 0.7383\n",
      "Training loss (for one batch) at step 90: 269.5658, Accuracy: 0.7327\n",
      "Training loss (for one batch) at step 100: 265.0572, Accuracy: 0.7414\n",
      "Training loss (for one batch) at step 110: 268.2136, Accuracy: 0.7436\n",
      "---- Training ----\n",
      "Training loss: 93.2595\n",
      "Training acc over epoch: 0.7420\n",
      "---- Validation ----\n",
      "Validation loss: 41.1958\n",
      "Validation acc: 0.6926\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 293.8212, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 282.5918, Accuracy: 0.5895\n",
      "Training loss (for one batch) at step 20: 271.4612, Accuracy: 0.6477\n",
      "Training loss (for one batch) at step 30: 273.5898, Accuracy: 0.7044\n",
      "Training loss (for one batch) at step 40: 253.2301, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 50: 256.6041, Accuracy: 0.7598\n",
      "Training loss (for one batch) at step 60: 258.3581, Accuracy: 0.7707\n",
      "Training loss (for one batch) at step 70: 286.2978, Accuracy: 0.7576\n",
      "Training loss (for one batch) at step 80: 285.5204, Accuracy: 0.7392\n",
      "Training loss (for one batch) at step 90: 267.5554, Accuracy: 0.7380\n",
      "Training loss (for one batch) at step 100: 254.6432, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 110: 254.0948, Accuracy: 0.7461\n",
      "---- Training ----\n",
      "Training loss: 96.1490\n",
      "Training acc over epoch: 0.7454\n",
      "---- Validation ----\n",
      "Validation loss: 37.6507\n",
      "Validation acc: 0.6934\n",
      "Time taken: 18.06s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 297.1882, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 304.1988, Accuracy: 0.5689\n",
      "Training loss (for one batch) at step 20: 258.0792, Accuracy: 0.6481\n",
      "Training loss (for one batch) at step 30: 258.4738, Accuracy: 0.7019\n",
      "Training loss (for one batch) at step 40: 238.5735, Accuracy: 0.7376\n",
      "Training loss (for one batch) at step 50: 250.7787, Accuracy: 0.7589\n",
      "Training loss (for one batch) at step 60: 259.6043, Accuracy: 0.7704\n",
      "Training loss (for one batch) at step 70: 277.5020, Accuracy: 0.7548\n",
      "Training loss (for one batch) at step 80: 289.4103, Accuracy: 0.7347\n",
      "Training loss (for one batch) at step 90: 271.1290, Accuracy: 0.7329\n",
      "Training loss (for one batch) at step 100: 242.0009, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 110: 283.8973, Accuracy: 0.7441\n",
      "---- Training ----\n",
      "Training loss: 86.2200\n",
      "Training acc over epoch: 0.7423\n",
      "---- Validation ----\n",
      "Validation loss: 31.5343\n",
      "Validation acc: 0.6827\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 314.6202, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 310.6665, Accuracy: 0.5710\n",
      "Training loss (for one batch) at step 20: 264.1341, Accuracy: 0.6417\n",
      "Training loss (for one batch) at step 30: 255.7527, Accuracy: 0.7019\n",
      "Training loss (for one batch) at step 40: 258.9349, Accuracy: 0.7365\n",
      "Training loss (for one batch) at step 50: 243.8345, Accuracy: 0.7547\n",
      "Training loss (for one batch) at step 60: 261.6107, Accuracy: 0.7673\n",
      "Training loss (for one batch) at step 70: 315.4879, Accuracy: 0.7552\n",
      "Training loss (for one batch) at step 80: 300.3745, Accuracy: 0.7367\n",
      "Training loss (for one batch) at step 90: 264.1093, Accuracy: 0.7346\n",
      "Training loss (for one batch) at step 100: 260.5432, Accuracy: 0.7423\n",
      "Training loss (for one batch) at step 110: 267.9050, Accuracy: 0.7461\n",
      "---- Training ----\n",
      "Training loss: 87.8268\n",
      "Training acc over epoch: 0.7440\n",
      "---- Validation ----\n",
      "Validation loss: 45.3958\n",
      "Validation acc: 0.6878\n",
      "Time taken: 18.14s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 294.3781, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 272.0781, Accuracy: 0.5909\n",
      "Training loss (for one batch) at step 20: 258.3133, Accuracy: 0.6525\n",
      "Training loss (for one batch) at step 30: 246.0431, Accuracy: 0.7021\n",
      "Training loss (for one batch) at step 40: 226.2047, Accuracy: 0.7399\n",
      "Training loss (for one batch) at step 50: 242.9589, Accuracy: 0.7615\n",
      "Training loss (for one batch) at step 60: 256.7179, Accuracy: 0.7718\n",
      "Training loss (for one batch) at step 70: 282.5248, Accuracy: 0.7568\n",
      "Training loss (for one batch) at step 80: 294.3885, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 90: 252.7389, Accuracy: 0.7359\n",
      "Training loss (for one batch) at step 100: 254.6742, Accuracy: 0.7434\n",
      "Training loss (for one batch) at step 110: 267.3833, Accuracy: 0.7466\n",
      "---- Training ----\n",
      "Training loss: 93.4951\n",
      "Training acc over epoch: 0.7448\n",
      "---- Validation ----\n",
      "Validation loss: 46.7647\n",
      "Validation acc: 0.6889\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 328.3976, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 287.1467, Accuracy: 0.5668\n",
      "Training loss (for one batch) at step 20: 268.2081, Accuracy: 0.6458\n",
      "Training loss (for one batch) at step 30: 285.3760, Accuracy: 0.7041\n",
      "Training loss (for one batch) at step 40: 228.2689, Accuracy: 0.7355\n",
      "Training loss (for one batch) at step 50: 232.0428, Accuracy: 0.7595\n",
      "Training loss (for one batch) at step 60: 256.1522, Accuracy: 0.7709\n",
      "Training loss (for one batch) at step 70: 273.3411, Accuracy: 0.7556\n",
      "Training loss (for one batch) at step 80: 294.1026, Accuracy: 0.7373\n",
      "Training loss (for one batch) at step 90: 280.9667, Accuracy: 0.7328\n",
      "Training loss (for one batch) at step 100: 252.2258, Accuracy: 0.7415\n",
      "Training loss (for one batch) at step 110: 258.8657, Accuracy: 0.7458\n",
      "---- Training ----\n",
      "Training loss: 80.5753\n",
      "Training acc over epoch: 0.7433\n",
      "---- Validation ----\n",
      "Validation loss: 38.6101\n",
      "Validation acc: 0.6994\n",
      "Time taken: 18.11s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 303.6317, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 299.4503, Accuracy: 0.5710\n",
      "Training loss (for one batch) at step 20: 240.7332, Accuracy: 0.6376\n",
      "Training loss (for one batch) at step 30: 278.8052, Accuracy: 0.7014\n",
      "Training loss (for one batch) at step 40: 241.2958, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 50: 245.5802, Accuracy: 0.7569\n",
      "Training loss (for one batch) at step 60: 253.0746, Accuracy: 0.7696\n",
      "Training loss (for one batch) at step 70: 273.4936, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 80: 268.4773, Accuracy: 0.7371\n",
      "Training loss (for one batch) at step 90: 243.2809, Accuracy: 0.7359\n",
      "Training loss (for one batch) at step 100: 235.0917, Accuracy: 0.7430\n",
      "Training loss (for one batch) at step 110: 267.4965, Accuracy: 0.7454\n",
      "---- Training ----\n",
      "Training loss: 92.6194\n",
      "Training acc over epoch: 0.7440\n",
      "---- Validation ----\n",
      "Validation loss: 37.8305\n",
      "Validation acc: 0.6937\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 283.4732, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 273.4992, Accuracy: 0.5845\n",
      "Training loss (for one batch) at step 20: 246.1567, Accuracy: 0.6469\n",
      "Training loss (for one batch) at step 30: 250.4653, Accuracy: 0.7049\n",
      "Training loss (for one batch) at step 40: 239.8485, Accuracy: 0.7365\n",
      "Training loss (for one batch) at step 50: 232.3038, Accuracy: 0.7577\n",
      "Training loss (for one batch) at step 60: 267.0692, Accuracy: 0.7687\n",
      "Training loss (for one batch) at step 70: 297.7450, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 80: 282.3038, Accuracy: 0.7366\n",
      "Training loss (for one batch) at step 90: 252.6550, Accuracy: 0.7345\n",
      "Training loss (for one batch) at step 100: 243.1965, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 110: 274.9308, Accuracy: 0.7444\n",
      "---- Training ----\n",
      "Training loss: 78.8792\n",
      "Training acc over epoch: 0.7421\n",
      "---- Validation ----\n",
      "Validation loss: 39.4375\n",
      "Validation acc: 0.6875\n",
      "Time taken: 17.89s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 299.3896, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 275.8232, Accuracy: 0.5661\n",
      "Training loss (for one batch) at step 20: 254.9364, Accuracy: 0.6414\n",
      "Training loss (for one batch) at step 30: 254.9932, Accuracy: 0.6963\n",
      "Training loss (for one batch) at step 40: 238.2741, Accuracy: 0.7338\n",
      "Training loss (for one batch) at step 50: 248.2536, Accuracy: 0.7561\n",
      "Training loss (for one batch) at step 60: 259.5765, Accuracy: 0.7682\n",
      "Training loss (for one batch) at step 70: 260.2430, Accuracy: 0.7534\n",
      "Training loss (for one batch) at step 80: 277.4265, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 90: 253.8930, Accuracy: 0.7333\n",
      "Training loss (for one batch) at step 100: 244.7553, Accuracy: 0.7420\n",
      "Training loss (for one batch) at step 110: 255.9536, Accuracy: 0.7439\n",
      "---- Training ----\n",
      "Training loss: 86.0468\n",
      "Training acc over epoch: 0.7423\n",
      "---- Validation ----\n",
      "Validation loss: 57.2929\n",
      "Validation acc: 0.6835\n",
      "Time taken: 18.10s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 314.1424, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 277.9846, Accuracy: 0.5866\n",
      "Training loss (for one batch) at step 20: 240.0362, Accuracy: 0.6510\n",
      "Training loss (for one batch) at step 30: 235.6055, Accuracy: 0.7049\n",
      "Training loss (for one batch) at step 40: 242.2368, Accuracy: 0.7359\n",
      "Training loss (for one batch) at step 50: 234.1078, Accuracy: 0.7589\n",
      "Training loss (for one batch) at step 60: 256.6858, Accuracy: 0.7697\n",
      "Training loss (for one batch) at step 70: 271.5934, Accuracy: 0.7566\n",
      "Training loss (for one batch) at step 80: 266.2751, Accuracy: 0.7381\n",
      "Training loss (for one batch) at step 90: 250.7452, Accuracy: 0.7340\n",
      "Training loss (for one batch) at step 100: 235.3285, Accuracy: 0.7430\n",
      "Training loss (for one batch) at step 110: 259.2957, Accuracy: 0.7464\n",
      "---- Training ----\n",
      "Training loss: 84.2069\n",
      "Training acc over epoch: 0.7450\n",
      "---- Validation ----\n",
      "Validation loss: 29.4722\n",
      "Validation acc: 0.6857\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 285.6259, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 272.3867, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 20: 261.7605, Accuracy: 0.6488\n",
      "Training loss (for one batch) at step 30: 261.3526, Accuracy: 0.7011\n",
      "Training loss (for one batch) at step 40: 258.5201, Accuracy: 0.7334\n",
      "Training loss (for one batch) at step 50: 234.8916, Accuracy: 0.7575\n",
      "Training loss (for one batch) at step 60: 249.2440, Accuracy: 0.7679\n",
      "Training loss (for one batch) at step 70: 283.0882, Accuracy: 0.7530\n",
      "Training loss (for one batch) at step 80: 270.7507, Accuracy: 0.7361\n",
      "Training loss (for one batch) at step 90: 263.6819, Accuracy: 0.7339\n",
      "Training loss (for one batch) at step 100: 249.8926, Accuracy: 0.7418\n",
      "Training loss (for one batch) at step 110: 268.7972, Accuracy: 0.7457\n",
      "---- Training ----\n",
      "Training loss: 110.1659\n",
      "Training acc over epoch: 0.7431\n",
      "---- Validation ----\n",
      "Validation loss: 46.7033\n",
      "Validation acc: 0.6900\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 283.7037, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 310.7492, Accuracy: 0.5710\n",
      "Training loss (for one batch) at step 20: 243.0588, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 30: 233.1916, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 40: 242.5117, Accuracy: 0.7346\n",
      "Training loss (for one batch) at step 50: 235.7285, Accuracy: 0.7589\n",
      "Training loss (for one batch) at step 60: 265.7158, Accuracy: 0.7673\n",
      "Training loss (for one batch) at step 70: 290.3942, Accuracy: 0.7499\n",
      "Training loss (for one batch) at step 80: 260.9955, Accuracy: 0.7339\n",
      "Training loss (for one batch) at step 90: 266.7946, Accuracy: 0.7321\n",
      "Training loss (for one batch) at step 100: 220.4068, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 110: 271.5555, Accuracy: 0.7437\n",
      "---- Training ----\n",
      "Training loss: 89.0229\n",
      "Training acc over epoch: 0.7423\n",
      "---- Validation ----\n",
      "Validation loss: 49.6744\n",
      "Validation acc: 0.6857\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 274.6882, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 313.1259, Accuracy: 0.5767\n",
      "Training loss (for one batch) at step 20: 239.3200, Accuracy: 0.6510\n",
      "Training loss (for one batch) at step 30: 234.7863, Accuracy: 0.7039\n",
      "Training loss (for one batch) at step 40: 240.5226, Accuracy: 0.7374\n",
      "Training loss (for one batch) at step 50: 235.6370, Accuracy: 0.7584\n",
      "Training loss (for one batch) at step 60: 271.1364, Accuracy: 0.7693\n",
      "Training loss (for one batch) at step 70: 282.0175, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 80: 277.4274, Accuracy: 0.7363\n",
      "Training loss (for one batch) at step 90: 235.7904, Accuracy: 0.7322\n",
      "Training loss (for one batch) at step 100: 237.4993, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 110: 258.3508, Accuracy: 0.7439\n",
      "---- Training ----\n",
      "Training loss: 97.5441\n",
      "Training acc over epoch: 0.7429\n",
      "---- Validation ----\n",
      "Validation loss: 51.4057\n",
      "Validation acc: 0.6771\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 293.5628, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 273.5640, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 20: 251.2196, Accuracy: 0.6410\n",
      "Training loss (for one batch) at step 30: 235.7779, Accuracy: 0.7009\n",
      "Training loss (for one batch) at step 40: 237.5705, Accuracy: 0.7363\n",
      "Training loss (for one batch) at step 50: 229.4784, Accuracy: 0.7592\n",
      "Training loss (for one batch) at step 60: 248.5119, Accuracy: 0.7672\n",
      "Training loss (for one batch) at step 70: 276.7556, Accuracy: 0.7554\n",
      "Training loss (for one batch) at step 80: 270.6762, Accuracy: 0.7377\n",
      "Training loss (for one batch) at step 90: 265.7437, Accuracy: 0.7339\n",
      "Training loss (for one batch) at step 100: 251.7309, Accuracy: 0.7408\n",
      "Training loss (for one batch) at step 110: 255.0679, Accuracy: 0.7428\n",
      "---- Training ----\n",
      "Training loss: 91.0537\n",
      "Training acc over epoch: 0.7416\n",
      "---- Validation ----\n",
      "Validation loss: 41.7528\n",
      "Validation acc: 0.6891\n",
      "Time taken: 17.89s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 285.0463, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 286.6040, Accuracy: 0.5732\n",
      "Training loss (for one batch) at step 20: 245.4203, Accuracy: 0.6432\n",
      "Training loss (for one batch) at step 30: 232.7859, Accuracy: 0.7014\n",
      "Training loss (for one batch) at step 40: 236.3261, Accuracy: 0.7338\n",
      "Training loss (for one batch) at step 50: 228.1999, Accuracy: 0.7584\n",
      "Training loss (for one batch) at step 60: 254.2390, Accuracy: 0.7697\n",
      "Training loss (for one batch) at step 70: 279.0651, Accuracy: 0.7537\n",
      "Training loss (for one batch) at step 80: 248.8044, Accuracy: 0.7338\n",
      "Training loss (for one batch) at step 90: 256.7817, Accuracy: 0.7316\n",
      "Training loss (for one batch) at step 100: 243.8127, Accuracy: 0.7385\n",
      "Training loss (for one batch) at step 110: 242.3580, Accuracy: 0.7423\n",
      "---- Training ----\n",
      "Training loss: 89.7378\n",
      "Training acc over epoch: 0.7417\n",
      "---- Validation ----\n",
      "Validation loss: 45.8772\n",
      "Validation acc: 0.6848\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 281.4136, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 303.4765, Accuracy: 0.5589\n",
      "Training loss (for one batch) at step 20: 237.9846, Accuracy: 0.6306\n",
      "Training loss (for one batch) at step 30: 226.1493, Accuracy: 0.6935\n",
      "Training loss (for one batch) at step 40: 241.8910, Accuracy: 0.7327\n",
      "Training loss (for one batch) at step 50: 229.9904, Accuracy: 0.7535\n",
      "Training loss (for one batch) at step 60: 242.0904, Accuracy: 0.7637\n",
      "Training loss (for one batch) at step 70: 264.6233, Accuracy: 0.7499\n",
      "Training loss (for one batch) at step 80: 276.4667, Accuracy: 0.7318\n",
      "Training loss (for one batch) at step 90: 250.8253, Accuracy: 0.7295\n",
      "Training loss (for one batch) at step 100: 242.6978, Accuracy: 0.7374\n",
      "Training loss (for one batch) at step 110: 266.8287, Accuracy: 0.7397\n",
      "---- Training ----\n",
      "Training loss: 75.8086\n",
      "Training acc over epoch: 0.7394\n",
      "---- Validation ----\n",
      "Validation loss: 33.9649\n",
      "Validation acc: 0.6878\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 295.2195, Accuracy: 0.4922\n",
      "Training loss (for one batch) at step 10: 281.5051, Accuracy: 0.5611\n",
      "Training loss (for one batch) at step 20: 233.7424, Accuracy: 0.6399\n",
      "Training loss (for one batch) at step 30: 237.9650, Accuracy: 0.7009\n",
      "Training loss (for one batch) at step 40: 236.0430, Accuracy: 0.7334\n",
      "Training loss (for one batch) at step 50: 234.9387, Accuracy: 0.7552\n",
      "Training loss (for one batch) at step 60: 252.5164, Accuracy: 0.7659\n",
      "Training loss (for one batch) at step 70: 259.5708, Accuracy: 0.7524\n",
      "Training loss (for one batch) at step 80: 272.2090, Accuracy: 0.7346\n",
      "Training loss (for one batch) at step 90: 238.4891, Accuracy: 0.7335\n",
      "Training loss (for one batch) at step 100: 247.2480, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 110: 255.3088, Accuracy: 0.7441\n",
      "---- Training ----\n",
      "Training loss: 87.4246\n",
      "Training acc over epoch: 0.7423\n",
      "---- Validation ----\n",
      "Validation loss: 34.3258\n",
      "Validation acc: 0.6929\n",
      "Time taken: 17.87s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 305.5810, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 280.7851, Accuracy: 0.5639\n",
      "Training loss (for one batch) at step 20: 241.8580, Accuracy: 0.6354\n",
      "Training loss (for one batch) at step 30: 239.7863, Accuracy: 0.6935\n",
      "Training loss (for one batch) at step 40: 234.3663, Accuracy: 0.7300\n",
      "Training loss (for one batch) at step 50: 243.4241, Accuracy: 0.7546\n",
      "Training loss (for one batch) at step 60: 246.7332, Accuracy: 0.7668\n",
      "Training loss (for one batch) at step 70: 298.0425, Accuracy: 0.7537\n",
      "Training loss (for one batch) at step 80: 282.7244, Accuracy: 0.7328\n",
      "Training loss (for one batch) at step 90: 239.2988, Accuracy: 0.7319\n",
      "Training loss (for one batch) at step 100: 246.7887, Accuracy: 0.7404\n",
      "Training loss (for one batch) at step 110: 246.1331, Accuracy: 0.7439\n",
      "---- Training ----\n",
      "Training loss: 87.4634\n",
      "Training acc over epoch: 0.7416\n",
      "---- Validation ----\n",
      "Validation loss: 59.1115\n",
      "Validation acc: 0.6625\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 294.4995, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 284.5107, Accuracy: 0.5611\n",
      "Training loss (for one batch) at step 20: 242.0238, Accuracy: 0.6403\n",
      "Training loss (for one batch) at step 30: 239.2169, Accuracy: 0.6966\n",
      "Training loss (for one batch) at step 40: 230.0911, Accuracy: 0.7315\n",
      "Training loss (for one batch) at step 50: 245.6299, Accuracy: 0.7570\n",
      "Training loss (for one batch) at step 60: 274.8079, Accuracy: 0.7665\n",
      "Training loss (for one batch) at step 70: 267.6530, Accuracy: 0.7525\n",
      "Training loss (for one batch) at step 80: 264.3918, Accuracy: 0.7304\n",
      "Training loss (for one batch) at step 90: 241.2708, Accuracy: 0.7294\n",
      "Training loss (for one batch) at step 100: 245.7113, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 110: 247.6961, Accuracy: 0.7424\n",
      "---- Training ----\n",
      "Training loss: 69.2080\n",
      "Training acc over epoch: 0.7399\n",
      "---- Validation ----\n",
      "Validation loss: 52.1703\n",
      "Validation acc: 0.6924\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 309.4761, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 307.7412, Accuracy: 0.5852\n",
      "Training loss (for one batch) at step 20: 238.7654, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 30: 245.9013, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 40: 237.7057, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 50: 235.5756, Accuracy: 0.7609\n",
      "Training loss (for one batch) at step 60: 258.3214, Accuracy: 0.7687\n",
      "Training loss (for one batch) at step 70: 278.8842, Accuracy: 0.7558\n",
      "Training loss (for one batch) at step 80: 281.9483, Accuracy: 0.7367\n",
      "Training loss (for one batch) at step 90: 253.5552, Accuracy: 0.7319\n",
      "Training loss (for one batch) at step 100: 243.4571, Accuracy: 0.7392\n",
      "Training loss (for one batch) at step 110: 254.1934, Accuracy: 0.7431\n",
      "---- Training ----\n",
      "Training loss: 95.4959\n",
      "Training acc over epoch: 0.7407\n",
      "---- Validation ----\n",
      "Validation loss: 74.3442\n",
      "Validation acc: 0.6945\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 282.0179, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 262.6240, Accuracy: 0.5575\n",
      "Training loss (for one batch) at step 20: 246.3426, Accuracy: 0.6376\n",
      "Training loss (for one batch) at step 30: 242.3797, Accuracy: 0.6951\n",
      "Training loss (for one batch) at step 40: 233.8284, Accuracy: 0.7294\n",
      "Training loss (for one batch) at step 50: 220.6564, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 60: 230.8317, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 70: 275.5970, Accuracy: 0.7512\n",
      "Training loss (for one batch) at step 80: 275.6655, Accuracy: 0.7324\n",
      "Training loss (for one batch) at step 90: 248.6035, Accuracy: 0.7290\n",
      "Training loss (for one batch) at step 100: 251.1032, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 110: 264.3093, Accuracy: 0.7420\n",
      "---- Training ----\n",
      "Training loss: 72.0554\n",
      "Training acc over epoch: 0.7402\n",
      "---- Validation ----\n",
      "Validation loss: 34.4465\n",
      "Validation acc: 0.6811\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 265.8550, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 271.0053, Accuracy: 0.5675\n",
      "Training loss (for one batch) at step 20: 226.6228, Accuracy: 0.6421\n",
      "Training loss (for one batch) at step 30: 251.7069, Accuracy: 0.7006\n",
      "Training loss (for one batch) at step 40: 214.9770, Accuracy: 0.7334\n",
      "Training loss (for one batch) at step 50: 221.3165, Accuracy: 0.7590\n",
      "Training loss (for one batch) at step 60: 234.8333, Accuracy: 0.7684\n",
      "Training loss (for one batch) at step 70: 268.5005, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 80: 258.3396, Accuracy: 0.7332\n",
      "Training loss (for one batch) at step 90: 248.9751, Accuracy: 0.7304\n",
      "Training loss (for one batch) at step 100: 233.3921, Accuracy: 0.7399\n",
      "Training loss (for one batch) at step 110: 239.6943, Accuracy: 0.7428\n",
      "---- Training ----\n",
      "Training loss: 78.3450\n",
      "Training acc over epoch: 0.7406\n",
      "---- Validation ----\n",
      "Validation loss: 80.5267\n",
      "Validation acc: 0.6819\n",
      "Time taken: 18.09s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 294.1859, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 295.7042, Accuracy: 0.5689\n",
      "Training loss (for one batch) at step 20: 249.9026, Accuracy: 0.6380\n",
      "Training loss (for one batch) at step 30: 231.2395, Accuracy: 0.6973\n",
      "Training loss (for one batch) at step 40: 229.6555, Accuracy: 0.7315\n",
      "Training loss (for one batch) at step 50: 240.9147, Accuracy: 0.7549\n",
      "Training loss (for one batch) at step 60: 243.5538, Accuracy: 0.7664\n",
      "Training loss (for one batch) at step 70: 251.7569, Accuracy: 0.7520\n",
      "Training loss (for one batch) at step 80: 276.8836, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 90: 257.5078, Accuracy: 0.7300\n",
      "Training loss (for one batch) at step 100: 246.5664, Accuracy: 0.7390\n",
      "Training loss (for one batch) at step 110: 246.2580, Accuracy: 0.7416\n",
      "---- Training ----\n",
      "Training loss: 95.0808\n",
      "Training acc over epoch: 0.7402\n",
      "---- Validation ----\n",
      "Validation loss: 58.8402\n",
      "Validation acc: 0.6722\n",
      "Time taken: 18.22s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 296.5462, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 265.7381, Accuracy: 0.5682\n",
      "Training loss (for one batch) at step 20: 241.9274, Accuracy: 0.6354\n",
      "Training loss (for one batch) at step 30: 226.4177, Accuracy: 0.6961\n",
      "Training loss (for one batch) at step 40: 257.7669, Accuracy: 0.7325\n",
      "Training loss (for one batch) at step 50: 234.0056, Accuracy: 0.7535\n",
      "Training loss (for one batch) at step 60: 271.6477, Accuracy: 0.7641\n",
      "Training loss (for one batch) at step 70: 252.1110, Accuracy: 0.7496\n",
      "Training loss (for one batch) at step 80: 277.3274, Accuracy: 0.7310\n",
      "Training loss (for one batch) at step 90: 235.2462, Accuracy: 0.7281\n",
      "Training loss (for one batch) at step 100: 238.1906, Accuracy: 0.7369\n",
      "Training loss (for one batch) at step 110: 260.9415, Accuracy: 0.7393\n",
      "---- Training ----\n",
      "Training loss: 76.3196\n",
      "Training acc over epoch: 0.7383\n",
      "---- Validation ----\n",
      "Validation loss: 52.8873\n",
      "Validation acc: 0.6840\n",
      "Time taken: 17.88s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 276.0002, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 278.3455, Accuracy: 0.5589\n",
      "Training loss (for one batch) at step 20: 254.1975, Accuracy: 0.6421\n",
      "Training loss (for one batch) at step 30: 239.2872, Accuracy: 0.7009\n",
      "Training loss (for one batch) at step 40: 259.1494, Accuracy: 0.7353\n",
      "Training loss (for one batch) at step 50: 230.4866, Accuracy: 0.7584\n",
      "Training loss (for one batch) at step 60: 257.6346, Accuracy: 0.7691\n",
      "Training loss (for one batch) at step 70: 261.0026, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 80: 258.8334, Accuracy: 0.7357\n",
      "Training loss (for one batch) at step 90: 236.8181, Accuracy: 0.7322\n",
      "Training loss (for one batch) at step 100: 237.6814, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 110: 241.9022, Accuracy: 0.7437\n",
      "---- Training ----\n",
      "Training loss: 82.9883\n",
      "Training acc over epoch: 0.7419\n",
      "---- Validation ----\n",
      "Validation loss: 41.0283\n",
      "Validation acc: 0.6835\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 266.6599, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 275.9619, Accuracy: 0.5668\n",
      "Training loss (for one batch) at step 20: 250.5344, Accuracy: 0.6414\n",
      "Training loss (for one batch) at step 30: 225.7507, Accuracy: 0.6961\n",
      "Training loss (for one batch) at step 40: 235.6221, Accuracy: 0.7332\n",
      "Training loss (for one batch) at step 50: 234.3696, Accuracy: 0.7593\n",
      "Training loss (for one batch) at step 60: 248.6567, Accuracy: 0.7683\n",
      "Training loss (for one batch) at step 70: 255.0343, Accuracy: 0.7526\n",
      "Training loss (for one batch) at step 80: 254.5258, Accuracy: 0.7333\n",
      "Training loss (for one batch) at step 90: 246.8832, Accuracy: 0.7288\n",
      "Training loss (for one batch) at step 100: 230.6004, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 110: 247.3655, Accuracy: 0.7423\n",
      "---- Training ----\n",
      "Training loss: 76.6974\n",
      "Training acc over epoch: 0.7407\n",
      "---- Validation ----\n",
      "Validation loss: 49.4535\n",
      "Validation acc: 0.6698\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 267.7571, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 262.6904, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 20: 234.0729, Accuracy: 0.6306\n",
      "Training loss (for one batch) at step 30: 244.0417, Accuracy: 0.6946\n",
      "Training loss (for one batch) at step 40: 241.3341, Accuracy: 0.7330\n",
      "Training loss (for one batch) at step 50: 218.1543, Accuracy: 0.7572\n",
      "Training loss (for one batch) at step 60: 238.8057, Accuracy: 0.7669\n",
      "Training loss (for one batch) at step 70: 259.7919, Accuracy: 0.7533\n",
      "Training loss (for one batch) at step 80: 260.7364, Accuracy: 0.7345\n",
      "Training loss (for one batch) at step 90: 246.2911, Accuracy: 0.7316\n",
      "Training loss (for one batch) at step 100: 247.6988, Accuracy: 0.7388\n",
      "Training loss (for one batch) at step 110: 229.9102, Accuracy: 0.7422\n",
      "---- Training ----\n",
      "Training loss: 78.5451\n",
      "Training acc over epoch: 0.7407\n",
      "---- Validation ----\n",
      "Validation loss: 47.1604\n",
      "Validation acc: 0.6918\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 274.9728, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 266.8153, Accuracy: 0.5668\n",
      "Training loss (for one batch) at step 20: 244.3999, Accuracy: 0.6388\n",
      "Training loss (for one batch) at step 30: 251.1800, Accuracy: 0.7014\n",
      "Training loss (for one batch) at step 40: 229.0267, Accuracy: 0.7389\n",
      "Training loss (for one batch) at step 50: 233.3134, Accuracy: 0.7592\n",
      "Training loss (for one batch) at step 60: 240.5788, Accuracy: 0.7678\n",
      "Training loss (for one batch) at step 70: 255.1921, Accuracy: 0.7522\n",
      "Training loss (for one batch) at step 80: 275.1123, Accuracy: 0.7345\n",
      "Training loss (for one batch) at step 90: 231.3898, Accuracy: 0.7323\n",
      "Training loss (for one batch) at step 100: 231.3710, Accuracy: 0.7396\n",
      "Training loss (for one batch) at step 110: 241.5333, Accuracy: 0.7431\n",
      "---- Training ----\n",
      "Training loss: 79.0159\n",
      "Training acc over epoch: 0.7409\n",
      "---- Validation ----\n",
      "Validation loss: 65.7439\n",
      "Validation acc: 0.6789\n",
      "Time taken: 17.86s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 268.1813, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 260.2836, Accuracy: 0.5639\n",
      "Training loss (for one batch) at step 20: 243.9486, Accuracy: 0.6395\n",
      "Training loss (for one batch) at step 30: 237.3372, Accuracy: 0.6956\n",
      "Training loss (for one batch) at step 40: 224.2977, Accuracy: 0.7338\n",
      "Training loss (for one batch) at step 50: 238.1889, Accuracy: 0.7566\n",
      "Training loss (for one batch) at step 60: 257.3026, Accuracy: 0.7663\n",
      "Training loss (for one batch) at step 70: 247.3007, Accuracy: 0.7514\n",
      "Training loss (for one batch) at step 80: 258.7944, Accuracy: 0.7314\n",
      "Training loss (for one batch) at step 90: 238.1530, Accuracy: 0.7295\n",
      "Training loss (for one batch) at step 100: 256.1844, Accuracy: 0.7388\n",
      "Training loss (for one batch) at step 110: 237.0409, Accuracy: 0.7399\n",
      "---- Training ----\n",
      "Training loss: 99.4119\n",
      "Training acc over epoch: 0.7398\n",
      "---- Validation ----\n",
      "Validation loss: 44.0594\n",
      "Validation acc: 0.6913\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 255.6325, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 265.8754, Accuracy: 0.5845\n",
      "Training loss (for one batch) at step 20: 246.6658, Accuracy: 0.6496\n",
      "Training loss (for one batch) at step 30: 240.6214, Accuracy: 0.7029\n",
      "Training loss (for one batch) at step 40: 218.6956, Accuracy: 0.7346\n",
      "Training loss (for one batch) at step 50: 241.5414, Accuracy: 0.7574\n",
      "Training loss (for one batch) at step 60: 227.2470, Accuracy: 0.7700\n",
      "Training loss (for one batch) at step 70: 269.0234, Accuracy: 0.7537\n",
      "Training loss (for one batch) at step 80: 271.5462, Accuracy: 0.7334\n",
      "Training loss (for one batch) at step 90: 229.2477, Accuracy: 0.7297\n",
      "Training loss (for one batch) at step 100: 242.9979, Accuracy: 0.7392\n",
      "Training loss (for one batch) at step 110: 253.5293, Accuracy: 0.7424\n",
      "---- Training ----\n",
      "Training loss: 82.6324\n",
      "Training acc over epoch: 0.7417\n",
      "---- Validation ----\n",
      "Validation loss: 40.5847\n",
      "Validation acc: 0.6674\n",
      "Time taken: 20.12s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 263.0512, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 253.3737, Accuracy: 0.5753\n",
      "Training loss (for one batch) at step 20: 242.1643, Accuracy: 0.6417\n",
      "Training loss (for one batch) at step 30: 214.6735, Accuracy: 0.6991\n",
      "Training loss (for one batch) at step 40: 233.8396, Accuracy: 0.7348\n",
      "Training loss (for one batch) at step 50: 228.9272, Accuracy: 0.7566\n",
      "Training loss (for one batch) at step 60: 243.1628, Accuracy: 0.7692\n",
      "Training loss (for one batch) at step 70: 274.0907, Accuracy: 0.7526\n",
      "Training loss (for one batch) at step 80: 257.4792, Accuracy: 0.7320\n",
      "Training loss (for one batch) at step 90: 245.7162, Accuracy: 0.7297\n",
      "Training loss (for one batch) at step 100: 232.5009, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 110: 253.1346, Accuracy: 0.7410\n",
      "---- Training ----\n",
      "Training loss: 74.0109\n",
      "Training acc over epoch: 0.7390\n",
      "---- Validation ----\n",
      "Validation loss: 56.0645\n",
      "Validation acc: 0.6889\n",
      "Time taken: 17.87s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 298.7214, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 283.2349, Accuracy: 0.5533\n",
      "Training loss (for one batch) at step 20: 243.0612, Accuracy: 0.6354\n",
      "Training loss (for one batch) at step 30: 243.7053, Accuracy: 0.6946\n",
      "Training loss (for one batch) at step 40: 230.0155, Accuracy: 0.7330\n",
      "Training loss (for one batch) at step 50: 227.5627, Accuracy: 0.7544\n",
      "Training loss (for one batch) at step 60: 245.1997, Accuracy: 0.7654\n",
      "Training loss (for one batch) at step 70: 256.0416, Accuracy: 0.7525\n",
      "Training loss (for one batch) at step 80: 278.1062, Accuracy: 0.7329\n",
      "Training loss (for one batch) at step 90: 236.1328, Accuracy: 0.7297\n",
      "Training loss (for one batch) at step 100: 219.4484, Accuracy: 0.7375\n",
      "Training loss (for one batch) at step 110: 242.4190, Accuracy: 0.7420\n",
      "---- Training ----\n",
      "Training loss: 73.8328\n",
      "Training acc over epoch: 0.7395\n",
      "---- Validation ----\n",
      "Validation loss: 59.5850\n",
      "Validation acc: 0.6830\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 279.5186, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 280.3720, Accuracy: 0.5710\n",
      "Training loss (for one batch) at step 20: 244.7222, Accuracy: 0.6362\n",
      "Training loss (for one batch) at step 30: 224.1921, Accuracy: 0.6923\n",
      "Training loss (for one batch) at step 40: 234.5428, Accuracy: 0.7329\n",
      "Training loss (for one batch) at step 50: 242.5277, Accuracy: 0.7586\n",
      "Training loss (for one batch) at step 60: 237.6857, Accuracy: 0.7691\n",
      "Training loss (for one batch) at step 70: 246.1440, Accuracy: 0.7562\n",
      "Training loss (for one batch) at step 80: 237.0030, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 90: 248.0767, Accuracy: 0.7306\n",
      "Training loss (for one batch) at step 100: 233.2806, Accuracy: 0.7392\n",
      "Training loss (for one batch) at step 110: 227.8817, Accuracy: 0.7426\n",
      "---- Training ----\n",
      "Training loss: 82.4901\n",
      "Training acc over epoch: 0.7406\n",
      "---- Validation ----\n",
      "Validation loss: 47.0608\n",
      "Validation acc: 0.6816\n",
      "Time taken: 17.86s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 272.0815, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 251.0725, Accuracy: 0.5774\n",
      "Training loss (for one batch) at step 20: 222.8852, Accuracy: 0.6447\n",
      "Training loss (for one batch) at step 30: 221.3313, Accuracy: 0.7009\n",
      "Training loss (for one batch) at step 40: 215.3159, Accuracy: 0.7353\n",
      "Training loss (for one batch) at step 50: 218.2918, Accuracy: 0.7575\n",
      "Training loss (for one batch) at step 60: 233.7887, Accuracy: 0.7697\n",
      "Training loss (for one batch) at step 70: 265.2099, Accuracy: 0.7508\n",
      "Training loss (for one batch) at step 80: 254.5317, Accuracy: 0.7311\n",
      "Training loss (for one batch) at step 90: 231.3546, Accuracy: 0.7298\n",
      "Training loss (for one batch) at step 100: 229.4597, Accuracy: 0.7399\n",
      "Training loss (for one batch) at step 110: 255.0263, Accuracy: 0.7432\n",
      "---- Training ----\n",
      "Training loss: 83.5288\n",
      "Training acc over epoch: 0.7404\n",
      "---- Validation ----\n",
      "Validation loss: 41.7952\n",
      "Validation acc: 0.6722\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 276.3387, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 245.0872, Accuracy: 0.5618\n",
      "Training loss (for one batch) at step 20: 247.2123, Accuracy: 0.6336\n",
      "Training loss (for one batch) at step 30: 250.7831, Accuracy: 0.6961\n",
      "Training loss (for one batch) at step 40: 219.7015, Accuracy: 0.7313\n",
      "Training loss (for one batch) at step 50: 218.2768, Accuracy: 0.7540\n",
      "Training loss (for one batch) at step 60: 238.1574, Accuracy: 0.7654\n",
      "Training loss (for one batch) at step 70: 242.9560, Accuracy: 0.7504\n",
      "Training loss (for one batch) at step 80: 271.9422, Accuracy: 0.7323\n",
      "Training loss (for one batch) at step 90: 231.9753, Accuracy: 0.7292\n",
      "Training loss (for one batch) at step 100: 231.3157, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 110: 248.1601, Accuracy: 0.7415\n",
      "---- Training ----\n",
      "Training loss: 86.2747\n",
      "Training acc over epoch: 0.7396\n",
      "---- Validation ----\n",
      "Validation loss: 44.1654\n",
      "Validation acc: 0.7004\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 263.7280, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 261.0621, Accuracy: 0.5682\n",
      "Training loss (for one batch) at step 20: 238.4970, Accuracy: 0.6350\n",
      "Training loss (for one batch) at step 30: 226.5882, Accuracy: 0.6923\n",
      "Training loss (for one batch) at step 40: 227.2632, Accuracy: 0.7306\n",
      "Training loss (for one batch) at step 50: 220.1435, Accuracy: 0.7532\n",
      "Training loss (for one batch) at step 60: 239.2637, Accuracy: 0.7622\n",
      "Training loss (for one batch) at step 70: 236.5441, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 80: 257.4069, Accuracy: 0.7308\n",
      "Training loss (for one batch) at step 90: 228.9763, Accuracy: 0.7283\n",
      "Training loss (for one batch) at step 100: 223.4008, Accuracy: 0.7365\n",
      "Training loss (for one batch) at step 110: 267.3905, Accuracy: 0.7395\n",
      "---- Training ----\n",
      "Training loss: 77.6078\n",
      "Training acc over epoch: 0.7381\n",
      "---- Validation ----\n",
      "Validation loss: 48.1897\n",
      "Validation acc: 0.6792\n",
      "Time taken: 18.12s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 283.1357, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 268.4211, Accuracy: 0.5653\n",
      "Training loss (for one batch) at step 20: 219.4780, Accuracy: 0.6481\n",
      "Training loss (for one batch) at step 30: 233.0909, Accuracy: 0.7026\n",
      "Training loss (for one batch) at step 40: 219.7374, Accuracy: 0.7338\n",
      "Training loss (for one batch) at step 50: 214.1039, Accuracy: 0.7558\n",
      "Training loss (for one batch) at step 60: 221.0749, Accuracy: 0.7668\n",
      "Training loss (for one batch) at step 70: 256.2932, Accuracy: 0.7530\n",
      "Training loss (for one batch) at step 80: 258.8146, Accuracy: 0.7322\n",
      "Training loss (for one batch) at step 90: 246.1871, Accuracy: 0.7309\n",
      "Training loss (for one batch) at step 100: 235.1280, Accuracy: 0.7380\n",
      "Training loss (for one batch) at step 110: 234.8354, Accuracy: 0.7416\n",
      "---- Training ----\n",
      "Training loss: 72.8526\n",
      "Training acc over epoch: 0.7398\n",
      "---- Validation ----\n",
      "Validation loss: 41.8454\n",
      "Validation acc: 0.6854\n",
      "Time taken: 18.06s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 253.0573, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 253.5025, Accuracy: 0.5760\n",
      "Training loss (for one batch) at step 20: 224.2238, Accuracy: 0.6373\n",
      "Training loss (for one batch) at step 30: 227.7302, Accuracy: 0.6988\n",
      "Training loss (for one batch) at step 40: 228.6647, Accuracy: 0.7346\n",
      "Training loss (for one batch) at step 50: 211.7145, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 60: 255.1819, Accuracy: 0.7634\n",
      "Training loss (for one batch) at step 70: 255.5378, Accuracy: 0.7518\n",
      "Training loss (for one batch) at step 80: 239.5411, Accuracy: 0.7321\n",
      "Training loss (for one batch) at step 90: 241.6398, Accuracy: 0.7305\n",
      "Training loss (for one batch) at step 100: 221.1436, Accuracy: 0.7392\n",
      "Training loss (for one batch) at step 110: 239.3735, Accuracy: 0.7412\n",
      "---- Training ----\n",
      "Training loss: 88.5113\n",
      "Training acc over epoch: 0.7394\n",
      "---- Validation ----\n",
      "Validation loss: 59.6691\n",
      "Validation acc: 0.6940\n",
      "Time taken: 17.83s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 257.9823, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 255.5309, Accuracy: 0.5689\n",
      "Training loss (for one batch) at step 20: 244.5792, Accuracy: 0.6343\n",
      "Training loss (for one batch) at step 30: 220.5018, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 40: 225.5830, Accuracy: 0.7321\n",
      "Training loss (for one batch) at step 50: 223.6267, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 60: 231.7315, Accuracy: 0.7655\n",
      "Training loss (for one batch) at step 70: 267.0723, Accuracy: 0.7504\n",
      "Training loss (for one batch) at step 80: 268.4522, Accuracy: 0.7318\n",
      "Training loss (for one batch) at step 90: 241.3783, Accuracy: 0.7291\n",
      "Training loss (for one batch) at step 100: 236.6359, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 110: 225.8063, Accuracy: 0.7411\n",
      "---- Training ----\n",
      "Training loss: 89.3922\n",
      "Training acc over epoch: 0.7395\n",
      "---- Validation ----\n",
      "Validation loss: 41.9518\n",
      "Validation acc: 0.6784\n",
      "Time taken: 18.09s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 269.4629, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 263.0571, Accuracy: 0.5639\n",
      "Training loss (for one batch) at step 20: 250.3971, Accuracy: 0.6324\n",
      "Training loss (for one batch) at step 30: 222.0322, Accuracy: 0.6998\n",
      "Training loss (for one batch) at step 40: 227.8006, Accuracy: 0.7346\n",
      "Training loss (for one batch) at step 50: 246.8567, Accuracy: 0.7560\n",
      "Training loss (for one batch) at step 60: 230.0543, Accuracy: 0.7652\n",
      "Training loss (for one batch) at step 70: 254.7611, Accuracy: 0.7501\n",
      "Training loss (for one batch) at step 80: 230.5176, Accuracy: 0.7308\n",
      "Training loss (for one batch) at step 90: 233.5055, Accuracy: 0.7282\n",
      "Training loss (for one batch) at step 100: 237.2140, Accuracy: 0.7365\n",
      "Training loss (for one batch) at step 110: 230.4406, Accuracy: 0.7392\n",
      "---- Training ----\n",
      "Training loss: 86.2178\n",
      "Training acc over epoch: 0.7380\n",
      "---- Validation ----\n",
      "Validation loss: 32.6743\n",
      "Validation acc: 0.6975\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 257.7142, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 263.6550, Accuracy: 0.5575\n",
      "Training loss (for one batch) at step 20: 236.5479, Accuracy: 0.6373\n",
      "Training loss (for one batch) at step 30: 241.4721, Accuracy: 0.6966\n",
      "Training loss (for one batch) at step 40: 235.7587, Accuracy: 0.7323\n",
      "Training loss (for one batch) at step 50: 218.3976, Accuracy: 0.7577\n",
      "Training loss (for one batch) at step 60: 237.1107, Accuracy: 0.7669\n",
      "Training loss (for one batch) at step 70: 264.2675, Accuracy: 0.7511\n",
      "Training loss (for one batch) at step 80: 260.2835, Accuracy: 0.7298\n",
      "Training loss (for one batch) at step 90: 230.5358, Accuracy: 0.7283\n",
      "Training loss (for one batch) at step 100: 224.8430, Accuracy: 0.7365\n",
      "Training loss (for one batch) at step 110: 244.8646, Accuracy: 0.7392\n",
      "---- Training ----\n",
      "Training loss: 75.7855\n",
      "Training acc over epoch: 0.7374\n",
      "---- Validation ----\n",
      "Validation loss: 34.3113\n",
      "Validation acc: 0.6773\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 267.7176, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 252.6041, Accuracy: 0.5788\n",
      "Training loss (for one batch) at step 20: 242.2010, Accuracy: 0.6466\n",
      "Training loss (for one batch) at step 30: 235.2730, Accuracy: 0.6991\n",
      "Training loss (for one batch) at step 40: 217.6448, Accuracy: 0.7319\n",
      "Training loss (for one batch) at step 50: 212.0274, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 60: 242.3220, Accuracy: 0.7682\n",
      "Training loss (for one batch) at step 70: 238.8305, Accuracy: 0.7541\n",
      "Training loss (for one batch) at step 80: 248.3194, Accuracy: 0.7337\n",
      "Training loss (for one batch) at step 90: 250.9072, Accuracy: 0.7300\n",
      "Training loss (for one batch) at step 100: 243.1312, Accuracy: 0.7392\n",
      "Training loss (for one batch) at step 110: 228.0376, Accuracy: 0.7413\n",
      "---- Training ----\n",
      "Training loss: 94.4970\n",
      "Training acc over epoch: 0.7394\n",
      "---- Validation ----\n",
      "Validation loss: 42.0389\n",
      "Validation acc: 0.6808\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 250.4626, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 263.6760, Accuracy: 0.5533\n",
      "Training loss (for one batch) at step 20: 230.3847, Accuracy: 0.6310\n",
      "Training loss (for one batch) at step 30: 240.1880, Accuracy: 0.6941\n",
      "Training loss (for one batch) at step 40: 226.0435, Accuracy: 0.7311\n",
      "Training loss (for one batch) at step 50: 222.2761, Accuracy: 0.7517\n",
      "Training loss (for one batch) at step 60: 238.9288, Accuracy: 0.7640\n",
      "Training loss (for one batch) at step 70: 253.9857, Accuracy: 0.7509\n",
      "Training loss (for one batch) at step 80: 243.3061, Accuracy: 0.7335\n",
      "Training loss (for one batch) at step 90: 222.2232, Accuracy: 0.7300\n",
      "Training loss (for one batch) at step 100: 257.6212, Accuracy: 0.7375\n",
      "Training loss (for one batch) at step 110: 255.8666, Accuracy: 0.7399\n",
      "---- Training ----\n",
      "Training loss: 87.4669\n",
      "Training acc over epoch: 0.7390\n",
      "---- Validation ----\n",
      "Validation loss: 50.5856\n",
      "Validation acc: 0.6655\n",
      "Time taken: 18.10s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 264.9142, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 266.4247, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 20: 254.1232, Accuracy: 0.6354\n",
      "Training loss (for one batch) at step 30: 224.4080, Accuracy: 0.6933\n",
      "Training loss (for one batch) at step 40: 244.3158, Accuracy: 0.7309\n",
      "Training loss (for one batch) at step 50: 215.0602, Accuracy: 0.7544\n",
      "Training loss (for one batch) at step 60: 222.8653, Accuracy: 0.7659\n",
      "Training loss (for one batch) at step 70: 266.5012, Accuracy: 0.7520\n",
      "Training loss (for one batch) at step 80: 257.9680, Accuracy: 0.7332\n",
      "Training loss (for one batch) at step 90: 228.9636, Accuracy: 0.7303\n",
      "Training loss (for one batch) at step 100: 246.6586, Accuracy: 0.7379\n",
      "Training loss (for one batch) at step 110: 236.2780, Accuracy: 0.7395\n",
      "---- Training ----\n",
      "Training loss: 71.8761\n",
      "Training acc over epoch: 0.7388\n",
      "---- Validation ----\n",
      "Validation loss: 39.1885\n",
      "Validation acc: 0.6795\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 241.3870, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 234.2711, Accuracy: 0.5831\n",
      "Training loss (for one batch) at step 20: 227.3569, Accuracy: 0.6477\n",
      "Training loss (for one batch) at step 30: 242.4861, Accuracy: 0.7014\n",
      "Training loss (for one batch) at step 40: 215.4250, Accuracy: 0.7361\n",
      "Training loss (for one batch) at step 50: 216.7588, Accuracy: 0.7577\n",
      "Training loss (for one batch) at step 60: 235.9885, Accuracy: 0.7687\n",
      "Training loss (for one batch) at step 70: 264.0907, Accuracy: 0.7520\n",
      "Training loss (for one batch) at step 80: 244.3965, Accuracy: 0.7313\n",
      "Training loss (for one batch) at step 90: 232.6864, Accuracy: 0.7293\n",
      "Training loss (for one batch) at step 100: 233.3904, Accuracy: 0.7377\n",
      "Training loss (for one batch) at step 110: 267.6207, Accuracy: 0.7399\n",
      "---- Training ----\n",
      "Training loss: 73.3506\n",
      "Training acc over epoch: 0.7388\n",
      "---- Validation ----\n",
      "Validation loss: 52.2746\n",
      "Validation acc: 0.6754\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 266.3795, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 279.3930, Accuracy: 0.5632\n",
      "Training loss (for one batch) at step 20: 227.6700, Accuracy: 0.6376\n",
      "Training loss (for one batch) at step 30: 218.8268, Accuracy: 0.6971\n",
      "Training loss (for one batch) at step 40: 222.9329, Accuracy: 0.7319\n",
      "Training loss (for one batch) at step 50: 211.4972, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 60: 244.0840, Accuracy: 0.7655\n",
      "Training loss (for one batch) at step 70: 246.8263, Accuracy: 0.7522\n",
      "Training loss (for one batch) at step 80: 260.3238, Accuracy: 0.7322\n",
      "Training loss (for one batch) at step 90: 214.6193, Accuracy: 0.7300\n",
      "Training loss (for one batch) at step 100: 221.9035, Accuracy: 0.7372\n",
      "Training loss (for one batch) at step 110: 220.1127, Accuracy: 0.7399\n",
      "---- Training ----\n",
      "Training loss: 85.4888\n",
      "Training acc over epoch: 0.7389\n",
      "---- Validation ----\n",
      "Validation loss: 57.6924\n",
      "Validation acc: 0.6779\n",
      "Time taken: 20.17s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 281.1530, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 248.2370, Accuracy: 0.5589\n",
      "Training loss (for one batch) at step 20: 235.1863, Accuracy: 0.6343\n",
      "Training loss (for one batch) at step 30: 215.8538, Accuracy: 0.6948\n",
      "Training loss (for one batch) at step 40: 219.5231, Accuracy: 0.7313\n",
      "Training loss (for one batch) at step 50: 230.7553, Accuracy: 0.7551\n",
      "Training loss (for one batch) at step 60: 220.5777, Accuracy: 0.7681\n",
      "Training loss (for one batch) at step 70: 248.9289, Accuracy: 0.7533\n",
      "Training loss (for one batch) at step 80: 250.9705, Accuracy: 0.7348\n",
      "Training loss (for one batch) at step 90: 233.5829, Accuracy: 0.7316\n",
      "Training loss (for one batch) at step 100: 218.1615, Accuracy: 0.7406\n",
      "Training loss (for one batch) at step 110: 242.6551, Accuracy: 0.7413\n",
      "---- Training ----\n",
      "Training loss: 76.5117\n",
      "Training acc over epoch: 0.7395\n",
      "---- Validation ----\n",
      "Validation loss: 43.4002\n",
      "Validation acc: 0.6677\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 279.9817, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 243.9810, Accuracy: 0.5611\n",
      "Training loss (for one batch) at step 20: 222.4850, Accuracy: 0.6347\n",
      "Training loss (for one batch) at step 30: 213.8205, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 40: 216.8864, Accuracy: 0.7334\n",
      "Training loss (for one batch) at step 50: 224.7096, Accuracy: 0.7567\n",
      "Training loss (for one batch) at step 60: 241.7156, Accuracy: 0.7673\n",
      "Training loss (for one batch) at step 70: 232.6609, Accuracy: 0.7512\n",
      "Training loss (for one batch) at step 80: 243.2746, Accuracy: 0.7307\n",
      "Training loss (for one batch) at step 90: 238.4461, Accuracy: 0.7297\n",
      "Training loss (for one batch) at step 100: 229.6225, Accuracy: 0.7369\n",
      "Training loss (for one batch) at step 110: 228.3440, Accuracy: 0.7404\n",
      "---- Training ----\n",
      "Training loss: 94.1117\n",
      "Training acc over epoch: 0.7394\n",
      "---- Validation ----\n",
      "Validation loss: 67.8392\n",
      "Validation acc: 0.6744\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 263.1129, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 273.6256, Accuracy: 0.5518\n",
      "Training loss (for one batch) at step 20: 226.7565, Accuracy: 0.6332\n",
      "Training loss (for one batch) at step 30: 230.8429, Accuracy: 0.6923\n",
      "Training loss (for one batch) at step 40: 218.1263, Accuracy: 0.7298\n",
      "Training loss (for one batch) at step 50: 206.4663, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 60: 238.1793, Accuracy: 0.7670\n",
      "Training loss (for one batch) at step 70: 262.7006, Accuracy: 0.7508\n",
      "Training loss (for one batch) at step 80: 244.6078, Accuracy: 0.7325\n",
      "Training loss (for one batch) at step 90: 227.5079, Accuracy: 0.7292\n",
      "Training loss (for one batch) at step 100: 235.7408, Accuracy: 0.7391\n",
      "Training loss (for one batch) at step 110: 234.5813, Accuracy: 0.7419\n",
      "---- Training ----\n",
      "Training loss: 73.9101\n",
      "Training acc over epoch: 0.7389\n",
      "---- Validation ----\n",
      "Validation loss: 43.1355\n",
      "Validation acc: 0.6822\n",
      "Time taken: 18.06s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 271.6664, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 249.6798, Accuracy: 0.5632\n",
      "Training loss (for one batch) at step 20: 221.5703, Accuracy: 0.6354\n",
      "Training loss (for one batch) at step 30: 219.0112, Accuracy: 0.6968\n",
      "Training loss (for one batch) at step 40: 234.0433, Accuracy: 0.7325\n",
      "Training loss (for one batch) at step 50: 224.4583, Accuracy: 0.7572\n",
      "Training loss (for one batch) at step 60: 238.1541, Accuracy: 0.7664\n",
      "Training loss (for one batch) at step 70: 258.2557, Accuracy: 0.7508\n",
      "Training loss (for one batch) at step 80: 251.5608, Accuracy: 0.7300\n",
      "Training loss (for one batch) at step 90: 250.0591, Accuracy: 0.7285\n",
      "Training loss (for one batch) at step 100: 264.8481, Accuracy: 0.7371\n",
      "Training loss (for one batch) at step 110: 253.1054, Accuracy: 0.7420\n",
      "---- Training ----\n",
      "Training loss: 77.4676\n",
      "Training acc over epoch: 0.7389\n",
      "---- Validation ----\n",
      "Validation loss: 62.1039\n",
      "Validation acc: 0.7061\n",
      "Time taken: 18.37s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 291.4749, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 253.9220, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 20: 236.4900, Accuracy: 0.6436\n",
      "Training loss (for one batch) at step 30: 218.3257, Accuracy: 0.6958\n",
      "Training loss (for one batch) at step 40: 230.2511, Accuracy: 0.7327\n",
      "Training loss (for one batch) at step 50: 213.1155, Accuracy: 0.7560\n",
      "Training loss (for one batch) at step 60: 234.7758, Accuracy: 0.7661\n",
      "Training loss (for one batch) at step 70: 260.7384, Accuracy: 0.7533\n",
      "Training loss (for one batch) at step 80: 239.4569, Accuracy: 0.7343\n",
      "Training loss (for one batch) at step 90: 229.2941, Accuracy: 0.7290\n",
      "Training loss (for one batch) at step 100: 222.6622, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 110: 216.3281, Accuracy: 0.7413\n",
      "---- Training ----\n",
      "Training loss: 72.2014\n",
      "Training acc over epoch: 0.7386\n",
      "---- Validation ----\n",
      "Validation loss: 73.8053\n",
      "Validation acc: 0.6781\n",
      "Time taken: 18.29s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 283.3244, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 241.2849, Accuracy: 0.5717\n",
      "Training loss (for one batch) at step 20: 228.2295, Accuracy: 0.6429\n",
      "Training loss (for one batch) at step 30: 239.1945, Accuracy: 0.6966\n",
      "Training loss (for one batch) at step 40: 237.9710, Accuracy: 0.7330\n",
      "Training loss (for one batch) at step 50: 223.5759, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 60: 231.6492, Accuracy: 0.7665\n",
      "Training loss (for one batch) at step 70: 237.7724, Accuracy: 0.7518\n",
      "Training loss (for one batch) at step 80: 253.8033, Accuracy: 0.7332\n",
      "Training loss (for one batch) at step 90: 225.3199, Accuracy: 0.7297\n",
      "Training loss (for one batch) at step 100: 225.6440, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 110: 234.9955, Accuracy: 0.7420\n",
      "---- Training ----\n",
      "Training loss: 77.9192\n",
      "Training acc over epoch: 0.7398\n",
      "---- Validation ----\n",
      "Validation loss: 97.2171\n",
      "Validation acc: 0.6900\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 241.3145, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 251.2184, Accuracy: 0.5518\n",
      "Training loss (for one batch) at step 20: 238.5674, Accuracy: 0.6324\n",
      "Training loss (for one batch) at step 30: 213.8150, Accuracy: 0.6928\n",
      "Training loss (for one batch) at step 40: 230.1111, Accuracy: 0.7313\n",
      "Training loss (for one batch) at step 50: 240.9598, Accuracy: 0.7549\n",
      "Training loss (for one batch) at step 60: 235.1849, Accuracy: 0.7650\n",
      "Training loss (for one batch) at step 70: 230.8304, Accuracy: 0.7508\n",
      "Training loss (for one batch) at step 80: 249.7541, Accuracy: 0.7284\n",
      "Training loss (for one batch) at step 90: 227.4128, Accuracy: 0.7261\n",
      "Training loss (for one batch) at step 100: 229.0972, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 110: 229.1041, Accuracy: 0.7390\n",
      "---- Training ----\n",
      "Training loss: 74.8282\n",
      "Training acc over epoch: 0.7376\n",
      "---- Validation ----\n",
      "Validation loss: 41.3461\n",
      "Validation acc: 0.6900\n",
      "Time taken: 18.06s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 249.6657, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 257.1348, Accuracy: 0.5632\n",
      "Training loss (for one batch) at step 20: 236.8766, Accuracy: 0.6358\n",
      "Training loss (for one batch) at step 30: 237.8032, Accuracy: 0.6925\n",
      "Training loss (for one batch) at step 40: 224.9985, Accuracy: 0.7300\n",
      "Training loss (for one batch) at step 50: 236.0486, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 60: 239.6638, Accuracy: 0.7651\n",
      "Training loss (for one batch) at step 70: 238.5624, Accuracy: 0.7498\n",
      "Training loss (for one batch) at step 80: 249.9945, Accuracy: 0.7290\n",
      "Training loss (for one batch) at step 90: 234.8297, Accuracy: 0.7276\n",
      "Training loss (for one batch) at step 100: 224.9250, Accuracy: 0.7366\n",
      "Training loss (for one batch) at step 110: 218.6208, Accuracy: 0.7402\n",
      "---- Training ----\n",
      "Training loss: 76.5430\n",
      "Training acc over epoch: 0.7377\n",
      "---- Validation ----\n",
      "Validation loss: 42.5457\n",
      "Validation acc: 0.6883\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 261.0668, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 256.0447, Accuracy: 0.5597\n",
      "Training loss (for one batch) at step 20: 235.6386, Accuracy: 0.6302\n",
      "Training loss (for one batch) at step 30: 248.8476, Accuracy: 0.6865\n",
      "Training loss (for one batch) at step 40: 213.1675, Accuracy: 0.7268\n",
      "Training loss (for one batch) at step 50: 224.7421, Accuracy: 0.7518\n",
      "Training loss (for one batch) at step 60: 216.9353, Accuracy: 0.7615\n",
      "Training loss (for one batch) at step 70: 243.9461, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 80: 247.3505, Accuracy: 0.7263\n",
      "Training loss (for one batch) at step 90: 218.3486, Accuracy: 0.7248\n",
      "Training loss (for one batch) at step 100: 247.3891, Accuracy: 0.7346\n",
      "Training loss (for one batch) at step 110: 233.9532, Accuracy: 0.7366\n",
      "---- Training ----\n",
      "Training loss: 71.2337\n",
      "Training acc over epoch: 0.7352\n",
      "---- Validation ----\n",
      "Validation loss: 33.4809\n",
      "Validation acc: 0.6889\n",
      "Time taken: 20.16s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 251.1159, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 265.9473, Accuracy: 0.5554\n",
      "Training loss (for one batch) at step 20: 216.6205, Accuracy: 0.6365\n",
      "Training loss (for one batch) at step 30: 205.9852, Accuracy: 0.6978\n",
      "Training loss (for one batch) at step 40: 230.7407, Accuracy: 0.7330\n",
      "Training loss (for one batch) at step 50: 222.8511, Accuracy: 0.7535\n",
      "Training loss (for one batch) at step 60: 228.6746, Accuracy: 0.7681\n",
      "Training loss (for one batch) at step 70: 239.0980, Accuracy: 0.7520\n",
      "Training loss (for one batch) at step 80: 235.5048, Accuracy: 0.7326\n",
      "Training loss (for one batch) at step 90: 222.8591, Accuracy: 0.7300\n",
      "Training loss (for one batch) at step 100: 230.7834, Accuracy: 0.7379\n",
      "Training loss (for one batch) at step 110: 219.4847, Accuracy: 0.7411\n",
      "---- Training ----\n",
      "Training loss: 69.9590\n",
      "Training acc over epoch: 0.7386\n",
      "---- Validation ----\n",
      "Validation loss: 41.7905\n",
      "Validation acc: 0.6953\n",
      "Time taken: 18.08s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 246.2235, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 265.0908, Accuracy: 0.5646\n",
      "Training loss (for one batch) at step 20: 224.6186, Accuracy: 0.6410\n",
      "Training loss (for one batch) at step 30: 233.1906, Accuracy: 0.6968\n",
      "Training loss (for one batch) at step 40: 210.8442, Accuracy: 0.7323\n",
      "Training loss (for one batch) at step 50: 218.5906, Accuracy: 0.7532\n",
      "Training loss (for one batch) at step 60: 228.5363, Accuracy: 0.7678\n",
      "Training loss (for one batch) at step 70: 240.8849, Accuracy: 0.7525\n",
      "Training loss (for one batch) at step 80: 250.8119, Accuracy: 0.7301\n",
      "Training loss (for one batch) at step 90: 216.7549, Accuracy: 0.7283\n",
      "Training loss (for one batch) at step 100: 235.8925, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 110: 242.1611, Accuracy: 0.7392\n",
      "---- Training ----\n",
      "Training loss: 70.8299\n",
      "Training acc over epoch: 0.7372\n",
      "---- Validation ----\n",
      "Validation loss: 36.7673\n",
      "Validation acc: 0.6873\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 272.1829, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 243.9350, Accuracy: 0.5582\n",
      "Training loss (for one batch) at step 20: 228.5878, Accuracy: 0.6343\n",
      "Training loss (for one batch) at step 30: 231.3692, Accuracy: 0.6946\n",
      "Training loss (for one batch) at step 40: 231.0118, Accuracy: 0.7302\n",
      "Training loss (for one batch) at step 50: 216.8176, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 60: 228.0655, Accuracy: 0.7645\n",
      "Training loss (for one batch) at step 70: 236.4976, Accuracy: 0.7498\n",
      "Training loss (for one batch) at step 80: 241.2082, Accuracy: 0.7297\n",
      "Training loss (for one batch) at step 90: 244.0401, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 100: 231.6447, Accuracy: 0.7367\n",
      "Training loss (for one batch) at step 110: 220.1989, Accuracy: 0.7381\n",
      "---- Training ----\n",
      "Training loss: 95.8673\n",
      "Training acc over epoch: 0.7370\n",
      "---- Validation ----\n",
      "Validation loss: 53.1690\n",
      "Validation acc: 0.6889\n",
      "Time taken: 18.09s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 268.6468, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 244.4461, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 20: 228.6943, Accuracy: 0.6336\n",
      "Training loss (for one batch) at step 30: 213.2010, Accuracy: 0.6928\n",
      "Training loss (for one batch) at step 40: 216.7122, Accuracy: 0.7298\n",
      "Training loss (for one batch) at step 50: 218.4925, Accuracy: 0.7508\n",
      "Training loss (for one batch) at step 60: 244.2097, Accuracy: 0.7632\n",
      "Training loss (for one batch) at step 70: 235.7902, Accuracy: 0.7502\n",
      "Training loss (for one batch) at step 80: 248.9908, Accuracy: 0.7313\n",
      "Training loss (for one batch) at step 90: 221.0378, Accuracy: 0.7289\n",
      "Training loss (for one batch) at step 100: 230.8194, Accuracy: 0.7380\n",
      "Training loss (for one batch) at step 110: 227.2632, Accuracy: 0.7404\n",
      "---- Training ----\n",
      "Training loss: 70.3762\n",
      "Training acc over epoch: 0.7384\n",
      "---- Validation ----\n",
      "Validation loss: 56.7767\n",
      "Validation acc: 0.6822\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 271.8881, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 233.1846, Accuracy: 0.5696\n",
      "Training loss (for one batch) at step 20: 241.2534, Accuracy: 0.6347\n",
      "Training loss (for one batch) at step 30: 221.3542, Accuracy: 0.6946\n",
      "Training loss (for one batch) at step 40: 200.9676, Accuracy: 0.7361\n",
      "Training loss (for one batch) at step 50: 221.6154, Accuracy: 0.7581\n",
      "Training loss (for one batch) at step 60: 232.7052, Accuracy: 0.7683\n",
      "Training loss (for one batch) at step 70: 253.5478, Accuracy: 0.7513\n",
      "Training loss (for one batch) at step 80: 240.2968, Accuracy: 0.7315\n",
      "Training loss (for one batch) at step 90: 245.4557, Accuracy: 0.7283\n",
      "Training loss (for one batch) at step 100: 226.5508, Accuracy: 0.7364\n",
      "Training loss (for one batch) at step 110: 242.9987, Accuracy: 0.7389\n",
      "---- Training ----\n",
      "Training loss: 74.4850\n",
      "Training acc over epoch: 0.7370\n",
      "---- Validation ----\n",
      "Validation loss: 38.2969\n",
      "Validation acc: 0.6827\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 273.4489, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 253.7388, Accuracy: 0.5526\n",
      "Training loss (for one batch) at step 20: 229.2443, Accuracy: 0.6313\n",
      "Training loss (for one batch) at step 30: 220.3438, Accuracy: 0.6925\n",
      "Training loss (for one batch) at step 40: 245.7458, Accuracy: 0.7281\n",
      "Training loss (for one batch) at step 50: 215.9518, Accuracy: 0.7534\n",
      "Training loss (for one batch) at step 60: 227.9637, Accuracy: 0.7620\n",
      "Training loss (for one batch) at step 70: 241.4292, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 80: 262.3780, Accuracy: 0.7302\n",
      "Training loss (for one batch) at step 90: 243.5154, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 100: 218.7466, Accuracy: 0.7348\n",
      "Training loss (for one batch) at step 110: 240.6332, Accuracy: 0.7368\n",
      "---- Training ----\n",
      "Training loss: 78.9325\n",
      "Training acc over epoch: 0.7365\n",
      "---- Validation ----\n",
      "Validation loss: 56.1758\n",
      "Validation acc: 0.6835\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 269.5222, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 267.1395, Accuracy: 0.5788\n",
      "Training loss (for one batch) at step 20: 225.2326, Accuracy: 0.6324\n",
      "Training loss (for one batch) at step 30: 226.8945, Accuracy: 0.6948\n",
      "Training loss (for one batch) at step 40: 224.5824, Accuracy: 0.7273\n",
      "Training loss (for one batch) at step 50: 211.5461, Accuracy: 0.7532\n",
      "Training loss (for one batch) at step 60: 225.6674, Accuracy: 0.7652\n",
      "Training loss (for one batch) at step 70: 232.8353, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 80: 251.6182, Accuracy: 0.7307\n",
      "Training loss (for one batch) at step 90: 223.4302, Accuracy: 0.7291\n",
      "Training loss (for one batch) at step 100: 220.1693, Accuracy: 0.7384\n",
      "Training loss (for one batch) at step 110: 229.7604, Accuracy: 0.7399\n",
      "---- Training ----\n",
      "Training loss: 72.0625\n",
      "Training acc over epoch: 0.7382\n",
      "---- Validation ----\n",
      "Validation loss: 56.9843\n",
      "Validation acc: 0.6797\n",
      "Time taken: 18.00s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.6771\n",
      "Validation AUC: 0.6774\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 713.2382, Accuracy: 0.4922\n",
      "Training loss (for one batch) at step 10: 565.6422, Accuracy: 0.4972\n",
      "Training loss (for one batch) at step 20: 572.4861, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 30: 518.8548, Accuracy: 0.5154\n",
      "Training loss (for one batch) at step 40: 494.2432, Accuracy: 0.5116\n",
      "Training loss (for one batch) at step 50: 501.1718, Accuracy: 0.5103\n",
      "Training loss (for one batch) at step 60: 485.5132, Accuracy: 0.5118\n",
      "Training loss (for one batch) at step 70: 479.9048, Accuracy: 0.5109\n",
      "Training loss (for one batch) at step 80: 483.5883, Accuracy: 0.5133\n",
      "Training loss (for one batch) at step 90: 474.5203, Accuracy: 0.5149\n",
      "Training loss (for one batch) at step 100: 482.2653, Accuracy: 0.5173\n",
      "Training loss (for one batch) at step 110: 461.4325, Accuracy: 0.5187\n",
      "---- Training ----\n",
      "Training loss: 145.1949\n",
      "Training acc over epoch: 0.5198\n",
      "---- Validation ----\n",
      "Validation loss: 34.6571\n",
      "Validation acc: 0.5258\n",
      "Time taken: 19.43s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 474.3368, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 454.8407, Accuracy: 0.5277\n",
      "Training loss (for one batch) at step 20: 454.2844, Accuracy: 0.5298\n",
      "Training loss (for one batch) at step 30: 453.0553, Accuracy: 0.5217\n",
      "Training loss (for one batch) at step 40: 456.2932, Accuracy: 0.5236\n",
      "Training loss (for one batch) at step 50: 450.7604, Accuracy: 0.5262\n",
      "Training loss (for one batch) at step 60: 454.7165, Accuracy: 0.5224\n",
      "Training loss (for one batch) at step 70: 455.7408, Accuracy: 0.5248\n",
      "Training loss (for one batch) at step 80: 439.0036, Accuracy: 0.5279\n",
      "Training loss (for one batch) at step 90: 447.2094, Accuracy: 0.5270\n",
      "Training loss (for one batch) at step 100: 451.3160, Accuracy: 0.5295\n",
      "Training loss (for one batch) at step 110: 451.1636, Accuracy: 0.5279\n",
      "---- Training ----\n",
      "Training loss: 138.1868\n",
      "Training acc over epoch: 0.5288\n",
      "---- Validation ----\n",
      "Validation loss: 35.1690\n",
      "Validation acc: 0.4882\n",
      "Time taken: 17.88s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 447.3755, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 448.2865, Accuracy: 0.5398\n",
      "Training loss (for one batch) at step 20: 448.4109, Accuracy: 0.5621\n",
      "Training loss (for one batch) at step 30: 449.0939, Accuracy: 0.5509\n",
      "Training loss (for one batch) at step 40: 443.6700, Accuracy: 0.5524\n",
      "Training loss (for one batch) at step 50: 447.9161, Accuracy: 0.5484\n",
      "Training loss (for one batch) at step 60: 445.7690, Accuracy: 0.5524\n",
      "Training loss (for one batch) at step 70: 448.1533, Accuracy: 0.5548\n",
      "Training loss (for one batch) at step 80: 444.7117, Accuracy: 0.5556\n",
      "Training loss (for one batch) at step 90: 443.1992, Accuracy: 0.5571\n",
      "Training loss (for one batch) at step 100: 445.5533, Accuracy: 0.5553\n",
      "Training loss (for one batch) at step 110: 442.8353, Accuracy: 0.5564\n",
      "---- Training ----\n",
      "Training loss: 140.1367\n",
      "Training acc over epoch: 0.5555\n",
      "---- Validation ----\n",
      "Validation loss: 35.1250\n",
      "Validation acc: 0.5572\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 441.4126, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 444.7135, Accuracy: 0.5497\n",
      "Training loss (for one batch) at step 20: 443.8328, Accuracy: 0.5480\n",
      "Training loss (for one batch) at step 30: 440.1877, Accuracy: 0.5441\n",
      "Training loss (for one batch) at step 40: 444.0029, Accuracy: 0.5554\n",
      "Training loss (for one batch) at step 50: 440.9434, Accuracy: 0.5524\n",
      "Training loss (for one batch) at step 60: 444.4489, Accuracy: 0.5511\n",
      "Training loss (for one batch) at step 70: 443.7312, Accuracy: 0.5503\n",
      "Training loss (for one batch) at step 80: 445.6663, Accuracy: 0.5532\n",
      "Training loss (for one batch) at step 90: 442.9351, Accuracy: 0.5531\n",
      "Training loss (for one batch) at step 100: 444.3079, Accuracy: 0.5553\n",
      "Training loss (for one batch) at step 110: 442.2720, Accuracy: 0.5558\n",
      "---- Training ----\n",
      "Training loss: 137.0899\n",
      "Training acc over epoch: 0.5551\n",
      "---- Validation ----\n",
      "Validation loss: 34.9991\n",
      "Validation acc: 0.5766\n",
      "Time taken: 18.15s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 445.7632, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 444.0348, Accuracy: 0.5746\n",
      "Training loss (for one batch) at step 20: 439.8688, Accuracy: 0.5822\n",
      "Training loss (for one batch) at step 30: 444.6125, Accuracy: 0.5736\n",
      "Training loss (for one batch) at step 40: 441.8448, Accuracy: 0.5720\n",
      "Training loss (for one batch) at step 50: 441.6668, Accuracy: 0.5689\n",
      "Training loss (for one batch) at step 60: 446.2039, Accuracy: 0.5736\n",
      "Training loss (for one batch) at step 70: 441.2789, Accuracy: 0.5727\n",
      "Training loss (for one batch) at step 80: 441.4560, Accuracy: 0.5742\n",
      "Training loss (for one batch) at step 90: 442.5817, Accuracy: 0.5725\n",
      "Training loss (for one batch) at step 100: 441.6858, Accuracy: 0.5720\n",
      "Training loss (for one batch) at step 110: 437.0862, Accuracy: 0.5701\n",
      "---- Training ----\n",
      "Training loss: 138.5478\n",
      "Training acc over epoch: 0.5716\n",
      "---- Validation ----\n",
      "Validation loss: 33.6922\n",
      "Validation acc: 0.6024\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 442.7315, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 443.5394, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 20: 445.2452, Accuracy: 0.5789\n",
      "Training loss (for one batch) at step 30: 437.1738, Accuracy: 0.5789\n",
      "Training loss (for one batch) at step 40: 439.4641, Accuracy: 0.5812\n",
      "Training loss (for one batch) at step 50: 439.3034, Accuracy: 0.5846\n",
      "Training loss (for one batch) at step 60: 442.6890, Accuracy: 0.5877\n",
      "Training loss (for one batch) at step 70: 441.9823, Accuracy: 0.5879\n",
      "Training loss (for one batch) at step 80: 440.7915, Accuracy: 0.5856\n",
      "Training loss (for one batch) at step 90: 440.4863, Accuracy: 0.5864\n",
      "Training loss (for one batch) at step 100: 440.7787, Accuracy: 0.5887\n",
      "Training loss (for one batch) at step 110: 444.6599, Accuracy: 0.5887\n",
      "---- Training ----\n",
      "Training loss: 137.8891\n",
      "Training acc over epoch: 0.5895\n",
      "---- Validation ----\n",
      "Validation loss: 34.5344\n",
      "Validation acc: 0.5825\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 445.7049, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 440.4948, Accuracy: 0.6001\n",
      "Training loss (for one batch) at step 20: 437.5548, Accuracy: 0.5952\n",
      "Training loss (for one batch) at step 30: 439.7397, Accuracy: 0.5920\n",
      "Training loss (for one batch) at step 40: 439.8594, Accuracy: 0.5979\n",
      "Training loss (for one batch) at step 50: 438.9850, Accuracy: 0.5999\n",
      "Training loss (for one batch) at step 60: 442.6040, Accuracy: 0.6031\n",
      "Training loss (for one batch) at step 70: 445.9505, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 80: 436.8982, Accuracy: 0.5997\n",
      "Training loss (for one batch) at step 90: 437.5883, Accuracy: 0.5980\n",
      "Training loss (for one batch) at step 100: 439.7868, Accuracy: 0.6022\n",
      "Training loss (for one batch) at step 110: 442.5836, Accuracy: 0.6021\n",
      "---- Training ----\n",
      "Training loss: 140.6670\n",
      "Training acc over epoch: 0.6024\n",
      "---- Validation ----\n",
      "Validation loss: 34.8837\n",
      "Validation acc: 0.6048\n",
      "Time taken: 17.86s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 440.5475, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 439.1479, Accuracy: 0.6165\n",
      "Training loss (for one batch) at step 20: 441.1161, Accuracy: 0.6008\n",
      "Training loss (for one batch) at step 30: 431.4503, Accuracy: 0.6091\n",
      "Training loss (for one batch) at step 40: 434.4366, Accuracy: 0.6101\n",
      "Training loss (for one batch) at step 50: 432.7416, Accuracy: 0.6066\n",
      "Training loss (for one batch) at step 60: 429.4419, Accuracy: 0.6100\n",
      "Training loss (for one batch) at step 70: 437.4375, Accuracy: 0.6115\n",
      "Training loss (for one batch) at step 80: 438.7029, Accuracy: 0.6114\n",
      "Training loss (for one batch) at step 90: 442.4746, Accuracy: 0.6104\n",
      "Training loss (for one batch) at step 100: 441.4110, Accuracy: 0.6128\n",
      "Training loss (for one batch) at step 110: 444.1062, Accuracy: 0.6132\n",
      "---- Training ----\n",
      "Training loss: 133.1165\n",
      "Training acc over epoch: 0.6141\n",
      "---- Validation ----\n",
      "Validation loss: 34.4280\n",
      "Validation acc: 0.6198\n",
      "Time taken: 18.05s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 442.2639, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 440.4158, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 20: 438.4780, Accuracy: 0.6060\n",
      "Training loss (for one batch) at step 30: 432.0598, Accuracy: 0.6162\n",
      "Training loss (for one batch) at step 40: 434.0311, Accuracy: 0.6212\n",
      "Training loss (for one batch) at step 50: 427.0319, Accuracy: 0.6209\n",
      "Training loss (for one batch) at step 60: 433.0190, Accuracy: 0.6218\n",
      "Training loss (for one batch) at step 70: 438.7999, Accuracy: 0.6224\n",
      "Training loss (for one batch) at step 80: 438.5455, Accuracy: 0.6209\n",
      "Training loss (for one batch) at step 90: 438.4234, Accuracy: 0.6215\n",
      "Training loss (for one batch) at step 100: 436.3053, Accuracy: 0.6238\n",
      "Training loss (for one batch) at step 110: 445.8897, Accuracy: 0.6235\n",
      "---- Training ----\n",
      "Training loss: 135.9771\n",
      "Training acc over epoch: 0.6241\n",
      "---- Validation ----\n",
      "Validation loss: 33.6169\n",
      "Validation acc: 0.5897\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 442.0222, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 442.4632, Accuracy: 0.6136\n",
      "Training loss (for one batch) at step 20: 431.3635, Accuracy: 0.6090\n",
      "Training loss (for one batch) at step 30: 430.5446, Accuracy: 0.6152\n",
      "Training loss (for one batch) at step 40: 433.5817, Accuracy: 0.6223\n",
      "Training loss (for one batch) at step 50: 431.6441, Accuracy: 0.6252\n",
      "Training loss (for one batch) at step 60: 430.9709, Accuracy: 0.6332\n",
      "Training loss (for one batch) at step 70: 438.7442, Accuracy: 0.6346\n",
      "Training loss (for one batch) at step 80: 433.2205, Accuracy: 0.6344\n",
      "Training loss (for one batch) at step 90: 433.9677, Accuracy: 0.6336\n",
      "Training loss (for one batch) at step 100: 429.3556, Accuracy: 0.6334\n",
      "Training loss (for one batch) at step 110: 433.3140, Accuracy: 0.6350\n",
      "---- Training ----\n",
      "Training loss: 139.9416\n",
      "Training acc over epoch: 0.6339\n",
      "---- Validation ----\n",
      "Validation loss: 35.1330\n",
      "Validation acc: 0.6252\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 445.1794, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 436.3640, Accuracy: 0.6151\n",
      "Training loss (for one batch) at step 20: 437.2695, Accuracy: 0.6243\n",
      "Training loss (for one batch) at step 30: 438.3604, Accuracy: 0.6331\n",
      "Training loss (for one batch) at step 40: 426.8278, Accuracy: 0.6395\n",
      "Training loss (for one batch) at step 50: 421.8260, Accuracy: 0.6428\n",
      "Training loss (for one batch) at step 60: 425.5212, Accuracy: 0.6506\n",
      "Training loss (for one batch) at step 70: 430.5264, Accuracy: 0.6502\n",
      "Training loss (for one batch) at step 80: 439.6065, Accuracy: 0.6447\n",
      "Training loss (for one batch) at step 90: 439.5242, Accuracy: 0.6417\n",
      "Training loss (for one batch) at step 100: 420.8910, Accuracy: 0.6443\n",
      "Training loss (for one batch) at step 110: 423.7189, Accuracy: 0.6448\n",
      "---- Training ----\n",
      "Training loss: 128.3784\n",
      "Training acc over epoch: 0.6459\n",
      "---- Validation ----\n",
      "Validation loss: 35.1295\n",
      "Validation acc: 0.6265\n",
      "Time taken: 18.09s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 431.5003, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 428.0816, Accuracy: 0.6428\n",
      "Training loss (for one batch) at step 20: 428.3899, Accuracy: 0.6399\n",
      "Training loss (for one batch) at step 30: 429.5227, Accuracy: 0.6404\n",
      "Training loss (for one batch) at step 40: 414.7394, Accuracy: 0.6463\n",
      "Training loss (for one batch) at step 50: 415.7535, Accuracy: 0.6472\n",
      "Training loss (for one batch) at step 60: 419.9242, Accuracy: 0.6513\n",
      "Training loss (for one batch) at step 70: 432.8101, Accuracy: 0.6520\n",
      "Training loss (for one batch) at step 80: 425.5964, Accuracy: 0.6470\n",
      "Training loss (for one batch) at step 90: 431.9255, Accuracy: 0.6436\n",
      "Training loss (for one batch) at step 100: 420.8092, Accuracy: 0.6468\n",
      "Training loss (for one batch) at step 110: 409.7079, Accuracy: 0.6498\n",
      "---- Training ----\n",
      "Training loss: 135.6506\n",
      "Training acc over epoch: 0.6498\n",
      "---- Validation ----\n",
      "Validation loss: 33.7071\n",
      "Validation acc: 0.6459\n",
      "Time taken: 18.09s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 441.9911, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 437.8061, Accuracy: 0.6300\n",
      "Training loss (for one batch) at step 20: 423.2886, Accuracy: 0.6280\n",
      "Training loss (for one batch) at step 30: 415.1621, Accuracy: 0.6366\n",
      "Training loss (for one batch) at step 40: 409.7921, Accuracy: 0.6458\n",
      "Training loss (for one batch) at step 50: 415.8656, Accuracy: 0.6461\n",
      "Training loss (for one batch) at step 60: 410.9129, Accuracy: 0.6492\n",
      "Training loss (for one batch) at step 70: 412.4601, Accuracy: 0.6524\n",
      "Training loss (for one batch) at step 80: 424.8198, Accuracy: 0.6508\n",
      "Training loss (for one batch) at step 90: 428.5768, Accuracy: 0.6500\n",
      "Training loss (for one batch) at step 100: 408.4781, Accuracy: 0.6502\n",
      "Training loss (for one batch) at step 110: 414.6111, Accuracy: 0.6499\n",
      "---- Training ----\n",
      "Training loss: 126.3999\n",
      "Training acc over epoch: 0.6505\n",
      "---- Validation ----\n",
      "Validation loss: 41.0043\n",
      "Validation acc: 0.6185\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 431.2697, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 423.3218, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 20: 415.9481, Accuracy: 0.6414\n",
      "Training loss (for one batch) at step 30: 422.4833, Accuracy: 0.6419\n",
      "Training loss (for one batch) at step 40: 413.9778, Accuracy: 0.6553\n",
      "Training loss (for one batch) at step 50: 385.4997, Accuracy: 0.6628\n",
      "Training loss (for one batch) at step 60: 393.4684, Accuracy: 0.6664\n",
      "Training loss (for one batch) at step 70: 409.4337, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 80: 408.3964, Accuracy: 0.6610\n",
      "Training loss (for one batch) at step 90: 398.8838, Accuracy: 0.6601\n",
      "Training loss (for one batch) at step 100: 405.7005, Accuracy: 0.6625\n",
      "Training loss (for one batch) at step 110: 411.2349, Accuracy: 0.6641\n",
      "---- Training ----\n",
      "Training loss: 125.0602\n",
      "Training acc over epoch: 0.6661\n",
      "---- Validation ----\n",
      "Validation loss: 38.7796\n",
      "Validation acc: 0.5854\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 417.7766, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 425.1318, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 20: 406.1588, Accuracy: 0.6425\n",
      "Training loss (for one batch) at step 30: 396.8243, Accuracy: 0.6512\n",
      "Training loss (for one batch) at step 40: 380.2388, Accuracy: 0.6606\n",
      "Training loss (for one batch) at step 50: 398.1008, Accuracy: 0.6691\n",
      "Training loss (for one batch) at step 60: 393.7436, Accuracy: 0.6735\n",
      "Training loss (for one batch) at step 70: 401.8644, Accuracy: 0.6690\n",
      "Training loss (for one batch) at step 80: 395.4245, Accuracy: 0.6651\n",
      "Training loss (for one batch) at step 90: 396.6245, Accuracy: 0.6610\n",
      "Training loss (for one batch) at step 100: 392.0248, Accuracy: 0.6649\n",
      "Training loss (for one batch) at step 110: 402.8328, Accuracy: 0.6653\n",
      "---- Training ----\n",
      "Training loss: 125.7588\n",
      "Training acc over epoch: 0.6650\n",
      "---- Validation ----\n",
      "Validation loss: 34.6731\n",
      "Validation acc: 0.6064\n",
      "Time taken: 20.19s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 402.2747, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 408.8167, Accuracy: 0.6207\n",
      "Training loss (for one batch) at step 20: 396.2032, Accuracy: 0.6429\n",
      "Training loss (for one batch) at step 30: 370.2003, Accuracy: 0.6547\n",
      "Training loss (for one batch) at step 40: 373.2780, Accuracy: 0.6694\n",
      "Training loss (for one batch) at step 50: 377.1993, Accuracy: 0.6788\n",
      "Training loss (for one batch) at step 60: 381.6260, Accuracy: 0.6824\n",
      "Training loss (for one batch) at step 70: 390.2901, Accuracy: 0.6784\n",
      "Training loss (for one batch) at step 80: 395.9927, Accuracy: 0.6684\n",
      "Training loss (for one batch) at step 90: 384.8393, Accuracy: 0.6645\n",
      "Training loss (for one batch) at step 100: 365.7827, Accuracy: 0.6658\n",
      "Training loss (for one batch) at step 110: 387.1370, Accuracy: 0.6696\n",
      "---- Training ----\n",
      "Training loss: 125.8060\n",
      "Training acc over epoch: 0.6687\n",
      "---- Validation ----\n",
      "Validation loss: 40.2257\n",
      "Validation acc: 0.6462\n",
      "Time taken: 17.88s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 420.7879, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 391.5366, Accuracy: 0.6335\n",
      "Training loss (for one batch) at step 20: 380.8219, Accuracy: 0.6451\n",
      "Training loss (for one batch) at step 30: 372.0549, Accuracy: 0.6603\n",
      "Training loss (for one batch) at step 40: 362.2480, Accuracy: 0.6734\n",
      "Training loss (for one batch) at step 50: 362.8330, Accuracy: 0.6805\n",
      "Training loss (for one batch) at step 60: 375.2258, Accuracy: 0.6844\n",
      "Training loss (for one batch) at step 70: 380.5325, Accuracy: 0.6796\n",
      "Training loss (for one batch) at step 80: 393.9846, Accuracy: 0.6689\n",
      "Training loss (for one batch) at step 90: 376.6279, Accuracy: 0.6659\n",
      "Training loss (for one batch) at step 100: 369.0342, Accuracy: 0.6694\n",
      "Training loss (for one batch) at step 110: 377.1989, Accuracy: 0.6692\n",
      "---- Training ----\n",
      "Training loss: 119.4053\n",
      "Training acc over epoch: 0.6703\n",
      "---- Validation ----\n",
      "Validation loss: 40.7540\n",
      "Validation acc: 0.6497\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 397.8052, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 384.2949, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 20: 374.4977, Accuracy: 0.6391\n",
      "Training loss (for one batch) at step 30: 354.5113, Accuracy: 0.6613\n",
      "Training loss (for one batch) at step 40: 374.0159, Accuracy: 0.6772\n",
      "Training loss (for one batch) at step 50: 349.1510, Accuracy: 0.6858\n",
      "Training loss (for one batch) at step 60: 373.2856, Accuracy: 0.6898\n",
      "Training loss (for one batch) at step 70: 383.4818, Accuracy: 0.6846\n",
      "Training loss (for one batch) at step 80: 378.2300, Accuracy: 0.6760\n",
      "Training loss (for one batch) at step 90: 367.6711, Accuracy: 0.6755\n",
      "Training loss (for one batch) at step 100: 359.0920, Accuracy: 0.6754\n",
      "Training loss (for one batch) at step 110: 380.6885, Accuracy: 0.6766\n",
      "---- Training ----\n",
      "Training loss: 114.8886\n",
      "Training acc over epoch: 0.6750\n",
      "---- Validation ----\n",
      "Validation loss: 32.4808\n",
      "Validation acc: 0.6357\n",
      "Time taken: 18.08s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 382.9478, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 386.9024, Accuracy: 0.6023\n",
      "Training loss (for one batch) at step 20: 343.3194, Accuracy: 0.6376\n",
      "Training loss (for one batch) at step 30: 346.2848, Accuracy: 0.6628\n",
      "Training loss (for one batch) at step 40: 356.7274, Accuracy: 0.6772\n",
      "Training loss (for one batch) at step 50: 347.5408, Accuracy: 0.6922\n",
      "Training loss (for one batch) at step 60: 364.5546, Accuracy: 0.6960\n",
      "Training loss (for one batch) at step 70: 374.4901, Accuracy: 0.6916\n",
      "Training loss (for one batch) at step 80: 361.6920, Accuracy: 0.6802\n",
      "Training loss (for one batch) at step 90: 347.0066, Accuracy: 0.6739\n",
      "Training loss (for one batch) at step 100: 356.2219, Accuracy: 0.6767\n",
      "Training loss (for one batch) at step 110: 355.4553, Accuracy: 0.6769\n",
      "---- Training ----\n",
      "Training loss: 105.4512\n",
      "Training acc over epoch: 0.6762\n",
      "---- Validation ----\n",
      "Validation loss: 42.1873\n",
      "Validation acc: 0.6373\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 377.1564, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 369.8462, Accuracy: 0.6349\n",
      "Training loss (for one batch) at step 20: 360.3085, Accuracy: 0.6469\n",
      "Training loss (for one batch) at step 30: 334.3696, Accuracy: 0.6648\n",
      "Training loss (for one batch) at step 40: 333.6827, Accuracy: 0.6793\n",
      "Training loss (for one batch) at step 50: 346.0504, Accuracy: 0.6909\n",
      "Training loss (for one batch) at step 60: 344.9240, Accuracy: 0.6935\n",
      "Training loss (for one batch) at step 70: 353.3820, Accuracy: 0.6838\n",
      "Training loss (for one batch) at step 80: 364.9658, Accuracy: 0.6733\n",
      "Training loss (for one batch) at step 90: 339.0846, Accuracy: 0.6700\n",
      "Training loss (for one batch) at step 100: 340.0163, Accuracy: 0.6741\n",
      "Training loss (for one batch) at step 110: 364.9297, Accuracy: 0.6754\n",
      "---- Training ----\n",
      "Training loss: 105.0752\n",
      "Training acc over epoch: 0.6752\n",
      "---- Validation ----\n",
      "Validation loss: 41.2221\n",
      "Validation acc: 0.6378\n",
      "Time taken: 18.05s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 367.1472, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 362.6543, Accuracy: 0.6349\n",
      "Training loss (for one batch) at step 20: 330.6395, Accuracy: 0.6473\n",
      "Training loss (for one batch) at step 30: 327.2588, Accuracy: 0.6736\n",
      "Training loss (for one batch) at step 40: 326.6131, Accuracy: 0.6892\n",
      "Training loss (for one batch) at step 50: 329.7874, Accuracy: 0.6947\n",
      "Training loss (for one batch) at step 60: 344.3569, Accuracy: 0.6979\n",
      "Training loss (for one batch) at step 70: 357.3354, Accuracy: 0.6889\n",
      "Training loss (for one batch) at step 80: 355.1234, Accuracy: 0.6767\n",
      "Training loss (for one batch) at step 90: 324.7793, Accuracy: 0.6744\n",
      "Training loss (for one batch) at step 100: 326.6044, Accuracy: 0.6771\n",
      "Training loss (for one batch) at step 110: 328.2118, Accuracy: 0.6776\n",
      "---- Training ----\n",
      "Training loss: 108.0690\n",
      "Training acc over epoch: 0.6769\n",
      "---- Validation ----\n",
      "Validation loss: 34.9045\n",
      "Validation acc: 0.6515\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 372.1674, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 365.2562, Accuracy: 0.6193\n",
      "Training loss (for one batch) at step 20: 326.6883, Accuracy: 0.6466\n",
      "Training loss (for one batch) at step 30: 327.1672, Accuracy: 0.6615\n",
      "Training loss (for one batch) at step 40: 322.5688, Accuracy: 0.6810\n",
      "Training loss (for one batch) at step 50: 324.1768, Accuracy: 0.6912\n",
      "Training loss (for one batch) at step 60: 320.0630, Accuracy: 0.6962\n",
      "Training loss (for one batch) at step 70: 351.9343, Accuracy: 0.6878\n",
      "Training loss (for one batch) at step 80: 347.0974, Accuracy: 0.6777\n",
      "Training loss (for one batch) at step 90: 320.4592, Accuracy: 0.6745\n",
      "Training loss (for one batch) at step 100: 318.5446, Accuracy: 0.6766\n",
      "Training loss (for one batch) at step 110: 337.3168, Accuracy: 0.6786\n",
      "---- Training ----\n",
      "Training loss: 94.8677\n",
      "Training acc over epoch: 0.6782\n",
      "---- Validation ----\n",
      "Validation loss: 53.7715\n",
      "Validation acc: 0.6300\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 350.3799, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 354.3809, Accuracy: 0.6229\n",
      "Training loss (for one batch) at step 20: 358.9854, Accuracy: 0.6469\n",
      "Training loss (for one batch) at step 30: 316.3149, Accuracy: 0.6691\n",
      "Training loss (for one batch) at step 40: 295.6522, Accuracy: 0.6841\n",
      "Training loss (for one batch) at step 50: 298.7925, Accuracy: 0.6961\n",
      "Training loss (for one batch) at step 60: 321.1999, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 70: 320.7517, Accuracy: 0.6918\n",
      "Training loss (for one batch) at step 80: 327.5638, Accuracy: 0.6775\n",
      "Training loss (for one batch) at step 90: 329.4355, Accuracy: 0.6764\n",
      "Training loss (for one batch) at step 100: 308.3401, Accuracy: 0.6807\n",
      "Training loss (for one batch) at step 110: 324.5541, Accuracy: 0.6826\n",
      "---- Training ----\n",
      "Training loss: 98.2665\n",
      "Training acc over epoch: 0.6820\n",
      "---- Validation ----\n",
      "Validation loss: 35.8406\n",
      "Validation acc: 0.6394\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 361.1605, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 310.5206, Accuracy: 0.5916\n",
      "Training loss (for one batch) at step 20: 335.8506, Accuracy: 0.6347\n",
      "Training loss (for one batch) at step 30: 309.6134, Accuracy: 0.6623\n",
      "Training loss (for one batch) at step 40: 317.0099, Accuracy: 0.6791\n",
      "Training loss (for one batch) at step 50: 303.1926, Accuracy: 0.6906\n",
      "Training loss (for one batch) at step 60: 322.7894, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 70: 319.3210, Accuracy: 0.6877\n",
      "Training loss (for one batch) at step 80: 310.9719, Accuracy: 0.6746\n",
      "Training loss (for one batch) at step 90: 323.0313, Accuracy: 0.6729\n",
      "Training loss (for one batch) at step 100: 322.8864, Accuracy: 0.6768\n",
      "Training loss (for one batch) at step 110: 314.9888, Accuracy: 0.6778\n",
      "---- Training ----\n",
      "Training loss: 125.0779\n",
      "Training acc over epoch: 0.6780\n",
      "---- Validation ----\n",
      "Validation loss: 53.5465\n",
      "Validation acc: 0.6454\n",
      "Time taken: 17.89s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 338.2444, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 333.8692, Accuracy: 0.5923\n",
      "Training loss (for one batch) at step 20: 302.4320, Accuracy: 0.6354\n",
      "Training loss (for one batch) at step 30: 340.4812, Accuracy: 0.6663\n",
      "Training loss (for one batch) at step 40: 294.1064, Accuracy: 0.6770\n",
      "Training loss (for one batch) at step 50: 322.7135, Accuracy: 0.6936\n",
      "Training loss (for one batch) at step 60: 301.2976, Accuracy: 0.6979\n",
      "Training loss (for one batch) at step 70: 332.6234, Accuracy: 0.6889\n",
      "Training loss (for one batch) at step 80: 312.0420, Accuracy: 0.6754\n",
      "Training loss (for one batch) at step 90: 307.0148, Accuracy: 0.6738\n",
      "Training loss (for one batch) at step 100: 298.1872, Accuracy: 0.6764\n",
      "Training loss (for one batch) at step 110: 310.4341, Accuracy: 0.6798\n",
      "---- Training ----\n",
      "Training loss: 107.3118\n",
      "Training acc over epoch: 0.6780\n",
      "---- Validation ----\n",
      "Validation loss: 42.8478\n",
      "Validation acc: 0.6497\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 322.0734, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 330.4127, Accuracy: 0.6023\n",
      "Training loss (for one batch) at step 20: 323.6413, Accuracy: 0.6388\n",
      "Training loss (for one batch) at step 30: 304.3169, Accuracy: 0.6653\n",
      "Training loss (for one batch) at step 40: 297.2115, Accuracy: 0.6820\n",
      "Training loss (for one batch) at step 50: 288.6042, Accuracy: 0.6947\n",
      "Training loss (for one batch) at step 60: 282.1761, Accuracy: 0.7012\n",
      "Training loss (for one batch) at step 70: 322.8974, Accuracy: 0.6896\n",
      "Training loss (for one batch) at step 80: 306.4985, Accuracy: 0.6783\n",
      "Training loss (for one batch) at step 90: 326.2828, Accuracy: 0.6741\n",
      "Training loss (for one batch) at step 100: 285.3299, Accuracy: 0.6788\n",
      "Training loss (for one batch) at step 110: 296.8353, Accuracy: 0.6792\n",
      "---- Training ----\n",
      "Training loss: 102.2683\n",
      "Training acc over epoch: 0.6783\n",
      "---- Validation ----\n",
      "Validation loss: 44.2879\n",
      "Validation acc: 0.6462\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 315.4058, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 308.6534, Accuracy: 0.5966\n",
      "Training loss (for one batch) at step 20: 306.2131, Accuracy: 0.6350\n",
      "Training loss (for one batch) at step 30: 288.6845, Accuracy: 0.6648\n",
      "Training loss (for one batch) at step 40: 287.7975, Accuracy: 0.6862\n",
      "Training loss (for one batch) at step 50: 290.5685, Accuracy: 0.6959\n",
      "Training loss (for one batch) at step 60: 322.2577, Accuracy: 0.6986\n",
      "Training loss (for one batch) at step 70: 337.6751, Accuracy: 0.6864\n",
      "Training loss (for one batch) at step 80: 310.5621, Accuracy: 0.6744\n",
      "Training loss (for one batch) at step 90: 297.1405, Accuracy: 0.6704\n",
      "Training loss (for one batch) at step 100: 285.2321, Accuracy: 0.6771\n",
      "Training loss (for one batch) at step 110: 307.9047, Accuracy: 0.6766\n",
      "---- Training ----\n",
      "Training loss: 97.0136\n",
      "Training acc over epoch: 0.6754\n",
      "---- Validation ----\n",
      "Validation loss: 39.3933\n",
      "Validation acc: 0.6585\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 322.7236, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 318.9171, Accuracy: 0.6065\n",
      "Training loss (for one batch) at step 20: 297.6094, Accuracy: 0.6436\n",
      "Training loss (for one batch) at step 30: 287.9349, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 40: 282.4534, Accuracy: 0.6837\n",
      "Training loss (for one batch) at step 50: 288.5999, Accuracy: 0.6936\n",
      "Training loss (for one batch) at step 60: 321.4345, Accuracy: 0.7002\n",
      "Training loss (for one batch) at step 70: 296.3285, Accuracy: 0.6867\n",
      "Training loss (for one batch) at step 80: 316.0302, Accuracy: 0.6731\n",
      "Training loss (for one batch) at step 90: 294.7823, Accuracy: 0.6730\n",
      "Training loss (for one batch) at step 100: 291.3372, Accuracy: 0.6765\n",
      "Training loss (for one batch) at step 110: 270.5588, Accuracy: 0.6768\n",
      "---- Training ----\n",
      "Training loss: 93.9323\n",
      "Training acc over epoch: 0.6762\n",
      "---- Validation ----\n",
      "Validation loss: 37.8185\n",
      "Validation acc: 0.6359\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 317.7528, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 318.3338, Accuracy: 0.6044\n",
      "Training loss (for one batch) at step 20: 293.9225, Accuracy: 0.6332\n",
      "Training loss (for one batch) at step 30: 275.4675, Accuracy: 0.6583\n",
      "Training loss (for one batch) at step 40: 293.3062, Accuracy: 0.6803\n",
      "Training loss (for one batch) at step 50: 276.8123, Accuracy: 0.6918\n",
      "Training loss (for one batch) at step 60: 298.6218, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 70: 303.1834, Accuracy: 0.6878\n",
      "Training loss (for one batch) at step 80: 297.4655, Accuracy: 0.6769\n",
      "Training loss (for one batch) at step 90: 291.6742, Accuracy: 0.6718\n",
      "Training loss (for one batch) at step 100: 286.7797, Accuracy: 0.6760\n",
      "Training loss (for one batch) at step 110: 304.9691, Accuracy: 0.6774\n",
      "---- Training ----\n",
      "Training loss: 97.9789\n",
      "Training acc over epoch: 0.6769\n",
      "---- Validation ----\n",
      "Validation loss: 55.6707\n",
      "Validation acc: 0.6397\n",
      "Time taken: 18.12s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 316.9099, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 301.1693, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 20: 290.9957, Accuracy: 0.6276\n",
      "Training loss (for one batch) at step 30: 299.7485, Accuracy: 0.6578\n",
      "Training loss (for one batch) at step 40: 279.1696, Accuracy: 0.6770\n",
      "Training loss (for one batch) at step 50: 293.7192, Accuracy: 0.6881\n",
      "Training loss (for one batch) at step 60: 290.3352, Accuracy: 0.6965\n",
      "Training loss (for one batch) at step 70: 297.6807, Accuracy: 0.6916\n",
      "Training loss (for one batch) at step 80: 308.5107, Accuracy: 0.6773\n",
      "Training loss (for one batch) at step 90: 285.6612, Accuracy: 0.6745\n",
      "Training loss (for one batch) at step 100: 268.8959, Accuracy: 0.6771\n",
      "Training loss (for one batch) at step 110: 288.1691, Accuracy: 0.6774\n",
      "---- Training ----\n",
      "Training loss: 92.2591\n",
      "Training acc over epoch: 0.6781\n",
      "---- Validation ----\n",
      "Validation loss: 47.1155\n",
      "Validation acc: 0.6464\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 323.4889, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 313.8449, Accuracy: 0.6001\n",
      "Training loss (for one batch) at step 20: 284.5727, Accuracy: 0.6228\n",
      "Training loss (for one batch) at step 30: 284.3168, Accuracy: 0.6583\n",
      "Training loss (for one batch) at step 40: 255.3247, Accuracy: 0.6801\n",
      "Training loss (for one batch) at step 50: 284.4904, Accuracy: 0.6909\n",
      "Training loss (for one batch) at step 60: 294.2430, Accuracy: 0.6976\n",
      "Training loss (for one batch) at step 70: 310.3615, Accuracy: 0.6895\n",
      "Training loss (for one batch) at step 80: 281.3292, Accuracy: 0.6750\n",
      "Training loss (for one batch) at step 90: 285.7642, Accuracy: 0.6708\n",
      "Training loss (for one batch) at step 100: 286.5591, Accuracy: 0.6769\n",
      "Training loss (for one batch) at step 110: 286.0782, Accuracy: 0.6760\n",
      "---- Training ----\n",
      "Training loss: 91.3843\n",
      "Training acc over epoch: 0.6761\n",
      "---- Validation ----\n",
      "Validation loss: 44.2816\n",
      "Validation acc: 0.6427\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 295.6713, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 294.4841, Accuracy: 0.5916\n",
      "Training loss (for one batch) at step 20: 274.9896, Accuracy: 0.6298\n",
      "Training loss (for one batch) at step 30: 266.7711, Accuracy: 0.6605\n",
      "Training loss (for one batch) at step 40: 282.2892, Accuracy: 0.6795\n",
      "Training loss (for one batch) at step 50: 262.8755, Accuracy: 0.6935\n",
      "Training loss (for one batch) at step 60: 278.6822, Accuracy: 0.6971\n",
      "Training loss (for one batch) at step 70: 308.7669, Accuracy: 0.6855\n",
      "Training loss (for one batch) at step 80: 301.5269, Accuracy: 0.6741\n",
      "Training loss (for one batch) at step 90: 294.7954, Accuracy: 0.6704\n",
      "Training loss (for one batch) at step 100: 275.9825, Accuracy: 0.6747\n",
      "Training loss (for one batch) at step 110: 271.7517, Accuracy: 0.6770\n",
      "---- Training ----\n",
      "Training loss: 92.3996\n",
      "Training acc over epoch: 0.6761\n",
      "---- Validation ----\n",
      "Validation loss: 33.7436\n",
      "Validation acc: 0.6515\n",
      "Time taken: 18.13s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 284.9555, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 291.4059, Accuracy: 0.6072\n",
      "Training loss (for one batch) at step 20: 281.8972, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 30: 276.4483, Accuracy: 0.6585\n",
      "Training loss (for one batch) at step 40: 268.2607, Accuracy: 0.6808\n",
      "Training loss (for one batch) at step 50: 260.5991, Accuracy: 0.6913\n",
      "Training loss (for one batch) at step 60: 285.2042, Accuracy: 0.6944\n",
      "Training loss (for one batch) at step 70: 281.3798, Accuracy: 0.6849\n",
      "Training loss (for one batch) at step 80: 296.7425, Accuracy: 0.6705\n",
      "Training loss (for one batch) at step 90: 285.5240, Accuracy: 0.6698\n",
      "Training loss (for one batch) at step 100: 277.8974, Accuracy: 0.6741\n",
      "Training loss (for one batch) at step 110: 295.1300, Accuracy: 0.6750\n",
      "---- Training ----\n",
      "Training loss: 96.9165\n",
      "Training acc over epoch: 0.6736\n",
      "---- Validation ----\n",
      "Validation loss: 43.5934\n",
      "Validation acc: 0.6411\n",
      "Time taken: 18.06s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 287.5439, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 313.6797, Accuracy: 0.6072\n",
      "Training loss (for one batch) at step 20: 301.5068, Accuracy: 0.6306\n",
      "Training loss (for one batch) at step 30: 289.2065, Accuracy: 0.6573\n",
      "Training loss (for one batch) at step 40: 275.9918, Accuracy: 0.6793\n",
      "Training loss (for one batch) at step 50: 259.2873, Accuracy: 0.6912\n",
      "Training loss (for one batch) at step 60: 289.3066, Accuracy: 0.6960\n",
      "Training loss (for one batch) at step 70: 290.9865, Accuracy: 0.6846\n",
      "Training loss (for one batch) at step 80: 277.4861, Accuracy: 0.6706\n",
      "Training loss (for one batch) at step 90: 269.1396, Accuracy: 0.6694\n",
      "Training loss (for one batch) at step 100: 259.2956, Accuracy: 0.6737\n",
      "Training loss (for one batch) at step 110: 255.1975, Accuracy: 0.6752\n",
      "---- Training ----\n",
      "Training loss: 95.4696\n",
      "Training acc over epoch: 0.6744\n",
      "---- Validation ----\n",
      "Validation loss: 44.5897\n",
      "Validation acc: 0.6480\n",
      "Time taken: 18.05s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 287.5032, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 296.1346, Accuracy: 0.6080\n",
      "Training loss (for one batch) at step 20: 258.1904, Accuracy: 0.6336\n",
      "Training loss (for one batch) at step 30: 272.8995, Accuracy: 0.6610\n",
      "Training loss (for one batch) at step 40: 268.5227, Accuracy: 0.6799\n",
      "Training loss (for one batch) at step 50: 253.1370, Accuracy: 0.6873\n",
      "Training loss (for one batch) at step 60: 287.0242, Accuracy: 0.6926\n",
      "Training loss (for one batch) at step 70: 278.6381, Accuracy: 0.6830\n",
      "Training loss (for one batch) at step 80: 295.3018, Accuracy: 0.6707\n",
      "Training loss (for one batch) at step 90: 246.3027, Accuracy: 0.6691\n",
      "Training loss (for one batch) at step 100: 259.7681, Accuracy: 0.6740\n",
      "Training loss (for one batch) at step 110: 267.7615, Accuracy: 0.6762\n",
      "---- Training ----\n",
      "Training loss: 96.4179\n",
      "Training acc over epoch: 0.6742\n",
      "---- Validation ----\n",
      "Validation loss: 38.4463\n",
      "Validation acc: 0.6290\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 285.8960, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 296.5284, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 20: 274.3869, Accuracy: 0.6324\n",
      "Training loss (for one batch) at step 30: 278.5335, Accuracy: 0.6610\n",
      "Training loss (for one batch) at step 40: 255.6366, Accuracy: 0.6747\n",
      "Training loss (for one batch) at step 50: 253.4033, Accuracy: 0.6883\n",
      "Training loss (for one batch) at step 60: 280.6614, Accuracy: 0.6948\n",
      "Training loss (for one batch) at step 70: 287.2792, Accuracy: 0.6846\n",
      "Training loss (for one batch) at step 80: 293.8996, Accuracy: 0.6700\n",
      "Training loss (for one batch) at step 90: 286.2721, Accuracy: 0.6679\n",
      "Training loss (for one batch) at step 100: 280.4457, Accuracy: 0.6720\n",
      "Training loss (for one batch) at step 110: 250.3300, Accuracy: 0.6734\n",
      "---- Training ----\n",
      "Training loss: 84.6477\n",
      "Training acc over epoch: 0.6738\n",
      "---- Validation ----\n",
      "Validation loss: 40.4138\n",
      "Validation acc: 0.6373\n",
      "Time taken: 18.08s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 298.9532, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 284.8357, Accuracy: 0.5817\n",
      "Training loss (for one batch) at step 20: 272.9135, Accuracy: 0.6324\n",
      "Training loss (for one batch) at step 30: 260.8520, Accuracy: 0.6595\n",
      "Training loss (for one batch) at step 40: 264.6314, Accuracy: 0.6799\n",
      "Training loss (for one batch) at step 50: 277.8818, Accuracy: 0.6936\n",
      "Training loss (for one batch) at step 60: 255.6625, Accuracy: 0.6945\n",
      "Training loss (for one batch) at step 70: 295.2930, Accuracy: 0.6853\n",
      "Training loss (for one batch) at step 80: 283.0558, Accuracy: 0.6713\n",
      "Training loss (for one batch) at step 90: 269.6849, Accuracy: 0.6694\n",
      "Training loss (for one batch) at step 100: 265.5017, Accuracy: 0.6728\n",
      "Training loss (for one batch) at step 110: 269.9602, Accuracy: 0.6735\n",
      "---- Training ----\n",
      "Training loss: 97.2076\n",
      "Training acc over epoch: 0.6731\n",
      "---- Validation ----\n",
      "Validation loss: 60.9573\n",
      "Validation acc: 0.6282\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 294.2681, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 299.6577, Accuracy: 0.6087\n",
      "Training loss (for one batch) at step 20: 277.6195, Accuracy: 0.6425\n",
      "Training loss (for one batch) at step 30: 259.1262, Accuracy: 0.6626\n",
      "Training loss (for one batch) at step 40: 268.7010, Accuracy: 0.6761\n",
      "Training loss (for one batch) at step 50: 264.8115, Accuracy: 0.6889\n",
      "Training loss (for one batch) at step 60: 287.5270, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 70: 295.0222, Accuracy: 0.6882\n",
      "Training loss (for one batch) at step 80: 270.6045, Accuracy: 0.6735\n",
      "Training loss (for one batch) at step 90: 266.3172, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 100: 286.4821, Accuracy: 0.6763\n",
      "Training loss (for one batch) at step 110: 268.4180, Accuracy: 0.6770\n",
      "---- Training ----\n",
      "Training loss: 93.4271\n",
      "Training acc over epoch: 0.6769\n",
      "---- Validation ----\n",
      "Validation loss: 73.4028\n",
      "Validation acc: 0.6351\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 304.9612, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 264.6426, Accuracy: 0.5952\n",
      "Training loss (for one batch) at step 20: 272.9127, Accuracy: 0.6343\n",
      "Training loss (for one batch) at step 30: 258.0925, Accuracy: 0.6620\n",
      "Training loss (for one batch) at step 40: 244.0344, Accuracy: 0.6789\n",
      "Training loss (for one batch) at step 50: 259.1888, Accuracy: 0.6900\n",
      "Training loss (for one batch) at step 60: 267.7538, Accuracy: 0.6968\n",
      "Training loss (for one batch) at step 70: 277.4805, Accuracy: 0.6836\n",
      "Training loss (for one batch) at step 80: 299.6213, Accuracy: 0.6734\n",
      "Training loss (for one batch) at step 90: 267.4702, Accuracy: 0.6701\n",
      "Training loss (for one batch) at step 100: 264.5982, Accuracy: 0.6749\n",
      "Training loss (for one batch) at step 110: 252.9935, Accuracy: 0.6762\n",
      "---- Training ----\n",
      "Training loss: 85.0485\n",
      "Training acc over epoch: 0.6750\n",
      "---- Validation ----\n",
      "Validation loss: 37.1285\n",
      "Validation acc: 0.6459\n",
      "Time taken: 18.06s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 304.4697, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 282.0305, Accuracy: 0.5980\n",
      "Training loss (for one batch) at step 20: 265.1854, Accuracy: 0.6362\n",
      "Training loss (for one batch) at step 30: 266.9551, Accuracy: 0.6618\n",
      "Training loss (for one batch) at step 40: 261.7966, Accuracy: 0.6824\n",
      "Training loss (for one batch) at step 50: 266.6916, Accuracy: 0.6935\n",
      "Training loss (for one batch) at step 60: 269.2784, Accuracy: 0.6970\n",
      "Training loss (for one batch) at step 70: 238.1109, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 80: 273.2242, Accuracy: 0.6740\n",
      "Training loss (for one batch) at step 90: 264.1142, Accuracy: 0.6720\n",
      "Training loss (for one batch) at step 100: 255.9581, Accuracy: 0.6769\n",
      "Training loss (for one batch) at step 110: 264.2314, Accuracy: 0.6774\n",
      "---- Training ----\n",
      "Training loss: 84.2386\n",
      "Training acc over epoch: 0.6757\n",
      "---- Validation ----\n",
      "Validation loss: 41.9104\n",
      "Validation acc: 0.6381\n",
      "Time taken: 18.07s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 301.8897, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 302.1731, Accuracy: 0.5959\n",
      "Training loss (for one batch) at step 20: 254.4904, Accuracy: 0.6380\n",
      "Training loss (for one batch) at step 30: 273.9876, Accuracy: 0.6605\n",
      "Training loss (for one batch) at step 40: 241.0614, Accuracy: 0.6822\n",
      "Training loss (for one batch) at step 50: 268.1376, Accuracy: 0.6935\n",
      "Training loss (for one batch) at step 60: 249.2813, Accuracy: 0.7009\n",
      "Training loss (for one batch) at step 70: 263.5834, Accuracy: 0.6883\n",
      "Training loss (for one batch) at step 80: 259.8416, Accuracy: 0.6734\n",
      "Training loss (for one batch) at step 90: 274.1905, Accuracy: 0.6708\n",
      "Training loss (for one batch) at step 100: 259.1569, Accuracy: 0.6753\n",
      "Training loss (for one batch) at step 110: 268.8780, Accuracy: 0.6764\n",
      "---- Training ----\n",
      "Training loss: 80.6633\n",
      "Training acc over epoch: 0.6762\n",
      "---- Validation ----\n",
      "Validation loss: 48.4448\n",
      "Validation acc: 0.6405\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 289.3752, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 287.6594, Accuracy: 0.5980\n",
      "Training loss (for one batch) at step 20: 262.1455, Accuracy: 0.6298\n",
      "Training loss (for one batch) at step 30: 276.0059, Accuracy: 0.6583\n",
      "Training loss (for one batch) at step 40: 249.9386, Accuracy: 0.6761\n",
      "Training loss (for one batch) at step 50: 251.1743, Accuracy: 0.6900\n",
      "Training loss (for one batch) at step 60: 256.0414, Accuracy: 0.6952\n",
      "Training loss (for one batch) at step 70: 278.7011, Accuracy: 0.6857\n",
      "Training loss (for one batch) at step 80: 276.8915, Accuracy: 0.6732\n",
      "Training loss (for one batch) at step 90: 268.2662, Accuracy: 0.6684\n",
      "Training loss (for one batch) at step 100: 257.9880, Accuracy: 0.6733\n",
      "Training loss (for one batch) at step 110: 270.0763, Accuracy: 0.6755\n",
      "---- Training ----\n",
      "Training loss: 94.5254\n",
      "Training acc over epoch: 0.6742\n",
      "---- Validation ----\n",
      "Validation loss: 79.2819\n",
      "Validation acc: 0.6196\n",
      "Time taken: 17.89s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 282.6270, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 264.3157, Accuracy: 0.5902\n",
      "Training loss (for one batch) at step 20: 263.4290, Accuracy: 0.6209\n",
      "Training loss (for one batch) at step 30: 246.8172, Accuracy: 0.6530\n",
      "Training loss (for one batch) at step 40: 265.3979, Accuracy: 0.6766\n",
      "Training loss (for one batch) at step 50: 252.0313, Accuracy: 0.6898\n",
      "Training loss (for one batch) at step 60: 276.5964, Accuracy: 0.6954\n",
      "Training loss (for one batch) at step 70: 275.1111, Accuracy: 0.6829\n",
      "Training loss (for one batch) at step 80: 287.4301, Accuracy: 0.6718\n",
      "Training loss (for one batch) at step 90: 258.6741, Accuracy: 0.6684\n",
      "Training loss (for one batch) at step 100: 264.7816, Accuracy: 0.6728\n",
      "Training loss (for one batch) at step 110: 263.2928, Accuracy: 0.6753\n",
      "---- Training ----\n",
      "Training loss: 93.7701\n",
      "Training acc over epoch: 0.6740\n",
      "---- Validation ----\n",
      "Validation loss: 62.2945\n",
      "Validation acc: 0.6359\n",
      "Time taken: 18.19s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 266.3078, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 309.7728, Accuracy: 0.5810\n",
      "Training loss (for one batch) at step 20: 257.8456, Accuracy: 0.6287\n",
      "Training loss (for one batch) at step 30: 272.2661, Accuracy: 0.6590\n",
      "Training loss (for one batch) at step 40: 265.2413, Accuracy: 0.6749\n",
      "Training loss (for one batch) at step 50: 248.0137, Accuracy: 0.6889\n",
      "Training loss (for one batch) at step 60: 247.7488, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 70: 275.4558, Accuracy: 0.6867\n",
      "Training loss (for one batch) at step 80: 257.7468, Accuracy: 0.6706\n",
      "Training loss (for one batch) at step 90: 251.3121, Accuracy: 0.6692\n",
      "Training loss (for one batch) at step 100: 275.9735, Accuracy: 0.6733\n",
      "Training loss (for one batch) at step 110: 243.9374, Accuracy: 0.6738\n",
      "---- Training ----\n",
      "Training loss: 82.6743\n",
      "Training acc over epoch: 0.6734\n",
      "---- Validation ----\n",
      "Validation loss: 43.4932\n",
      "Validation acc: 0.6378\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 274.6571, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 265.4450, Accuracy: 0.5888\n",
      "Training loss (for one batch) at step 20: 261.3907, Accuracy: 0.6265\n",
      "Training loss (for one batch) at step 30: 231.7129, Accuracy: 0.6583\n",
      "Training loss (for one batch) at step 40: 234.5127, Accuracy: 0.6764\n",
      "Training loss (for one batch) at step 50: 244.5832, Accuracy: 0.6892\n",
      "Training loss (for one batch) at step 60: 254.3265, Accuracy: 0.6974\n",
      "Training loss (for one batch) at step 70: 278.5872, Accuracy: 0.6856\n",
      "Training loss (for one batch) at step 80: 275.1272, Accuracy: 0.6715\n",
      "Training loss (for one batch) at step 90: 261.4950, Accuracy: 0.6690\n",
      "Training loss (for one batch) at step 100: 248.0922, Accuracy: 0.6743\n",
      "Training loss (for one batch) at step 110: 247.3461, Accuracy: 0.6743\n",
      "---- Training ----\n",
      "Training loss: 78.5229\n",
      "Training acc over epoch: 0.6739\n",
      "---- Validation ----\n",
      "Validation loss: 43.3144\n",
      "Validation acc: 0.6531\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 252.7578, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 275.8142, Accuracy: 0.6023\n",
      "Training loss (for one batch) at step 20: 279.3598, Accuracy: 0.6350\n",
      "Training loss (for one batch) at step 30: 246.2735, Accuracy: 0.6631\n",
      "Training loss (for one batch) at step 40: 239.8820, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 50: 271.5031, Accuracy: 0.6936\n",
      "Training loss (for one batch) at step 60: 279.6529, Accuracy: 0.6974\n",
      "Training loss (for one batch) at step 70: 266.8412, Accuracy: 0.6852\n",
      "Training loss (for one batch) at step 80: 257.6720, Accuracy: 0.6754\n",
      "Training loss (for one batch) at step 90: 243.2833, Accuracy: 0.6692\n",
      "Training loss (for one batch) at step 100: 269.1353, Accuracy: 0.6739\n",
      "Training loss (for one batch) at step 110: 254.2550, Accuracy: 0.6751\n",
      "---- Training ----\n",
      "Training loss: 81.2275\n",
      "Training acc over epoch: 0.6747\n",
      "---- Validation ----\n",
      "Validation loss: 55.9317\n",
      "Validation acc: 0.6456\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 260.2778, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 275.1057, Accuracy: 0.6030\n",
      "Training loss (for one batch) at step 20: 266.3962, Accuracy: 0.6354\n",
      "Training loss (for one batch) at step 30: 246.7548, Accuracy: 0.6613\n",
      "Training loss (for one batch) at step 40: 241.4856, Accuracy: 0.6827\n",
      "Training loss (for one batch) at step 50: 240.5988, Accuracy: 0.6944\n",
      "Training loss (for one batch) at step 60: 261.0689, Accuracy: 0.6929\n",
      "Training loss (for one batch) at step 70: 261.6797, Accuracy: 0.6843\n",
      "Training loss (for one batch) at step 80: 244.5917, Accuracy: 0.6703\n",
      "Training loss (for one batch) at step 90: 246.1138, Accuracy: 0.6681\n",
      "Training loss (for one batch) at step 100: 249.7860, Accuracy: 0.6734\n",
      "Training loss (for one batch) at step 110: 236.8946, Accuracy: 0.6738\n",
      "---- Training ----\n",
      "Training loss: 83.2455\n",
      "Training acc over epoch: 0.6732\n",
      "---- Validation ----\n",
      "Validation loss: 59.1125\n",
      "Validation acc: 0.6359\n",
      "Time taken: 18.27s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 267.8145, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 270.0772, Accuracy: 0.5973\n",
      "Training loss (for one batch) at step 20: 250.4429, Accuracy: 0.6310\n",
      "Training loss (for one batch) at step 30: 250.3196, Accuracy: 0.6610\n",
      "Training loss (for one batch) at step 40: 246.1710, Accuracy: 0.6782\n",
      "Training loss (for one batch) at step 50: 248.3650, Accuracy: 0.6906\n",
      "Training loss (for one batch) at step 60: 228.6526, Accuracy: 0.6971\n",
      "Training loss (for one batch) at step 70: 275.9854, Accuracy: 0.6872\n",
      "Training loss (for one batch) at step 80: 255.8995, Accuracy: 0.6727\n",
      "Training loss (for one batch) at step 90: 241.2446, Accuracy: 0.6704\n",
      "Training loss (for one batch) at step 100: 249.0837, Accuracy: 0.6744\n",
      "Training loss (for one batch) at step 110: 250.3093, Accuracy: 0.6765\n",
      "---- Training ----\n",
      "Training loss: 71.8127\n",
      "Training acc over epoch: 0.6752\n",
      "---- Validation ----\n",
      "Validation loss: 49.0540\n",
      "Validation acc: 0.6386\n",
      "Time taken: 18.07s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 264.2938, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 266.4023, Accuracy: 0.5831\n",
      "Training loss (for one batch) at step 20: 257.0665, Accuracy: 0.6310\n",
      "Training loss (for one batch) at step 30: 243.4016, Accuracy: 0.6593\n",
      "Training loss (for one batch) at step 40: 260.2841, Accuracy: 0.6778\n",
      "Training loss (for one batch) at step 50: 232.6260, Accuracy: 0.6906\n",
      "Training loss (for one batch) at step 60: 251.3794, Accuracy: 0.6952\n",
      "Training loss (for one batch) at step 70: 261.1005, Accuracy: 0.6842\n",
      "Training loss (for one batch) at step 80: 284.3145, Accuracy: 0.6710\n",
      "Training loss (for one batch) at step 90: 256.4799, Accuracy: 0.6676\n",
      "Training loss (for one batch) at step 100: 245.9436, Accuracy: 0.6724\n",
      "Training loss (for one batch) at step 110: 254.3429, Accuracy: 0.6733\n",
      "---- Training ----\n",
      "Training loss: 81.6497\n",
      "Training acc over epoch: 0.6720\n",
      "---- Validation ----\n",
      "Validation loss: 46.0648\n",
      "Validation acc: 0.6306\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 303.2997, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 263.0148, Accuracy: 0.6058\n",
      "Training loss (for one batch) at step 20: 266.7969, Accuracy: 0.6343\n",
      "Training loss (for one batch) at step 30: 241.7078, Accuracy: 0.6522\n",
      "Training loss (for one batch) at step 40: 234.8432, Accuracy: 0.6715\n",
      "Training loss (for one batch) at step 50: 246.6228, Accuracy: 0.6877\n",
      "Training loss (for one batch) at step 60: 232.5569, Accuracy: 0.6930\n",
      "Training loss (for one batch) at step 70: 234.2133, Accuracy: 0.6833\n",
      "Training loss (for one batch) at step 80: 273.8362, Accuracy: 0.6705\n",
      "Training loss (for one batch) at step 90: 256.7092, Accuracy: 0.6681\n",
      "Training loss (for one batch) at step 100: 251.2959, Accuracy: 0.6729\n",
      "Training loss (for one batch) at step 110: 250.2507, Accuracy: 0.6756\n",
      "---- Training ----\n",
      "Training loss: 84.1493\n",
      "Training acc over epoch: 0.6735\n",
      "---- Validation ----\n",
      "Validation loss: 54.0739\n",
      "Validation acc: 0.6330\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 292.0095, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 244.5307, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 20: 266.2852, Accuracy: 0.6298\n",
      "Training loss (for one batch) at step 30: 263.9462, Accuracy: 0.6590\n",
      "Training loss (for one batch) at step 40: 257.2309, Accuracy: 0.6772\n",
      "Training loss (for one batch) at step 50: 233.7704, Accuracy: 0.6912\n",
      "Training loss (for one batch) at step 60: 229.3533, Accuracy: 0.6954\n",
      "Training loss (for one batch) at step 70: 261.2345, Accuracy: 0.6858\n",
      "Training loss (for one batch) at step 80: 269.0093, Accuracy: 0.6713\n",
      "Training loss (for one batch) at step 90: 239.5087, Accuracy: 0.6702\n",
      "Training loss (for one batch) at step 100: 249.1293, Accuracy: 0.6756\n",
      "Training loss (for one batch) at step 110: 245.9393, Accuracy: 0.6762\n",
      "---- Training ----\n",
      "Training loss: 76.5866\n",
      "Training acc over epoch: 0.6747\n",
      "---- Validation ----\n",
      "Validation loss: 48.6011\n",
      "Validation acc: 0.6537\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 254.8730, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 253.3202, Accuracy: 0.5930\n",
      "Training loss (for one batch) at step 20: 238.6443, Accuracy: 0.6276\n",
      "Training loss (for one batch) at step 30: 233.4929, Accuracy: 0.6557\n",
      "Training loss (for one batch) at step 40: 235.4684, Accuracy: 0.6772\n",
      "Training loss (for one batch) at step 50: 231.9325, Accuracy: 0.6904\n",
      "Training loss (for one batch) at step 60: 264.3782, Accuracy: 0.6947\n",
      "Training loss (for one batch) at step 70: 278.7671, Accuracy: 0.6831\n",
      "Training loss (for one batch) at step 80: 246.1538, Accuracy: 0.6687\n",
      "Training loss (for one batch) at step 90: 276.4095, Accuracy: 0.6673\n",
      "Training loss (for one batch) at step 100: 234.7998, Accuracy: 0.6723\n",
      "Training loss (for one batch) at step 110: 251.0562, Accuracy: 0.6736\n",
      "---- Training ----\n",
      "Training loss: 82.9592\n",
      "Training acc over epoch: 0.6729\n",
      "---- Validation ----\n",
      "Validation loss: 47.1906\n",
      "Validation acc: 0.6368\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 292.7794, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 291.6601, Accuracy: 0.5952\n",
      "Training loss (for one batch) at step 20: 241.5376, Accuracy: 0.6291\n",
      "Training loss (for one batch) at step 30: 231.6342, Accuracy: 0.6623\n",
      "Training loss (for one batch) at step 40: 236.3747, Accuracy: 0.6799\n",
      "Training loss (for one batch) at step 50: 232.4701, Accuracy: 0.6933\n",
      "Training loss (for one batch) at step 60: 233.4533, Accuracy: 0.6977\n",
      "Training loss (for one batch) at step 70: 251.0496, Accuracy: 0.6854\n",
      "Training loss (for one batch) at step 80: 252.0229, Accuracy: 0.6729\n",
      "Training loss (for one batch) at step 90: 220.9860, Accuracy: 0.6669\n",
      "Training loss (for one batch) at step 100: 237.1414, Accuracy: 0.6737\n",
      "Training loss (for one batch) at step 110: 239.9420, Accuracy: 0.6739\n",
      "---- Training ----\n",
      "Training loss: 94.3604\n",
      "Training acc over epoch: 0.6742\n",
      "---- Validation ----\n",
      "Validation loss: 52.8516\n",
      "Validation acc: 0.6448\n",
      "Time taken: 18.11s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 283.5112, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 252.0031, Accuracy: 0.6072\n",
      "Training loss (for one batch) at step 20: 246.9476, Accuracy: 0.6343\n",
      "Training loss (for one batch) at step 30: 250.9044, Accuracy: 0.6638\n",
      "Training loss (for one batch) at step 40: 252.5773, Accuracy: 0.6791\n",
      "Training loss (for one batch) at step 50: 234.3558, Accuracy: 0.6919\n",
      "Training loss (for one batch) at step 60: 232.2760, Accuracy: 0.6956\n",
      "Training loss (for one batch) at step 70: 253.1357, Accuracy: 0.6872\n",
      "Training loss (for one batch) at step 80: 241.9730, Accuracy: 0.6710\n",
      "Training loss (for one batch) at step 90: 232.5439, Accuracy: 0.6678\n",
      "Training loss (for one batch) at step 100: 228.7252, Accuracy: 0.6732\n",
      "Training loss (for one batch) at step 110: 237.0430, Accuracy: 0.6734\n",
      "---- Training ----\n",
      "Training loss: 78.2889\n",
      "Training acc over epoch: 0.6731\n",
      "---- Validation ----\n",
      "Validation loss: 42.0125\n",
      "Validation acc: 0.6491\n",
      "Time taken: 18.09s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 265.4194, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 249.6892, Accuracy: 0.5874\n",
      "Training loss (for one batch) at step 20: 255.4488, Accuracy: 0.6287\n",
      "Training loss (for one batch) at step 30: 235.0187, Accuracy: 0.6585\n",
      "Training loss (for one batch) at step 40: 243.8880, Accuracy: 0.6749\n",
      "Training loss (for one batch) at step 50: 231.3364, Accuracy: 0.6877\n",
      "Training loss (for one batch) at step 60: 263.6176, Accuracy: 0.6936\n",
      "Training loss (for one batch) at step 70: 238.8308, Accuracy: 0.6842\n",
      "Training loss (for one batch) at step 80: 241.5981, Accuracy: 0.6710\n",
      "Training loss (for one batch) at step 90: 228.8587, Accuracy: 0.6684\n",
      "Training loss (for one batch) at step 100: 228.2262, Accuracy: 0.6747\n",
      "Training loss (for one batch) at step 110: 241.4419, Accuracy: 0.6753\n",
      "---- Training ----\n",
      "Training loss: 79.7943\n",
      "Training acc over epoch: 0.6742\n",
      "---- Validation ----\n",
      "Validation loss: 55.2478\n",
      "Validation acc: 0.6451\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 256.1316, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 268.7191, Accuracy: 0.5945\n",
      "Training loss (for one batch) at step 20: 256.6562, Accuracy: 0.6243\n",
      "Training loss (for one batch) at step 30: 244.6098, Accuracy: 0.6537\n",
      "Training loss (for one batch) at step 40: 234.8908, Accuracy: 0.6799\n",
      "Training loss (for one batch) at step 50: 243.1493, Accuracy: 0.6922\n",
      "Training loss (for one batch) at step 60: 237.7090, Accuracy: 0.6988\n",
      "Training loss (for one batch) at step 70: 254.5808, Accuracy: 0.6844\n",
      "Training loss (for one batch) at step 80: 255.2714, Accuracy: 0.6695\n",
      "Training loss (for one batch) at step 90: 227.3226, Accuracy: 0.6670\n",
      "Training loss (for one batch) at step 100: 263.7126, Accuracy: 0.6731\n",
      "Training loss (for one batch) at step 110: 228.5986, Accuracy: 0.6734\n",
      "---- Training ----\n",
      "Training loss: 80.2013\n",
      "Training acc over epoch: 0.6730\n",
      "---- Validation ----\n",
      "Validation loss: 46.0374\n",
      "Validation acc: 0.6435\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 257.3939, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 260.2831, Accuracy: 0.5923\n",
      "Training loss (for one batch) at step 20: 255.1714, Accuracy: 0.6239\n",
      "Training loss (for one batch) at step 30: 247.4670, Accuracy: 0.6583\n",
      "Training loss (for one batch) at step 40: 247.8583, Accuracy: 0.6772\n",
      "Training loss (for one batch) at step 50: 273.6803, Accuracy: 0.6883\n",
      "Training loss (for one batch) at step 60: 242.6316, Accuracy: 0.6917\n",
      "Training loss (for one batch) at step 70: 243.7984, Accuracy: 0.6857\n",
      "Training loss (for one batch) at step 80: 271.5358, Accuracy: 0.6711\n",
      "Training loss (for one batch) at step 90: 239.6063, Accuracy: 0.6662\n",
      "Training loss (for one batch) at step 100: 239.7342, Accuracy: 0.6709\n",
      "Training loss (for one batch) at step 110: 275.8598, Accuracy: 0.6724\n",
      "---- Training ----\n",
      "Training loss: 78.1956\n",
      "Training acc over epoch: 0.6713\n",
      "---- Validation ----\n",
      "Validation loss: 45.3578\n",
      "Validation acc: 0.6534\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 263.8450, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 269.4283, Accuracy: 0.5952\n",
      "Training loss (for one batch) at step 20: 243.4392, Accuracy: 0.6276\n",
      "Training loss (for one batch) at step 30: 241.4828, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 40: 226.1360, Accuracy: 0.6789\n",
      "Training loss (for one batch) at step 50: 233.9120, Accuracy: 0.6903\n",
      "Training loss (for one batch) at step 60: 249.1915, Accuracy: 0.6940\n",
      "Training loss (for one batch) at step 70: 266.7167, Accuracy: 0.6828\n",
      "Training loss (for one batch) at step 80: 247.8843, Accuracy: 0.6710\n",
      "Training loss (for one batch) at step 90: 228.8316, Accuracy: 0.6685\n",
      "Training loss (for one batch) at step 100: 227.8295, Accuracy: 0.6730\n",
      "Training loss (for one batch) at step 110: 250.2902, Accuracy: 0.6751\n",
      "---- Training ----\n",
      "Training loss: 77.6986\n",
      "Training acc over epoch: 0.6742\n",
      "---- Validation ----\n",
      "Validation loss: 54.9486\n",
      "Validation acc: 0.6462\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 249.3685, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 249.7164, Accuracy: 0.6030\n",
      "Training loss (for one batch) at step 20: 252.2044, Accuracy: 0.6302\n",
      "Training loss (for one batch) at step 30: 236.5892, Accuracy: 0.6557\n",
      "Training loss (for one batch) at step 40: 238.9627, Accuracy: 0.6789\n",
      "Training loss (for one batch) at step 50: 240.9742, Accuracy: 0.6878\n",
      "Training loss (for one batch) at step 60: 239.5691, Accuracy: 0.6931\n",
      "Training loss (for one batch) at step 70: 285.2472, Accuracy: 0.6842\n",
      "Training loss (for one batch) at step 80: 274.5722, Accuracy: 0.6694\n",
      "Training loss (for one batch) at step 90: 244.2500, Accuracy: 0.6680\n",
      "Training loss (for one batch) at step 100: 236.7205, Accuracy: 0.6723\n",
      "Training loss (for one batch) at step 110: 246.2989, Accuracy: 0.6721\n",
      "---- Training ----\n",
      "Training loss: 76.1518\n",
      "Training acc over epoch: 0.6715\n",
      "---- Validation ----\n",
      "Validation loss: 50.3123\n",
      "Validation acc: 0.6330\n",
      "Time taken: 18.05s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 257.9314, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 262.6848, Accuracy: 0.5810\n",
      "Training loss (for one batch) at step 20: 242.1028, Accuracy: 0.6254\n",
      "Training loss (for one batch) at step 30: 220.5317, Accuracy: 0.6540\n",
      "Training loss (for one batch) at step 40: 229.2356, Accuracy: 0.6778\n",
      "Training loss (for one batch) at step 50: 227.5823, Accuracy: 0.6877\n",
      "Training loss (for one batch) at step 60: 223.8825, Accuracy: 0.6943\n",
      "Training loss (for one batch) at step 70: 245.8560, Accuracy: 0.6819\n",
      "Training loss (for one batch) at step 80: 229.4588, Accuracy: 0.6689\n",
      "Training loss (for one batch) at step 90: 248.5717, Accuracy: 0.6667\n",
      "Training loss (for one batch) at step 100: 249.2292, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 110: 258.1555, Accuracy: 0.6734\n",
      "---- Training ----\n",
      "Training loss: 84.5791\n",
      "Training acc over epoch: 0.6727\n",
      "---- Validation ----\n",
      "Validation loss: 35.0184\n",
      "Validation acc: 0.6373\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 266.1254, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 241.7576, Accuracy: 0.5803\n",
      "Training loss (for one batch) at step 20: 261.7072, Accuracy: 0.6213\n",
      "Training loss (for one batch) at step 30: 242.7632, Accuracy: 0.6537\n",
      "Training loss (for one batch) at step 40: 251.9479, Accuracy: 0.6761\n",
      "Training loss (for one batch) at step 50: 233.7898, Accuracy: 0.6855\n",
      "Training loss (for one batch) at step 60: 232.1778, Accuracy: 0.6942\n",
      "Training loss (for one batch) at step 70: 240.6984, Accuracy: 0.6855\n",
      "Training loss (for one batch) at step 80: 256.6681, Accuracy: 0.6704\n",
      "Training loss (for one batch) at step 90: 231.0077, Accuracy: 0.6671\n",
      "Training loss (for one batch) at step 100: 240.1456, Accuracy: 0.6715\n",
      "Training loss (for one batch) at step 110: 244.0921, Accuracy: 0.6726\n",
      "---- Training ----\n",
      "Training loss: 98.3360\n",
      "Training acc over epoch: 0.6720\n",
      "---- Validation ----\n",
      "Validation loss: 38.9976\n",
      "Validation acc: 0.6376\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 250.4442, Accuracy: 0.4844\n",
      "Training loss (for one batch) at step 10: 240.6271, Accuracy: 0.5888\n",
      "Training loss (for one batch) at step 20: 225.5034, Accuracy: 0.6272\n",
      "Training loss (for one batch) at step 30: 237.4776, Accuracy: 0.6573\n",
      "Training loss (for one batch) at step 40: 235.5128, Accuracy: 0.6795\n",
      "Training loss (for one batch) at step 50: 239.2104, Accuracy: 0.6936\n",
      "Training loss (for one batch) at step 60: 239.8687, Accuracy: 0.6966\n",
      "Training loss (for one batch) at step 70: 231.0071, Accuracy: 0.6855\n",
      "Training loss (for one batch) at step 80: 255.4333, Accuracy: 0.6693\n",
      "Training loss (for one batch) at step 90: 225.3631, Accuracy: 0.6679\n",
      "Training loss (for one batch) at step 100: 256.5710, Accuracy: 0.6733\n",
      "Training loss (for one batch) at step 110: 239.7030, Accuracy: 0.6749\n",
      "---- Training ----\n",
      "Training loss: 91.3927\n",
      "Training acc over epoch: 0.6722\n",
      "---- Validation ----\n",
      "Validation loss: 45.1703\n",
      "Validation acc: 0.6397\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 286.2927, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 241.3430, Accuracy: 0.5930\n",
      "Training loss (for one batch) at step 20: 235.6722, Accuracy: 0.6280\n",
      "Training loss (for one batch) at step 30: 227.7679, Accuracy: 0.6560\n",
      "Training loss (for one batch) at step 40: 225.2744, Accuracy: 0.6751\n",
      "Training loss (for one batch) at step 50: 221.4083, Accuracy: 0.6898\n",
      "Training loss (for one batch) at step 60: 241.6766, Accuracy: 0.6928\n",
      "Training loss (for one batch) at step 70: 259.7438, Accuracy: 0.6843\n",
      "Training loss (for one batch) at step 80: 239.3817, Accuracy: 0.6708\n",
      "Training loss (for one batch) at step 90: 242.7428, Accuracy: 0.6683\n",
      "Training loss (for one batch) at step 100: 226.5972, Accuracy: 0.6733\n",
      "Training loss (for one batch) at step 110: 224.1494, Accuracy: 0.6742\n",
      "---- Training ----\n",
      "Training loss: 71.9183\n",
      "Training acc over epoch: 0.6733\n",
      "---- Validation ----\n",
      "Validation loss: 43.9968\n",
      "Validation acc: 0.6400\n",
      "Time taken: 17.88s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 256.3684, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 236.6819, Accuracy: 0.5930\n",
      "Training loss (for one batch) at step 20: 249.6076, Accuracy: 0.6306\n",
      "Training loss (for one batch) at step 30: 230.4398, Accuracy: 0.6666\n",
      "Training loss (for one batch) at step 40: 233.4827, Accuracy: 0.6801\n",
      "Training loss (for one batch) at step 50: 221.4896, Accuracy: 0.6895\n",
      "Training loss (for one batch) at step 60: 251.6713, Accuracy: 0.6981\n",
      "Training loss (for one batch) at step 70: 228.2330, Accuracy: 0.6863\n",
      "Training loss (for one batch) at step 80: 244.6209, Accuracy: 0.6728\n",
      "Training loss (for one batch) at step 90: 236.7530, Accuracy: 0.6708\n",
      "Training loss (for one batch) at step 100: 245.0533, Accuracy: 0.6731\n",
      "Training loss (for one batch) at step 110: 215.9663, Accuracy: 0.6750\n",
      "---- Training ----\n",
      "Training loss: 81.2769\n",
      "Training acc over epoch: 0.6741\n",
      "---- Validation ----\n",
      "Validation loss: 43.7801\n",
      "Validation acc: 0.6282\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 249.7320, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 254.1074, Accuracy: 0.5909\n",
      "Training loss (for one batch) at step 20: 226.6445, Accuracy: 0.6280\n",
      "Training loss (for one batch) at step 30: 236.4409, Accuracy: 0.6552\n",
      "Training loss (for one batch) at step 40: 244.2865, Accuracy: 0.6761\n",
      "Training loss (for one batch) at step 50: 248.1428, Accuracy: 0.6867\n",
      "Training loss (for one batch) at step 60: 217.1680, Accuracy: 0.6933\n",
      "Training loss (for one batch) at step 70: 239.0036, Accuracy: 0.6836\n",
      "Training loss (for one batch) at step 80: 267.1929, Accuracy: 0.6692\n",
      "Training loss (for one batch) at step 90: 238.6666, Accuracy: 0.6672\n",
      "Training loss (for one batch) at step 100: 284.7793, Accuracy: 0.6730\n",
      "Training loss (for one batch) at step 110: 234.9662, Accuracy: 0.6726\n",
      "---- Training ----\n",
      "Training loss: 85.7607\n",
      "Training acc over epoch: 0.6722\n",
      "---- Validation ----\n",
      "Validation loss: 53.7028\n",
      "Validation acc: 0.6257\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 259.6672, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 253.7701, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 20: 222.1382, Accuracy: 0.6295\n",
      "Training loss (for one batch) at step 30: 227.4127, Accuracy: 0.6593\n",
      "Training loss (for one batch) at step 40: 223.3754, Accuracy: 0.6774\n",
      "Training loss (for one batch) at step 50: 222.9947, Accuracy: 0.6906\n",
      "Training loss (for one batch) at step 60: 241.8775, Accuracy: 0.6957\n",
      "Training loss (for one batch) at step 70: 251.2471, Accuracy: 0.6844\n",
      "Training loss (for one batch) at step 80: 260.0486, Accuracy: 0.6700\n",
      "Training loss (for one batch) at step 90: 225.1973, Accuracy: 0.6654\n",
      "Training loss (for one batch) at step 100: 232.1892, Accuracy: 0.6696\n",
      "Training loss (for one batch) at step 110: 229.9847, Accuracy: 0.6710\n",
      "---- Training ----\n",
      "Training loss: 79.7734\n",
      "Training acc over epoch: 0.6718\n",
      "---- Validation ----\n",
      "Validation loss: 34.0141\n",
      "Validation acc: 0.6421\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 241.8677, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 234.2102, Accuracy: 0.5746\n",
      "Training loss (for one batch) at step 20: 248.4031, Accuracy: 0.6176\n",
      "Training loss (for one batch) at step 30: 224.3153, Accuracy: 0.6585\n",
      "Training loss (for one batch) at step 40: 229.5558, Accuracy: 0.6782\n",
      "Training loss (for one batch) at step 50: 229.3276, Accuracy: 0.6895\n",
      "Training loss (for one batch) at step 60: 224.4406, Accuracy: 0.6957\n",
      "Training loss (for one batch) at step 70: 232.7027, Accuracy: 0.6843\n",
      "Training loss (for one batch) at step 80: 239.8191, Accuracy: 0.6704\n",
      "Training loss (for one batch) at step 90: 228.1953, Accuracy: 0.6685\n",
      "Training loss (for one batch) at step 100: 250.3626, Accuracy: 0.6731\n",
      "Training loss (for one batch) at step 110: 231.9074, Accuracy: 0.6744\n",
      "---- Training ----\n",
      "Training loss: 76.7796\n",
      "Training acc over epoch: 0.6730\n",
      "---- Validation ----\n",
      "Validation loss: 56.9288\n",
      "Validation acc: 0.6378\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 268.8567, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 242.6687, Accuracy: 0.6072\n",
      "Training loss (for one batch) at step 20: 223.3704, Accuracy: 0.6261\n",
      "Training loss (for one batch) at step 30: 245.1586, Accuracy: 0.6550\n",
      "Training loss (for one batch) at step 40: 236.6288, Accuracy: 0.6711\n",
      "Training loss (for one batch) at step 50: 232.4514, Accuracy: 0.6878\n",
      "Training loss (for one batch) at step 60: 231.0042, Accuracy: 0.6929\n",
      "Training loss (for one batch) at step 70: 252.1174, Accuracy: 0.6822\n",
      "Training loss (for one batch) at step 80: 229.9797, Accuracy: 0.6682\n",
      "Training loss (for one batch) at step 90: 219.3165, Accuracy: 0.6667\n",
      "Training loss (for one batch) at step 100: 224.4634, Accuracy: 0.6701\n",
      "Training loss (for one batch) at step 110: 239.4290, Accuracy: 0.6721\n",
      "---- Training ----\n",
      "Training loss: 68.2152\n",
      "Training acc over epoch: 0.6705\n",
      "---- Validation ----\n",
      "Validation loss: 42.1592\n",
      "Validation acc: 0.6392\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 244.3468, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 244.1742, Accuracy: 0.5952\n",
      "Training loss (for one batch) at step 20: 224.2615, Accuracy: 0.6205\n",
      "Training loss (for one batch) at step 30: 231.8363, Accuracy: 0.6532\n",
      "Training loss (for one batch) at step 40: 237.0179, Accuracy: 0.6768\n",
      "Training loss (for one batch) at step 50: 225.1052, Accuracy: 0.6909\n",
      "Training loss (for one batch) at step 60: 240.3724, Accuracy: 0.6956\n",
      "Training loss (for one batch) at step 70: 239.8938, Accuracy: 0.6846\n",
      "Training loss (for one batch) at step 80: 234.1931, Accuracy: 0.6705\n",
      "Training loss (for one batch) at step 90: 233.7589, Accuracy: 0.6672\n",
      "Training loss (for one batch) at step 100: 237.0553, Accuracy: 0.6720\n",
      "Training loss (for one batch) at step 110: 234.0732, Accuracy: 0.6729\n",
      "---- Training ----\n",
      "Training loss: 85.4947\n",
      "Training acc over epoch: 0.6723\n",
      "---- Validation ----\n",
      "Validation loss: 48.5743\n",
      "Validation acc: 0.6419\n",
      "Time taken: 18.10s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 252.2986, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 241.1621, Accuracy: 0.5909\n",
      "Training loss (for one batch) at step 20: 236.4147, Accuracy: 0.6306\n",
      "Training loss (for one batch) at step 30: 225.9041, Accuracy: 0.6595\n",
      "Training loss (for one batch) at step 40: 230.2821, Accuracy: 0.6764\n",
      "Training loss (for one batch) at step 50: 226.7961, Accuracy: 0.6887\n",
      "Training loss (for one batch) at step 60: 231.1037, Accuracy: 0.6939\n",
      "Training loss (for one batch) at step 70: 239.6575, Accuracy: 0.6829\n",
      "Training loss (for one batch) at step 80: 239.2483, Accuracy: 0.6697\n",
      "Training loss (for one batch) at step 90: 233.1156, Accuracy: 0.6660\n",
      "Training loss (for one batch) at step 100: 252.2038, Accuracy: 0.6718\n",
      "Training loss (for one batch) at step 110: 237.1276, Accuracy: 0.6727\n",
      "---- Training ----\n",
      "Training loss: 78.8166\n",
      "Training acc over epoch: 0.6717\n",
      "---- Validation ----\n",
      "Validation loss: 38.4365\n",
      "Validation acc: 0.6483\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 225.5216, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 235.0249, Accuracy: 0.6001\n",
      "Training loss (for one batch) at step 20: 244.7000, Accuracy: 0.6224\n",
      "Training loss (for one batch) at step 30: 219.2367, Accuracy: 0.6560\n",
      "Training loss (for one batch) at step 40: 225.9834, Accuracy: 0.6774\n",
      "Training loss (for one batch) at step 50: 229.6408, Accuracy: 0.6907\n",
      "Training loss (for one batch) at step 60: 255.7227, Accuracy: 0.6951\n",
      "Training loss (for one batch) at step 70: 232.8761, Accuracy: 0.6836\n",
      "Training loss (for one batch) at step 80: 217.5200, Accuracy: 0.6707\n",
      "Training loss (for one batch) at step 90: 228.5850, Accuracy: 0.6670\n",
      "Training loss (for one batch) at step 100: 239.1635, Accuracy: 0.6725\n",
      "Training loss (for one batch) at step 110: 240.4844, Accuracy: 0.6730\n",
      "---- Training ----\n",
      "Training loss: 73.2495\n",
      "Training acc over epoch: 0.6724\n",
      "---- Validation ----\n",
      "Validation loss: 39.2964\n",
      "Validation acc: 0.6464\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 237.9257, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 239.8058, Accuracy: 0.5909\n",
      "Training loss (for one batch) at step 20: 246.9629, Accuracy: 0.6246\n",
      "Training loss (for one batch) at step 30: 225.7476, Accuracy: 0.6565\n",
      "Training loss (for one batch) at step 40: 241.8627, Accuracy: 0.6782\n",
      "Training loss (for one batch) at step 50: 221.5637, Accuracy: 0.6901\n",
      "Training loss (for one batch) at step 60: 235.1640, Accuracy: 0.6956\n",
      "Training loss (for one batch) at step 70: 241.7331, Accuracy: 0.6845\n",
      "Training loss (for one batch) at step 80: 237.8837, Accuracy: 0.6723\n",
      "Training loss (for one batch) at step 90: 214.2638, Accuracy: 0.6687\n",
      "Training loss (for one batch) at step 100: 236.1598, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 110: 229.9052, Accuracy: 0.6740\n",
      "---- Training ----\n",
      "Training loss: 76.6935\n",
      "Training acc over epoch: 0.6743\n",
      "---- Validation ----\n",
      "Validation loss: 41.2540\n",
      "Validation acc: 0.6268\n",
      "Time taken: 17.89s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 241.8999, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 238.4507, Accuracy: 0.6080\n",
      "Training loss (for one batch) at step 20: 255.6915, Accuracy: 0.6332\n",
      "Training loss (for one batch) at step 30: 236.1323, Accuracy: 0.6633\n",
      "Training loss (for one batch) at step 40: 221.3639, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 50: 235.8250, Accuracy: 0.6926\n",
      "Training loss (for one batch) at step 60: 222.3641, Accuracy: 0.6995\n",
      "Training loss (for one batch) at step 70: 218.1036, Accuracy: 0.6887\n",
      "Training loss (for one batch) at step 80: 253.9781, Accuracy: 0.6725\n",
      "Training loss (for one batch) at step 90: 228.3629, Accuracy: 0.6704\n",
      "Training loss (for one batch) at step 100: 233.9605, Accuracy: 0.6745\n",
      "Training loss (for one batch) at step 110: 221.6720, Accuracy: 0.6753\n",
      "---- Training ----\n",
      "Training loss: 78.4253\n",
      "Training acc over epoch: 0.6740\n",
      "---- Validation ----\n",
      "Validation loss: 59.8627\n",
      "Validation acc: 0.6279\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 251.3079, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 255.5507, Accuracy: 0.5874\n",
      "Training loss (for one batch) at step 20: 232.9077, Accuracy: 0.6217\n",
      "Training loss (for one batch) at step 30: 215.9335, Accuracy: 0.6557\n",
      "Training loss (for one batch) at step 40: 233.8502, Accuracy: 0.6751\n",
      "Training loss (for one batch) at step 50: 244.9827, Accuracy: 0.6895\n",
      "Training loss (for one batch) at step 60: 239.3479, Accuracy: 0.6929\n",
      "Training loss (for one batch) at step 70: 244.8673, Accuracy: 0.6816\n",
      "Training loss (for one batch) at step 80: 238.2256, Accuracy: 0.6689\n",
      "Training loss (for one batch) at step 90: 227.3377, Accuracy: 0.6654\n",
      "Training loss (for one batch) at step 100: 229.4742, Accuracy: 0.6700\n",
      "Training loss (for one batch) at step 110: 228.1094, Accuracy: 0.6731\n",
      "---- Training ----\n",
      "Training loss: 73.9518\n",
      "Training acc over epoch: 0.6704\n",
      "---- Validation ----\n",
      "Validation loss: 47.0320\n",
      "Validation acc: 0.6212\n",
      "Time taken: 18.05s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 262.6347, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 230.6416, Accuracy: 0.5959\n",
      "Training loss (for one batch) at step 20: 227.6680, Accuracy: 0.6306\n",
      "Training loss (for one batch) at step 30: 244.0167, Accuracy: 0.6588\n",
      "Training loss (for one batch) at step 40: 212.3924, Accuracy: 0.6744\n",
      "Training loss (for one batch) at step 50: 220.3053, Accuracy: 0.6877\n",
      "Training loss (for one batch) at step 60: 251.9309, Accuracy: 0.6943\n",
      "Training loss (for one batch) at step 70: 245.6408, Accuracy: 0.6846\n",
      "Training loss (for one batch) at step 80: 250.7688, Accuracy: 0.6724\n",
      "Training loss (for one batch) at step 90: 246.1529, Accuracy: 0.6692\n",
      "Training loss (for one batch) at step 100: 233.9062, Accuracy: 0.6716\n",
      "Training loss (for one batch) at step 110: 223.8318, Accuracy: 0.6738\n",
      "---- Training ----\n",
      "Training loss: 81.6851\n",
      "Training acc over epoch: 0.6723\n",
      "---- Validation ----\n",
      "Validation loss: 57.6929\n",
      "Validation acc: 0.6282\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 242.5468, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 250.2296, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 20: 241.6120, Accuracy: 0.6306\n",
      "Training loss (for one batch) at step 30: 227.2221, Accuracy: 0.6598\n",
      "Training loss (for one batch) at step 40: 242.4512, Accuracy: 0.6753\n",
      "Training loss (for one batch) at step 50: 231.6771, Accuracy: 0.6898\n",
      "Training loss (for one batch) at step 60: 243.2720, Accuracy: 0.6956\n",
      "Training loss (for one batch) at step 70: 230.2022, Accuracy: 0.6823\n",
      "Training loss (for one batch) at step 80: 227.6747, Accuracy: 0.6698\n",
      "Training loss (for one batch) at step 90: 239.5804, Accuracy: 0.6682\n",
      "Training loss (for one batch) at step 100: 233.0672, Accuracy: 0.6718\n",
      "Training loss (for one batch) at step 110: 229.6259, Accuracy: 0.6732\n",
      "---- Training ----\n",
      "Training loss: 92.2721\n",
      "Training acc over epoch: 0.6722\n",
      "---- Validation ----\n",
      "Validation loss: 47.6950\n",
      "Validation acc: 0.6252\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 244.9579, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 238.1766, Accuracy: 0.5774\n",
      "Training loss (for one batch) at step 20: 234.0876, Accuracy: 0.6105\n",
      "Training loss (for one batch) at step 30: 234.2388, Accuracy: 0.6527\n",
      "Training loss (for one batch) at step 40: 219.2756, Accuracy: 0.6778\n",
      "Training loss (for one batch) at step 50: 235.1396, Accuracy: 0.6880\n",
      "Training loss (for one batch) at step 60: 222.9921, Accuracy: 0.6917\n",
      "Training loss (for one batch) at step 70: 224.6227, Accuracy: 0.6832\n",
      "Training loss (for one batch) at step 80: 233.4993, Accuracy: 0.6697\n",
      "Training loss (for one batch) at step 90: 227.0033, Accuracy: 0.6677\n",
      "Training loss (for one batch) at step 100: 222.4683, Accuracy: 0.6715\n",
      "Training loss (for one batch) at step 110: 215.6080, Accuracy: 0.6720\n",
      "---- Training ----\n",
      "Training loss: 65.3145\n",
      "Training acc over epoch: 0.6713\n",
      "---- Validation ----\n",
      "Validation loss: 40.8512\n",
      "Validation acc: 0.6365\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 246.2153, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 248.9704, Accuracy: 0.5895\n",
      "Training loss (for one batch) at step 20: 232.4501, Accuracy: 0.6243\n",
      "Training loss (for one batch) at step 30: 209.3011, Accuracy: 0.6610\n",
      "Training loss (for one batch) at step 40: 233.0713, Accuracy: 0.6757\n",
      "Training loss (for one batch) at step 50: 238.5280, Accuracy: 0.6883\n",
      "Training loss (for one batch) at step 60: 254.4806, Accuracy: 0.6945\n",
      "Training loss (for one batch) at step 70: 240.8346, Accuracy: 0.6818\n",
      "Training loss (for one batch) at step 80: 236.7011, Accuracy: 0.6695\n",
      "Training loss (for one batch) at step 90: 236.1270, Accuracy: 0.6671\n",
      "Training loss (for one batch) at step 100: 240.0680, Accuracy: 0.6711\n",
      "Training loss (for one batch) at step 110: 225.0581, Accuracy: 0.6725\n",
      "---- Training ----\n",
      "Training loss: 75.0595\n",
      "Training acc over epoch: 0.6718\n",
      "---- Validation ----\n",
      "Validation loss: 61.0603\n",
      "Validation acc: 0.6384\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 256.4292, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 235.2335, Accuracy: 0.5902\n",
      "Training loss (for one batch) at step 20: 231.2395, Accuracy: 0.6261\n",
      "Training loss (for one batch) at step 30: 236.1836, Accuracy: 0.6565\n",
      "Training loss (for one batch) at step 40: 226.8183, Accuracy: 0.6745\n",
      "Training loss (for one batch) at step 50: 209.6289, Accuracy: 0.6870\n",
      "Training loss (for one batch) at step 60: 214.7027, Accuracy: 0.6928\n",
      "Training loss (for one batch) at step 70: 238.1673, Accuracy: 0.6808\n",
      "Training loss (for one batch) at step 80: 214.7583, Accuracy: 0.6667\n",
      "Training loss (for one batch) at step 90: 215.5106, Accuracy: 0.6655\n",
      "Training loss (for one batch) at step 100: 244.4870, Accuracy: 0.6699\n",
      "Training loss (for one batch) at step 110: 213.1612, Accuracy: 0.6713\n",
      "---- Training ----\n",
      "Training loss: 81.4779\n",
      "Training acc over epoch: 0.6713\n",
      "---- Validation ----\n",
      "Validation loss: 41.0483\n",
      "Validation acc: 0.6424\n",
      "Time taken: 17.89s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 245.5630, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 225.6745, Accuracy: 0.6051\n",
      "Training loss (for one batch) at step 20: 245.7502, Accuracy: 0.6291\n",
      "Training loss (for one batch) at step 30: 224.6379, Accuracy: 0.6573\n",
      "Training loss (for one batch) at step 40: 223.0167, Accuracy: 0.6784\n",
      "Training loss (for one batch) at step 50: 233.2455, Accuracy: 0.6904\n",
      "Training loss (for one batch) at step 60: 246.1132, Accuracy: 0.6958\n",
      "Training loss (for one batch) at step 70: 236.0037, Accuracy: 0.6830\n",
      "Training loss (for one batch) at step 80: 236.9483, Accuracy: 0.6691\n",
      "Training loss (for one batch) at step 90: 227.8611, Accuracy: 0.6664\n",
      "Training loss (for one batch) at step 100: 233.4048, Accuracy: 0.6707\n",
      "Training loss (for one batch) at step 110: 217.4988, Accuracy: 0.6730\n",
      "---- Training ----\n",
      "Training loss: 66.1886\n",
      "Training acc over epoch: 0.6717\n",
      "---- Validation ----\n",
      "Validation loss: 81.5741\n",
      "Validation acc: 0.6386\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 236.4341, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 234.9990, Accuracy: 0.5952\n",
      "Training loss (for one batch) at step 20: 219.8011, Accuracy: 0.6254\n",
      "Training loss (for one batch) at step 30: 249.4482, Accuracy: 0.6618\n",
      "Training loss (for one batch) at step 40: 237.3010, Accuracy: 0.6770\n",
      "Training loss (for one batch) at step 50: 214.1185, Accuracy: 0.6893\n",
      "Training loss (for one batch) at step 60: 229.1192, Accuracy: 0.6938\n",
      "Training loss (for one batch) at step 70: 245.3797, Accuracy: 0.6840\n",
      "Training loss (for one batch) at step 80: 241.1926, Accuracy: 0.6677\n",
      "Training loss (for one batch) at step 90: 235.5333, Accuracy: 0.6656\n",
      "Training loss (for one batch) at step 100: 228.3984, Accuracy: 0.6709\n",
      "Training loss (for one batch) at step 110: 211.1008, Accuracy: 0.6733\n",
      "---- Training ----\n",
      "Training loss: 75.7745\n",
      "Training acc over epoch: 0.6718\n",
      "---- Validation ----\n",
      "Validation loss: 38.6587\n",
      "Validation acc: 0.6413\n",
      "Time taken: 18.16s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 226.0734, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 241.5905, Accuracy: 0.5866\n",
      "Training loss (for one batch) at step 20: 221.7039, Accuracy: 0.6291\n",
      "Training loss (for one batch) at step 30: 216.9749, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 40: 223.3140, Accuracy: 0.6785\n",
      "Training loss (for one batch) at step 50: 229.5071, Accuracy: 0.6907\n",
      "Training loss (for one batch) at step 60: 233.8256, Accuracy: 0.6956\n",
      "Training loss (for one batch) at step 70: 220.8121, Accuracy: 0.6828\n",
      "Training loss (for one batch) at step 80: 237.6178, Accuracy: 0.6681\n",
      "Training loss (for one batch) at step 90: 232.8609, Accuracy: 0.6668\n",
      "Training loss (for one batch) at step 100: 215.5293, Accuracy: 0.6720\n",
      "Training loss (for one batch) at step 110: 218.9284, Accuracy: 0.6719\n",
      "---- Training ----\n",
      "Training loss: 77.2216\n",
      "Training acc over epoch: 0.6717\n",
      "---- Validation ----\n",
      "Validation loss: 40.5400\n",
      "Validation acc: 0.6233\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 247.3860, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 246.0281, Accuracy: 0.5739\n",
      "Training loss (for one batch) at step 20: 229.8938, Accuracy: 0.6272\n",
      "Training loss (for one batch) at step 30: 221.6215, Accuracy: 0.6542\n",
      "Training loss (for one batch) at step 40: 219.4533, Accuracy: 0.6744\n",
      "Training loss (for one batch) at step 50: 215.2558, Accuracy: 0.6877\n",
      "Training loss (for one batch) at step 60: 221.6357, Accuracy: 0.6945\n",
      "Training loss (for one batch) at step 70: 233.2094, Accuracy: 0.6819\n",
      "Training loss (for one batch) at step 80: 244.8542, Accuracy: 0.6660\n",
      "Training loss (for one batch) at step 90: 208.0036, Accuracy: 0.6662\n",
      "Training loss (for one batch) at step 100: 215.0841, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 110: 232.0537, Accuracy: 0.6710\n",
      "---- Training ----\n",
      "Training loss: 64.8253\n",
      "Training acc over epoch: 0.6705\n",
      "---- Validation ----\n",
      "Validation loss: 67.3125\n",
      "Validation acc: 0.6392\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 241.8127, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 225.1669, Accuracy: 0.6009\n",
      "Training loss (for one batch) at step 20: 215.5201, Accuracy: 0.6298\n",
      "Training loss (for one batch) at step 30: 223.3533, Accuracy: 0.6585\n",
      "Training loss (for one batch) at step 40: 222.5120, Accuracy: 0.6761\n",
      "Training loss (for one batch) at step 50: 212.7787, Accuracy: 0.6892\n",
      "Training loss (for one batch) at step 60: 224.2294, Accuracy: 0.6949\n",
      "Training loss (for one batch) at step 70: 219.2010, Accuracy: 0.6832\n",
      "Training loss (for one batch) at step 80: 224.9021, Accuracy: 0.6659\n",
      "Training loss (for one batch) at step 90: 230.9488, Accuracy: 0.6648\n",
      "Training loss (for one batch) at step 100: 229.2733, Accuracy: 0.6699\n",
      "Training loss (for one batch) at step 110: 221.6875, Accuracy: 0.6718\n",
      "---- Training ----\n",
      "Training loss: 77.0523\n",
      "Training acc over epoch: 0.6709\n",
      "---- Validation ----\n",
      "Validation loss: 56.0492\n",
      "Validation acc: 0.6397\n",
      "Time taken: 18.08s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 237.0263, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 231.3153, Accuracy: 0.5845\n",
      "Training loss (for one batch) at step 20: 223.8219, Accuracy: 0.6213\n",
      "Training loss (for one batch) at step 30: 218.0853, Accuracy: 0.6542\n",
      "Training loss (for one batch) at step 40: 212.1548, Accuracy: 0.6780\n",
      "Training loss (for one batch) at step 50: 229.1280, Accuracy: 0.6872\n",
      "Training loss (for one batch) at step 60: 237.1796, Accuracy: 0.6934\n",
      "Training loss (for one batch) at step 70: 258.4189, Accuracy: 0.6828\n",
      "Training loss (for one batch) at step 80: 243.1951, Accuracy: 0.6672\n",
      "Training loss (for one batch) at step 90: 226.2070, Accuracy: 0.6646\n",
      "Training loss (for one batch) at step 100: 218.7652, Accuracy: 0.6692\n",
      "Training loss (for one batch) at step 110: 232.1648, Accuracy: 0.6708\n",
      "---- Training ----\n",
      "Training loss: 69.4417\n",
      "Training acc over epoch: 0.6709\n",
      "---- Validation ----\n",
      "Validation loss: 44.7421\n",
      "Validation acc: 0.6343\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 233.7851, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 232.6833, Accuracy: 0.6037\n",
      "Training loss (for one batch) at step 20: 221.5332, Accuracy: 0.6235\n",
      "Training loss (for one batch) at step 30: 221.4157, Accuracy: 0.6557\n",
      "Training loss (for one batch) at step 40: 211.2579, Accuracy: 0.6814\n",
      "Training loss (for one batch) at step 50: 213.4773, Accuracy: 0.6893\n",
      "Training loss (for one batch) at step 60: 223.3904, Accuracy: 0.6966\n",
      "Training loss (for one batch) at step 70: 228.8546, Accuracy: 0.6839\n",
      "Training loss (for one batch) at step 80: 223.9083, Accuracy: 0.6698\n",
      "Training loss (for one batch) at step 90: 232.2429, Accuracy: 0.6682\n",
      "Training loss (for one batch) at step 100: 219.2443, Accuracy: 0.6718\n",
      "Training loss (for one batch) at step 110: 227.9619, Accuracy: 0.6723\n",
      "---- Training ----\n",
      "Training loss: 78.0797\n",
      "Training acc over epoch: 0.6717\n",
      "---- Validation ----\n",
      "Validation loss: 58.9916\n",
      "Validation acc: 0.6311\n",
      "Time taken: 17.88s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 250.8266, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 230.2938, Accuracy: 0.5732\n",
      "Training loss (for one batch) at step 20: 215.0990, Accuracy: 0.6246\n",
      "Training loss (for one batch) at step 30: 210.0666, Accuracy: 0.6600\n",
      "Training loss (for one batch) at step 40: 215.1096, Accuracy: 0.6806\n",
      "Training loss (for one batch) at step 50: 228.6249, Accuracy: 0.6906\n",
      "Training loss (for one batch) at step 60: 224.4523, Accuracy: 0.6951\n",
      "Training loss (for one batch) at step 70: 217.2043, Accuracy: 0.6846\n",
      "Training loss (for one batch) at step 80: 217.8924, Accuracy: 0.6698\n",
      "Training loss (for one batch) at step 90: 216.9178, Accuracy: 0.6670\n",
      "Training loss (for one batch) at step 100: 215.5990, Accuracy: 0.6707\n",
      "Training loss (for one batch) at step 110: 223.3298, Accuracy: 0.6721\n",
      "---- Training ----\n",
      "Training loss: 92.2143\n",
      "Training acc over epoch: 0.6717\n",
      "---- Validation ----\n",
      "Validation loss: 54.1336\n",
      "Validation acc: 0.6343\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 231.4514, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 220.0385, Accuracy: 0.5831\n",
      "Training loss (for one batch) at step 20: 236.8554, Accuracy: 0.6213\n",
      "Training loss (for one batch) at step 30: 222.2844, Accuracy: 0.6542\n",
      "Training loss (for one batch) at step 40: 250.1759, Accuracy: 0.6747\n",
      "Training loss (for one batch) at step 50: 211.2964, Accuracy: 0.6884\n",
      "Training loss (for one batch) at step 60: 213.7811, Accuracy: 0.6934\n",
      "Training loss (for one batch) at step 70: 238.7998, Accuracy: 0.6832\n",
      "Training loss (for one batch) at step 80: 230.9326, Accuracy: 0.6692\n",
      "Training loss (for one batch) at step 90: 214.8037, Accuracy: 0.6633\n",
      "Training loss (for one batch) at step 100: 233.6052, Accuracy: 0.6706\n",
      "Training loss (for one batch) at step 110: 225.0012, Accuracy: 0.6724\n",
      "---- Training ----\n",
      "Training loss: 62.5435\n",
      "Training acc over epoch: 0.6717\n",
      "---- Validation ----\n",
      "Validation loss: 37.0035\n",
      "Validation acc: 0.6386\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 245.0289, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 245.6328, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 20: 226.9604, Accuracy: 0.6150\n",
      "Training loss (for one batch) at step 30: 223.4482, Accuracy: 0.6550\n",
      "Training loss (for one batch) at step 40: 218.4203, Accuracy: 0.6749\n",
      "Training loss (for one batch) at step 50: 213.6415, Accuracy: 0.6887\n",
      "Training loss (for one batch) at step 60: 222.3403, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 70: 230.3503, Accuracy: 0.6823\n",
      "Training loss (for one batch) at step 80: 216.8182, Accuracy: 0.6692\n",
      "Training loss (for one batch) at step 90: 224.2042, Accuracy: 0.6640\n",
      "Training loss (for one batch) at step 100: 212.1326, Accuracy: 0.6701\n",
      "Training loss (for one batch) at step 110: 218.9151, Accuracy: 0.6706\n",
      "---- Training ----\n",
      "Training loss: 74.0208\n",
      "Training acc over epoch: 0.6707\n",
      "---- Validation ----\n",
      "Validation loss: 45.9563\n",
      "Validation acc: 0.6325\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 238.2462, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 233.4532, Accuracy: 0.5881\n",
      "Training loss (for one batch) at step 20: 223.2238, Accuracy: 0.6302\n",
      "Training loss (for one batch) at step 30: 217.7050, Accuracy: 0.6603\n",
      "Training loss (for one batch) at step 40: 233.2412, Accuracy: 0.6812\n",
      "Training loss (for one batch) at step 50: 211.7430, Accuracy: 0.6904\n",
      "Training loss (for one batch) at step 60: 224.1139, Accuracy: 0.6944\n",
      "Training loss (for one batch) at step 70: 253.2200, Accuracy: 0.6855\n",
      "Training loss (for one batch) at step 80: 236.7832, Accuracy: 0.6711\n",
      "Training loss (for one batch) at step 90: 228.1775, Accuracy: 0.6670\n",
      "Training loss (for one batch) at step 100: 215.1400, Accuracy: 0.6717\n",
      "Training loss (for one batch) at step 110: 221.2631, Accuracy: 0.6721\n",
      "---- Training ----\n",
      "Training loss: 76.1628\n",
      "Training acc over epoch: 0.6720\n",
      "---- Validation ----\n",
      "Validation loss: 53.2433\n",
      "Validation acc: 0.6427\n",
      "Time taken: 17.87s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 266.0916, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 245.7348, Accuracy: 0.5945\n",
      "Training loss (for one batch) at step 20: 213.8327, Accuracy: 0.6239\n",
      "Training loss (for one batch) at step 30: 215.4523, Accuracy: 0.6550\n",
      "Training loss (for one batch) at step 40: 204.6829, Accuracy: 0.6776\n",
      "Training loss (for one batch) at step 50: 221.9106, Accuracy: 0.6883\n",
      "Training loss (for one batch) at step 60: 223.5896, Accuracy: 0.6924\n",
      "Training loss (for one batch) at step 70: 238.1022, Accuracy: 0.6817\n",
      "Training loss (for one batch) at step 80: 228.3129, Accuracy: 0.6672\n",
      "Training loss (for one batch) at step 90: 213.0155, Accuracy: 0.6651\n",
      "Training loss (for one batch) at step 100: 227.9188, Accuracy: 0.6706\n",
      "Training loss (for one batch) at step 110: 247.4112, Accuracy: 0.6722\n",
      "---- Training ----\n",
      "Training loss: 78.6314\n",
      "Training acc over epoch: 0.6711\n",
      "---- Validation ----\n",
      "Validation loss: 36.8882\n",
      "Validation acc: 0.6193\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 256.4381, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 233.9118, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 20: 222.5927, Accuracy: 0.6194\n",
      "Training loss (for one batch) at step 30: 241.8585, Accuracy: 0.6499\n",
      "Training loss (for one batch) at step 40: 216.5892, Accuracy: 0.6734\n",
      "Training loss (for one batch) at step 50: 230.7978, Accuracy: 0.6878\n",
      "Training loss (for one batch) at step 60: 233.2511, Accuracy: 0.6945\n",
      "Training loss (for one batch) at step 70: 228.9742, Accuracy: 0.6831\n",
      "Training loss (for one batch) at step 80: 235.0181, Accuracy: 0.6685\n",
      "Training loss (for one batch) at step 90: 241.7724, Accuracy: 0.6665\n",
      "Training loss (for one batch) at step 100: 232.1073, Accuracy: 0.6709\n",
      "Training loss (for one batch) at step 110: 235.3074, Accuracy: 0.6721\n",
      "---- Training ----\n",
      "Training loss: 82.4581\n",
      "Training acc over epoch: 0.6711\n",
      "---- Validation ----\n",
      "Validation loss: 43.2827\n",
      "Validation acc: 0.6386\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 226.0680, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 224.6015, Accuracy: 0.5881\n",
      "Training loss (for one batch) at step 20: 240.5770, Accuracy: 0.6228\n",
      "Training loss (for one batch) at step 30: 213.3506, Accuracy: 0.6570\n",
      "Training loss (for one batch) at step 40: 209.1279, Accuracy: 0.6780\n",
      "Training loss (for one batch) at step 50: 221.0021, Accuracy: 0.6886\n",
      "Training loss (for one batch) at step 60: 224.1543, Accuracy: 0.6934\n",
      "Training loss (for one batch) at step 70: 246.7735, Accuracy: 0.6838\n",
      "Training loss (for one batch) at step 80: 221.1842, Accuracy: 0.6690\n",
      "Training loss (for one batch) at step 90: 222.6327, Accuracy: 0.6662\n",
      "Training loss (for one batch) at step 100: 234.9020, Accuracy: 0.6714\n",
      "Training loss (for one batch) at step 110: 218.2582, Accuracy: 0.6728\n",
      "---- Training ----\n",
      "Training loss: 71.9785\n",
      "Training acc over epoch: 0.6720\n",
      "---- Validation ----\n",
      "Validation loss: 65.2876\n",
      "Validation acc: 0.6413\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 236.7165, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 230.8567, Accuracy: 0.5916\n",
      "Training loss (for one batch) at step 20: 211.3404, Accuracy: 0.6287\n",
      "Training loss (for one batch) at step 30: 215.5209, Accuracy: 0.6626\n",
      "Training loss (for one batch) at step 40: 245.5270, Accuracy: 0.6803\n",
      "Training loss (for one batch) at step 50: 229.0574, Accuracy: 0.6896\n",
      "Training loss (for one batch) at step 60: 220.6434, Accuracy: 0.6965\n",
      "Training loss (for one batch) at step 70: 235.9355, Accuracy: 0.6835\n",
      "Training loss (for one batch) at step 80: 225.7399, Accuracy: 0.6699\n",
      "Training loss (for one batch) at step 90: 230.0755, Accuracy: 0.6672\n",
      "Training loss (for one batch) at step 100: 231.2059, Accuracy: 0.6708\n",
      "Training loss (for one batch) at step 110: 226.4211, Accuracy: 0.6729\n",
      "---- Training ----\n",
      "Training loss: 82.8179\n",
      "Training acc over epoch: 0.6712\n",
      "---- Validation ----\n",
      "Validation loss: 49.7493\n",
      "Validation acc: 0.6308\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 244.6561, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 225.7593, Accuracy: 0.5845\n",
      "Training loss (for one batch) at step 20: 223.0654, Accuracy: 0.6298\n",
      "Training loss (for one batch) at step 30: 222.1305, Accuracy: 0.6583\n",
      "Training loss (for one batch) at step 40: 231.6181, Accuracy: 0.6736\n",
      "Training loss (for one batch) at step 50: 211.9634, Accuracy: 0.6900\n",
      "Training loss (for one batch) at step 60: 222.0281, Accuracy: 0.6947\n",
      "Training loss (for one batch) at step 70: 254.6325, Accuracy: 0.6786\n",
      "Training loss (for one batch) at step 80: 217.2991, Accuracy: 0.6689\n",
      "Training loss (for one batch) at step 90: 208.7552, Accuracy: 0.6659\n",
      "Training loss (for one batch) at step 100: 219.9454, Accuracy: 0.6689\n",
      "Training loss (for one batch) at step 110: 214.8116, Accuracy: 0.6699\n",
      "---- Training ----\n",
      "Training loss: 73.3432\n",
      "Training acc over epoch: 0.6709\n",
      "---- Validation ----\n",
      "Validation loss: 40.1281\n",
      "Validation acc: 0.6177\n",
      "Time taken: 18.25s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 235.3328, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 222.0459, Accuracy: 0.5923\n",
      "Training loss (for one batch) at step 20: 217.9034, Accuracy: 0.6246\n",
      "Training loss (for one batch) at step 30: 226.5935, Accuracy: 0.6542\n",
      "Training loss (for one batch) at step 40: 218.8800, Accuracy: 0.6766\n",
      "Training loss (for one batch) at step 50: 235.3473, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 60: 214.2955, Accuracy: 0.6926\n",
      "Training loss (for one batch) at step 70: 249.9191, Accuracy: 0.6856\n",
      "Training loss (for one batch) at step 80: 239.9889, Accuracy: 0.6681\n",
      "Training loss (for one batch) at step 90: 222.0666, Accuracy: 0.6672\n",
      "Training loss (for one batch) at step 100: 229.7129, Accuracy: 0.6717\n",
      "Training loss (for one batch) at step 110: 224.3131, Accuracy: 0.6744\n",
      "---- Training ----\n",
      "Training loss: 100.1637\n",
      "Training acc over epoch: 0.6720\n",
      "---- Validation ----\n",
      "Validation loss: 39.8803\n",
      "Validation acc: 0.6163\n",
      "Time taken: 17.87s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 227.4702, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 262.6419, Accuracy: 0.6009\n",
      "Training loss (for one batch) at step 20: 217.3419, Accuracy: 0.6269\n",
      "Training loss (for one batch) at step 30: 216.6343, Accuracy: 0.6618\n",
      "Training loss (for one batch) at step 40: 233.6531, Accuracy: 0.6816\n",
      "Training loss (for one batch) at step 50: 217.5619, Accuracy: 0.6913\n",
      "Training loss (for one batch) at step 60: 217.5949, Accuracy: 0.6962\n",
      "Training loss (for one batch) at step 70: 248.6660, Accuracy: 0.6850\n",
      "Training loss (for one batch) at step 80: 221.0914, Accuracy: 0.6700\n",
      "Training loss (for one batch) at step 90: 253.4702, Accuracy: 0.6672\n",
      "Training loss (for one batch) at step 100: 220.4668, Accuracy: 0.6713\n",
      "Training loss (for one batch) at step 110: 224.4549, Accuracy: 0.6723\n",
      "---- Training ----\n",
      "Training loss: 77.4669\n",
      "Training acc over epoch: 0.6715\n",
      "---- Validation ----\n",
      "Validation loss: 39.2112\n",
      "Validation acc: 0.6359\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 233.5600, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 217.2596, Accuracy: 0.6001\n",
      "Training loss (for one batch) at step 20: 214.5708, Accuracy: 0.6246\n",
      "Training loss (for one batch) at step 30: 207.3273, Accuracy: 0.6565\n",
      "Training loss (for one batch) at step 40: 223.9134, Accuracy: 0.6799\n",
      "Training loss (for one batch) at step 50: 208.2046, Accuracy: 0.6892\n",
      "Training loss (for one batch) at step 60: 216.3130, Accuracy: 0.6935\n",
      "Training loss (for one batch) at step 70: 246.9294, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 80: 230.9220, Accuracy: 0.6659\n",
      "Training loss (for one batch) at step 90: 231.6758, Accuracy: 0.6656\n",
      "Training loss (for one batch) at step 100: 221.6452, Accuracy: 0.6686\n",
      "Training loss (for one batch) at step 110: 222.5067, Accuracy: 0.6703\n",
      "---- Training ----\n",
      "Training loss: 75.9754\n",
      "Training acc over epoch: 0.6691\n",
      "---- Validation ----\n",
      "Validation loss: 30.1348\n",
      "Validation acc: 0.6233\n",
      "Time taken: 20.16s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 232.6587, Accuracy: 0.4688\n",
      "Training loss (for one batch) at step 10: 216.5422, Accuracy: 0.5874\n",
      "Training loss (for one batch) at step 20: 215.3673, Accuracy: 0.6224\n",
      "Training loss (for one batch) at step 30: 224.2793, Accuracy: 0.6512\n",
      "Training loss (for one batch) at step 40: 220.7957, Accuracy: 0.6749\n",
      "Training loss (for one batch) at step 50: 227.1339, Accuracy: 0.6881\n",
      "Training loss (for one batch) at step 60: 242.8651, Accuracy: 0.6948\n",
      "Training loss (for one batch) at step 70: 220.2481, Accuracy: 0.6818\n",
      "Training loss (for one batch) at step 80: 238.0030, Accuracy: 0.6691\n",
      "Training loss (for one batch) at step 90: 223.7595, Accuracy: 0.6671\n",
      "Training loss (for one batch) at step 100: 219.0185, Accuracy: 0.6698\n",
      "Training loss (for one batch) at step 110: 225.4870, Accuracy: 0.6708\n",
      "---- Training ----\n",
      "Training loss: 65.7805\n",
      "Training acc over epoch: 0.6708\n",
      "---- Validation ----\n",
      "Validation loss: 31.2539\n",
      "Validation acc: 0.6333\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 235.4054, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 226.1524, Accuracy: 0.5959\n",
      "Training loss (for one batch) at step 20: 223.5211, Accuracy: 0.6257\n",
      "Training loss (for one batch) at step 30: 222.2887, Accuracy: 0.6545\n",
      "Training loss (for one batch) at step 40: 207.5072, Accuracy: 0.6806\n",
      "Training loss (for one batch) at step 50: 229.1656, Accuracy: 0.6904\n",
      "Training loss (for one batch) at step 60: 220.2190, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 70: 215.8251, Accuracy: 0.6840\n",
      "Training loss (for one batch) at step 80: 227.6728, Accuracy: 0.6665\n",
      "Training loss (for one batch) at step 90: 214.1060, Accuracy: 0.6648\n",
      "Training loss (for one batch) at step 100: 240.6787, Accuracy: 0.6697\n",
      "Training loss (for one batch) at step 110: 224.5476, Accuracy: 0.6712\n",
      "---- Training ----\n",
      "Training loss: 73.6228\n",
      "Training acc over epoch: 0.6705\n",
      "---- Validation ----\n",
      "Validation loss: 68.0637\n",
      "Validation acc: 0.6279\n",
      "Time taken: 17.98s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.6260\n",
      "Validation AUC: 0.6258\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 662.4262, Accuracy: 0.4766\n",
      "Training loss (for one batch) at step 10: 545.7134, Accuracy: 0.5263\n",
      "Training loss (for one batch) at step 20: 514.3978, Accuracy: 0.5182\n",
      "Training loss (for one batch) at step 30: 498.8600, Accuracy: 0.5161\n",
      "Training loss (for one batch) at step 40: 487.9665, Accuracy: 0.5185\n",
      "Training loss (for one batch) at step 50: 511.1759, Accuracy: 0.5095\n",
      "Training loss (for one batch) at step 60: 483.5115, Accuracy: 0.5115\n",
      "Training loss (for one batch) at step 70: 480.3833, Accuracy: 0.5100\n",
      "Training loss (for one batch) at step 80: 463.9382, Accuracy: 0.5122\n",
      "Training loss (for one batch) at step 90: 458.9432, Accuracy: 0.5123\n",
      "Training loss (for one batch) at step 100: 469.2434, Accuracy: 0.5105\n",
      "Training loss (for one batch) at step 110: 471.4868, Accuracy: 0.5100\n",
      "---- Training ----\n",
      "Training loss: 139.2115\n",
      "Training acc over epoch: 0.5103\n",
      "---- Validation ----\n",
      "Validation loss: 34.6121\n",
      "Validation acc: 0.5234\n",
      "Time taken: 22.05s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 464.4659, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 455.8129, Accuracy: 0.4993\n",
      "Training loss (for one batch) at step 20: 454.4990, Accuracy: 0.4996\n",
      "Training loss (for one batch) at step 30: 451.5537, Accuracy: 0.5023\n",
      "Training loss (for one batch) at step 40: 448.3904, Accuracy: 0.5061\n",
      "Training loss (for one batch) at step 50: 446.4795, Accuracy: 0.5090\n",
      "Training loss (for one batch) at step 60: 445.2115, Accuracy: 0.5090\n",
      "Training loss (for one batch) at step 70: 449.7417, Accuracy: 0.5094\n",
      "Training loss (for one batch) at step 80: 452.8004, Accuracy: 0.5119\n",
      "Training loss (for one batch) at step 90: 445.7609, Accuracy: 0.5142\n",
      "Training loss (for one batch) at step 100: 443.1605, Accuracy: 0.5140\n",
      "Training loss (for one batch) at step 110: 444.6392, Accuracy: 0.5127\n",
      "---- Training ----\n",
      "Training loss: 139.0978\n",
      "Training acc over epoch: 0.5132\n",
      "---- Validation ----\n",
      "Validation loss: 35.0224\n",
      "Validation acc: 0.5218\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 447.2220, Accuracy: 0.4453\n",
      "Training loss (for one batch) at step 10: 448.4170, Accuracy: 0.5007\n",
      "Training loss (for one batch) at step 20: 443.7981, Accuracy: 0.5141\n",
      "Training loss (for one batch) at step 30: 443.9665, Accuracy: 0.5181\n",
      "Training loss (for one batch) at step 40: 446.5529, Accuracy: 0.5221\n",
      "Training loss (for one batch) at step 50: 448.7516, Accuracy: 0.5231\n",
      "Training loss (for one batch) at step 60: 442.2093, Accuracy: 0.5237\n",
      "Training loss (for one batch) at step 70: 445.6199, Accuracy: 0.5267\n",
      "Training loss (for one batch) at step 80: 444.0189, Accuracy: 0.5313\n",
      "Training loss (for one batch) at step 90: 446.8650, Accuracy: 0.5351\n",
      "Training loss (for one batch) at step 100: 445.0358, Accuracy: 0.5328\n",
      "Training loss (for one batch) at step 110: 443.2715, Accuracy: 0.5324\n",
      "---- Training ----\n",
      "Training loss: 140.6447\n",
      "Training acc over epoch: 0.5310\n",
      "---- Validation ----\n",
      "Validation loss: 34.6104\n",
      "Validation acc: 0.5298\n",
      "Time taken: 18.15s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 451.6310, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 447.5373, Accuracy: 0.5249\n",
      "Training loss (for one batch) at step 20: 438.9017, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 30: 440.6494, Accuracy: 0.5444\n",
      "Training loss (for one batch) at step 40: 443.5312, Accuracy: 0.5396\n",
      "Training loss (for one batch) at step 50: 445.1948, Accuracy: 0.5423\n",
      "Training loss (for one batch) at step 60: 442.6172, Accuracy: 0.5419\n",
      "Training loss (for one batch) at step 70: 457.0187, Accuracy: 0.5409\n",
      "Training loss (for one batch) at step 80: 444.6184, Accuracy: 0.5412\n",
      "Training loss (for one batch) at step 90: 443.4791, Accuracy: 0.5403\n",
      "Training loss (for one batch) at step 100: 442.9824, Accuracy: 0.5418\n",
      "Training loss (for one batch) at step 110: 440.7428, Accuracy: 0.5412\n",
      "---- Training ----\n",
      "Training loss: 137.7006\n",
      "Training acc over epoch: 0.5403\n",
      "---- Validation ----\n",
      "Validation loss: 34.7499\n",
      "Validation acc: 0.5484\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 441.3690, Accuracy: 0.4844\n",
      "Training loss (for one batch) at step 10: 444.0541, Accuracy: 0.5639\n",
      "Training loss (for one batch) at step 20: 441.9795, Accuracy: 0.5435\n",
      "Training loss (for one batch) at step 30: 434.4536, Accuracy: 0.5542\n",
      "Training loss (for one batch) at step 40: 451.2257, Accuracy: 0.5558\n",
      "Training loss (for one batch) at step 50: 439.4425, Accuracy: 0.5521\n",
      "Training loss (for one batch) at step 60: 440.9312, Accuracy: 0.5529\n",
      "Training loss (for one batch) at step 70: 439.8585, Accuracy: 0.5571\n",
      "Training loss (for one batch) at step 80: 440.9222, Accuracy: 0.5582\n",
      "Training loss (for one batch) at step 90: 441.5011, Accuracy: 0.5555\n",
      "Training loss (for one batch) at step 100: 441.0329, Accuracy: 0.5540\n",
      "Training loss (for one batch) at step 110: 436.7866, Accuracy: 0.5534\n",
      "---- Training ----\n",
      "Training loss: 137.4051\n",
      "Training acc over epoch: 0.5529\n",
      "---- Validation ----\n",
      "Validation loss: 35.3780\n",
      "Validation acc: 0.5411\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 448.4703, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 438.9943, Accuracy: 0.5511\n",
      "Training loss (for one batch) at step 20: 442.5528, Accuracy: 0.5413\n",
      "Training loss (for one batch) at step 30: 433.9771, Accuracy: 0.5441\n",
      "Training loss (for one batch) at step 40: 441.1127, Accuracy: 0.5425\n",
      "Training loss (for one batch) at step 50: 439.3487, Accuracy: 0.5412\n",
      "Training loss (for one batch) at step 60: 441.6974, Accuracy: 0.5450\n",
      "Training loss (for one batch) at step 70: 441.4359, Accuracy: 0.5493\n",
      "Training loss (for one batch) at step 80: 439.9815, Accuracy: 0.5468\n",
      "Training loss (for one batch) at step 90: 437.7731, Accuracy: 0.5455\n",
      "Training loss (for one batch) at step 100: 439.1786, Accuracy: 0.5422\n",
      "Training loss (for one batch) at step 110: 443.6682, Accuracy: 0.5418\n",
      "---- Training ----\n",
      "Training loss: 138.7627\n",
      "Training acc over epoch: 0.5409\n",
      "---- Validation ----\n",
      "Validation loss: 33.3924\n",
      "Validation acc: 0.5696\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 439.6804, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 441.4858, Accuracy: 0.5526\n",
      "Training loss (for one batch) at step 20: 441.8256, Accuracy: 0.5387\n",
      "Training loss (for one batch) at step 30: 434.4059, Accuracy: 0.5499\n",
      "Training loss (for one batch) at step 40: 430.6971, Accuracy: 0.5480\n",
      "Training loss (for one batch) at step 50: 432.5156, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 60: 443.5055, Accuracy: 0.5540\n",
      "Training loss (for one batch) at step 70: 438.8539, Accuracy: 0.5537\n",
      "Training loss (for one batch) at step 80: 441.9378, Accuracy: 0.5545\n",
      "Training loss (for one batch) at step 90: 439.0808, Accuracy: 0.5523\n",
      "Training loss (for one batch) at step 100: 441.5446, Accuracy: 0.5507\n",
      "Training loss (for one batch) at step 110: 440.0267, Accuracy: 0.5494\n",
      "---- Training ----\n",
      "Training loss: 137.7003\n",
      "Training acc over epoch: 0.5498\n",
      "---- Validation ----\n",
      "Validation loss: 33.8184\n",
      "Validation acc: 0.5073\n",
      "Time taken: 20.13s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 440.5164, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 439.1261, Accuracy: 0.5568\n",
      "Training loss (for one batch) at step 20: 437.0652, Accuracy: 0.5499\n",
      "Training loss (for one batch) at step 30: 430.8341, Accuracy: 0.5527\n",
      "Training loss (for one batch) at step 40: 432.8455, Accuracy: 0.5503\n",
      "Training loss (for one batch) at step 50: 426.4802, Accuracy: 0.5498\n",
      "Training loss (for one batch) at step 60: 447.1827, Accuracy: 0.5542\n",
      "Training loss (for one batch) at step 70: 439.1545, Accuracy: 0.5553\n",
      "Training loss (for one batch) at step 80: 438.9147, Accuracy: 0.5544\n",
      "Training loss (for one batch) at step 90: 439.0387, Accuracy: 0.5507\n",
      "Training loss (for one batch) at step 100: 431.3220, Accuracy: 0.5494\n",
      "Training loss (for one batch) at step 110: 441.9558, Accuracy: 0.5489\n",
      "---- Training ----\n",
      "Training loss: 141.9791\n",
      "Training acc over epoch: 0.5502\n",
      "---- Validation ----\n",
      "Validation loss: 35.0488\n",
      "Validation acc: 0.5030\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 442.0214, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 438.4322, Accuracy: 0.5099\n",
      "Training loss (for one batch) at step 20: 435.4795, Accuracy: 0.5149\n",
      "Training loss (for one batch) at step 30: 434.5359, Accuracy: 0.5285\n",
      "Training loss (for one batch) at step 40: 435.4965, Accuracy: 0.5349\n",
      "Training loss (for one batch) at step 50: 418.5628, Accuracy: 0.5444\n",
      "Training loss (for one batch) at step 60: 434.6460, Accuracy: 0.5511\n",
      "Training loss (for one batch) at step 70: 445.1588, Accuracy: 0.5568\n",
      "Training loss (for one batch) at step 80: 437.6920, Accuracy: 0.5562\n",
      "Training loss (for one batch) at step 90: 428.7245, Accuracy: 0.5519\n",
      "Training loss (for one batch) at step 100: 434.0108, Accuracy: 0.5514\n",
      "Training loss (for one batch) at step 110: 432.0950, Accuracy: 0.5512\n",
      "---- Training ----\n",
      "Training loss: 137.2291\n",
      "Training acc over epoch: 0.5498\n",
      "---- Validation ----\n",
      "Validation loss: 34.7444\n",
      "Validation acc: 0.5494\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 438.7614, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 440.4019, Accuracy: 0.5689\n",
      "Training loss (for one batch) at step 20: 430.4429, Accuracy: 0.5487\n",
      "Training loss (for one batch) at step 30: 425.6401, Accuracy: 0.5491\n",
      "Training loss (for one batch) at step 40: 419.9762, Accuracy: 0.5503\n",
      "Training loss (for one batch) at step 50: 417.2414, Accuracy: 0.5587\n",
      "Training loss (for one batch) at step 60: 434.4216, Accuracy: 0.5633\n",
      "Training loss (for one batch) at step 70: 431.9538, Accuracy: 0.5687\n",
      "Training loss (for one batch) at step 80: 444.9608, Accuracy: 0.5682\n",
      "Training loss (for one batch) at step 90: 428.7241, Accuracy: 0.5657\n",
      "Training loss (for one batch) at step 100: 431.1161, Accuracy: 0.5640\n",
      "Training loss (for one batch) at step 110: 432.7641, Accuracy: 0.5630\n",
      "---- Training ----\n",
      "Training loss: 137.4326\n",
      "Training acc over epoch: 0.5638\n",
      "---- Validation ----\n",
      "Validation loss: 35.8809\n",
      "Validation acc: 0.5172\n",
      "Time taken: 18.25s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 432.3838, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 432.2404, Accuracy: 0.5554\n",
      "Training loss (for one batch) at step 20: 428.8150, Accuracy: 0.5472\n",
      "Training loss (for one batch) at step 30: 422.8029, Accuracy: 0.5590\n",
      "Training loss (for one batch) at step 40: 420.7588, Accuracy: 0.5644\n",
      "Training loss (for one batch) at step 50: 405.1941, Accuracy: 0.5689\n",
      "Training loss (for one batch) at step 60: 421.3213, Accuracy: 0.5774\n",
      "Training loss (for one batch) at step 70: 434.2456, Accuracy: 0.5818\n",
      "Training loss (for one batch) at step 80: 437.6532, Accuracy: 0.5788\n",
      "Training loss (for one batch) at step 90: 439.0010, Accuracy: 0.5771\n",
      "Training loss (for one batch) at step 100: 420.7511, Accuracy: 0.5760\n",
      "Training loss (for one batch) at step 110: 435.2184, Accuracy: 0.5766\n",
      "---- Training ----\n",
      "Training loss: 134.1974\n",
      "Training acc over epoch: 0.5758\n",
      "---- Validation ----\n",
      "Validation loss: 36.6276\n",
      "Validation acc: 0.5648\n",
      "Time taken: 17.89s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 434.1175, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 434.7554, Accuracy: 0.5568\n",
      "Training loss (for one batch) at step 20: 426.9974, Accuracy: 0.5499\n",
      "Training loss (for one batch) at step 30: 429.6546, Accuracy: 0.5537\n",
      "Training loss (for one batch) at step 40: 409.8864, Accuracy: 0.5705\n",
      "Training loss (for one batch) at step 50: 404.4022, Accuracy: 0.5789\n",
      "Training loss (for one batch) at step 60: 431.3120, Accuracy: 0.5871\n",
      "Training loss (for one batch) at step 70: 419.4954, Accuracy: 0.5904\n",
      "Training loss (for one batch) at step 80: 429.6183, Accuracy: 0.5855\n",
      "Training loss (for one batch) at step 90: 425.3070, Accuracy: 0.5844\n",
      "Training loss (for one batch) at step 100: 425.5832, Accuracy: 0.5835\n",
      "Training loss (for one batch) at step 110: 429.2909, Accuracy: 0.5857\n",
      "---- Training ----\n",
      "Training loss: 130.0588\n",
      "Training acc over epoch: 0.5854\n",
      "---- Validation ----\n",
      "Validation loss: 33.7638\n",
      "Validation acc: 0.5473\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 443.0013, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 433.2241, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 20: 431.4388, Accuracy: 0.5502\n",
      "Training loss (for one batch) at step 30: 421.5908, Accuracy: 0.5640\n",
      "Training loss (for one batch) at step 40: 414.3428, Accuracy: 0.5716\n",
      "Training loss (for one batch) at step 50: 411.3451, Accuracy: 0.5826\n",
      "Training loss (for one batch) at step 60: 402.5962, Accuracy: 0.5880\n",
      "Training loss (for one batch) at step 70: 421.8287, Accuracy: 0.5884\n",
      "Training loss (for one batch) at step 80: 424.2215, Accuracy: 0.5882\n",
      "Training loss (for one batch) at step 90: 415.1837, Accuracy: 0.5857\n",
      "Training loss (for one batch) at step 100: 404.6530, Accuracy: 0.5856\n",
      "Training loss (for one batch) at step 110: 422.2010, Accuracy: 0.5864\n",
      "---- Training ----\n",
      "Training loss: 139.1644\n",
      "Training acc over epoch: 0.5862\n",
      "---- Validation ----\n",
      "Validation loss: 39.3108\n",
      "Validation acc: 0.5185\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 426.5133, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 417.2206, Accuracy: 0.5661\n",
      "Training loss (for one batch) at step 20: 427.1909, Accuracy: 0.5621\n",
      "Training loss (for one batch) at step 30: 407.4935, Accuracy: 0.5673\n",
      "Training loss (for one batch) at step 40: 396.0978, Accuracy: 0.5776\n",
      "Training loss (for one batch) at step 50: 377.5636, Accuracy: 0.5919\n",
      "Training loss (for one batch) at step 60: 410.6965, Accuracy: 0.5975\n",
      "Training loss (for one batch) at step 70: 427.3627, Accuracy: 0.5998\n",
      "Training loss (for one batch) at step 80: 421.0372, Accuracy: 0.5945\n",
      "Training loss (for one batch) at step 90: 407.7764, Accuracy: 0.5913\n",
      "Training loss (for one batch) at step 100: 404.4244, Accuracy: 0.5931\n",
      "Training loss (for one batch) at step 110: 419.0045, Accuracy: 0.5940\n",
      "---- Training ----\n",
      "Training loss: 127.4672\n",
      "Training acc over epoch: 0.5945\n",
      "---- Validation ----\n",
      "Validation loss: 34.3955\n",
      "Validation acc: 0.5387\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 433.6424, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 425.8246, Accuracy: 0.5902\n",
      "Training loss (for one batch) at step 20: 400.7649, Accuracy: 0.5830\n",
      "Training loss (for one batch) at step 30: 397.9484, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 40: 385.5715, Accuracy: 0.5993\n",
      "Training loss (for one batch) at step 50: 381.9070, Accuracy: 0.6089\n",
      "Training loss (for one batch) at step 60: 389.0320, Accuracy: 0.6148\n",
      "Training loss (for one batch) at step 70: 411.4778, Accuracy: 0.6132\n",
      "Training loss (for one batch) at step 80: 412.6824, Accuracy: 0.6046\n",
      "Training loss (for one batch) at step 90: 395.9481, Accuracy: 0.6042\n",
      "Training loss (for one batch) at step 100: 389.6769, Accuracy: 0.6054\n",
      "Training loss (for one batch) at step 110: 415.2520, Accuracy: 0.6042\n",
      "---- Training ----\n",
      "Training loss: 130.7809\n",
      "Training acc over epoch: 0.6040\n",
      "---- Validation ----\n",
      "Validation loss: 36.5420\n",
      "Validation acc: 0.5720\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 427.5525, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 425.8667, Accuracy: 0.5760\n",
      "Training loss (for one batch) at step 20: 398.9584, Accuracy: 0.5651\n",
      "Training loss (for one batch) at step 30: 396.5172, Accuracy: 0.5789\n",
      "Training loss (for one batch) at step 40: 379.9034, Accuracy: 0.5966\n",
      "Training loss (for one batch) at step 50: 382.9998, Accuracy: 0.6074\n",
      "Training loss (for one batch) at step 60: 391.8923, Accuracy: 0.6128\n",
      "Training loss (for one batch) at step 70: 411.2969, Accuracy: 0.6088\n",
      "Training loss (for one batch) at step 80: 398.5922, Accuracy: 0.6004\n",
      "Training loss (for one batch) at step 90: 390.7520, Accuracy: 0.5987\n",
      "Training loss (for one batch) at step 100: 376.4403, Accuracy: 0.5993\n",
      "Training loss (for one batch) at step 110: 388.3271, Accuracy: 0.5998\n",
      "---- Training ----\n",
      "Training loss: 128.1668\n",
      "Training acc over epoch: 0.5988\n",
      "---- Validation ----\n",
      "Validation loss: 34.1009\n",
      "Validation acc: 0.5669\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 417.8164, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 390.7065, Accuracy: 0.5746\n",
      "Training loss (for one batch) at step 20: 400.8216, Accuracy: 0.5774\n",
      "Training loss (for one batch) at step 30: 362.4372, Accuracy: 0.5925\n",
      "Training loss (for one batch) at step 40: 366.0790, Accuracy: 0.6048\n",
      "Training loss (for one batch) at step 50: 355.2462, Accuracy: 0.6173\n",
      "Training loss (for one batch) at step 60: 375.8932, Accuracy: 0.6231\n",
      "Training loss (for one batch) at step 70: 399.4672, Accuracy: 0.6233\n",
      "Training loss (for one batch) at step 80: 390.5368, Accuracy: 0.6148\n",
      "Training loss (for one batch) at step 90: 382.6284, Accuracy: 0.6100\n",
      "Training loss (for one batch) at step 100: 361.9726, Accuracy: 0.6096\n",
      "Training loss (for one batch) at step 110: 372.0836, Accuracy: 0.6094\n",
      "---- Training ----\n",
      "Training loss: 112.1955\n",
      "Training acc over epoch: 0.6083\n",
      "---- Validation ----\n",
      "Validation loss: 37.0182\n",
      "Validation acc: 0.5580\n",
      "Time taken: 18.09s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 411.3630, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 399.9952, Accuracy: 0.5668\n",
      "Training loss (for one batch) at step 20: 373.8917, Accuracy: 0.5792\n",
      "Training loss (for one batch) at step 30: 386.6196, Accuracy: 0.5877\n",
      "Training loss (for one batch) at step 40: 359.2013, Accuracy: 0.6037\n",
      "Training loss (for one batch) at step 50: 347.0751, Accuracy: 0.6150\n",
      "Training loss (for one batch) at step 60: 358.3569, Accuracy: 0.6232\n",
      "Training loss (for one batch) at step 70: 390.1625, Accuracy: 0.6216\n",
      "Training loss (for one batch) at step 80: 390.5319, Accuracy: 0.6156\n",
      "Training loss (for one batch) at step 90: 382.6260, Accuracy: 0.6120\n",
      "Training loss (for one batch) at step 100: 350.6657, Accuracy: 0.6101\n",
      "Training loss (for one batch) at step 110: 371.2299, Accuracy: 0.6083\n",
      "---- Training ----\n",
      "Training loss: 124.8136\n",
      "Training acc over epoch: 0.6079\n",
      "---- Validation ----\n",
      "Validation loss: 33.9262\n",
      "Validation acc: 0.5462\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 380.9315, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 393.9684, Accuracy: 0.5540\n",
      "Training loss (for one batch) at step 20: 360.0802, Accuracy: 0.5733\n",
      "Training loss (for one batch) at step 30: 343.0844, Accuracy: 0.5869\n",
      "Training loss (for one batch) at step 40: 355.8009, Accuracy: 0.6033\n",
      "Training loss (for one batch) at step 50: 317.9263, Accuracy: 0.6157\n",
      "Training loss (for one batch) at step 60: 334.8518, Accuracy: 0.6219\n",
      "Training loss (for one batch) at step 70: 362.9250, Accuracy: 0.6160\n",
      "Training loss (for one batch) at step 80: 372.9334, Accuracy: 0.6060\n",
      "Training loss (for one batch) at step 90: 364.3756, Accuracy: 0.6018\n",
      "Training loss (for one batch) at step 100: 364.7950, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 110: 359.1743, Accuracy: 0.6023\n",
      "---- Training ----\n",
      "Training loss: 106.6357\n",
      "Training acc over epoch: 0.6024\n",
      "---- Validation ----\n",
      "Validation loss: 52.7010\n",
      "Validation acc: 0.5419\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 387.6915, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 367.4460, Accuracy: 0.5405\n",
      "Training loss (for one batch) at step 20: 356.3360, Accuracy: 0.5707\n",
      "Training loss (for one batch) at step 30: 346.8845, Accuracy: 0.5817\n",
      "Training loss (for one batch) at step 40: 352.5142, Accuracy: 0.6027\n",
      "Training loss (for one batch) at step 50: 304.8904, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 60: 347.1606, Accuracy: 0.6223\n",
      "Training loss (for one batch) at step 70: 355.6887, Accuracy: 0.6153\n",
      "Training loss (for one batch) at step 80: 340.3306, Accuracy: 0.6093\n",
      "Training loss (for one batch) at step 90: 379.3769, Accuracy: 0.6070\n",
      "Training loss (for one batch) at step 100: 355.5235, Accuracy: 0.6062\n",
      "Training loss (for one batch) at step 110: 344.3325, Accuracy: 0.6053\n",
      "---- Training ----\n",
      "Training loss: 115.0444\n",
      "Training acc over epoch: 0.6038\n",
      "---- Validation ----\n",
      "Validation loss: 37.6055\n",
      "Validation acc: 0.5433\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 371.7920, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 375.6959, Accuracy: 0.5348\n",
      "Training loss (for one batch) at step 20: 364.4932, Accuracy: 0.5428\n",
      "Training loss (for one batch) at step 30: 330.6269, Accuracy: 0.5685\n",
      "Training loss (for one batch) at step 40: 317.9832, Accuracy: 0.5865\n",
      "Training loss (for one batch) at step 50: 312.9214, Accuracy: 0.6008\n",
      "Training loss (for one batch) at step 60: 358.9862, Accuracy: 0.6095\n",
      "Training loss (for one batch) at step 70: 362.7503, Accuracy: 0.6041\n",
      "Training loss (for one batch) at step 80: 344.8056, Accuracy: 0.5965\n",
      "Training loss (for one batch) at step 90: 333.4118, Accuracy: 0.5912\n",
      "Training loss (for one batch) at step 100: 341.2097, Accuracy: 0.5951\n",
      "Training loss (for one batch) at step 110: 357.2692, Accuracy: 0.5943\n",
      "---- Training ----\n",
      "Training loss: 104.2680\n",
      "Training acc over epoch: 0.5926\n",
      "---- Validation ----\n",
      "Validation loss: 35.0036\n",
      "Validation acc: 0.5680\n",
      "Time taken: 18.06s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 364.9409, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 368.3418, Accuracy: 0.5604\n",
      "Training loss (for one batch) at step 20: 332.4609, Accuracy: 0.5707\n",
      "Training loss (for one batch) at step 30: 308.8484, Accuracy: 0.5912\n",
      "Training loss (for one batch) at step 40: 303.9115, Accuracy: 0.5998\n",
      "Training loss (for one batch) at step 50: 310.8663, Accuracy: 0.6124\n",
      "Training loss (for one batch) at step 60: 314.8709, Accuracy: 0.6200\n",
      "Training loss (for one batch) at step 70: 355.1782, Accuracy: 0.6120\n",
      "Training loss (for one batch) at step 80: 370.1238, Accuracy: 0.6020\n",
      "Training loss (for one batch) at step 90: 324.0208, Accuracy: 0.6019\n",
      "Training loss (for one batch) at step 100: 328.0156, Accuracy: 0.6013\n",
      "Training loss (for one batch) at step 110: 322.1227, Accuracy: 0.6002\n",
      "---- Training ----\n",
      "Training loss: 119.3878\n",
      "Training acc over epoch: 0.5998\n",
      "---- Validation ----\n",
      "Validation loss: 37.3622\n",
      "Validation acc: 0.5572\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 365.1330, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 341.0060, Accuracy: 0.5291\n",
      "Training loss (for one batch) at step 20: 325.1730, Accuracy: 0.5532\n",
      "Training loss (for one batch) at step 30: 308.3493, Accuracy: 0.5796\n",
      "Training loss (for one batch) at step 40: 327.8166, Accuracy: 0.5955\n",
      "Training loss (for one batch) at step 50: 308.6484, Accuracy: 0.6074\n",
      "Training loss (for one batch) at step 60: 325.0230, Accuracy: 0.6144\n",
      "Training loss (for one batch) at step 70: 349.4953, Accuracy: 0.6072\n",
      "Training loss (for one batch) at step 80: 358.8245, Accuracy: 0.5991\n",
      "Training loss (for one batch) at step 90: 326.5371, Accuracy: 0.5955\n",
      "Training loss (for one batch) at step 100: 317.2561, Accuracy: 0.5965\n",
      "Training loss (for one batch) at step 110: 319.9146, Accuracy: 0.5952\n",
      "---- Training ----\n",
      "Training loss: 111.6935\n",
      "Training acc over epoch: 0.5937\n",
      "---- Validation ----\n",
      "Validation loss: 43.2959\n",
      "Validation acc: 0.5161\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 349.5056, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 368.9397, Accuracy: 0.5178\n",
      "Training loss (for one batch) at step 20: 320.5616, Accuracy: 0.5539\n",
      "Training loss (for one batch) at step 30: 316.0702, Accuracy: 0.5789\n",
      "Training loss (for one batch) at step 40: 318.2724, Accuracy: 0.5945\n",
      "Training loss (for one batch) at step 50: 309.6787, Accuracy: 0.6091\n",
      "Training loss (for one batch) at step 60: 297.4419, Accuracy: 0.6159\n",
      "Training loss (for one batch) at step 70: 340.5873, Accuracy: 0.6072\n",
      "Training loss (for one batch) at step 80: 318.9521, Accuracy: 0.5977\n",
      "Training loss (for one batch) at step 90: 311.3144, Accuracy: 0.5930\n",
      "Training loss (for one batch) at step 100: 314.3059, Accuracy: 0.5950\n",
      "Training loss (for one batch) at step 110: 338.7032, Accuracy: 0.5950\n",
      "---- Training ----\n",
      "Training loss: 101.4551\n",
      "Training acc over epoch: 0.5938\n",
      "---- Validation ----\n",
      "Validation loss: 48.4468\n",
      "Validation acc: 0.5293\n",
      "Time taken: 20.13s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 376.0479, Accuracy: 0.4844\n",
      "Training loss (for one batch) at step 10: 335.4128, Accuracy: 0.5149\n",
      "Training loss (for one batch) at step 20: 318.2690, Accuracy: 0.5543\n",
      "Training loss (for one batch) at step 30: 311.2610, Accuracy: 0.5806\n",
      "Training loss (for one batch) at step 40: 301.3139, Accuracy: 0.5947\n",
      "Training loss (for one batch) at step 50: 295.7582, Accuracy: 0.6092\n",
      "Training loss (for one batch) at step 60: 312.1797, Accuracy: 0.6157\n",
      "Training loss (for one batch) at step 70: 329.2359, Accuracy: 0.6081\n",
      "Training loss (for one batch) at step 80: 325.7912, Accuracy: 0.5949\n",
      "Training loss (for one batch) at step 90: 302.8586, Accuracy: 0.5931\n",
      "Training loss (for one batch) at step 100: 287.8165, Accuracy: 0.5949\n",
      "Training loss (for one batch) at step 110: 306.5355, Accuracy: 0.5952\n",
      "---- Training ----\n",
      "Training loss: 98.5738\n",
      "Training acc over epoch: 0.5937\n",
      "---- Validation ----\n",
      "Validation loss: 34.1009\n",
      "Validation acc: 0.5664\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 348.1212, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 335.2758, Accuracy: 0.5185\n",
      "Training loss (for one batch) at step 20: 306.4157, Accuracy: 0.5465\n",
      "Training loss (for one batch) at step 30: 278.9676, Accuracy: 0.5726\n",
      "Training loss (for one batch) at step 40: 306.1937, Accuracy: 0.5939\n",
      "Training loss (for one batch) at step 50: 279.0922, Accuracy: 0.6080\n",
      "Training loss (for one batch) at step 60: 295.8158, Accuracy: 0.6130\n",
      "Training loss (for one batch) at step 70: 308.7955, Accuracy: 0.6049\n",
      "Training loss (for one batch) at step 80: 336.7770, Accuracy: 0.5943\n",
      "Training loss (for one batch) at step 90: 315.4727, Accuracy: 0.5921\n",
      "Training loss (for one batch) at step 100: 310.3629, Accuracy: 0.5929\n",
      "Training loss (for one batch) at step 110: 324.0363, Accuracy: 0.5935\n",
      "---- Training ----\n",
      "Training loss: 110.9187\n",
      "Training acc over epoch: 0.5928\n",
      "---- Validation ----\n",
      "Validation loss: 31.7894\n",
      "Validation acc: 0.5508\n",
      "Time taken: 18.06s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 352.1563, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 317.1533, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 20: 292.4302, Accuracy: 0.5461\n",
      "Training loss (for one batch) at step 30: 310.9555, Accuracy: 0.5736\n",
      "Training loss (for one batch) at step 40: 292.1719, Accuracy: 0.5966\n",
      "Training loss (for one batch) at step 50: 300.8617, Accuracy: 0.6101\n",
      "Training loss (for one batch) at step 60: 303.7487, Accuracy: 0.6174\n",
      "Training loss (for one batch) at step 70: 314.8904, Accuracy: 0.6059\n",
      "Training loss (for one batch) at step 80: 325.5847, Accuracy: 0.5965\n",
      "Training loss (for one batch) at step 90: 295.0218, Accuracy: 0.5934\n",
      "Training loss (for one batch) at step 100: 301.4508, Accuracy: 0.5967\n",
      "Training loss (for one batch) at step 110: 316.0793, Accuracy: 0.5958\n",
      "---- Training ----\n",
      "Training loss: 103.7870\n",
      "Training acc over epoch: 0.5940\n",
      "---- Validation ----\n",
      "Validation loss: 40.5558\n",
      "Validation acc: 0.5500\n",
      "Time taken: 18.05s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 333.3929, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 337.5926, Accuracy: 0.5185\n",
      "Training loss (for one batch) at step 20: 285.1572, Accuracy: 0.5499\n",
      "Training loss (for one batch) at step 30: 306.8811, Accuracy: 0.5754\n",
      "Training loss (for one batch) at step 40: 285.4247, Accuracy: 0.5913\n",
      "Training loss (for one batch) at step 50: 275.8073, Accuracy: 0.6080\n",
      "Training loss (for one batch) at step 60: 284.9221, Accuracy: 0.6165\n",
      "Training loss (for one batch) at step 70: 305.3604, Accuracy: 0.6072\n",
      "Training loss (for one batch) at step 80: 334.6581, Accuracy: 0.5955\n",
      "Training loss (for one batch) at step 90: 287.5798, Accuracy: 0.5919\n",
      "Training loss (for one batch) at step 100: 297.1891, Accuracy: 0.5937\n",
      "Training loss (for one batch) at step 110: 318.2257, Accuracy: 0.5923\n",
      "---- Training ----\n",
      "Training loss: 115.2761\n",
      "Training acc over epoch: 0.5922\n",
      "---- Validation ----\n",
      "Validation loss: 39.4271\n",
      "Validation acc: 0.5040\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 336.9187, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 327.8905, Accuracy: 0.5142\n",
      "Training loss (for one batch) at step 20: 283.5628, Accuracy: 0.5409\n",
      "Training loss (for one batch) at step 30: 281.4858, Accuracy: 0.5678\n",
      "Training loss (for one batch) at step 40: 297.1913, Accuracy: 0.5865\n",
      "Training loss (for one batch) at step 50: 290.7086, Accuracy: 0.6019\n",
      "Training loss (for one batch) at step 60: 303.9041, Accuracy: 0.6096\n",
      "Training loss (for one batch) at step 70: 304.6153, Accuracy: 0.6007\n",
      "Training loss (for one batch) at step 80: 316.5984, Accuracy: 0.5892\n",
      "Training loss (for one batch) at step 90: 289.7618, Accuracy: 0.5876\n",
      "Training loss (for one batch) at step 100: 287.8652, Accuracy: 0.5899\n",
      "Training loss (for one batch) at step 110: 316.5272, Accuracy: 0.5897\n",
      "---- Training ----\n",
      "Training loss: 108.8726\n",
      "Training acc over epoch: 0.5889\n",
      "---- Validation ----\n",
      "Validation loss: 48.9442\n",
      "Validation acc: 0.5279\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 317.0278, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 312.5121, Accuracy: 0.5028\n",
      "Training loss (for one batch) at step 20: 291.6169, Accuracy: 0.5443\n",
      "Training loss (for one batch) at step 30: 270.9793, Accuracy: 0.5754\n",
      "Training loss (for one batch) at step 40: 269.8431, Accuracy: 0.5922\n",
      "Training loss (for one batch) at step 50: 288.5775, Accuracy: 0.6081\n",
      "Training loss (for one batch) at step 60: 281.9285, Accuracy: 0.6154\n",
      "Training loss (for one batch) at step 70: 305.3718, Accuracy: 0.6055\n",
      "Training loss (for one batch) at step 80: 308.5653, Accuracy: 0.5927\n",
      "Training loss (for one batch) at step 90: 286.4529, Accuracy: 0.5907\n",
      "Training loss (for one batch) at step 100: 279.2425, Accuracy: 0.5927\n",
      "Training loss (for one batch) at step 110: 307.9590, Accuracy: 0.5897\n",
      "---- Training ----\n",
      "Training loss: 103.0094\n",
      "Training acc over epoch: 0.5893\n",
      "---- Validation ----\n",
      "Validation loss: 38.3677\n",
      "Validation acc: 0.5309\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 322.7952, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 314.6217, Accuracy: 0.5220\n",
      "Training loss (for one batch) at step 20: 304.5536, Accuracy: 0.5432\n",
      "Training loss (for one batch) at step 30: 266.7549, Accuracy: 0.5713\n",
      "Training loss (for one batch) at step 40: 297.9502, Accuracy: 0.5901\n",
      "Training loss (for one batch) at step 50: 280.3769, Accuracy: 0.6045\n",
      "Training loss (for one batch) at step 60: 286.8449, Accuracy: 0.6101\n",
      "Training loss (for one batch) at step 70: 333.8560, Accuracy: 0.6012\n",
      "Training loss (for one batch) at step 80: 295.4065, Accuracy: 0.5909\n",
      "Training loss (for one batch) at step 90: 302.7221, Accuracy: 0.5875\n",
      "Training loss (for one batch) at step 100: 273.5707, Accuracy: 0.5888\n",
      "Training loss (for one batch) at step 110: 293.4901, Accuracy: 0.5880\n",
      "---- Training ----\n",
      "Training loss: 92.5975\n",
      "Training acc over epoch: 0.5867\n",
      "---- Validation ----\n",
      "Validation loss: 34.1503\n",
      "Validation acc: 0.5255\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 318.4218, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 329.0599, Accuracy: 0.4964\n",
      "Training loss (for one batch) at step 20: 295.1314, Accuracy: 0.5260\n",
      "Training loss (for one batch) at step 30: 273.1694, Accuracy: 0.5612\n",
      "Training loss (for one batch) at step 40: 296.8255, Accuracy: 0.5856\n",
      "Training loss (for one batch) at step 50: 283.8503, Accuracy: 0.6009\n",
      "Training loss (for one batch) at step 60: 280.5665, Accuracy: 0.6099\n",
      "Training loss (for one batch) at step 70: 311.6682, Accuracy: 0.5985\n",
      "Training loss (for one batch) at step 80: 311.2309, Accuracy: 0.5900\n",
      "Training loss (for one batch) at step 90: 290.3203, Accuracy: 0.5850\n",
      "Training loss (for one batch) at step 100: 273.0550, Accuracy: 0.5873\n",
      "Training loss (for one batch) at step 110: 295.3663, Accuracy: 0.5883\n",
      "---- Training ----\n",
      "Training loss: 107.1299\n",
      "Training acc over epoch: 0.5873\n",
      "---- Validation ----\n",
      "Validation loss: 40.5181\n",
      "Validation acc: 0.5427\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 316.4179, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 326.8358, Accuracy: 0.5199\n",
      "Training loss (for one batch) at step 20: 270.5151, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 30: 286.3289, Accuracy: 0.5731\n",
      "Training loss (for one batch) at step 40: 269.7329, Accuracy: 0.5873\n",
      "Training loss (for one batch) at step 50: 300.3089, Accuracy: 0.6025\n",
      "Training loss (for one batch) at step 60: 276.6140, Accuracy: 0.6089\n",
      "Training loss (for one batch) at step 70: 294.4033, Accuracy: 0.5996\n",
      "Training loss (for one batch) at step 80: 309.4300, Accuracy: 0.5884\n",
      "Training loss (for one batch) at step 90: 293.4871, Accuracy: 0.5866\n",
      "Training loss (for one batch) at step 100: 279.3829, Accuracy: 0.5886\n",
      "Training loss (for one batch) at step 110: 296.0191, Accuracy: 0.5890\n",
      "---- Training ----\n",
      "Training loss: 93.6199\n",
      "Training acc over epoch: 0.5873\n",
      "---- Validation ----\n",
      "Validation loss: 44.5529\n",
      "Validation acc: 0.5274\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 315.1598, Accuracy: 0.4688\n",
      "Training loss (for one batch) at step 10: 304.3727, Accuracy: 0.5099\n",
      "Training loss (for one batch) at step 20: 277.1299, Accuracy: 0.5372\n",
      "Training loss (for one batch) at step 30: 281.9774, Accuracy: 0.5655\n",
      "Training loss (for one batch) at step 40: 273.0220, Accuracy: 0.5882\n",
      "Training loss (for one batch) at step 50: 260.6363, Accuracy: 0.6040\n",
      "Training loss (for one batch) at step 60: 291.1504, Accuracy: 0.6131\n",
      "Training loss (for one batch) at step 70: 296.2838, Accuracy: 0.6026\n",
      "Training loss (for one batch) at step 80: 308.5289, Accuracy: 0.5896\n",
      "Training loss (for one batch) at step 90: 291.6562, Accuracy: 0.5871\n",
      "Training loss (for one batch) at step 100: 278.5995, Accuracy: 0.5876\n",
      "Training loss (for one batch) at step 110: 285.8193, Accuracy: 0.5894\n",
      "---- Training ----\n",
      "Training loss: 84.8522\n",
      "Training acc over epoch: 0.5882\n",
      "---- Validation ----\n",
      "Validation loss: 45.7036\n",
      "Validation acc: 0.5535\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 301.9274, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 321.0457, Accuracy: 0.5163\n",
      "Training loss (for one batch) at step 20: 279.0196, Accuracy: 0.5439\n",
      "Training loss (for one batch) at step 30: 267.3795, Accuracy: 0.5711\n",
      "Training loss (for one batch) at step 40: 289.4306, Accuracy: 0.5837\n",
      "Training loss (for one batch) at step 50: 261.1254, Accuracy: 0.6037\n",
      "Training loss (for one batch) at step 60: 260.9998, Accuracy: 0.6109\n",
      "Training loss (for one batch) at step 70: 295.0069, Accuracy: 0.5995\n",
      "Training loss (for one batch) at step 80: 284.7101, Accuracy: 0.5881\n",
      "Training loss (for one batch) at step 90: 289.2833, Accuracy: 0.5874\n",
      "Training loss (for one batch) at step 100: 290.7044, Accuracy: 0.5879\n",
      "Training loss (for one batch) at step 110: 276.8117, Accuracy: 0.5872\n",
      "---- Training ----\n",
      "Training loss: 105.1560\n",
      "Training acc over epoch: 0.5860\n",
      "---- Validation ----\n",
      "Validation loss: 39.9163\n",
      "Validation acc: 0.5656\n",
      "Time taken: 18.10s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 309.4264, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 321.4885, Accuracy: 0.4879\n",
      "Training loss (for one batch) at step 20: 272.7451, Accuracy: 0.5335\n",
      "Training loss (for one batch) at step 30: 286.2874, Accuracy: 0.5633\n",
      "Training loss (for one batch) at step 40: 268.8068, Accuracy: 0.5817\n",
      "Training loss (for one batch) at step 50: 260.5315, Accuracy: 0.5982\n",
      "Training loss (for one batch) at step 60: 274.2149, Accuracy: 0.6091\n",
      "Training loss (for one batch) at step 70: 278.0581, Accuracy: 0.5998\n",
      "Training loss (for one batch) at step 80: 283.0947, Accuracy: 0.5880\n",
      "Training loss (for one batch) at step 90: 278.3122, Accuracy: 0.5841\n",
      "Training loss (for one batch) at step 100: 269.5312, Accuracy: 0.5878\n",
      "Training loss (for one batch) at step 110: 292.4723, Accuracy: 0.5873\n",
      "---- Training ----\n",
      "Training loss: 103.1703\n",
      "Training acc over epoch: 0.5863\n",
      "---- Validation ----\n",
      "Validation loss: 45.6985\n",
      "Validation acc: 0.5422\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 294.9184, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 309.3112, Accuracy: 0.5099\n",
      "Training loss (for one batch) at step 20: 293.3369, Accuracy: 0.5346\n",
      "Training loss (for one batch) at step 30: 278.1029, Accuracy: 0.5693\n",
      "Training loss (for one batch) at step 40: 259.3688, Accuracy: 0.5846\n",
      "Training loss (for one batch) at step 50: 266.4805, Accuracy: 0.5991\n",
      "Training loss (for one batch) at step 60: 260.1281, Accuracy: 0.6041\n",
      "Training loss (for one batch) at step 70: 266.1415, Accuracy: 0.5964\n",
      "Training loss (for one batch) at step 80: 285.9123, Accuracy: 0.5860\n",
      "Training loss (for one batch) at step 90: 278.0205, Accuracy: 0.5840\n",
      "Training loss (for one batch) at step 100: 269.6964, Accuracy: 0.5875\n",
      "Training loss (for one batch) at step 110: 271.9523, Accuracy: 0.5872\n",
      "---- Training ----\n",
      "Training loss: 93.4557\n",
      "Training acc over epoch: 0.5854\n",
      "---- Validation ----\n",
      "Validation loss: 34.7150\n",
      "Validation acc: 0.5438\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 318.5939, Accuracy: 0.4922\n",
      "Training loss (for one batch) at step 10: 298.9454, Accuracy: 0.4950\n",
      "Training loss (for one batch) at step 20: 291.6376, Accuracy: 0.5286\n",
      "Training loss (for one batch) at step 30: 265.6010, Accuracy: 0.5635\n",
      "Training loss (for one batch) at step 40: 260.6932, Accuracy: 0.5827\n",
      "Training loss (for one batch) at step 50: 247.7527, Accuracy: 0.5974\n",
      "Training loss (for one batch) at step 60: 272.9066, Accuracy: 0.6058\n",
      "Training loss (for one batch) at step 70: 294.7712, Accuracy: 0.5979\n",
      "Training loss (for one batch) at step 80: 279.4054, Accuracy: 0.5881\n",
      "Training loss (for one batch) at step 90: 275.9176, Accuracy: 0.5836\n",
      "Training loss (for one batch) at step 100: 279.6098, Accuracy: 0.5856\n",
      "Training loss (for one batch) at step 110: 283.7378, Accuracy: 0.5847\n",
      "---- Training ----\n",
      "Training loss: 95.7139\n",
      "Training acc over epoch: 0.5850\n",
      "---- Validation ----\n",
      "Validation loss: 42.1140\n",
      "Validation acc: 0.5376\n",
      "Time taken: 18.09s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 297.7319, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 283.2427, Accuracy: 0.5021\n",
      "Training loss (for one batch) at step 20: 260.1104, Accuracy: 0.5346\n",
      "Training loss (for one batch) at step 30: 273.0117, Accuracy: 0.5630\n",
      "Training loss (for one batch) at step 40: 266.5757, Accuracy: 0.5838\n",
      "Training loss (for one batch) at step 50: 238.6713, Accuracy: 0.6009\n",
      "Training loss (for one batch) at step 60: 264.1671, Accuracy: 0.6101\n",
      "Training loss (for one batch) at step 70: 293.2166, Accuracy: 0.5995\n",
      "Training loss (for one batch) at step 80: 295.3197, Accuracy: 0.5869\n",
      "Training loss (for one batch) at step 90: 276.7075, Accuracy: 0.5837\n",
      "Training loss (for one batch) at step 100: 257.4606, Accuracy: 0.5869\n",
      "Training loss (for one batch) at step 110: 289.8516, Accuracy: 0.5865\n",
      "---- Training ----\n",
      "Training loss: 95.7967\n",
      "Training acc over epoch: 0.5845\n",
      "---- Validation ----\n",
      "Validation loss: 36.8295\n",
      "Validation acc: 0.5371\n",
      "Time taken: 17.89s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 285.9889, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 291.9610, Accuracy: 0.5036\n",
      "Training loss (for one batch) at step 20: 279.0328, Accuracy: 0.5253\n",
      "Training loss (for one batch) at step 30: 253.2589, Accuracy: 0.5653\n",
      "Training loss (for one batch) at step 40: 266.2087, Accuracy: 0.5800\n",
      "Training loss (for one batch) at step 50: 258.7432, Accuracy: 0.5980\n",
      "Training loss (for one batch) at step 60: 269.9702, Accuracy: 0.6062\n",
      "Training loss (for one batch) at step 70: 280.3948, Accuracy: 0.5945\n",
      "Training loss (for one batch) at step 80: 274.5065, Accuracy: 0.5827\n",
      "Training loss (for one batch) at step 90: 270.5555, Accuracy: 0.5822\n",
      "Training loss (for one batch) at step 100: 278.8987, Accuracy: 0.5851\n",
      "Training loss (for one batch) at step 110: 269.4236, Accuracy: 0.5861\n",
      "---- Training ----\n",
      "Training loss: 91.9809\n",
      "Training acc over epoch: 0.5839\n",
      "---- Validation ----\n",
      "Validation loss: 34.2253\n",
      "Validation acc: 0.5400\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 300.3762, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 269.2448, Accuracy: 0.4844\n",
      "Training loss (for one batch) at step 20: 253.0769, Accuracy: 0.5309\n",
      "Training loss (for one batch) at step 30: 255.7221, Accuracy: 0.5597\n",
      "Training loss (for one batch) at step 40: 268.9955, Accuracy: 0.5816\n",
      "Training loss (for one batch) at step 50: 248.0191, Accuracy: 0.5987\n",
      "Training loss (for one batch) at step 60: 264.2494, Accuracy: 0.6049\n",
      "Training loss (for one batch) at step 70: 284.5376, Accuracy: 0.5962\n",
      "Training loss (for one batch) at step 80: 280.0222, Accuracy: 0.5841\n",
      "Training loss (for one batch) at step 90: 265.9032, Accuracy: 0.5852\n",
      "Training loss (for one batch) at step 100: 268.3227, Accuracy: 0.5850\n",
      "Training loss (for one batch) at step 110: 256.3286, Accuracy: 0.5845\n",
      "---- Training ----\n",
      "Training loss: 88.1280\n",
      "Training acc over epoch: 0.5836\n",
      "---- Validation ----\n",
      "Validation loss: 50.4402\n",
      "Validation acc: 0.5384\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 310.6599, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 293.3831, Accuracy: 0.4993\n",
      "Training loss (for one batch) at step 20: 273.5047, Accuracy: 0.5231\n",
      "Training loss (for one batch) at step 30: 250.2128, Accuracy: 0.5653\n",
      "Training loss (for one batch) at step 40: 256.5298, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 50: 246.9741, Accuracy: 0.6019\n",
      "Training loss (for one batch) at step 60: 265.7238, Accuracy: 0.6104\n",
      "Training loss (for one batch) at step 70: 284.4450, Accuracy: 0.6009\n",
      "Training loss (for one batch) at step 80: 274.2007, Accuracy: 0.5872\n",
      "Training loss (for one batch) at step 90: 284.1573, Accuracy: 0.5856\n",
      "Training loss (for one batch) at step 100: 272.6129, Accuracy: 0.5869\n",
      "Training loss (for one batch) at step 110: 274.9952, Accuracy: 0.5875\n",
      "---- Training ----\n",
      "Training loss: 85.4726\n",
      "Training acc over epoch: 0.5845\n",
      "---- Validation ----\n",
      "Validation loss: 44.5504\n",
      "Validation acc: 0.5790\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 324.1299, Accuracy: 0.4844\n",
      "Training loss (for one batch) at step 10: 280.7296, Accuracy: 0.4936\n",
      "Training loss (for one batch) at step 20: 277.9634, Accuracy: 0.5335\n",
      "Training loss (for one batch) at step 30: 264.3245, Accuracy: 0.5567\n",
      "Training loss (for one batch) at step 40: 260.9792, Accuracy: 0.5791\n",
      "Training loss (for one batch) at step 50: 244.3352, Accuracy: 0.5987\n",
      "Training loss (for one batch) at step 60: 246.3666, Accuracy: 0.6091\n",
      "Training loss (for one batch) at step 70: 293.8871, Accuracy: 0.5999\n",
      "Training loss (for one batch) at step 80: 291.6179, Accuracy: 0.5869\n",
      "Training loss (for one batch) at step 90: 276.7440, Accuracy: 0.5824\n",
      "Training loss (for one batch) at step 100: 265.7303, Accuracy: 0.5850\n",
      "Training loss (for one batch) at step 110: 272.1583, Accuracy: 0.5837\n",
      "---- Training ----\n",
      "Training loss: 86.9090\n",
      "Training acc over epoch: 0.5837\n",
      "---- Validation ----\n",
      "Validation loss: 37.7815\n",
      "Validation acc: 0.5690\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 277.1659, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 300.2464, Accuracy: 0.4964\n",
      "Training loss (for one batch) at step 20: 261.9005, Accuracy: 0.5383\n",
      "Training loss (for one batch) at step 30: 259.4760, Accuracy: 0.5678\n",
      "Training loss (for one batch) at step 40: 267.4605, Accuracy: 0.5863\n",
      "Training loss (for one batch) at step 50: 257.4218, Accuracy: 0.6009\n",
      "Training loss (for one batch) at step 60: 259.0788, Accuracy: 0.6086\n",
      "Training loss (for one batch) at step 70: 272.2740, Accuracy: 0.6001\n",
      "Training loss (for one batch) at step 80: 282.7693, Accuracy: 0.5877\n",
      "Training loss (for one batch) at step 90: 283.6051, Accuracy: 0.5844\n",
      "Training loss (for one batch) at step 100: 264.1234, Accuracy: 0.5855\n",
      "Training loss (for one batch) at step 110: 259.0204, Accuracy: 0.5850\n",
      "---- Training ----\n",
      "Training loss: 85.2601\n",
      "Training acc over epoch: 0.5845\n",
      "---- Validation ----\n",
      "Validation loss: 47.4780\n",
      "Validation acc: 0.5476\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 272.2225, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 274.2923, Accuracy: 0.4936\n",
      "Training loss (for one batch) at step 20: 264.1672, Accuracy: 0.5272\n",
      "Training loss (for one batch) at step 30: 247.4597, Accuracy: 0.5595\n",
      "Training loss (for one batch) at step 40: 251.4662, Accuracy: 0.5783\n",
      "Training loss (for one batch) at step 50: 242.8278, Accuracy: 0.6017\n",
      "Training loss (for one batch) at step 60: 265.1650, Accuracy: 0.6068\n",
      "Training loss (for one batch) at step 70: 272.2077, Accuracy: 0.5969\n",
      "Training loss (for one batch) at step 80: 279.6696, Accuracy: 0.5850\n",
      "Training loss (for one batch) at step 90: 258.5727, Accuracy: 0.5816\n",
      "Training loss (for one batch) at step 100: 288.6331, Accuracy: 0.5827\n",
      "Training loss (for one batch) at step 110: 266.5473, Accuracy: 0.5826\n",
      "---- Training ----\n",
      "Training loss: 90.2811\n",
      "Training acc over epoch: 0.5817\n",
      "---- Validation ----\n",
      "Validation loss: 39.3298\n",
      "Validation acc: 0.5457\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 284.2970, Accuracy: 0.4609\n",
      "Training loss (for one batch) at step 10: 296.4689, Accuracy: 0.5149\n",
      "Training loss (for one batch) at step 20: 261.6919, Accuracy: 0.5372\n",
      "Training loss (for one batch) at step 30: 247.5201, Accuracy: 0.5620\n",
      "Training loss (for one batch) at step 40: 261.6075, Accuracy: 0.5837\n",
      "Training loss (for one batch) at step 50: 246.8213, Accuracy: 0.5996\n",
      "Training loss (for one batch) at step 60: 257.9724, Accuracy: 0.6084\n",
      "Training loss (for one batch) at step 70: 275.6190, Accuracy: 0.6022\n",
      "Training loss (for one batch) at step 80: 282.3141, Accuracy: 0.5866\n",
      "Training loss (for one batch) at step 90: 261.8163, Accuracy: 0.5839\n",
      "Training loss (for one batch) at step 100: 265.3821, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 110: 247.5517, Accuracy: 0.5859\n",
      "---- Training ----\n",
      "Training loss: 99.5840\n",
      "Training acc over epoch: 0.5853\n",
      "---- Validation ----\n",
      "Validation loss: 55.6647\n",
      "Validation acc: 0.5551\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 292.0361, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 275.4599, Accuracy: 0.4858\n",
      "Training loss (for one batch) at step 20: 269.4971, Accuracy: 0.5275\n",
      "Training loss (for one batch) at step 30: 241.6399, Accuracy: 0.5600\n",
      "Training loss (for one batch) at step 40: 256.9263, Accuracy: 0.5793\n",
      "Training loss (for one batch) at step 50: 241.1304, Accuracy: 0.5939\n",
      "Training loss (for one batch) at step 60: 240.0018, Accuracy: 0.6057\n",
      "Training loss (for one batch) at step 70: 268.4530, Accuracy: 0.5951\n",
      "Training loss (for one batch) at step 80: 281.5782, Accuracy: 0.5823\n",
      "Training loss (for one batch) at step 90: 257.7155, Accuracy: 0.5801\n",
      "Training loss (for one batch) at step 100: 267.5223, Accuracy: 0.5821\n",
      "Training loss (for one batch) at step 110: 276.4839, Accuracy: 0.5831\n",
      "---- Training ----\n",
      "Training loss: 92.7360\n",
      "Training acc over epoch: 0.5817\n",
      "---- Validation ----\n",
      "Validation loss: 43.5562\n",
      "Validation acc: 0.5446\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 289.4941, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 280.1057, Accuracy: 0.5014\n",
      "Training loss (for one batch) at step 20: 245.5484, Accuracy: 0.5357\n",
      "Training loss (for one batch) at step 30: 243.4601, Accuracy: 0.5585\n",
      "Training loss (for one batch) at step 40: 257.8469, Accuracy: 0.5804\n",
      "Training loss (for one batch) at step 50: 254.3820, Accuracy: 0.5979\n",
      "Training loss (for one batch) at step 60: 256.0093, Accuracy: 0.6091\n",
      "Training loss (for one batch) at step 70: 261.1533, Accuracy: 0.5967\n",
      "Training loss (for one batch) at step 80: 270.0894, Accuracy: 0.5857\n",
      "Training loss (for one batch) at step 90: 238.5742, Accuracy: 0.5840\n",
      "Training loss (for one batch) at step 100: 244.8654, Accuracy: 0.5856\n",
      "Training loss (for one batch) at step 110: 261.8825, Accuracy: 0.5842\n",
      "---- Training ----\n",
      "Training loss: 92.2801\n",
      "Training acc over epoch: 0.5831\n",
      "---- Validation ----\n",
      "Validation loss: 43.8282\n",
      "Validation acc: 0.5430\n",
      "Time taken: 17.89s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 268.2579, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 286.4885, Accuracy: 0.4830\n",
      "Training loss (for one batch) at step 20: 266.3098, Accuracy: 0.5279\n",
      "Training loss (for one batch) at step 30: 257.2870, Accuracy: 0.5635\n",
      "Training loss (for one batch) at step 40: 243.4767, Accuracy: 0.5838\n",
      "Training loss (for one batch) at step 50: 240.4714, Accuracy: 0.5985\n",
      "Training loss (for one batch) at step 60: 256.9474, Accuracy: 0.6055\n",
      "Training loss (for one batch) at step 70: 283.9081, Accuracy: 0.5979\n",
      "Training loss (for one batch) at step 80: 260.4376, Accuracy: 0.5861\n",
      "Training loss (for one batch) at step 90: 246.9469, Accuracy: 0.5815\n",
      "Training loss (for one batch) at step 100: 267.5471, Accuracy: 0.5833\n",
      "Training loss (for one batch) at step 110: 255.8212, Accuracy: 0.5814\n",
      "---- Training ----\n",
      "Training loss: 93.1488\n",
      "Training acc over epoch: 0.5816\n",
      "---- Validation ----\n",
      "Validation loss: 56.0338\n",
      "Validation acc: 0.5290\n",
      "Time taken: 18.10s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 271.6486, Accuracy: 0.4844\n",
      "Training loss (for one batch) at step 10: 288.5750, Accuracy: 0.5014\n",
      "Training loss (for one batch) at step 20: 237.5300, Accuracy: 0.5357\n",
      "Training loss (for one batch) at step 30: 246.8000, Accuracy: 0.5582\n",
      "Training loss (for one batch) at step 40: 254.0736, Accuracy: 0.5817\n",
      "Training loss (for one batch) at step 50: 239.5953, Accuracy: 0.6006\n",
      "Training loss (for one batch) at step 60: 238.4240, Accuracy: 0.6085\n",
      "Training loss (for one batch) at step 70: 249.5934, Accuracy: 0.5974\n",
      "Training loss (for one batch) at step 80: 278.7367, Accuracy: 0.5845\n",
      "Training loss (for one batch) at step 90: 270.6893, Accuracy: 0.5844\n",
      "Training loss (for one batch) at step 100: 249.0324, Accuracy: 0.5854\n",
      "Training loss (for one batch) at step 110: 258.8942, Accuracy: 0.5840\n",
      "---- Training ----\n",
      "Training loss: 79.7960\n",
      "Training acc over epoch: 0.5834\n",
      "---- Validation ----\n",
      "Validation loss: 42.8893\n",
      "Validation acc: 0.5210\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 279.1523, Accuracy: 0.4844\n",
      "Training loss (for one batch) at step 10: 268.3654, Accuracy: 0.4822\n",
      "Training loss (for one batch) at step 20: 259.6309, Accuracy: 0.5182\n",
      "Training loss (for one batch) at step 30: 255.0555, Accuracy: 0.5559\n",
      "Training loss (for one batch) at step 40: 244.6514, Accuracy: 0.5776\n",
      "Training loss (for one batch) at step 50: 250.3880, Accuracy: 0.5964\n",
      "Training loss (for one batch) at step 60: 250.2511, Accuracy: 0.6069\n",
      "Training loss (for one batch) at step 70: 263.0996, Accuracy: 0.5957\n",
      "Training loss (for one batch) at step 80: 269.5110, Accuracy: 0.5838\n",
      "Training loss (for one batch) at step 90: 267.6049, Accuracy: 0.5798\n",
      "Training loss (for one batch) at step 100: 264.9543, Accuracy: 0.5830\n",
      "Training loss (for one batch) at step 110: 244.4377, Accuracy: 0.5825\n",
      "---- Training ----\n",
      "Training loss: 97.7667\n",
      "Training acc over epoch: 0.5815\n",
      "---- Validation ----\n",
      "Validation loss: 51.1935\n",
      "Validation acc: 0.5564\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 295.5471, Accuracy: 0.4531\n",
      "Training loss (for one batch) at step 10: 269.8538, Accuracy: 0.4986\n",
      "Training loss (for one batch) at step 20: 254.1675, Accuracy: 0.5257\n",
      "Training loss (for one batch) at step 30: 236.7630, Accuracy: 0.5605\n",
      "Training loss (for one batch) at step 40: 255.2201, Accuracy: 0.5800\n",
      "Training loss (for one batch) at step 50: 240.3291, Accuracy: 0.5983\n",
      "Training loss (for one batch) at step 60: 263.7911, Accuracy: 0.6062\n",
      "Training loss (for one batch) at step 70: 270.3448, Accuracy: 0.5963\n",
      "Training loss (for one batch) at step 80: 283.1637, Accuracy: 0.5850\n",
      "Training loss (for one batch) at step 90: 261.6876, Accuracy: 0.5814\n",
      "Training loss (for one batch) at step 100: 231.8082, Accuracy: 0.5847\n",
      "Training loss (for one batch) at step 110: 279.2557, Accuracy: 0.5834\n",
      "---- Training ----\n",
      "Training loss: 82.9269\n",
      "Training acc over epoch: 0.5827\n",
      "---- Validation ----\n",
      "Validation loss: 48.7420\n",
      "Validation acc: 0.5207\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 309.4567, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 289.8906, Accuracy: 0.4943\n",
      "Training loss (for one batch) at step 20: 254.2994, Accuracy: 0.5301\n",
      "Training loss (for one batch) at step 30: 267.3609, Accuracy: 0.5612\n",
      "Training loss (for one batch) at step 40: 259.6485, Accuracy: 0.5791\n",
      "Training loss (for one batch) at step 50: 245.6159, Accuracy: 0.5971\n",
      "Training loss (for one batch) at step 60: 249.2649, Accuracy: 0.6075\n",
      "Training loss (for one batch) at step 70: 259.7451, Accuracy: 0.5980\n",
      "Training loss (for one batch) at step 80: 259.7042, Accuracy: 0.5850\n",
      "Training loss (for one batch) at step 90: 262.4592, Accuracy: 0.5813\n",
      "Training loss (for one batch) at step 100: 244.2433, Accuracy: 0.5842\n",
      "Training loss (for one batch) at step 110: 262.1859, Accuracy: 0.5846\n",
      "---- Training ----\n",
      "Training loss: 91.4574\n",
      "Training acc over epoch: 0.5836\n",
      "---- Validation ----\n",
      "Validation loss: 37.6237\n",
      "Validation acc: 0.5236\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 270.0117, Accuracy: 0.4297\n",
      "Training loss (for one batch) at step 10: 271.9380, Accuracy: 0.4872\n",
      "Training loss (for one batch) at step 20: 227.7256, Accuracy: 0.5223\n",
      "Training loss (for one batch) at step 30: 237.7934, Accuracy: 0.5565\n",
      "Training loss (for one batch) at step 40: 248.1289, Accuracy: 0.5774\n",
      "Training loss (for one batch) at step 50: 222.9953, Accuracy: 0.5982\n",
      "Training loss (for one batch) at step 60: 250.9137, Accuracy: 0.6067\n",
      "Training loss (for one batch) at step 70: 277.7647, Accuracy: 0.5967\n",
      "Training loss (for one batch) at step 80: 263.0146, Accuracy: 0.5858\n",
      "Training loss (for one batch) at step 90: 261.5404, Accuracy: 0.5817\n",
      "Training loss (for one batch) at step 100: 247.6515, Accuracy: 0.5840\n",
      "Training loss (for one batch) at step 110: 254.6451, Accuracy: 0.5843\n",
      "---- Training ----\n",
      "Training loss: 85.1637\n",
      "Training acc over epoch: 0.5833\n",
      "---- Validation ----\n",
      "Validation loss: 45.0148\n",
      "Validation acc: 0.5438\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 257.6040, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 264.5750, Accuracy: 0.4929\n",
      "Training loss (for one batch) at step 20: 244.5573, Accuracy: 0.5272\n",
      "Training loss (for one batch) at step 30: 236.2404, Accuracy: 0.5607\n",
      "Training loss (for one batch) at step 40: 239.1064, Accuracy: 0.5751\n",
      "Training loss (for one batch) at step 50: 226.5456, Accuracy: 0.5965\n",
      "Training loss (for one batch) at step 60: 247.6005, Accuracy: 0.6080\n",
      "Training loss (for one batch) at step 70: 259.9078, Accuracy: 0.5966\n",
      "Training loss (for one batch) at step 80: 291.4406, Accuracy: 0.5849\n",
      "Training loss (for one batch) at step 90: 256.2648, Accuracy: 0.5810\n",
      "Training loss (for one batch) at step 100: 253.9041, Accuracy: 0.5853\n",
      "Training loss (for one batch) at step 110: 256.9847, Accuracy: 0.5835\n",
      "---- Training ----\n",
      "Training loss: 73.1082\n",
      "Training acc over epoch: 0.5821\n",
      "---- Validation ----\n",
      "Validation loss: 38.7156\n",
      "Validation acc: 0.5129\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 284.1650, Accuracy: 0.4922\n",
      "Training loss (for one batch) at step 10: 272.3909, Accuracy: 0.4893\n",
      "Training loss (for one batch) at step 20: 236.1472, Accuracy: 0.5346\n",
      "Training loss (for one batch) at step 30: 273.5273, Accuracy: 0.5620\n",
      "Training loss (for one batch) at step 40: 250.7722, Accuracy: 0.5816\n",
      "Training loss (for one batch) at step 50: 247.0453, Accuracy: 0.6002\n",
      "Training loss (for one batch) at step 60: 250.7838, Accuracy: 0.6071\n",
      "Training loss (for one batch) at step 70: 272.6750, Accuracy: 0.5962\n",
      "Training loss (for one batch) at step 80: 249.6929, Accuracy: 0.5855\n",
      "Training loss (for one batch) at step 90: 260.9512, Accuracy: 0.5834\n",
      "Training loss (for one batch) at step 100: 258.2595, Accuracy: 0.5841\n",
      "Training loss (for one batch) at step 110: 252.1943, Accuracy: 0.5844\n",
      "---- Training ----\n",
      "Training loss: 90.0357\n",
      "Training acc over epoch: 0.5831\n",
      "---- Validation ----\n",
      "Validation loss: 40.5121\n",
      "Validation acc: 0.5521\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 254.4691, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 281.1499, Accuracy: 0.4993\n",
      "Training loss (for one batch) at step 20: 256.8018, Accuracy: 0.5298\n",
      "Training loss (for one batch) at step 30: 239.2281, Accuracy: 0.5635\n",
      "Training loss (for one batch) at step 40: 246.2748, Accuracy: 0.5812\n",
      "Training loss (for one batch) at step 50: 240.0906, Accuracy: 0.5973\n",
      "Training loss (for one batch) at step 60: 244.5437, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 70: 274.5242, Accuracy: 0.5989\n",
      "Training loss (for one batch) at step 80: 261.7487, Accuracy: 0.5856\n",
      "Training loss (for one batch) at step 90: 240.8444, Accuracy: 0.5831\n",
      "Training loss (for one batch) at step 100: 264.9223, Accuracy: 0.5841\n",
      "Training loss (for one batch) at step 110: 258.7471, Accuracy: 0.5835\n",
      "---- Training ----\n",
      "Training loss: 90.4424\n",
      "Training acc over epoch: 0.5819\n",
      "---- Validation ----\n",
      "Validation loss: 41.7288\n",
      "Validation acc: 0.5449\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 279.3711, Accuracy: 0.4531\n",
      "Training loss (for one batch) at step 10: 272.8243, Accuracy: 0.4830\n",
      "Training loss (for one batch) at step 20: 242.5015, Accuracy: 0.5193\n",
      "Training loss (for one batch) at step 30: 247.8403, Accuracy: 0.5582\n",
      "Training loss (for one batch) at step 40: 219.6111, Accuracy: 0.5814\n",
      "Training loss (for one batch) at step 50: 227.9421, Accuracy: 0.5962\n",
      "Training loss (for one batch) at step 60: 233.3285, Accuracy: 0.6058\n",
      "Training loss (for one batch) at step 70: 276.7704, Accuracy: 0.5932\n",
      "Training loss (for one batch) at step 80: 243.4881, Accuracy: 0.5847\n",
      "Training loss (for one batch) at step 90: 235.2722, Accuracy: 0.5820\n",
      "Training loss (for one batch) at step 100: 246.2848, Accuracy: 0.5833\n",
      "Training loss (for one batch) at step 110: 260.3714, Accuracy: 0.5815\n",
      "---- Training ----\n",
      "Training loss: 91.8808\n",
      "Training acc over epoch: 0.5807\n",
      "---- Validation ----\n",
      "Validation loss: 41.0444\n",
      "Validation acc: 0.5336\n",
      "Time taken: 18.07s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 284.0485, Accuracy: 0.4922\n",
      "Training loss (for one batch) at step 10: 251.9005, Accuracy: 0.4808\n",
      "Training loss (for one batch) at step 20: 248.8699, Accuracy: 0.5249\n",
      "Training loss (for one batch) at step 30: 247.1950, Accuracy: 0.5517\n",
      "Training loss (for one batch) at step 40: 236.1462, Accuracy: 0.5753\n",
      "Training loss (for one batch) at step 50: 229.7433, Accuracy: 0.5982\n",
      "Training loss (for one batch) at step 60: 232.2222, Accuracy: 0.6058\n",
      "Training loss (for one batch) at step 70: 257.7904, Accuracy: 0.5955\n",
      "Training loss (for one batch) at step 80: 240.3268, Accuracy: 0.5821\n",
      "Training loss (for one batch) at step 90: 256.5085, Accuracy: 0.5796\n",
      "Training loss (for one batch) at step 100: 233.4851, Accuracy: 0.5819\n",
      "Training loss (for one batch) at step 110: 259.5067, Accuracy: 0.5821\n",
      "---- Training ----\n",
      "Training loss: 97.2332\n",
      "Training acc over epoch: 0.5808\n",
      "---- Validation ----\n",
      "Validation loss: 39.9009\n",
      "Validation acc: 0.5572\n",
      "Time taken: 18.32s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 279.8375, Accuracy: 0.4609\n",
      "Training loss (for one batch) at step 10: 265.9576, Accuracy: 0.4872\n",
      "Training loss (for one batch) at step 20: 222.1118, Accuracy: 0.5171\n",
      "Training loss (for one batch) at step 30: 242.1201, Accuracy: 0.5562\n",
      "Training loss (for one batch) at step 40: 227.0934, Accuracy: 0.5776\n",
      "Training loss (for one batch) at step 50: 235.6762, Accuracy: 0.5973\n",
      "Training loss (for one batch) at step 60: 262.7052, Accuracy: 0.6053\n",
      "Training loss (for one batch) at step 70: 268.6545, Accuracy: 0.5945\n",
      "Training loss (for one batch) at step 80: 259.2292, Accuracy: 0.5849\n",
      "Training loss (for one batch) at step 90: 263.6913, Accuracy: 0.5811\n",
      "Training loss (for one batch) at step 100: 244.4052, Accuracy: 0.5845\n",
      "Training loss (for one batch) at step 110: 258.4222, Accuracy: 0.5827\n",
      "---- Training ----\n",
      "Training loss: 83.6910\n",
      "Training acc over epoch: 0.5825\n",
      "---- Validation ----\n",
      "Validation loss: 43.6906\n",
      "Validation acc: 0.5258\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 274.5031, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 248.1882, Accuracy: 0.4858\n",
      "Training loss (for one batch) at step 20: 237.5432, Accuracy: 0.5216\n",
      "Training loss (for one batch) at step 30: 232.8833, Accuracy: 0.5575\n",
      "Training loss (for one batch) at step 40: 242.0352, Accuracy: 0.5806\n",
      "Training loss (for one batch) at step 50: 227.8966, Accuracy: 0.5951\n",
      "Training loss (for one batch) at step 60: 232.5564, Accuracy: 0.6050\n",
      "Training loss (for one batch) at step 70: 257.3928, Accuracy: 0.5939\n",
      "Training loss (for one batch) at step 80: 249.1918, Accuracy: 0.5833\n",
      "Training loss (for one batch) at step 90: 265.7131, Accuracy: 0.5804\n",
      "Training loss (for one batch) at step 100: 242.4230, Accuracy: 0.5821\n",
      "Training loss (for one batch) at step 110: 233.0994, Accuracy: 0.5815\n",
      "---- Training ----\n",
      "Training loss: 78.5241\n",
      "Training acc over epoch: 0.5806\n",
      "---- Validation ----\n",
      "Validation loss: 40.6970\n",
      "Validation acc: 0.5392\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 253.0137, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 276.6012, Accuracy: 0.4922\n",
      "Training loss (for one batch) at step 20: 238.9309, Accuracy: 0.5171\n",
      "Training loss (for one batch) at step 30: 229.2668, Accuracy: 0.5557\n",
      "Training loss (for one batch) at step 40: 241.7563, Accuracy: 0.5783\n",
      "Training loss (for one batch) at step 50: 246.9415, Accuracy: 0.5974\n",
      "Training loss (for one batch) at step 60: 245.4050, Accuracy: 0.6049\n",
      "Training loss (for one batch) at step 70: 268.2385, Accuracy: 0.5942\n",
      "Training loss (for one batch) at step 80: 251.4801, Accuracy: 0.5829\n",
      "Training loss (for one batch) at step 90: 241.3791, Accuracy: 0.5798\n",
      "Training loss (for one batch) at step 100: 237.3452, Accuracy: 0.5823\n",
      "Training loss (for one batch) at step 110: 244.0985, Accuracy: 0.5812\n",
      "---- Training ----\n",
      "Training loss: 94.0801\n",
      "Training acc over epoch: 0.5809\n",
      "---- Validation ----\n",
      "Validation loss: 63.0662\n",
      "Validation acc: 0.5365\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 265.4897, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 254.9734, Accuracy: 0.5142\n",
      "Training loss (for one batch) at step 20: 236.6504, Accuracy: 0.5316\n",
      "Training loss (for one batch) at step 30: 241.4116, Accuracy: 0.5638\n",
      "Training loss (for one batch) at step 40: 238.5608, Accuracy: 0.5816\n",
      "Training loss (for one batch) at step 50: 244.6517, Accuracy: 0.5996\n",
      "Training loss (for one batch) at step 60: 238.7114, Accuracy: 0.6060\n",
      "Training loss (for one batch) at step 70: 263.0273, Accuracy: 0.5973\n",
      "Training loss (for one batch) at step 80: 264.2585, Accuracy: 0.5848\n",
      "Training loss (for one batch) at step 90: 248.7464, Accuracy: 0.5822\n",
      "Training loss (for one batch) at step 100: 259.2065, Accuracy: 0.5824\n",
      "Training loss (for one batch) at step 110: 259.8818, Accuracy: 0.5824\n",
      "---- Training ----\n",
      "Training loss: 92.6229\n",
      "Training acc over epoch: 0.5807\n",
      "---- Validation ----\n",
      "Validation loss: 49.8678\n",
      "Validation acc: 0.5228\n",
      "Time taken: 20.15s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 276.5442, Accuracy: 0.4453\n",
      "Training loss (for one batch) at step 10: 270.6687, Accuracy: 0.4773\n",
      "Training loss (for one batch) at step 20: 248.9351, Accuracy: 0.5223\n",
      "Training loss (for one batch) at step 30: 255.2399, Accuracy: 0.5539\n",
      "Training loss (for one batch) at step 40: 226.7928, Accuracy: 0.5751\n",
      "Training loss (for one batch) at step 50: 247.3261, Accuracy: 0.5934\n",
      "Training loss (for one batch) at step 60: 251.6407, Accuracy: 0.6044\n",
      "Training loss (for one batch) at step 70: 250.2428, Accuracy: 0.5929\n",
      "Training loss (for one batch) at step 80: 257.6634, Accuracy: 0.5820\n",
      "Training loss (for one batch) at step 90: 251.4559, Accuracy: 0.5765\n",
      "Training loss (for one batch) at step 100: 243.3740, Accuracy: 0.5797\n",
      "Training loss (for one batch) at step 110: 260.5621, Accuracy: 0.5802\n",
      "---- Training ----\n",
      "Training loss: 77.3725\n",
      "Training acc over epoch: 0.5788\n",
      "---- Validation ----\n",
      "Validation loss: 38.3833\n",
      "Validation acc: 0.5416\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 257.3199, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 250.4321, Accuracy: 0.4886\n",
      "Training loss (for one batch) at step 20: 231.5250, Accuracy: 0.5275\n",
      "Training loss (for one batch) at step 30: 233.6881, Accuracy: 0.5610\n",
      "Training loss (for one batch) at step 40: 243.3339, Accuracy: 0.5816\n",
      "Training loss (for one batch) at step 50: 235.9985, Accuracy: 0.5983\n",
      "Training loss (for one batch) at step 60: 229.0974, Accuracy: 0.6096\n",
      "Training loss (for one batch) at step 70: 248.7146, Accuracy: 0.5977\n",
      "Training loss (for one batch) at step 80: 244.7442, Accuracy: 0.5843\n",
      "Training loss (for one batch) at step 90: 236.9464, Accuracy: 0.5819\n",
      "Training loss (for one batch) at step 100: 249.7964, Accuracy: 0.5838\n",
      "Training loss (for one batch) at step 110: 255.3785, Accuracy: 0.5835\n",
      "---- Training ----\n",
      "Training loss: 85.5708\n",
      "Training acc over epoch: 0.5810\n",
      "---- Validation ----\n",
      "Validation loss: 43.1075\n",
      "Validation acc: 0.5446\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 244.3520, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 262.4280, Accuracy: 0.5057\n",
      "Training loss (for one batch) at step 20: 234.4513, Accuracy: 0.5335\n",
      "Training loss (for one batch) at step 30: 260.5092, Accuracy: 0.5590\n",
      "Training loss (for one batch) at step 40: 224.0052, Accuracy: 0.5779\n",
      "Training loss (for one batch) at step 50: 233.0637, Accuracy: 0.5997\n",
      "Training loss (for one batch) at step 60: 252.7663, Accuracy: 0.6057\n",
      "Training loss (for one batch) at step 70: 239.0984, Accuracy: 0.5936\n",
      "Training loss (for one batch) at step 80: 248.1564, Accuracy: 0.5823\n",
      "Training loss (for one batch) at step 90: 256.7450, Accuracy: 0.5794\n",
      "Training loss (for one batch) at step 100: 246.6768, Accuracy: 0.5825\n",
      "Training loss (for one batch) at step 110: 264.2881, Accuracy: 0.5827\n",
      "---- Training ----\n",
      "Training loss: 94.8022\n",
      "Training acc over epoch: 0.5813\n",
      "---- Validation ----\n",
      "Validation loss: 36.0725\n",
      "Validation acc: 0.5231\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 253.8724, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 259.2785, Accuracy: 0.4872\n",
      "Training loss (for one batch) at step 20: 243.5466, Accuracy: 0.5253\n",
      "Training loss (for one batch) at step 30: 235.5929, Accuracy: 0.5544\n",
      "Training loss (for one batch) at step 40: 269.1944, Accuracy: 0.5751\n",
      "Training loss (for one batch) at step 50: 223.2256, Accuracy: 0.5941\n",
      "Training loss (for one batch) at step 60: 225.2421, Accuracy: 0.6067\n",
      "Training loss (for one batch) at step 70: 246.7379, Accuracy: 0.5963\n",
      "Training loss (for one batch) at step 80: 252.1557, Accuracy: 0.5832\n",
      "Training loss (for one batch) at step 90: 227.0506, Accuracy: 0.5812\n",
      "Training loss (for one batch) at step 100: 250.3761, Accuracy: 0.5825\n",
      "Training loss (for one batch) at step 110: 240.5908, Accuracy: 0.5820\n",
      "---- Training ----\n",
      "Training loss: 100.0587\n",
      "Training acc over epoch: 0.5806\n",
      "---- Validation ----\n",
      "Validation loss: 48.3609\n",
      "Validation acc: 0.5505\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 261.8040, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 266.3824, Accuracy: 0.4922\n",
      "Training loss (for one batch) at step 20: 236.2529, Accuracy: 0.5253\n",
      "Training loss (for one batch) at step 30: 237.5914, Accuracy: 0.5570\n",
      "Training loss (for one batch) at step 40: 233.4205, Accuracy: 0.5783\n",
      "Training loss (for one batch) at step 50: 230.7604, Accuracy: 0.5973\n",
      "Training loss (for one batch) at step 60: 235.2626, Accuracy: 0.6039\n",
      "Training loss (for one batch) at step 70: 260.8230, Accuracy: 0.5963\n",
      "Training loss (for one batch) at step 80: 257.5063, Accuracy: 0.5815\n",
      "Training loss (for one batch) at step 90: 242.0746, Accuracy: 0.5805\n",
      "Training loss (for one batch) at step 100: 232.8005, Accuracy: 0.5817\n",
      "Training loss (for one batch) at step 110: 233.2614, Accuracy: 0.5815\n",
      "---- Training ----\n",
      "Training loss: 78.2815\n",
      "Training acc over epoch: 0.5808\n",
      "---- Validation ----\n",
      "Validation loss: 55.7824\n",
      "Validation acc: 0.5296\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 264.2963, Accuracy: 0.4844\n",
      "Training loss (for one batch) at step 10: 270.7791, Accuracy: 0.4957\n",
      "Training loss (for one batch) at step 20: 233.3230, Accuracy: 0.5242\n",
      "Training loss (for one batch) at step 30: 227.8410, Accuracy: 0.5587\n",
      "Training loss (for one batch) at step 40: 243.7058, Accuracy: 0.5793\n",
      "Training loss (for one batch) at step 50: 236.3136, Accuracy: 0.5979\n",
      "Training loss (for one batch) at step 60: 230.4383, Accuracy: 0.6075\n",
      "Training loss (for one batch) at step 70: 257.3045, Accuracy: 0.5969\n",
      "Training loss (for one batch) at step 80: 247.8772, Accuracy: 0.5840\n",
      "Training loss (for one batch) at step 90: 219.0341, Accuracy: 0.5803\n",
      "Training loss (for one batch) at step 100: 238.9716, Accuracy: 0.5820\n",
      "Training loss (for one batch) at step 110: 238.2644, Accuracy: 0.5804\n",
      "---- Training ----\n",
      "Training loss: 86.8089\n",
      "Training acc over epoch: 0.5812\n",
      "---- Validation ----\n",
      "Validation loss: 52.6224\n",
      "Validation acc: 0.5596\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 271.9508, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 242.3186, Accuracy: 0.4844\n",
      "Training loss (for one batch) at step 20: 224.6288, Accuracy: 0.5197\n",
      "Training loss (for one batch) at step 30: 246.5161, Accuracy: 0.5602\n",
      "Training loss (for one batch) at step 40: 238.9519, Accuracy: 0.5823\n",
      "Training loss (for one batch) at step 50: 214.5516, Accuracy: 0.5988\n",
      "Training loss (for one batch) at step 60: 226.4796, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 70: 259.3549, Accuracy: 0.5979\n",
      "Training loss (for one batch) at step 80: 241.4919, Accuracy: 0.5847\n",
      "Training loss (for one batch) at step 90: 233.7836, Accuracy: 0.5816\n",
      "Training loss (for one batch) at step 100: 241.2729, Accuracy: 0.5825\n",
      "Training loss (for one batch) at step 110: 242.4447, Accuracy: 0.5835\n",
      "---- Training ----\n",
      "Training loss: 72.9397\n",
      "Training acc over epoch: 0.5822\n",
      "---- Validation ----\n",
      "Validation loss: 50.3776\n",
      "Validation acc: 0.5425\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 258.0434, Accuracy: 0.4766\n",
      "Training loss (for one batch) at step 10: 256.2918, Accuracy: 0.4950\n",
      "Training loss (for one batch) at step 20: 237.9548, Accuracy: 0.5298\n",
      "Training loss (for one batch) at step 30: 232.0926, Accuracy: 0.5587\n",
      "Training loss (for one batch) at step 40: 239.7516, Accuracy: 0.5779\n",
      "Training loss (for one batch) at step 50: 225.7669, Accuracy: 0.5980\n",
      "Training loss (for one batch) at step 60: 264.8157, Accuracy: 0.6087\n",
      "Training loss (for one batch) at step 70: 272.1874, Accuracy: 0.5982\n",
      "Training loss (for one batch) at step 80: 238.7531, Accuracy: 0.5858\n",
      "Training loss (for one batch) at step 90: 242.4555, Accuracy: 0.5836\n",
      "Training loss (for one batch) at step 100: 229.4414, Accuracy: 0.5834\n",
      "Training loss (for one batch) at step 110: 239.0564, Accuracy: 0.5838\n",
      "---- Training ----\n",
      "Training loss: 84.4391\n",
      "Training acc over epoch: 0.5826\n",
      "---- Validation ----\n",
      "Validation loss: 43.1993\n",
      "Validation acc: 0.5454\n",
      "Time taken: 18.13s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 254.5163, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 256.1705, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 20: 226.3957, Accuracy: 0.5238\n",
      "Training loss (for one batch) at step 30: 236.8235, Accuracy: 0.5610\n",
      "Training loss (for one batch) at step 40: 219.4708, Accuracy: 0.5787\n",
      "Training loss (for one batch) at step 50: 243.0634, Accuracy: 0.5973\n",
      "Training loss (for one batch) at step 60: 230.2335, Accuracy: 0.6078\n",
      "Training loss (for one batch) at step 70: 278.2397, Accuracy: 0.5973\n",
      "Training loss (for one batch) at step 80: 235.3896, Accuracy: 0.5815\n",
      "Training loss (for one batch) at step 90: 236.8398, Accuracy: 0.5801\n",
      "Training loss (for one batch) at step 100: 236.9721, Accuracy: 0.5820\n",
      "Training loss (for one batch) at step 110: 224.3971, Accuracy: 0.5814\n",
      "---- Training ----\n",
      "Training loss: 73.8180\n",
      "Training acc over epoch: 0.5807\n",
      "---- Validation ----\n",
      "Validation loss: 49.5300\n",
      "Validation acc: 0.5438\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 244.2906, Accuracy: 0.4688\n",
      "Training loss (for one batch) at step 10: 246.2256, Accuracy: 0.4886\n",
      "Training loss (for one batch) at step 20: 247.6471, Accuracy: 0.5223\n",
      "Training loss (for one batch) at step 30: 224.6769, Accuracy: 0.5590\n",
      "Training loss (for one batch) at step 40: 228.0001, Accuracy: 0.5789\n",
      "Training loss (for one batch) at step 50: 230.1987, Accuracy: 0.5982\n",
      "Training loss (for one batch) at step 60: 246.3878, Accuracy: 0.6057\n",
      "Training loss (for one batch) at step 70: 254.6959, Accuracy: 0.5960\n",
      "Training loss (for one batch) at step 80: 237.9302, Accuracy: 0.5836\n",
      "Training loss (for one batch) at step 90: 238.0154, Accuracy: 0.5802\n",
      "Training loss (for one batch) at step 100: 245.8547, Accuracy: 0.5809\n",
      "Training loss (for one batch) at step 110: 241.3098, Accuracy: 0.5824\n",
      "---- Training ----\n",
      "Training loss: 81.7093\n",
      "Training acc over epoch: 0.5807\n",
      "---- Validation ----\n",
      "Validation loss: 40.3069\n",
      "Validation acc: 0.5320\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 250.0505, Accuracy: 0.4453\n",
      "Training loss (for one batch) at step 10: 268.5451, Accuracy: 0.4695\n",
      "Training loss (for one batch) at step 20: 244.9105, Accuracy: 0.5249\n",
      "Training loss (for one batch) at step 30: 243.7366, Accuracy: 0.5587\n",
      "Training loss (for one batch) at step 40: 225.2501, Accuracy: 0.5791\n",
      "Training loss (for one batch) at step 50: 215.8075, Accuracy: 0.5957\n",
      "Training loss (for one batch) at step 60: 234.7479, Accuracy: 0.6046\n",
      "Training loss (for one batch) at step 70: 254.3763, Accuracy: 0.5964\n",
      "Training loss (for one batch) at step 80: 271.9354, Accuracy: 0.5840\n",
      "Training loss (for one batch) at step 90: 251.6536, Accuracy: 0.5798\n",
      "Training loss (for one batch) at step 100: 243.8345, Accuracy: 0.5839\n",
      "Training loss (for one batch) at step 110: 244.2051, Accuracy: 0.5831\n",
      "---- Training ----\n",
      "Training loss: 80.3180\n",
      "Training acc over epoch: 0.5820\n",
      "---- Validation ----\n",
      "Validation loss: 37.2846\n",
      "Validation acc: 0.5556\n",
      "Time taken: 18.06s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 260.9990, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 261.5331, Accuracy: 0.4801\n",
      "Training loss (for one batch) at step 20: 233.1601, Accuracy: 0.5119\n",
      "Training loss (for one batch) at step 30: 230.6781, Accuracy: 0.5544\n",
      "Training loss (for one batch) at step 40: 222.0302, Accuracy: 0.5793\n",
      "Training loss (for one batch) at step 50: 215.7982, Accuracy: 0.5973\n",
      "Training loss (for one batch) at step 60: 239.5905, Accuracy: 0.6049\n",
      "Training loss (for one batch) at step 70: 265.6412, Accuracy: 0.5957\n",
      "Training loss (for one batch) at step 80: 239.9733, Accuracy: 0.5843\n",
      "Training loss (for one batch) at step 90: 225.8993, Accuracy: 0.5793\n",
      "Training loss (for one batch) at step 100: 227.6297, Accuracy: 0.5818\n",
      "Training loss (for one batch) at step 110: 239.0799, Accuracy: 0.5811\n",
      "---- Training ----\n",
      "Training loss: 68.0809\n",
      "Training acc over epoch: 0.5800\n",
      "---- Validation ----\n",
      "Validation loss: 58.3585\n",
      "Validation acc: 0.5621\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 247.4727, Accuracy: 0.4531\n",
      "Training loss (for one batch) at step 10: 270.0424, Accuracy: 0.4723\n",
      "Training loss (for one batch) at step 20: 224.4600, Accuracy: 0.5201\n",
      "Training loss (for one batch) at step 30: 245.0848, Accuracy: 0.5532\n",
      "Training loss (for one batch) at step 40: 225.7791, Accuracy: 0.5756\n",
      "Training loss (for one batch) at step 50: 222.5094, Accuracy: 0.5951\n",
      "Training loss (for one batch) at step 60: 234.9923, Accuracy: 0.6055\n",
      "Training loss (for one batch) at step 70: 239.3572, Accuracy: 0.5947\n",
      "Training loss (for one batch) at step 80: 231.0773, Accuracy: 0.5789\n",
      "Training loss (for one batch) at step 90: 247.2275, Accuracy: 0.5772\n",
      "Training loss (for one batch) at step 100: 220.8116, Accuracy: 0.5819\n",
      "Training loss (for one batch) at step 110: 239.0958, Accuracy: 0.5802\n",
      "---- Training ----\n",
      "Training loss: 72.5251\n",
      "Training acc over epoch: 0.5799\n",
      "---- Validation ----\n",
      "Validation loss: 38.5178\n",
      "Validation acc: 0.5433\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 261.2149, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 250.4043, Accuracy: 0.4936\n",
      "Training loss (for one batch) at step 20: 228.7386, Accuracy: 0.5238\n",
      "Training loss (for one batch) at step 30: 221.0831, Accuracy: 0.5565\n",
      "Training loss (for one batch) at step 40: 221.7227, Accuracy: 0.5755\n",
      "Training loss (for one batch) at step 50: 232.8636, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 60: 233.0425, Accuracy: 0.6031\n",
      "Training loss (for one batch) at step 70: 251.8329, Accuracy: 0.5963\n",
      "Training loss (for one batch) at step 80: 258.7179, Accuracy: 0.5822\n",
      "Training loss (for one batch) at step 90: 227.4762, Accuracy: 0.5817\n",
      "Training loss (for one batch) at step 100: 244.0294, Accuracy: 0.5826\n",
      "Training loss (for one batch) at step 110: 237.3302, Accuracy: 0.5828\n",
      "---- Training ----\n",
      "Training loss: 78.5653\n",
      "Training acc over epoch: 0.5810\n",
      "---- Validation ----\n",
      "Validation loss: 35.8122\n",
      "Validation acc: 0.5513\n",
      "Time taken: 18.15s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 261.6232, Accuracy: 0.4688\n",
      "Training loss (for one batch) at step 10: 253.0668, Accuracy: 0.4773\n",
      "Training loss (for one batch) at step 20: 242.4203, Accuracy: 0.5231\n",
      "Training loss (for one batch) at step 30: 225.1526, Accuracy: 0.5559\n",
      "Training loss (for one batch) at step 40: 231.5938, Accuracy: 0.5796\n",
      "Training loss (for one batch) at step 50: 225.6065, Accuracy: 0.5933\n",
      "Training loss (for one batch) at step 60: 245.0941, Accuracy: 0.6040\n",
      "Training loss (for one batch) at step 70: 256.7863, Accuracy: 0.5921\n",
      "Training loss (for one batch) at step 80: 243.4756, Accuracy: 0.5816\n",
      "Training loss (for one batch) at step 90: 233.8833, Accuracy: 0.5789\n",
      "Training loss (for one batch) at step 100: 236.7379, Accuracy: 0.5811\n",
      "Training loss (for one batch) at step 110: 244.6075, Accuracy: 0.5826\n",
      "---- Training ----\n",
      "Training loss: 73.0101\n",
      "Training acc over epoch: 0.5809\n",
      "---- Validation ----\n",
      "Validation loss: 57.8058\n",
      "Validation acc: 0.5320\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 241.2244, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 268.8456, Accuracy: 0.4950\n",
      "Training loss (for one batch) at step 20: 220.6225, Accuracy: 0.5197\n",
      "Training loss (for one batch) at step 30: 228.6801, Accuracy: 0.5552\n",
      "Training loss (for one batch) at step 40: 223.7803, Accuracy: 0.5755\n",
      "Training loss (for one batch) at step 50: 209.8507, Accuracy: 0.5948\n",
      "Training loss (for one batch) at step 60: 247.8938, Accuracy: 0.6055\n",
      "Training loss (for one batch) at step 70: 242.4427, Accuracy: 0.5975\n",
      "Training loss (for one batch) at step 80: 255.9946, Accuracy: 0.5829\n",
      "Training loss (for one batch) at step 90: 233.2665, Accuracy: 0.5798\n",
      "Training loss (for one batch) at step 100: 229.2001, Accuracy: 0.5814\n",
      "Training loss (for one batch) at step 110: 247.5170, Accuracy: 0.5827\n",
      "---- Training ----\n",
      "Training loss: 78.0180\n",
      "Training acc over epoch: 0.5815\n",
      "---- Validation ----\n",
      "Validation loss: 40.6873\n",
      "Validation acc: 0.5317\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 248.4314, Accuracy: 0.4453\n",
      "Training loss (for one batch) at step 10: 244.6740, Accuracy: 0.4751\n",
      "Training loss (for one batch) at step 20: 231.9276, Accuracy: 0.5249\n",
      "Training loss (for one batch) at step 30: 206.7981, Accuracy: 0.5585\n",
      "Training loss (for one batch) at step 40: 228.4025, Accuracy: 0.5795\n",
      "Training loss (for one batch) at step 50: 227.0467, Accuracy: 0.5976\n",
      "Training loss (for one batch) at step 60: 234.9462, Accuracy: 0.6071\n",
      "Training loss (for one batch) at step 70: 263.1625, Accuracy: 0.5943\n",
      "Training loss (for one batch) at step 80: 241.9871, Accuracy: 0.5823\n",
      "Training loss (for one batch) at step 90: 233.5751, Accuracy: 0.5773\n",
      "Training loss (for one batch) at step 100: 227.7438, Accuracy: 0.5797\n",
      "Training loss (for one batch) at step 110: 264.7318, Accuracy: 0.5783\n",
      "---- Training ----\n",
      "Training loss: 82.2612\n",
      "Training acc over epoch: 0.5791\n",
      "---- Validation ----\n",
      "Validation loss: 51.5604\n",
      "Validation acc: 0.5159\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 238.7363, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 253.5643, Accuracy: 0.4815\n",
      "Training loss (for one batch) at step 20: 231.4704, Accuracy: 0.5175\n",
      "Training loss (for one batch) at step 30: 247.1990, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 40: 243.8077, Accuracy: 0.5745\n",
      "Training loss (for one batch) at step 50: 217.0904, Accuracy: 0.5901\n",
      "Training loss (for one batch) at step 60: 236.7022, Accuracy: 0.6044\n",
      "Training loss (for one batch) at step 70: 242.2695, Accuracy: 0.5929\n",
      "Training loss (for one batch) at step 80: 241.8591, Accuracy: 0.5802\n",
      "Training loss (for one batch) at step 90: 262.3592, Accuracy: 0.5784\n",
      "Training loss (for one batch) at step 100: 230.2594, Accuracy: 0.5808\n",
      "Training loss (for one batch) at step 110: 237.5470, Accuracy: 0.5806\n",
      "---- Training ----\n",
      "Training loss: 89.6102\n",
      "Training acc over epoch: 0.5797\n",
      "---- Validation ----\n",
      "Validation loss: 56.0882\n",
      "Validation acc: 0.5339\n",
      "Time taken: 17.88s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 248.3922, Accuracy: 0.3984\n",
      "Training loss (for one batch) at step 10: 256.7519, Accuracy: 0.4908\n",
      "Training loss (for one batch) at step 20: 227.1946, Accuracy: 0.5305\n",
      "Training loss (for one batch) at step 30: 215.4154, Accuracy: 0.5607\n",
      "Training loss (for one batch) at step 40: 220.6727, Accuracy: 0.5816\n",
      "Training loss (for one batch) at step 50: 231.8149, Accuracy: 0.5957\n",
      "Training loss (for one batch) at step 60: 221.6657, Accuracy: 0.6067\n",
      "Training loss (for one batch) at step 70: 228.3614, Accuracy: 0.5953\n",
      "Training loss (for one batch) at step 80: 256.8510, Accuracy: 0.5807\n",
      "Training loss (for one batch) at step 90: 248.8944, Accuracy: 0.5770\n",
      "Training loss (for one batch) at step 100: 225.3270, Accuracy: 0.5798\n",
      "Training loss (for one batch) at step 110: 251.0971, Accuracy: 0.5794\n",
      "---- Training ----\n",
      "Training loss: 72.1906\n",
      "Training acc over epoch: 0.5790\n",
      "---- Validation ----\n",
      "Validation loss: 46.5990\n",
      "Validation acc: 0.5384\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 244.0564, Accuracy: 0.4766\n",
      "Training loss (for one batch) at step 10: 247.6286, Accuracy: 0.4844\n",
      "Training loss (for one batch) at step 20: 226.5881, Accuracy: 0.5264\n",
      "Training loss (for one batch) at step 30: 229.7443, Accuracy: 0.5620\n",
      "Training loss (for one batch) at step 40: 236.2851, Accuracy: 0.5785\n",
      "Training loss (for one batch) at step 50: 223.8526, Accuracy: 0.5945\n",
      "Training loss (for one batch) at step 60: 230.4798, Accuracy: 0.6055\n",
      "Training loss (for one batch) at step 70: 256.8251, Accuracy: 0.5969\n",
      "Training loss (for one batch) at step 80: 255.4996, Accuracy: 0.5826\n",
      "Training loss (for one batch) at step 90: 230.6515, Accuracy: 0.5787\n",
      "Training loss (for one batch) at step 100: 242.9562, Accuracy: 0.5804\n",
      "Training loss (for one batch) at step 110: 239.5489, Accuracy: 0.5811\n",
      "---- Training ----\n",
      "Training loss: 68.2186\n",
      "Training acc over epoch: 0.5797\n",
      "---- Validation ----\n",
      "Validation loss: 50.9445\n",
      "Validation acc: 0.5199\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 234.2410, Accuracy: 0.4453\n",
      "Training loss (for one batch) at step 10: 264.6538, Accuracy: 0.4815\n",
      "Training loss (for one batch) at step 20: 223.0588, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 30: 212.7709, Accuracy: 0.5534\n",
      "Training loss (for one batch) at step 40: 226.6073, Accuracy: 0.5766\n",
      "Training loss (for one batch) at step 50: 228.1522, Accuracy: 0.5913\n",
      "Training loss (for one batch) at step 60: 214.5784, Accuracy: 0.6025\n",
      "Training loss (for one batch) at step 70: 224.7908, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 80: 259.3703, Accuracy: 0.5813\n",
      "Training loss (for one batch) at step 90: 234.2920, Accuracy: 0.5796\n",
      "Training loss (for one batch) at step 100: 222.5995, Accuracy: 0.5807\n",
      "Training loss (for one batch) at step 110: 229.8777, Accuracy: 0.5808\n",
      "---- Training ----\n",
      "Training loss: 76.9699\n",
      "Training acc over epoch: 0.5793\n",
      "---- Validation ----\n",
      "Validation loss: 58.6940\n",
      "Validation acc: 0.5244\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 249.8115, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 236.0854, Accuracy: 0.4893\n",
      "Training loss (for one batch) at step 20: 223.2401, Accuracy: 0.5253\n",
      "Training loss (for one batch) at step 30: 233.1284, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 40: 242.0652, Accuracy: 0.5789\n",
      "Training loss (for one batch) at step 50: 224.6577, Accuracy: 0.5977\n",
      "Training loss (for one batch) at step 60: 219.2402, Accuracy: 0.6055\n",
      "Training loss (for one batch) at step 70: 269.3882, Accuracy: 0.5946\n",
      "Training loss (for one batch) at step 80: 238.6253, Accuracy: 0.5812\n",
      "Training loss (for one batch) at step 90: 212.2781, Accuracy: 0.5777\n",
      "Training loss (for one batch) at step 100: 223.3704, Accuracy: 0.5824\n",
      "Training loss (for one batch) at step 110: 229.9612, Accuracy: 0.5822\n",
      "---- Training ----\n",
      "Training loss: 73.7246\n",
      "Training acc over epoch: 0.5807\n",
      "---- Validation ----\n",
      "Validation loss: 38.2004\n",
      "Validation acc: 0.5438\n",
      "Time taken: 18.22s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 251.7793, Accuracy: 0.4922\n",
      "Training loss (for one batch) at step 10: 244.4725, Accuracy: 0.4858\n",
      "Training loss (for one batch) at step 20: 232.5315, Accuracy: 0.5253\n",
      "Training loss (for one batch) at step 30: 215.8792, Accuracy: 0.5549\n",
      "Training loss (for one batch) at step 40: 227.5624, Accuracy: 0.5747\n",
      "Training loss (for one batch) at step 50: 221.7235, Accuracy: 0.5959\n",
      "Training loss (for one batch) at step 60: 254.0120, Accuracy: 0.6051\n",
      "Training loss (for one batch) at step 70: 240.5077, Accuracy: 0.5935\n",
      "Training loss (for one batch) at step 80: 230.7101, Accuracy: 0.5826\n",
      "Training loss (for one batch) at step 90: 229.8349, Accuracy: 0.5775\n",
      "Training loss (for one batch) at step 100: 216.9369, Accuracy: 0.5805\n",
      "Training loss (for one batch) at step 110: 232.4856, Accuracy: 0.5809\n",
      "---- Training ----\n",
      "Training loss: 94.5610\n",
      "Training acc over epoch: 0.5803\n",
      "---- Validation ----\n",
      "Validation loss: 42.9465\n",
      "Validation acc: 0.5494\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 239.1169, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 234.6108, Accuracy: 0.4858\n",
      "Training loss (for one batch) at step 20: 222.2220, Accuracy: 0.5253\n",
      "Training loss (for one batch) at step 30: 227.2715, Accuracy: 0.5590\n",
      "Training loss (for one batch) at step 40: 215.2959, Accuracy: 0.5785\n",
      "Training loss (for one batch) at step 50: 222.7330, Accuracy: 0.5971\n",
      "Training loss (for one batch) at step 60: 212.0133, Accuracy: 0.6080\n",
      "Training loss (for one batch) at step 70: 225.9141, Accuracy: 0.5972\n",
      "Training loss (for one batch) at step 80: 238.4238, Accuracy: 0.5818\n",
      "Training loss (for one batch) at step 90: 219.0687, Accuracy: 0.5795\n",
      "Training loss (for one batch) at step 100: 228.2913, Accuracy: 0.5815\n",
      "Training loss (for one batch) at step 110: 250.1550, Accuracy: 0.5812\n",
      "---- Training ----\n",
      "Training loss: 87.8927\n",
      "Training acc over epoch: 0.5804\n",
      "---- Validation ----\n",
      "Validation loss: 54.1879\n",
      "Validation acc: 0.5376\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 251.2926, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 230.3650, Accuracy: 0.4943\n",
      "Training loss (for one batch) at step 20: 230.2502, Accuracy: 0.5197\n",
      "Training loss (for one batch) at step 30: 228.0207, Accuracy: 0.5572\n",
      "Training loss (for one batch) at step 40: 221.1923, Accuracy: 0.5751\n",
      "Training loss (for one batch) at step 50: 222.3751, Accuracy: 0.5954\n",
      "Training loss (for one batch) at step 60: 210.1193, Accuracy: 0.6053\n",
      "Training loss (for one batch) at step 70: 241.6523, Accuracy: 0.5940\n",
      "Training loss (for one batch) at step 80: 233.9762, Accuracy: 0.5807\n",
      "Training loss (for one batch) at step 90: 230.1823, Accuracy: 0.5795\n",
      "Training loss (for one batch) at step 100: 236.8785, Accuracy: 0.5809\n",
      "Training loss (for one batch) at step 110: 230.6079, Accuracy: 0.5793\n",
      "---- Training ----\n",
      "Training loss: 70.1792\n",
      "Training acc over epoch: 0.5789\n",
      "---- Validation ----\n",
      "Validation loss: 45.7309\n",
      "Validation acc: 0.5287\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 258.3501, Accuracy: 0.4062\n",
      "Training loss (for one batch) at step 10: 250.1882, Accuracy: 0.4822\n",
      "Training loss (for one batch) at step 20: 222.4304, Accuracy: 0.5216\n",
      "Training loss (for one batch) at step 30: 217.7101, Accuracy: 0.5522\n",
      "Training loss (for one batch) at step 40: 228.4632, Accuracy: 0.5768\n",
      "Training loss (for one batch) at step 50: 224.6403, Accuracy: 0.5950\n",
      "Training loss (for one batch) at step 60: 235.6802, Accuracy: 0.6036\n",
      "Training loss (for one batch) at step 70: 233.8679, Accuracy: 0.5940\n",
      "Training loss (for one batch) at step 80: 215.6237, Accuracy: 0.5804\n",
      "Training loss (for one batch) at step 90: 230.4193, Accuracy: 0.5792\n",
      "Training loss (for one batch) at step 100: 240.1373, Accuracy: 0.5806\n",
      "Training loss (for one batch) at step 110: 222.3523, Accuracy: 0.5809\n",
      "---- Training ----\n",
      "Training loss: 79.7431\n",
      "Training acc over epoch: 0.5801\n",
      "---- Validation ----\n",
      "Validation loss: 37.4666\n",
      "Validation acc: 0.4901\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 245.2883, Accuracy: 0.4766\n",
      "Training loss (for one batch) at step 10: 238.2711, Accuracy: 0.4865\n",
      "Training loss (for one batch) at step 20: 221.9367, Accuracy: 0.5186\n",
      "Training loss (for one batch) at step 30: 210.1907, Accuracy: 0.5504\n",
      "Training loss (for one batch) at step 40: 214.7974, Accuracy: 0.5745\n",
      "Training loss (for one batch) at step 50: 232.0560, Accuracy: 0.5934\n",
      "Training loss (for one batch) at step 60: 217.4765, Accuracy: 0.6017\n",
      "Training loss (for one batch) at step 70: 237.1670, Accuracy: 0.5930\n",
      "Training loss (for one batch) at step 80: 233.5779, Accuracy: 0.5813\n",
      "Training loss (for one batch) at step 90: 237.5467, Accuracy: 0.5780\n",
      "Training loss (for one batch) at step 100: 242.4564, Accuracy: 0.5808\n",
      "Training loss (for one batch) at step 110: 229.0504, Accuracy: 0.5805\n",
      "---- Training ----\n",
      "Training loss: 71.1912\n",
      "Training acc over epoch: 0.5799\n",
      "---- Validation ----\n",
      "Validation loss: 50.0383\n",
      "Validation acc: 0.5801\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 243.2680, Accuracy: 0.4375\n",
      "Training loss (for one batch) at step 10: 258.4570, Accuracy: 0.4808\n",
      "Training loss (for one batch) at step 20: 231.4105, Accuracy: 0.5264\n",
      "Training loss (for one batch) at step 30: 209.3814, Accuracy: 0.5554\n",
      "Training loss (for one batch) at step 40: 223.0642, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 50: 232.0163, Accuracy: 0.5950\n",
      "Training loss (for one batch) at step 60: 220.1211, Accuracy: 0.6055\n",
      "Training loss (for one batch) at step 70: 231.8741, Accuracy: 0.5977\n",
      "Training loss (for one batch) at step 80: 248.2514, Accuracy: 0.5829\n",
      "Training loss (for one batch) at step 90: 225.4722, Accuracy: 0.5807\n",
      "Training loss (for one batch) at step 100: 237.1971, Accuracy: 0.5829\n",
      "Training loss (for one batch) at step 110: 217.8407, Accuracy: 0.5835\n",
      "---- Training ----\n",
      "Training loss: 87.0584\n",
      "Training acc over epoch: 0.5809\n",
      "---- Validation ----\n",
      "Validation loss: 45.9799\n",
      "Validation acc: 0.5449\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 248.1043, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 236.3842, Accuracy: 0.4730\n",
      "Training loss (for one batch) at step 20: 232.6961, Accuracy: 0.5246\n",
      "Training loss (for one batch) at step 30: 226.2912, Accuracy: 0.5557\n",
      "Training loss (for one batch) at step 40: 231.9247, Accuracy: 0.5768\n",
      "Training loss (for one batch) at step 50: 226.4866, Accuracy: 0.5947\n",
      "Training loss (for one batch) at step 60: 224.5302, Accuracy: 0.6072\n",
      "Training loss (for one batch) at step 70: 243.9015, Accuracy: 0.5936\n",
      "Training loss (for one batch) at step 80: 236.9618, Accuracy: 0.5796\n",
      "Training loss (for one batch) at step 90: 226.0911, Accuracy: 0.5774\n",
      "Training loss (for one batch) at step 100: 238.6670, Accuracy: 0.5813\n",
      "Training loss (for one batch) at step 110: 217.8437, Accuracy: 0.5799\n",
      "---- Training ----\n",
      "Training loss: 92.7365\n",
      "Training acc over epoch: 0.5796\n",
      "---- Validation ----\n",
      "Validation loss: 45.2075\n",
      "Validation acc: 0.5306\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 261.6663, Accuracy: 0.4844\n",
      "Training loss (for one batch) at step 10: 274.3812, Accuracy: 0.4822\n",
      "Training loss (for one batch) at step 20: 231.2690, Accuracy: 0.5275\n",
      "Training loss (for one batch) at step 30: 232.2430, Accuracy: 0.5630\n",
      "Training loss (for one batch) at step 40: 218.9476, Accuracy: 0.5774\n",
      "Training loss (for one batch) at step 50: 218.9977, Accuracy: 0.5964\n",
      "Training loss (for one batch) at step 60: 229.1349, Accuracy: 0.6075\n",
      "Training loss (for one batch) at step 70: 239.8208, Accuracy: 0.5974\n",
      "Training loss (for one batch) at step 80: 235.7207, Accuracy: 0.5854\n",
      "Training loss (for one batch) at step 90: 226.2351, Accuracy: 0.5815\n",
      "Training loss (for one batch) at step 100: 240.7433, Accuracy: 0.5837\n",
      "Training loss (for one batch) at step 110: 230.1122, Accuracy: 0.5826\n",
      "---- Training ----\n",
      "Training loss: 71.4562\n",
      "Training acc over epoch: 0.5821\n",
      "---- Validation ----\n",
      "Validation loss: 54.1003\n",
      "Validation acc: 0.5519\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 269.5852, Accuracy: 0.4609\n",
      "Training loss (for one batch) at step 10: 247.9939, Accuracy: 0.4766\n",
      "Training loss (for one batch) at step 20: 227.2812, Accuracy: 0.5216\n",
      "Training loss (for one batch) at step 30: 223.2295, Accuracy: 0.5519\n",
      "Training loss (for one batch) at step 40: 220.6485, Accuracy: 0.5787\n",
      "Training loss (for one batch) at step 50: 213.9449, Accuracy: 0.5970\n",
      "Training loss (for one batch) at step 60: 227.7082, Accuracy: 0.6049\n",
      "Training loss (for one batch) at step 70: 236.7448, Accuracy: 0.5947\n",
      "Training loss (for one batch) at step 80: 239.1097, Accuracy: 0.5806\n",
      "Training loss (for one batch) at step 90: 218.2455, Accuracy: 0.5788\n",
      "Training loss (for one batch) at step 100: 233.2792, Accuracy: 0.5812\n",
      "Training loss (for one batch) at step 110: 221.3243, Accuracy: 0.5816\n",
      "---- Training ----\n",
      "Training loss: 78.3138\n",
      "Training acc over epoch: 0.5807\n",
      "---- Validation ----\n",
      "Validation loss: 52.3138\n",
      "Validation acc: 0.5548\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 247.5615, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 265.6968, Accuracy: 0.4759\n",
      "Training loss (for one batch) at step 20: 224.9959, Accuracy: 0.5223\n",
      "Training loss (for one batch) at step 30: 211.3766, Accuracy: 0.5557\n",
      "Training loss (for one batch) at step 40: 228.2177, Accuracy: 0.5737\n",
      "Training loss (for one batch) at step 50: 217.4103, Accuracy: 0.5934\n",
      "Training loss (for one batch) at step 60: 226.5743, Accuracy: 0.6053\n",
      "Training loss (for one batch) at step 70: 234.0799, Accuracy: 0.5925\n",
      "Training loss (for one batch) at step 80: 245.0745, Accuracy: 0.5807\n",
      "Training loss (for one batch) at step 90: 241.0486, Accuracy: 0.5785\n",
      "Training loss (for one batch) at step 100: 241.7691, Accuracy: 0.5801\n",
      "Training loss (for one batch) at step 110: 247.3675, Accuracy: 0.5802\n",
      "---- Training ----\n",
      "Training loss: 85.1774\n",
      "Training acc over epoch: 0.5791\n",
      "---- Validation ----\n",
      "Validation loss: 36.2617\n",
      "Validation acc: 0.5594\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 250.6467, Accuracy: 0.4766\n",
      "Training loss (for one batch) at step 10: 264.5586, Accuracy: 0.4801\n",
      "Training loss (for one batch) at step 20: 230.3998, Accuracy: 0.5268\n",
      "Training loss (for one batch) at step 30: 230.2231, Accuracy: 0.5602\n",
      "Training loss (for one batch) at step 40: 245.9908, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 50: 223.1506, Accuracy: 0.5968\n",
      "Training loss (for one batch) at step 60: 232.3374, Accuracy: 0.6055\n",
      "Training loss (for one batch) at step 70: 235.8149, Accuracy: 0.5921\n",
      "Training loss (for one batch) at step 80: 221.7941, Accuracy: 0.5792\n",
      "Training loss (for one batch) at step 90: 227.6772, Accuracy: 0.5787\n",
      "Training loss (for one batch) at step 100: 225.8062, Accuracy: 0.5801\n",
      "Training loss (for one batch) at step 110: 241.8587, Accuracy: 0.5800\n",
      "---- Training ----\n",
      "Training loss: 71.6216\n",
      "Training acc over epoch: 0.5794\n",
      "---- Validation ----\n",
      "Validation loss: 61.2735\n",
      "Validation acc: 0.5513\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 259.0175, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 246.0610, Accuracy: 0.4858\n",
      "Training loss (for one batch) at step 20: 221.5757, Accuracy: 0.5253\n",
      "Training loss (for one batch) at step 30: 211.0335, Accuracy: 0.5522\n",
      "Training loss (for one batch) at step 40: 232.2225, Accuracy: 0.5755\n",
      "Training loss (for one batch) at step 50: 219.3569, Accuracy: 0.5941\n",
      "Training loss (for one batch) at step 60: 212.6240, Accuracy: 0.6051\n",
      "Training loss (for one batch) at step 70: 232.7579, Accuracy: 0.5935\n",
      "Training loss (for one batch) at step 80: 239.6111, Accuracy: 0.5825\n",
      "Training loss (for one batch) at step 90: 238.9458, Accuracy: 0.5774\n",
      "Training loss (for one batch) at step 100: 224.4031, Accuracy: 0.5801\n",
      "Training loss (for one batch) at step 110: 216.5845, Accuracy: 0.5812\n",
      "---- Training ----\n",
      "Training loss: 79.5718\n",
      "Training acc over epoch: 0.5796\n",
      "---- Validation ----\n",
      "Validation loss: 53.0669\n",
      "Validation acc: 0.5704\n",
      "Time taken: 17.89s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 228.1602, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 226.7787, Accuracy: 0.4943\n",
      "Training loss (for one batch) at step 20: 233.1175, Accuracy: 0.5249\n",
      "Training loss (for one batch) at step 30: 222.1660, Accuracy: 0.5597\n",
      "Training loss (for one batch) at step 40: 223.8939, Accuracy: 0.5779\n",
      "Training loss (for one batch) at step 50: 233.9696, Accuracy: 0.5957\n",
      "Training loss (for one batch) at step 60: 221.0678, Accuracy: 0.6041\n",
      "Training loss (for one batch) at step 70: 231.0716, Accuracy: 0.5966\n",
      "Training loss (for one batch) at step 80: 251.7421, Accuracy: 0.5833\n",
      "Training loss (for one batch) at step 90: 238.5770, Accuracy: 0.5788\n",
      "Training loss (for one batch) at step 100: 225.2157, Accuracy: 0.5825\n",
      "Training loss (for one batch) at step 110: 236.6024, Accuracy: 0.5823\n",
      "---- Training ----\n",
      "Training loss: 74.8092\n",
      "Training acc over epoch: 0.5807\n",
      "---- Validation ----\n",
      "Validation loss: 49.7426\n",
      "Validation acc: 0.5293\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 250.3132, Accuracy: 0.4688\n",
      "Training loss (for one batch) at step 10: 231.2384, Accuracy: 0.4773\n",
      "Training loss (for one batch) at step 20: 196.4554, Accuracy: 0.5242\n",
      "Training loss (for one batch) at step 30: 215.9160, Accuracy: 0.5575\n",
      "Training loss (for one batch) at step 40: 207.0403, Accuracy: 0.5793\n",
      "Training loss (for one batch) at step 50: 212.6655, Accuracy: 0.5950\n",
      "Training loss (for one batch) at step 60: 212.9760, Accuracy: 0.6048\n",
      "Training loss (for one batch) at step 70: 251.6565, Accuracy: 0.5929\n",
      "Training loss (for one batch) at step 80: 229.1468, Accuracy: 0.5790\n",
      "Training loss (for one batch) at step 90: 229.5944, Accuracy: 0.5769\n",
      "Training loss (for one batch) at step 100: 234.1059, Accuracy: 0.5799\n",
      "Training loss (for one batch) at step 110: 230.7432, Accuracy: 0.5807\n",
      "---- Training ----\n",
      "Training loss: 77.9475\n",
      "Training acc over epoch: 0.5794\n",
      "---- Validation ----\n",
      "Validation loss: 43.6891\n",
      "Validation acc: 0.5462\n",
      "Time taken: 20.14s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 250.3213, Accuracy: 0.4844\n",
      "Training loss (for one batch) at step 10: 263.2176, Accuracy: 0.4886\n",
      "Training loss (for one batch) at step 20: 211.4790, Accuracy: 0.5197\n",
      "Training loss (for one batch) at step 30: 231.5324, Accuracy: 0.5552\n",
      "Training loss (for one batch) at step 40: 221.4372, Accuracy: 0.5783\n",
      "Training loss (for one batch) at step 50: 210.5707, Accuracy: 0.5964\n",
      "Training loss (for one batch) at step 60: 214.0935, Accuracy: 0.6064\n",
      "Training loss (for one batch) at step 70: 238.8024, Accuracy: 0.5975\n",
      "Training loss (for one batch) at step 80: 260.3110, Accuracy: 0.5829\n",
      "Training loss (for one batch) at step 90: 218.1755, Accuracy: 0.5796\n",
      "Training loss (for one batch) at step 100: 229.4189, Accuracy: 0.5811\n",
      "Training loss (for one batch) at step 110: 217.8314, Accuracy: 0.5800\n",
      "---- Training ----\n",
      "Training loss: 72.5305\n",
      "Training acc over epoch: 0.5800\n",
      "---- Validation ----\n",
      "Validation loss: 61.9636\n",
      "Validation acc: 0.5540\n",
      "Time taken: 17.96s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.5447\n",
      "Validation AUC: 0.5453\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 677.5400, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 527.2578, Accuracy: 0.5007\n",
      "Training loss (for one batch) at step 20: 527.0426, Accuracy: 0.4963\n",
      "Training loss (for one batch) at step 30: 514.4397, Accuracy: 0.5008\n",
      "Training loss (for one batch) at step 40: 509.0315, Accuracy: 0.5063\n",
      "Training loss (for one batch) at step 50: 509.8824, Accuracy: 0.5126\n",
      "Training loss (for one batch) at step 60: 487.3205, Accuracy: 0.5127\n",
      "Training loss (for one batch) at step 70: 477.6264, Accuracy: 0.5114\n",
      "Training loss (for one batch) at step 80: 477.3275, Accuracy: 0.5096\n",
      "Training loss (for one batch) at step 90: 467.2506, Accuracy: 0.5116\n",
      "Training loss (for one batch) at step 100: 467.6099, Accuracy: 0.5115\n",
      "Training loss (for one batch) at step 110: 465.3520, Accuracy: 0.5123\n",
      "---- Training ----\n",
      "Training loss: 147.7681\n",
      "Training acc over epoch: 0.5105\n",
      "---- Validation ----\n",
      "Validation loss: 34.8449\n",
      "Validation acc: 0.5134\n",
      "Time taken: 19.50s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 461.1028, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 463.7830, Accuracy: 0.5092\n",
      "Training loss (for one batch) at step 20: 456.2914, Accuracy: 0.5130\n",
      "Training loss (for one batch) at step 30: 459.0096, Accuracy: 0.5197\n",
      "Training loss (for one batch) at step 40: 455.7117, Accuracy: 0.5196\n",
      "Training loss (for one batch) at step 50: 456.7456, Accuracy: 0.5224\n",
      "Training loss (for one batch) at step 60: 455.7623, Accuracy: 0.5257\n",
      "Training loss (for one batch) at step 70: 451.5349, Accuracy: 0.5289\n",
      "Training loss (for one batch) at step 80: 445.1207, Accuracy: 0.5294\n",
      "Training loss (for one batch) at step 90: 451.7380, Accuracy: 0.5274\n",
      "Training loss (for one batch) at step 100: 449.9266, Accuracy: 0.5274\n",
      "Training loss (for one batch) at step 110: 451.9378, Accuracy: 0.5269\n",
      "---- Training ----\n",
      "Training loss: 137.4657\n",
      "Training acc over epoch: 0.5281\n",
      "---- Validation ----\n",
      "Validation loss: 35.2316\n",
      "Validation acc: 0.5148\n",
      "Time taken: 18.12s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 449.8676, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 445.7104, Accuracy: 0.5376\n",
      "Training loss (for one batch) at step 20: 449.6295, Accuracy: 0.5305\n",
      "Training loss (for one batch) at step 30: 443.2007, Accuracy: 0.5310\n",
      "Training loss (for one batch) at step 40: 445.9121, Accuracy: 0.5389\n",
      "Training loss (for one batch) at step 50: 441.9507, Accuracy: 0.5420\n",
      "Training loss (for one batch) at step 60: 444.2783, Accuracy: 0.5465\n",
      "Training loss (for one batch) at step 70: 444.8375, Accuracy: 0.5513\n",
      "Training loss (for one batch) at step 80: 445.6870, Accuracy: 0.5496\n",
      "Training loss (for one batch) at step 90: 443.6789, Accuracy: 0.5480\n",
      "Training loss (for one batch) at step 100: 443.5528, Accuracy: 0.5442\n",
      "Training loss (for one batch) at step 110: 446.7892, Accuracy: 0.5457\n",
      "---- Training ----\n",
      "Training loss: 138.3528\n",
      "Training acc over epoch: 0.5457\n",
      "---- Validation ----\n",
      "Validation loss: 34.5975\n",
      "Validation acc: 0.5113\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 443.1257, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 442.4497, Accuracy: 0.5476\n",
      "Training loss (for one batch) at step 20: 439.9832, Accuracy: 0.5577\n",
      "Training loss (for one batch) at step 30: 438.4525, Accuracy: 0.5612\n",
      "Training loss (for one batch) at step 40: 448.6392, Accuracy: 0.5615\n",
      "Training loss (for one batch) at step 50: 441.8024, Accuracy: 0.5705\n",
      "Training loss (for one batch) at step 60: 442.7407, Accuracy: 0.5774\n",
      "Training loss (for one batch) at step 70: 451.1967, Accuracy: 0.5812\n",
      "Training loss (for one batch) at step 80: 441.6212, Accuracy: 0.5809\n",
      "Training loss (for one batch) at step 90: 440.7040, Accuracy: 0.5738\n",
      "Training loss (for one batch) at step 100: 445.0925, Accuracy: 0.5723\n",
      "Training loss (for one batch) at step 110: 444.5938, Accuracy: 0.5717\n",
      "---- Training ----\n",
      "Training loss: 138.8545\n",
      "Training acc over epoch: 0.5731\n",
      "---- Validation ----\n",
      "Validation loss: 34.0675\n",
      "Validation acc: 0.5857\n",
      "Time taken: 18.07s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 440.9661, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 439.1887, Accuracy: 0.5909\n",
      "Training loss (for one batch) at step 20: 438.5634, Accuracy: 0.5900\n",
      "Training loss (for one batch) at step 30: 439.0057, Accuracy: 0.5897\n",
      "Training loss (for one batch) at step 40: 445.3122, Accuracy: 0.5926\n",
      "Training loss (for one batch) at step 50: 436.7903, Accuracy: 0.5947\n",
      "Training loss (for one batch) at step 60: 438.9501, Accuracy: 0.5998\n",
      "Training loss (for one batch) at step 70: 439.0276, Accuracy: 0.6015\n",
      "Training loss (for one batch) at step 80: 445.4124, Accuracy: 0.6004\n",
      "Training loss (for one batch) at step 90: 440.7478, Accuracy: 0.5974\n",
      "Training loss (for one batch) at step 100: 442.6602, Accuracy: 0.5970\n",
      "Training loss (for one batch) at step 110: 442.4311, Accuracy: 0.5997\n",
      "---- Training ----\n",
      "Training loss: 137.5385\n",
      "Training acc over epoch: 0.6018\n",
      "---- Validation ----\n",
      "Validation loss: 33.8169\n",
      "Validation acc: 0.6290\n",
      "Time taken: 20.14s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 439.6383, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 442.1484, Accuracy: 0.6143\n",
      "Training loss (for one batch) at step 20: 436.9712, Accuracy: 0.6164\n",
      "Training loss (for one batch) at step 30: 437.0723, Accuracy: 0.6137\n",
      "Training loss (for one batch) at step 40: 433.3023, Accuracy: 0.6153\n",
      "Training loss (for one batch) at step 50: 437.3542, Accuracy: 0.6140\n",
      "Training loss (for one batch) at step 60: 437.8042, Accuracy: 0.6196\n",
      "Training loss (for one batch) at step 70: 447.5178, Accuracy: 0.6217\n",
      "Training loss (for one batch) at step 80: 443.7899, Accuracy: 0.6155\n",
      "Training loss (for one batch) at step 90: 437.7385, Accuracy: 0.6097\n",
      "Training loss (for one batch) at step 100: 438.1913, Accuracy: 0.6119\n",
      "Training loss (for one batch) at step 110: 442.6554, Accuracy: 0.6133\n",
      "---- Training ----\n",
      "Training loss: 137.3786\n",
      "Training acc over epoch: 0.6159\n",
      "---- Validation ----\n",
      "Validation loss: 34.9249\n",
      "Validation acc: 0.6077\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 443.0155, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 441.4937, Accuracy: 0.6129\n",
      "Training loss (for one batch) at step 20: 440.8985, Accuracy: 0.6161\n",
      "Training loss (for one batch) at step 30: 435.2487, Accuracy: 0.6174\n",
      "Training loss (for one batch) at step 40: 435.6241, Accuracy: 0.6200\n",
      "Training loss (for one batch) at step 50: 438.8408, Accuracy: 0.6242\n",
      "Training loss (for one batch) at step 60: 443.1677, Accuracy: 0.6263\n",
      "Training loss (for one batch) at step 70: 443.4136, Accuracy: 0.6320\n",
      "Training loss (for one batch) at step 80: 438.2280, Accuracy: 0.6276\n",
      "Training loss (for one batch) at step 90: 440.2523, Accuracy: 0.6191\n",
      "Training loss (for one batch) at step 100: 437.2000, Accuracy: 0.6214\n",
      "Training loss (for one batch) at step 110: 437.7059, Accuracy: 0.6231\n",
      "---- Training ----\n",
      "Training loss: 136.7878\n",
      "Training acc over epoch: 0.6235\n",
      "---- Validation ----\n",
      "Validation loss: 34.8250\n",
      "Validation acc: 0.6432\n",
      "Time taken: 18.09s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 442.2401, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 437.4727, Accuracy: 0.6534\n",
      "Training loss (for one batch) at step 20: 435.3585, Accuracy: 0.6272\n",
      "Training loss (for one batch) at step 30: 425.2613, Accuracy: 0.6280\n",
      "Training loss (for one batch) at step 40: 424.4753, Accuracy: 0.6321\n",
      "Training loss (for one batch) at step 50: 436.9777, Accuracy: 0.6374\n",
      "Training loss (for one batch) at step 60: 433.8094, Accuracy: 0.6466\n",
      "Training loss (for one batch) at step 70: 441.1221, Accuracy: 0.6483\n",
      "Training loss (for one batch) at step 80: 442.1133, Accuracy: 0.6433\n",
      "Training loss (for one batch) at step 90: 442.2928, Accuracy: 0.6388\n",
      "Training loss (for one batch) at step 100: 434.6010, Accuracy: 0.6413\n",
      "Training loss (for one batch) at step 110: 435.0294, Accuracy: 0.6447\n",
      "---- Training ----\n",
      "Training loss: 136.3921\n",
      "Training acc over epoch: 0.6466\n",
      "---- Validation ----\n",
      "Validation loss: 34.2985\n",
      "Validation acc: 0.6577\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 445.3143, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 440.8232, Accuracy: 0.6513\n",
      "Training loss (for one batch) at step 20: 435.5858, Accuracy: 0.6399\n",
      "Training loss (for one batch) at step 30: 431.8190, Accuracy: 0.6414\n",
      "Training loss (for one batch) at step 40: 433.8737, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 50: 419.1059, Accuracy: 0.6455\n",
      "Training loss (for one batch) at step 60: 440.0484, Accuracy: 0.6542\n",
      "Training loss (for one batch) at step 70: 440.0311, Accuracy: 0.6496\n",
      "Training loss (for one batch) at step 80: 442.4028, Accuracy: 0.6419\n",
      "Training loss (for one batch) at step 90: 437.1315, Accuracy: 0.6381\n",
      "Training loss (for one batch) at step 100: 430.9688, Accuracy: 0.6392\n",
      "Training loss (for one batch) at step 110: 444.2556, Accuracy: 0.6425\n",
      "---- Training ----\n",
      "Training loss: 135.5647\n",
      "Training acc over epoch: 0.6446\n",
      "---- Validation ----\n",
      "Validation loss: 34.5546\n",
      "Validation acc: 0.6800\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 441.5922, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 438.6135, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 20: 431.6356, Accuracy: 0.6399\n",
      "Training loss (for one batch) at step 30: 426.3022, Accuracy: 0.6436\n",
      "Training loss (for one batch) at step 40: 418.0488, Accuracy: 0.6580\n",
      "Training loss (for one batch) at step 50: 419.1416, Accuracy: 0.6627\n",
      "Training loss (for one batch) at step 60: 432.3138, Accuracy: 0.6665\n",
      "Training loss (for one batch) at step 70: 445.5821, Accuracy: 0.6701\n",
      "Training loss (for one batch) at step 80: 437.8487, Accuracy: 0.6637\n",
      "Training loss (for one batch) at step 90: 435.1443, Accuracy: 0.6550\n",
      "Training loss (for one batch) at step 100: 434.7368, Accuracy: 0.6553\n",
      "Training loss (for one batch) at step 110: 431.3957, Accuracy: 0.6579\n",
      "---- Training ----\n",
      "Training loss: 135.1331\n",
      "Training acc over epoch: 0.6575\n",
      "---- Validation ----\n",
      "Validation loss: 35.8716\n",
      "Validation acc: 0.6862\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 442.9442, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 435.1977, Accuracy: 0.6747\n",
      "Training loss (for one batch) at step 20: 431.2593, Accuracy: 0.6622\n",
      "Training loss (for one batch) at step 30: 427.2316, Accuracy: 0.6593\n",
      "Training loss (for one batch) at step 40: 419.4388, Accuracy: 0.6646\n",
      "Training loss (for one batch) at step 50: 425.3456, Accuracy: 0.6670\n",
      "Training loss (for one batch) at step 60: 432.2408, Accuracy: 0.6753\n",
      "Training loss (for one batch) at step 70: 439.8546, Accuracy: 0.6761\n",
      "Training loss (for one batch) at step 80: 436.5984, Accuracy: 0.6661\n",
      "Training loss (for one batch) at step 90: 430.8942, Accuracy: 0.6634\n",
      "Training loss (for one batch) at step 100: 431.8785, Accuracy: 0.6627\n",
      "Training loss (for one batch) at step 110: 425.6041, Accuracy: 0.6651\n",
      "---- Training ----\n",
      "Training loss: 133.5142\n",
      "Training acc over epoch: 0.6654\n",
      "---- Validation ----\n",
      "Validation loss: 35.5984\n",
      "Validation acc: 0.6894\n",
      "Time taken: 18.14s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 442.4069, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 435.0229, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 20: 433.3925, Accuracy: 0.6510\n",
      "Training loss (for one batch) at step 30: 417.8476, Accuracy: 0.6615\n",
      "Training loss (for one batch) at step 40: 414.0490, Accuracy: 0.6660\n",
      "Training loss (for one batch) at step 50: 402.7297, Accuracy: 0.6748\n",
      "Training loss (for one batch) at step 60: 415.4427, Accuracy: 0.6814\n",
      "Training loss (for one batch) at step 70: 441.4951, Accuracy: 0.6794\n",
      "Training loss (for one batch) at step 80: 436.4516, Accuracy: 0.6705\n",
      "Training loss (for one batch) at step 90: 427.0669, Accuracy: 0.6660\n",
      "Training loss (for one batch) at step 100: 426.5099, Accuracy: 0.6686\n",
      "Training loss (for one batch) at step 110: 427.7014, Accuracy: 0.6698\n",
      "---- Training ----\n",
      "Training loss: 136.5093\n",
      "Training acc over epoch: 0.6717\n",
      "---- Validation ----\n",
      "Validation loss: 35.6032\n",
      "Validation acc: 0.6625\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 434.9646, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 432.3917, Accuracy: 0.6946\n",
      "Training loss (for one batch) at step 20: 426.2344, Accuracy: 0.6708\n",
      "Training loss (for one batch) at step 30: 413.5922, Accuracy: 0.6731\n",
      "Training loss (for one batch) at step 40: 410.3545, Accuracy: 0.6827\n",
      "Training loss (for one batch) at step 50: 399.6827, Accuracy: 0.6880\n",
      "Training loss (for one batch) at step 60: 417.1844, Accuracy: 0.6980\n",
      "Training loss (for one batch) at step 70: 431.1058, Accuracy: 0.6998\n",
      "Training loss (for one batch) at step 80: 433.9646, Accuracy: 0.6834\n",
      "Training loss (for one batch) at step 90: 426.3384, Accuracy: 0.6753\n",
      "Training loss (for one batch) at step 100: 409.9780, Accuracy: 0.6751\n",
      "Training loss (for one batch) at step 110: 421.9767, Accuracy: 0.6786\n",
      "---- Training ----\n",
      "Training loss: 136.7208\n",
      "Training acc over epoch: 0.6797\n",
      "---- Validation ----\n",
      "Validation loss: 35.2261\n",
      "Validation acc: 0.6781\n",
      "Time taken: 18.10s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 440.9112, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 427.9575, Accuracy: 0.6541\n",
      "Training loss (for one batch) at step 20: 426.1088, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 30: 405.3340, Accuracy: 0.6741\n",
      "Training loss (for one batch) at step 40: 397.5968, Accuracy: 0.6824\n",
      "Training loss (for one batch) at step 50: 387.7794, Accuracy: 0.6938\n",
      "Training loss (for one batch) at step 60: 395.3245, Accuracy: 0.7020\n",
      "Training loss (for one batch) at step 70: 431.2564, Accuracy: 0.7028\n",
      "Training loss (for one batch) at step 80: 427.9378, Accuracy: 0.6948\n",
      "Training loss (for one batch) at step 90: 419.2815, Accuracy: 0.6887\n",
      "Training loss (for one batch) at step 100: 413.6127, Accuracy: 0.6881\n",
      "Training loss (for one batch) at step 110: 413.3763, Accuracy: 0.6899\n",
      "---- Training ----\n",
      "Training loss: 130.3849\n",
      "Training acc over epoch: 0.6903\n",
      "---- Validation ----\n",
      "Validation loss: 34.2440\n",
      "Validation acc: 0.6819\n",
      "Time taken: 18.08s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 437.2860, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 419.7730, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 20: 421.7390, Accuracy: 0.6815\n",
      "Training loss (for one batch) at step 30: 407.8433, Accuracy: 0.6913\n",
      "Training loss (for one batch) at step 40: 384.1682, Accuracy: 0.6997\n",
      "Training loss (for one batch) at step 50: 381.4254, Accuracy: 0.7089\n",
      "Training loss (for one batch) at step 60: 410.2364, Accuracy: 0.7249\n",
      "Training loss (for one batch) at step 70: 425.5060, Accuracy: 0.7215\n",
      "Training loss (for one batch) at step 80: 417.0913, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 90: 410.8119, Accuracy: 0.7045\n",
      "Training loss (for one batch) at step 100: 399.7813, Accuracy: 0.7061\n",
      "Training loss (for one batch) at step 110: 416.3462, Accuracy: 0.7071\n",
      "---- Training ----\n",
      "Training loss: 133.9022\n",
      "Training acc over epoch: 0.7080\n",
      "---- Validation ----\n",
      "Validation loss: 40.2014\n",
      "Validation acc: 0.6709\n",
      "Time taken: 17.94s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 422.9026, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 422.6345, Accuracy: 0.6662\n",
      "Training loss (for one batch) at step 20: 410.2787, Accuracy: 0.6734\n",
      "Training loss (for one batch) at step 30: 403.4281, Accuracy: 0.6898\n",
      "Training loss (for one batch) at step 40: 381.3812, Accuracy: 0.7048\n",
      "Training loss (for one batch) at step 50: 368.0678, Accuracy: 0.7165\n",
      "Training loss (for one batch) at step 60: 394.0327, Accuracy: 0.7226\n",
      "Training loss (for one batch) at step 70: 417.7659, Accuracy: 0.7210\n",
      "Training loss (for one batch) at step 80: 419.1255, Accuracy: 0.7122\n",
      "Training loss (for one batch) at step 90: 400.4723, Accuracy: 0.7077\n",
      "Training loss (for one batch) at step 100: 423.6627, Accuracy: 0.7082\n",
      "Training loss (for one batch) at step 110: 400.2282, Accuracy: 0.7104\n",
      "---- Training ----\n",
      "Training loss: 139.0004\n",
      "Training acc over epoch: 0.7098\n",
      "---- Validation ----\n",
      "Validation loss: 37.5521\n",
      "Validation acc: 0.6717\n",
      "Time taken: 18.06s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 436.6957, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 417.0157, Accuracy: 0.6634\n",
      "Training loss (for one batch) at step 20: 395.9910, Accuracy: 0.6678\n",
      "Training loss (for one batch) at step 30: 387.3728, Accuracy: 0.6817\n",
      "Training loss (for one batch) at step 40: 366.5551, Accuracy: 0.6980\n",
      "Training loss (for one batch) at step 50: 365.3398, Accuracy: 0.7100\n",
      "Training loss (for one batch) at step 60: 392.1518, Accuracy: 0.7196\n",
      "Training loss (for one batch) at step 70: 415.2406, Accuracy: 0.7173\n",
      "Training loss (for one batch) at step 80: 392.8368, Accuracy: 0.7094\n",
      "Training loss (for one batch) at step 90: 376.2574, Accuracy: 0.7063\n",
      "Training loss (for one batch) at step 100: 378.2379, Accuracy: 0.7106\n",
      "Training loss (for one batch) at step 110: 406.9533, Accuracy: 0.7132\n",
      "---- Training ----\n",
      "Training loss: 116.8410\n",
      "Training acc over epoch: 0.7129\n",
      "---- Validation ----\n",
      "Validation loss: 38.3360\n",
      "Validation acc: 0.6703\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 407.8758, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 417.8670, Accuracy: 0.6605\n",
      "Training loss (for one batch) at step 20: 394.2343, Accuracy: 0.6745\n",
      "Training loss (for one batch) at step 30: 383.5522, Accuracy: 0.6928\n",
      "Training loss (for one batch) at step 40: 362.6418, Accuracy: 0.7090\n",
      "Training loss (for one batch) at step 50: 353.7469, Accuracy: 0.7261\n",
      "Training loss (for one batch) at step 60: 372.0920, Accuracy: 0.7345\n",
      "Training loss (for one batch) at step 70: 415.5977, Accuracy: 0.7291\n",
      "Training loss (for one batch) at step 80: 400.0287, Accuracy: 0.7186\n",
      "Training loss (for one batch) at step 90: 383.4698, Accuracy: 0.7154\n",
      "Training loss (for one batch) at step 100: 384.1741, Accuracy: 0.7169\n",
      "Training loss (for one batch) at step 110: 390.0080, Accuracy: 0.7180\n",
      "---- Training ----\n",
      "Training loss: 124.1830\n",
      "Training acc over epoch: 0.7177\n",
      "---- Validation ----\n",
      "Validation loss: 42.3941\n",
      "Validation acc: 0.6572\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 430.1973, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 398.6588, Accuracy: 0.6683\n",
      "Training loss (for one batch) at step 20: 375.8174, Accuracy: 0.6815\n",
      "Training loss (for one batch) at step 30: 372.3807, Accuracy: 0.6930\n",
      "Training loss (for one batch) at step 40: 357.3067, Accuracy: 0.7168\n",
      "Training loss (for one batch) at step 50: 338.4877, Accuracy: 0.7292\n",
      "Training loss (for one batch) at step 60: 365.5336, Accuracy: 0.7369\n",
      "Training loss (for one batch) at step 70: 394.2451, Accuracy: 0.7296\n",
      "Training loss (for one batch) at step 80: 379.4297, Accuracy: 0.7207\n",
      "Training loss (for one batch) at step 90: 374.9705, Accuracy: 0.7191\n",
      "Training loss (for one batch) at step 100: 368.3189, Accuracy: 0.7215\n",
      "Training loss (for one batch) at step 110: 384.4204, Accuracy: 0.7252\n",
      "---- Training ----\n",
      "Training loss: 124.6768\n",
      "Training acc over epoch: 0.7248\n",
      "---- Validation ----\n",
      "Validation loss: 33.4987\n",
      "Validation acc: 0.6601\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 401.6783, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 389.7680, Accuracy: 0.6527\n",
      "Training loss (for one batch) at step 20: 373.4681, Accuracy: 0.6860\n",
      "Training loss (for one batch) at step 30: 351.7176, Accuracy: 0.7097\n",
      "Training loss (for one batch) at step 40: 348.9825, Accuracy: 0.7275\n",
      "Training loss (for one batch) at step 50: 322.9602, Accuracy: 0.7417\n",
      "Training loss (for one batch) at step 60: 357.0785, Accuracy: 0.7506\n",
      "Training loss (for one batch) at step 70: 363.8554, Accuracy: 0.7423\n",
      "Training loss (for one batch) at step 80: 398.9280, Accuracy: 0.7281\n",
      "Training loss (for one batch) at step 90: 355.3249, Accuracy: 0.7248\n",
      "Training loss (for one batch) at step 100: 350.2103, Accuracy: 0.7296\n",
      "Training loss (for one batch) at step 110: 361.8853, Accuracy: 0.7312\n",
      "---- Training ----\n",
      "Training loss: 129.4265\n",
      "Training acc over epoch: 0.7307\n",
      "---- Validation ----\n",
      "Validation loss: 40.2770\n",
      "Validation acc: 0.6768\n",
      "Time taken: 18.07s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 407.5491, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 393.0377, Accuracy: 0.6655\n",
      "Training loss (for one batch) at step 20: 348.7017, Accuracy: 0.6894\n",
      "Training loss (for one batch) at step 30: 346.4312, Accuracy: 0.7112\n",
      "Training loss (for one batch) at step 40: 340.4174, Accuracy: 0.7262\n",
      "Training loss (for one batch) at step 50: 334.7783, Accuracy: 0.7426\n",
      "Training loss (for one batch) at step 60: 341.8604, Accuracy: 0.7512\n",
      "Training loss (for one batch) at step 70: 373.8871, Accuracy: 0.7414\n",
      "Training loss (for one batch) at step 80: 361.1679, Accuracy: 0.7265\n",
      "Training loss (for one batch) at step 90: 342.2732, Accuracy: 0.7233\n",
      "Training loss (for one batch) at step 100: 345.2205, Accuracy: 0.7270\n",
      "Training loss (for one batch) at step 110: 354.9906, Accuracy: 0.7305\n",
      "---- Training ----\n",
      "Training loss: 115.6684\n",
      "Training acc over epoch: 0.7304\n",
      "---- Validation ----\n",
      "Validation loss: 37.4797\n",
      "Validation acc: 0.6529\n",
      "Time taken: 18.24s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 400.5788, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 375.6360, Accuracy: 0.6293\n",
      "Training loss (for one batch) at step 20: 356.6868, Accuracy: 0.6749\n",
      "Training loss (for one batch) at step 30: 354.8521, Accuracy: 0.7079\n",
      "Training loss (for one batch) at step 40: 335.7190, Accuracy: 0.7241\n",
      "Training loss (for one batch) at step 50: 327.7095, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 60: 342.4420, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 70: 381.1416, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 80: 370.4587, Accuracy: 0.7249\n",
      "Training loss (for one batch) at step 90: 334.2755, Accuracy: 0.7254\n",
      "Training loss (for one batch) at step 100: 335.4907, Accuracy: 0.7290\n",
      "Training loss (for one batch) at step 110: 334.7816, Accuracy: 0.7314\n",
      "---- Training ----\n",
      "Training loss: 123.7688\n",
      "Training acc over epoch: 0.7295\n",
      "---- Validation ----\n",
      "Validation loss: 42.0741\n",
      "Validation acc: 0.6569\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 371.9059, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 400.2674, Accuracy: 0.6655\n",
      "Training loss (for one batch) at step 20: 330.2487, Accuracy: 0.6879\n",
      "Training loss (for one batch) at step 30: 336.1929, Accuracy: 0.7182\n",
      "Training loss (for one batch) at step 40: 309.1772, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 50: 302.3669, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 60: 320.5067, Accuracy: 0.7579\n",
      "Training loss (for one batch) at step 70: 347.9514, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 80: 363.6509, Accuracy: 0.7308\n",
      "Training loss (for one batch) at step 90: 331.0626, Accuracy: 0.7279\n",
      "Training loss (for one batch) at step 100: 321.6481, Accuracy: 0.7317\n",
      "Training loss (for one batch) at step 110: 319.0298, Accuracy: 0.7333\n",
      "---- Training ----\n",
      "Training loss: 108.1616\n",
      "Training acc over epoch: 0.7324\n",
      "---- Validation ----\n",
      "Validation loss: 34.3813\n",
      "Validation acc: 0.6486\n",
      "Time taken: 18.00s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 361.0772, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 363.6588, Accuracy: 0.6435\n",
      "Training loss (for one batch) at step 20: 327.6697, Accuracy: 0.6864\n",
      "Training loss (for one batch) at step 30: 324.2749, Accuracy: 0.7175\n",
      "Training loss (for one batch) at step 40: 318.5091, Accuracy: 0.7365\n",
      "Training loss (for one batch) at step 50: 324.5610, Accuracy: 0.7518\n",
      "Training loss (for one batch) at step 60: 330.3206, Accuracy: 0.7601\n",
      "Training loss (for one batch) at step 70: 334.5927, Accuracy: 0.7463\n",
      "Training loss (for one batch) at step 80: 339.6418, Accuracy: 0.7319\n",
      "Training loss (for one batch) at step 90: 336.6673, Accuracy: 0.7299\n",
      "Training loss (for one batch) at step 100: 329.4517, Accuracy: 0.7338\n",
      "Training loss (for one batch) at step 110: 332.9890, Accuracy: 0.7359\n",
      "---- Training ----\n",
      "Training loss: 114.1748\n",
      "Training acc over epoch: 0.7341\n",
      "---- Validation ----\n",
      "Validation loss: 36.6363\n",
      "Validation acc: 0.6693\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 364.8050, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 353.2877, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 20: 322.0673, Accuracy: 0.6946\n",
      "Training loss (for one batch) at step 30: 327.4567, Accuracy: 0.7160\n",
      "Training loss (for one batch) at step 40: 306.4592, Accuracy: 0.7349\n",
      "Training loss (for one batch) at step 50: 304.3321, Accuracy: 0.7518\n",
      "Training loss (for one batch) at step 60: 301.9688, Accuracy: 0.7574\n",
      "Training loss (for one batch) at step 70: 338.4994, Accuracy: 0.7470\n",
      "Training loss (for one batch) at step 80: 349.0583, Accuracy: 0.7349\n",
      "Training loss (for one batch) at step 90: 316.7684, Accuracy: 0.7293\n",
      "Training loss (for one batch) at step 100: 313.0093, Accuracy: 0.7343\n",
      "Training loss (for one batch) at step 110: 339.5443, Accuracy: 0.7361\n",
      "---- Training ----\n",
      "Training loss: 122.2006\n",
      "Training acc over epoch: 0.7350\n",
      "---- Validation ----\n",
      "Validation loss: 45.7580\n",
      "Validation acc: 0.6728\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 358.0971, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 354.0914, Accuracy: 0.6541\n",
      "Training loss (for one batch) at step 20: 320.2042, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 30: 297.2733, Accuracy: 0.7198\n",
      "Training loss (for one batch) at step 40: 295.9992, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 50: 304.2208, Accuracy: 0.7552\n",
      "Training loss (for one batch) at step 60: 307.5898, Accuracy: 0.7627\n",
      "Training loss (for one batch) at step 70: 356.8196, Accuracy: 0.7493\n",
      "Training loss (for one batch) at step 80: 317.2883, Accuracy: 0.7323\n",
      "Training loss (for one batch) at step 90: 317.1992, Accuracy: 0.7300\n",
      "Training loss (for one batch) at step 100: 302.0543, Accuracy: 0.7348\n",
      "Training loss (for one batch) at step 110: 324.9415, Accuracy: 0.7377\n",
      "---- Training ----\n",
      "Training loss: 113.3515\n",
      "Training acc over epoch: 0.7371\n",
      "---- Validation ----\n",
      "Validation loss: 60.7370\n",
      "Validation acc: 0.6789\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 352.3473, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 331.1121, Accuracy: 0.6442\n",
      "Training loss (for one batch) at step 20: 320.0286, Accuracy: 0.6812\n",
      "Training loss (for one batch) at step 30: 298.3549, Accuracy: 0.7165\n",
      "Training loss (for one batch) at step 40: 316.5461, Accuracy: 0.7348\n",
      "Training loss (for one batch) at step 50: 283.1821, Accuracy: 0.7541\n",
      "Training loss (for one batch) at step 60: 303.5006, Accuracy: 0.7604\n",
      "Training loss (for one batch) at step 70: 338.2447, Accuracy: 0.7470\n",
      "Training loss (for one batch) at step 80: 330.2345, Accuracy: 0.7308\n",
      "Training loss (for one batch) at step 90: 316.2450, Accuracy: 0.7263\n",
      "Training loss (for one batch) at step 100: 304.5444, Accuracy: 0.7331\n",
      "Training loss (for one batch) at step 110: 317.3090, Accuracy: 0.7361\n",
      "---- Training ----\n",
      "Training loss: 91.0093\n",
      "Training acc over epoch: 0.7352\n",
      "---- Validation ----\n",
      "Validation loss: 33.2030\n",
      "Validation acc: 0.6754\n",
      "Time taken: 18.14s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 346.3674, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 322.4473, Accuracy: 0.6293\n",
      "Training loss (for one batch) at step 20: 320.3393, Accuracy: 0.6756\n",
      "Training loss (for one batch) at step 30: 296.7008, Accuracy: 0.7034\n",
      "Training loss (for one batch) at step 40: 285.1301, Accuracy: 0.7271\n",
      "Training loss (for one batch) at step 50: 302.3762, Accuracy: 0.7466\n",
      "Training loss (for one batch) at step 60: 316.2880, Accuracy: 0.7567\n",
      "Training loss (for one batch) at step 70: 336.9868, Accuracy: 0.7438\n",
      "Training loss (for one batch) at step 80: 327.0390, Accuracy: 0.7285\n",
      "Training loss (for one batch) at step 90: 304.4840, Accuracy: 0.7256\n",
      "Training loss (for one batch) at step 100: 295.5370, Accuracy: 0.7300\n",
      "Training loss (for one batch) at step 110: 308.3995, Accuracy: 0.7322\n",
      "---- Training ----\n",
      "Training loss: 106.8023\n",
      "Training acc over epoch: 0.7314\n",
      "---- Validation ----\n",
      "Validation loss: 41.3906\n",
      "Validation acc: 0.6599\n",
      "Time taken: 20.13s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 354.4771, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 309.9237, Accuracy: 0.6413\n",
      "Training loss (for one batch) at step 20: 292.5985, Accuracy: 0.6715\n",
      "Training loss (for one batch) at step 30: 303.8551, Accuracy: 0.7112\n",
      "Training loss (for one batch) at step 40: 295.0847, Accuracy: 0.7317\n",
      "Training loss (for one batch) at step 50: 298.5246, Accuracy: 0.7498\n",
      "Training loss (for one batch) at step 60: 299.0771, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 70: 299.8064, Accuracy: 0.7428\n",
      "Training loss (for one batch) at step 80: 313.7212, Accuracy: 0.7256\n",
      "Training loss (for one batch) at step 90: 290.1683, Accuracy: 0.7224\n",
      "Training loss (for one batch) at step 100: 294.5629, Accuracy: 0.7278\n",
      "Training loss (for one batch) at step 110: 304.0811, Accuracy: 0.7323\n",
      "---- Training ----\n",
      "Training loss: 106.0488\n",
      "Training acc over epoch: 0.7317\n",
      "---- Validation ----\n",
      "Validation loss: 40.5346\n",
      "Validation acc: 0.6746\n",
      "Time taken: 18.05s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 308.8344, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 321.7273, Accuracy: 0.6307\n",
      "Training loss (for one batch) at step 20: 291.3428, Accuracy: 0.6786\n",
      "Training loss (for one batch) at step 30: 281.3539, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 40: 278.6640, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 50: 271.8258, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 60: 281.3105, Accuracy: 0.7588\n",
      "Training loss (for one batch) at step 70: 321.0210, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 80: 340.2710, Accuracy: 0.7312\n",
      "Training loss (for one batch) at step 90: 289.5471, Accuracy: 0.7282\n",
      "Training loss (for one batch) at step 100: 289.7867, Accuracy: 0.7345\n",
      "Training loss (for one batch) at step 110: 295.0656, Accuracy: 0.7355\n",
      "---- Training ----\n",
      "Training loss: 87.1999\n",
      "Training acc over epoch: 0.7350\n",
      "---- Validation ----\n",
      "Validation loss: 46.7216\n",
      "Validation acc: 0.6701\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 342.9955, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 311.3927, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 20: 272.2823, Accuracy: 0.6778\n",
      "Training loss (for one batch) at step 30: 279.6952, Accuracy: 0.7175\n",
      "Training loss (for one batch) at step 40: 285.1746, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 50: 276.6848, Accuracy: 0.7560\n",
      "Training loss (for one batch) at step 60: 269.0377, Accuracy: 0.7629\n",
      "Training loss (for one batch) at step 70: 327.8965, Accuracy: 0.7506\n",
      "Training loss (for one batch) at step 80: 310.9942, Accuracy: 0.7323\n",
      "Training loss (for one batch) at step 90: 275.9772, Accuracy: 0.7285\n",
      "Training loss (for one batch) at step 100: 282.5269, Accuracy: 0.7338\n",
      "Training loss (for one batch) at step 110: 299.6716, Accuracy: 0.7349\n",
      "---- Training ----\n",
      "Training loss: 104.8308\n",
      "Training acc over epoch: 0.7340\n",
      "---- Validation ----\n",
      "Validation loss: 38.3446\n",
      "Validation acc: 0.6698\n",
      "Time taken: 20.11s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 334.8690, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 328.2430, Accuracy: 0.6364\n",
      "Training loss (for one batch) at step 20: 282.9771, Accuracy: 0.6704\n",
      "Training loss (for one batch) at step 30: 274.7057, Accuracy: 0.7162\n",
      "Training loss (for one batch) at step 40: 290.8675, Accuracy: 0.7365\n",
      "Training loss (for one batch) at step 50: 255.0603, Accuracy: 0.7520\n",
      "Training loss (for one batch) at step 60: 282.3368, Accuracy: 0.7573\n",
      "Training loss (for one batch) at step 70: 318.3558, Accuracy: 0.7480\n",
      "Training loss (for one batch) at step 80: 302.8484, Accuracy: 0.7306\n",
      "Training loss (for one batch) at step 90: 299.5011, Accuracy: 0.7278\n",
      "Training loss (for one batch) at step 100: 269.3702, Accuracy: 0.7338\n",
      "Training loss (for one batch) at step 110: 303.6206, Accuracy: 0.7351\n",
      "---- Training ----\n",
      "Training loss: 105.8966\n",
      "Training acc over epoch: 0.7339\n",
      "---- Validation ----\n",
      "Validation loss: 45.9874\n",
      "Validation acc: 0.6838\n",
      "Time taken: 18.02s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 327.3326, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 308.5670, Accuracy: 0.6349\n",
      "Training loss (for one batch) at step 20: 282.2899, Accuracy: 0.6789\n",
      "Training loss (for one batch) at step 30: 279.9346, Accuracy: 0.7160\n",
      "Training loss (for one batch) at step 40: 260.0546, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 50: 269.0388, Accuracy: 0.7537\n",
      "Training loss (for one batch) at step 60: 276.3603, Accuracy: 0.7596\n",
      "Training loss (for one batch) at step 70: 307.0468, Accuracy: 0.7464\n",
      "Training loss (for one batch) at step 80: 324.6514, Accuracy: 0.7315\n",
      "Training loss (for one batch) at step 90: 278.2849, Accuracy: 0.7271\n",
      "Training loss (for one batch) at step 100: 272.1329, Accuracy: 0.7317\n",
      "Training loss (for one batch) at step 110: 290.3389, Accuracy: 0.7332\n",
      "---- Training ----\n",
      "Training loss: 95.9765\n",
      "Training acc over epoch: 0.7327\n",
      "---- Validation ----\n",
      "Validation loss: 47.9384\n",
      "Validation acc: 0.6596\n",
      "Time taken: 20.11s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 334.5278, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 297.7230, Accuracy: 0.6378\n",
      "Training loss (for one batch) at step 20: 283.6589, Accuracy: 0.6786\n",
      "Training loss (for one batch) at step 30: 259.6560, Accuracy: 0.7175\n",
      "Training loss (for one batch) at step 40: 271.4629, Accuracy: 0.7401\n",
      "Training loss (for one batch) at step 50: 261.0134, Accuracy: 0.7570\n",
      "Training loss (for one batch) at step 60: 293.4715, Accuracy: 0.7634\n",
      "Training loss (for one batch) at step 70: 304.8796, Accuracy: 0.7466\n",
      "Training loss (for one batch) at step 80: 323.8611, Accuracy: 0.7301\n",
      "Training loss (for one batch) at step 90: 280.6294, Accuracy: 0.7290\n",
      "Training loss (for one batch) at step 100: 286.0403, Accuracy: 0.7341\n",
      "Training loss (for one batch) at step 110: 299.3188, Accuracy: 0.7351\n",
      "---- Training ----\n",
      "Training loss: 91.2835\n",
      "Training acc over epoch: 0.7345\n",
      "---- Validation ----\n",
      "Validation loss: 50.2659\n",
      "Validation acc: 0.6762\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 310.7419, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 302.8316, Accuracy: 0.6527\n",
      "Training loss (for one batch) at step 20: 276.7581, Accuracy: 0.6771\n",
      "Training loss (for one batch) at step 30: 265.5203, Accuracy: 0.7140\n",
      "Training loss (for one batch) at step 40: 285.6541, Accuracy: 0.7357\n",
      "Training loss (for one batch) at step 50: 245.2874, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 60: 299.2483, Accuracy: 0.7590\n",
      "Training loss (for one batch) at step 70: 308.1796, Accuracy: 0.7476\n",
      "Training loss (for one batch) at step 80: 302.0426, Accuracy: 0.7328\n",
      "Training loss (for one batch) at step 90: 273.5534, Accuracy: 0.7288\n",
      "Training loss (for one batch) at step 100: 272.3019, Accuracy: 0.7355\n",
      "Training loss (for one batch) at step 110: 285.3885, Accuracy: 0.7374\n",
      "---- Training ----\n",
      "Training loss: 84.5718\n",
      "Training acc over epoch: 0.7359\n",
      "---- Validation ----\n",
      "Validation loss: 51.4570\n",
      "Validation acc: 0.6762\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 324.2230, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 290.3568, Accuracy: 0.6293\n",
      "Training loss (for one batch) at step 20: 277.4526, Accuracy: 0.6808\n",
      "Training loss (for one batch) at step 30: 257.3596, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 40: 263.9052, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 50: 280.6666, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 60: 285.5131, Accuracy: 0.7622\n",
      "Training loss (for one batch) at step 70: 296.0513, Accuracy: 0.7476\n",
      "Training loss (for one batch) at step 80: 299.7420, Accuracy: 0.7312\n",
      "Training loss (for one batch) at step 90: 280.8835, Accuracy: 0.7278\n",
      "Training loss (for one batch) at step 100: 271.7388, Accuracy: 0.7321\n",
      "Training loss (for one batch) at step 110: 284.4058, Accuracy: 0.7344\n",
      "---- Training ----\n",
      "Training loss: 87.0597\n",
      "Training acc over epoch: 0.7334\n",
      "---- Validation ----\n",
      "Validation loss: 46.7773\n",
      "Validation acc: 0.6752\n",
      "Time taken: 18.19s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 299.4342, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 311.6229, Accuracy: 0.6349\n",
      "Training loss (for one batch) at step 20: 282.4103, Accuracy: 0.6860\n",
      "Training loss (for one batch) at step 30: 268.2230, Accuracy: 0.7230\n",
      "Training loss (for one batch) at step 40: 252.1020, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 50: 243.8722, Accuracy: 0.7572\n",
      "Training loss (for one batch) at step 60: 269.6983, Accuracy: 0.7613\n",
      "Training loss (for one batch) at step 70: 289.4284, Accuracy: 0.7469\n",
      "Training loss (for one batch) at step 80: 306.6369, Accuracy: 0.7329\n",
      "Training loss (for one batch) at step 90: 264.9327, Accuracy: 0.7290\n",
      "Training loss (for one batch) at step 100: 272.1355, Accuracy: 0.7326\n",
      "Training loss (for one batch) at step 110: 293.9930, Accuracy: 0.7363\n",
      "---- Training ----\n",
      "Training loss: 102.5400\n",
      "Training acc over epoch: 0.7362\n",
      "---- Validation ----\n",
      "Validation loss: 37.9158\n",
      "Validation acc: 0.6728\n",
      "Time taken: 18.29s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 330.2877, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 304.5608, Accuracy: 0.6335\n",
      "Training loss (for one batch) at step 20: 267.1126, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 30: 261.3792, Accuracy: 0.7137\n",
      "Training loss (for one batch) at step 40: 255.8877, Accuracy: 0.7355\n",
      "Training loss (for one batch) at step 50: 246.6846, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 60: 275.1450, Accuracy: 0.7601\n",
      "Training loss (for one batch) at step 70: 304.0559, Accuracy: 0.7447\n",
      "Training loss (for one batch) at step 80: 297.5794, Accuracy: 0.7276\n",
      "Training loss (for one batch) at step 90: 279.0464, Accuracy: 0.7260\n",
      "Training loss (for one batch) at step 100: 281.5925, Accuracy: 0.7322\n",
      "Training loss (for one batch) at step 110: 280.2008, Accuracy: 0.7339\n",
      "---- Training ----\n",
      "Training loss: 89.1788\n",
      "Training acc over epoch: 0.7333\n",
      "---- Validation ----\n",
      "Validation loss: 57.3243\n",
      "Validation acc: 0.6617\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 314.1158, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 328.7071, Accuracy: 0.6136\n",
      "Training loss (for one batch) at step 20: 274.4348, Accuracy: 0.6674\n",
      "Training loss (for one batch) at step 30: 261.5546, Accuracy: 0.7137\n",
      "Training loss (for one batch) at step 40: 252.4760, Accuracy: 0.7363\n",
      "Training loss (for one batch) at step 50: 249.8209, Accuracy: 0.7551\n",
      "Training loss (for one batch) at step 60: 255.5663, Accuracy: 0.7591\n",
      "Training loss (for one batch) at step 70: 288.8757, Accuracy: 0.7461\n",
      "Training loss (for one batch) at step 80: 271.7322, Accuracy: 0.7291\n",
      "Training loss (for one batch) at step 90: 277.4029, Accuracy: 0.7271\n",
      "Training loss (for one batch) at step 100: 256.3798, Accuracy: 0.7321\n",
      "Training loss (for one batch) at step 110: 279.4718, Accuracy: 0.7341\n",
      "---- Training ----\n",
      "Training loss: 90.3317\n",
      "Training acc over epoch: 0.7329\n",
      "---- Validation ----\n",
      "Validation loss: 72.8958\n",
      "Validation acc: 0.6497\n",
      "Time taken: 18.08s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 296.4911, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 307.3242, Accuracy: 0.6335\n",
      "Training loss (for one batch) at step 20: 265.5091, Accuracy: 0.6838\n",
      "Training loss (for one batch) at step 30: 256.7083, Accuracy: 0.7170\n",
      "Training loss (for one batch) at step 40: 253.6851, Accuracy: 0.7372\n",
      "Training loss (for one batch) at step 50: 261.3290, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 60: 267.0070, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 70: 270.2195, Accuracy: 0.7498\n",
      "Training loss (for one batch) at step 80: 288.2707, Accuracy: 0.7330\n",
      "Training loss (for one batch) at step 90: 250.4730, Accuracy: 0.7311\n",
      "Training loss (for one batch) at step 100: 261.7047, Accuracy: 0.7340\n",
      "Training loss (for one batch) at step 110: 270.2978, Accuracy: 0.7367\n",
      "---- Training ----\n",
      "Training loss: 86.7695\n",
      "Training acc over epoch: 0.7348\n",
      "---- Validation ----\n",
      "Validation loss: 38.3384\n",
      "Validation acc: 0.6738\n",
      "Time taken: 18.13s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 318.4425, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 304.7927, Accuracy: 0.6385\n",
      "Training loss (for one batch) at step 20: 269.0263, Accuracy: 0.6793\n",
      "Training loss (for one batch) at step 30: 259.7218, Accuracy: 0.7142\n",
      "Training loss (for one batch) at step 40: 262.3676, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 50: 267.1018, Accuracy: 0.7563\n",
      "Training loss (for one batch) at step 60: 258.4704, Accuracy: 0.7628\n",
      "Training loss (for one batch) at step 70: 266.0223, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 80: 273.2855, Accuracy: 0.7310\n",
      "Training loss (for one batch) at step 90: 249.1944, Accuracy: 0.7295\n",
      "Training loss (for one batch) at step 100: 251.6105, Accuracy: 0.7367\n",
      "Training loss (for one batch) at step 110: 270.4492, Accuracy: 0.7375\n",
      "---- Training ----\n",
      "Training loss: 89.1398\n",
      "Training acc over epoch: 0.7361\n",
      "---- Validation ----\n",
      "Validation loss: 51.7988\n",
      "Validation acc: 0.6792\n",
      "Time taken: 17.88s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 304.2964, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 294.1147, Accuracy: 0.6385\n",
      "Training loss (for one batch) at step 20: 261.8628, Accuracy: 0.6734\n",
      "Training loss (for one batch) at step 30: 251.7089, Accuracy: 0.7142\n",
      "Training loss (for one batch) at step 40: 276.5571, Accuracy: 0.7389\n",
      "Training loss (for one batch) at step 50: 242.0017, Accuracy: 0.7520\n",
      "Training loss (for one batch) at step 60: 255.6251, Accuracy: 0.7601\n",
      "Training loss (for one batch) at step 70: 269.4971, Accuracy: 0.7463\n",
      "Training loss (for one batch) at step 80: 272.3820, Accuracy: 0.7301\n",
      "Training loss (for one batch) at step 90: 272.7568, Accuracy: 0.7247\n",
      "Training loss (for one batch) at step 100: 255.3206, Accuracy: 0.7301\n",
      "Training loss (for one batch) at step 110: 250.9575, Accuracy: 0.7323\n",
      "---- Training ----\n",
      "Training loss: 96.4234\n",
      "Training acc over epoch: 0.7319\n",
      "---- Validation ----\n",
      "Validation loss: 39.4804\n",
      "Validation acc: 0.6725\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 311.5775, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 293.7550, Accuracy: 0.6413\n",
      "Training loss (for one batch) at step 20: 258.4114, Accuracy: 0.6830\n",
      "Training loss (for one batch) at step 30: 242.7548, Accuracy: 0.7218\n",
      "Training loss (for one batch) at step 40: 239.1879, Accuracy: 0.7420\n",
      "Training loss (for one batch) at step 50: 247.4594, Accuracy: 0.7574\n",
      "Training loss (for one batch) at step 60: 277.9070, Accuracy: 0.7661\n",
      "Training loss (for one batch) at step 70: 280.2439, Accuracy: 0.7508\n",
      "Training loss (for one batch) at step 80: 286.9670, Accuracy: 0.7343\n",
      "Training loss (for one batch) at step 90: 258.9827, Accuracy: 0.7306\n",
      "Training loss (for one batch) at step 100: 266.4514, Accuracy: 0.7347\n",
      "Training loss (for one batch) at step 110: 289.2812, Accuracy: 0.7361\n",
      "---- Training ----\n",
      "Training loss: 85.8109\n",
      "Training acc over epoch: 0.7357\n",
      "---- Validation ----\n",
      "Validation loss: 53.0536\n",
      "Validation acc: 0.6760\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 275.9476, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 316.4002, Accuracy: 0.6293\n",
      "Training loss (for one batch) at step 20: 271.4737, Accuracy: 0.6737\n",
      "Training loss (for one batch) at step 30: 246.5086, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 40: 242.0097, Accuracy: 0.7384\n",
      "Training loss (for one batch) at step 50: 252.3801, Accuracy: 0.7520\n",
      "Training loss (for one batch) at step 60: 274.9193, Accuracy: 0.7597\n",
      "Training loss (for one batch) at step 70: 279.5443, Accuracy: 0.7453\n",
      "Training loss (for one batch) at step 80: 282.4791, Accuracy: 0.7270\n",
      "Training loss (for one batch) at step 90: 235.7634, Accuracy: 0.7250\n",
      "Training loss (for one batch) at step 100: 266.2464, Accuracy: 0.7318\n",
      "Training loss (for one batch) at step 110: 269.5061, Accuracy: 0.7319\n",
      "---- Training ----\n",
      "Training loss: 90.0206\n",
      "Training acc over epoch: 0.7315\n",
      "---- Validation ----\n",
      "Validation loss: 31.6093\n",
      "Validation acc: 0.6762\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 315.9937, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 283.8582, Accuracy: 0.6271\n",
      "Training loss (for one batch) at step 20: 242.2581, Accuracy: 0.6760\n",
      "Training loss (for one batch) at step 30: 250.3015, Accuracy: 0.7210\n",
      "Training loss (for one batch) at step 40: 268.8276, Accuracy: 0.7426\n",
      "Training loss (for one batch) at step 50: 230.0823, Accuracy: 0.7570\n",
      "Training loss (for one batch) at step 60: 263.2577, Accuracy: 0.7650\n",
      "Training loss (for one batch) at step 70: 284.7201, Accuracy: 0.7472\n",
      "Training loss (for one batch) at step 80: 294.5785, Accuracy: 0.7320\n",
      "Training loss (for one batch) at step 90: 253.0169, Accuracy: 0.7296\n",
      "Training loss (for one batch) at step 100: 247.7315, Accuracy: 0.7335\n",
      "Training loss (for one batch) at step 110: 264.9911, Accuracy: 0.7346\n",
      "---- Training ----\n",
      "Training loss: 98.9186\n",
      "Training acc over epoch: 0.7336\n",
      "---- Validation ----\n",
      "Validation loss: 57.4414\n",
      "Validation acc: 0.6658\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 288.3197, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 285.6489, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 20: 254.0601, Accuracy: 0.6808\n",
      "Training loss (for one batch) at step 30: 267.4362, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 40: 255.0788, Accuracy: 0.7357\n",
      "Training loss (for one batch) at step 50: 247.9874, Accuracy: 0.7520\n",
      "Training loss (for one batch) at step 60: 259.1650, Accuracy: 0.7602\n",
      "Training loss (for one batch) at step 70: 275.8326, Accuracy: 0.7453\n",
      "Training loss (for one batch) at step 80: 275.8148, Accuracy: 0.7309\n",
      "Training loss (for one batch) at step 90: 262.4612, Accuracy: 0.7271\n",
      "Training loss (for one batch) at step 100: 230.9774, Accuracy: 0.7339\n",
      "Training loss (for one batch) at step 110: 290.1411, Accuracy: 0.7346\n",
      "---- Training ----\n",
      "Training loss: 88.9657\n",
      "Training acc over epoch: 0.7335\n",
      "---- Validation ----\n",
      "Validation loss: 54.8807\n",
      "Validation acc: 0.6620\n",
      "Time taken: 18.05s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 284.4004, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 297.3625, Accuracy: 0.6101\n",
      "Training loss (for one batch) at step 20: 248.6760, Accuracy: 0.6663\n",
      "Training loss (for one batch) at step 30: 252.2044, Accuracy: 0.7084\n",
      "Training loss (for one batch) at step 40: 246.2590, Accuracy: 0.7336\n",
      "Training loss (for one batch) at step 50: 252.0334, Accuracy: 0.7505\n",
      "Training loss (for one batch) at step 60: 257.9732, Accuracy: 0.7590\n",
      "Training loss (for one batch) at step 70: 277.9632, Accuracy: 0.7445\n",
      "Training loss (for one batch) at step 80: 267.6709, Accuracy: 0.7281\n",
      "Training loss (for one batch) at step 90: 245.4036, Accuracy: 0.7250\n",
      "Training loss (for one batch) at step 100: 246.8736, Accuracy: 0.7321\n",
      "Training loss (for one batch) at step 110: 261.0843, Accuracy: 0.7328\n",
      "---- Training ----\n",
      "Training loss: 76.3178\n",
      "Training acc over epoch: 0.7317\n",
      "---- Validation ----\n",
      "Validation loss: 39.2736\n",
      "Validation acc: 0.6668\n",
      "Time taken: 18.17s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 281.5793, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 281.2372, Accuracy: 0.6378\n",
      "Training loss (for one batch) at step 20: 233.0891, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 30: 243.9094, Accuracy: 0.7074\n",
      "Training loss (for one batch) at step 40: 255.3388, Accuracy: 0.7330\n",
      "Training loss (for one batch) at step 50: 239.1517, Accuracy: 0.7509\n",
      "Training loss (for one batch) at step 60: 269.0582, Accuracy: 0.7592\n",
      "Training loss (for one batch) at step 70: 277.3743, Accuracy: 0.7459\n",
      "Training loss (for one batch) at step 80: 306.1988, Accuracy: 0.7282\n",
      "Training loss (for one batch) at step 90: 262.6922, Accuracy: 0.7230\n",
      "Training loss (for one batch) at step 100: 248.2100, Accuracy: 0.7283\n",
      "Training loss (for one batch) at step 110: 260.7429, Accuracy: 0.7306\n",
      "---- Training ----\n",
      "Training loss: 86.9659\n",
      "Training acc over epoch: 0.7301\n",
      "---- Validation ----\n",
      "Validation loss: 47.8709\n",
      "Validation acc: 0.6746\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 279.2283, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 272.3054, Accuracy: 0.6257\n",
      "Training loss (for one batch) at step 20: 258.4987, Accuracy: 0.6715\n",
      "Training loss (for one batch) at step 30: 249.6547, Accuracy: 0.7094\n",
      "Training loss (for one batch) at step 40: 239.1669, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 50: 242.0776, Accuracy: 0.7495\n",
      "Training loss (for one batch) at step 60: 254.3877, Accuracy: 0.7609\n",
      "Training loss (for one batch) at step 70: 286.8487, Accuracy: 0.7487\n",
      "Training loss (for one batch) at step 80: 286.4264, Accuracy: 0.7306\n",
      "Training loss (for one batch) at step 90: 256.9532, Accuracy: 0.7257\n",
      "Training loss (for one batch) at step 100: 253.6686, Accuracy: 0.7317\n",
      "Training loss (for one batch) at step 110: 266.1665, Accuracy: 0.7345\n",
      "---- Training ----\n",
      "Training loss: 89.5884\n",
      "Training acc over epoch: 0.7328\n",
      "---- Validation ----\n",
      "Validation loss: 42.6229\n",
      "Validation acc: 0.6617\n",
      "Time taken: 17.91s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 292.3774, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 274.0599, Accuracy: 0.6307\n",
      "Training loss (for one batch) at step 20: 239.5379, Accuracy: 0.6778\n",
      "Training loss (for one batch) at step 30: 247.6785, Accuracy: 0.7177\n",
      "Training loss (for one batch) at step 40: 234.1958, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 50: 236.1231, Accuracy: 0.7589\n",
      "Training loss (for one batch) at step 60: 242.0237, Accuracy: 0.7638\n",
      "Training loss (for one batch) at step 70: 261.3029, Accuracy: 0.7492\n",
      "Training loss (for one batch) at step 80: 268.9589, Accuracy: 0.7329\n",
      "Training loss (for one batch) at step 90: 251.0399, Accuracy: 0.7301\n",
      "Training loss (for one batch) at step 100: 250.3916, Accuracy: 0.7346\n",
      "Training loss (for one batch) at step 110: 265.2814, Accuracy: 0.7369\n",
      "---- Training ----\n",
      "Training loss: 87.7785\n",
      "Training acc over epoch: 0.7354\n",
      "---- Validation ----\n",
      "Validation loss: 38.8373\n",
      "Validation acc: 0.6609\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 273.6293, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 267.8629, Accuracy: 0.6342\n",
      "Training loss (for one batch) at step 20: 247.8911, Accuracy: 0.6775\n",
      "Training loss (for one batch) at step 30: 216.8956, Accuracy: 0.7177\n",
      "Training loss (for one batch) at step 40: 244.6850, Accuracy: 0.7357\n",
      "Training loss (for one batch) at step 50: 223.3591, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 60: 250.9138, Accuracy: 0.7613\n",
      "Training loss (for one batch) at step 70: 263.4427, Accuracy: 0.7469\n",
      "Training loss (for one batch) at step 80: 291.9461, Accuracy: 0.7269\n",
      "Training loss (for one batch) at step 90: 250.9762, Accuracy: 0.7218\n",
      "Training loss (for one batch) at step 100: 239.8769, Accuracy: 0.7276\n",
      "Training loss (for one batch) at step 110: 267.8253, Accuracy: 0.7316\n",
      "---- Training ----\n",
      "Training loss: 95.0474\n",
      "Training acc over epoch: 0.7309\n",
      "---- Validation ----\n",
      "Validation loss: 50.8600\n",
      "Validation acc: 0.6609\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 265.7900, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 302.6430, Accuracy: 0.6385\n",
      "Training loss (for one batch) at step 20: 233.8444, Accuracy: 0.6793\n",
      "Training loss (for one batch) at step 30: 247.7783, Accuracy: 0.7122\n",
      "Training loss (for one batch) at step 40: 226.3882, Accuracy: 0.7357\n",
      "Training loss (for one batch) at step 50: 234.0573, Accuracy: 0.7517\n",
      "Training loss (for one batch) at step 60: 258.3963, Accuracy: 0.7610\n",
      "Training loss (for one batch) at step 70: 293.6039, Accuracy: 0.7464\n",
      "Training loss (for one batch) at step 80: 255.1743, Accuracy: 0.7278\n",
      "Training loss (for one batch) at step 90: 245.7472, Accuracy: 0.7251\n",
      "Training loss (for one batch) at step 100: 248.4350, Accuracy: 0.7325\n",
      "Training loss (for one batch) at step 110: 257.2317, Accuracy: 0.7342\n",
      "---- Training ----\n",
      "Training loss: 81.1973\n",
      "Training acc over epoch: 0.7322\n",
      "---- Validation ----\n",
      "Validation loss: 41.8759\n",
      "Validation acc: 0.6585\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 272.9426, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 277.0247, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 20: 244.0043, Accuracy: 0.6693\n",
      "Training loss (for one batch) at step 30: 234.7495, Accuracy: 0.7074\n",
      "Training loss (for one batch) at step 40: 232.0415, Accuracy: 0.7330\n",
      "Training loss (for one batch) at step 50: 255.5415, Accuracy: 0.7482\n",
      "Training loss (for one batch) at step 60: 246.0093, Accuracy: 0.7587\n",
      "Training loss (for one batch) at step 70: 266.0833, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 80: 277.6025, Accuracy: 0.7285\n",
      "Training loss (for one batch) at step 90: 253.2303, Accuracy: 0.7248\n",
      "Training loss (for one batch) at step 100: 243.0158, Accuracy: 0.7300\n",
      "Training loss (for one batch) at step 110: 259.2993, Accuracy: 0.7325\n",
      "---- Training ----\n",
      "Training loss: 77.6136\n",
      "Training acc over epoch: 0.7319\n",
      "---- Validation ----\n",
      "Validation loss: 32.7723\n",
      "Validation acc: 0.6677\n",
      "Time taken: 18.09s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 283.2306, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 261.9699, Accuracy: 0.6193\n",
      "Training loss (for one batch) at step 20: 230.8470, Accuracy: 0.6704\n",
      "Training loss (for one batch) at step 30: 235.7727, Accuracy: 0.7074\n",
      "Training loss (for one batch) at step 40: 225.4153, Accuracy: 0.7338\n",
      "Training loss (for one batch) at step 50: 235.1638, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 60: 259.9711, Accuracy: 0.7596\n",
      "Training loss (for one batch) at step 70: 264.7164, Accuracy: 0.7439\n",
      "Training loss (for one batch) at step 80: 261.2416, Accuracy: 0.7264\n",
      "Training loss (for one batch) at step 90: 240.2039, Accuracy: 0.7247\n",
      "Training loss (for one batch) at step 100: 249.1608, Accuracy: 0.7306\n",
      "Training loss (for one batch) at step 110: 254.8216, Accuracy: 0.7328\n",
      "---- Training ----\n",
      "Training loss: 81.0749\n",
      "Training acc over epoch: 0.7324\n",
      "---- Validation ----\n",
      "Validation loss: 61.8171\n",
      "Validation acc: 0.6655\n",
      "Time taken: 18.29s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 271.0247, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 273.5632, Accuracy: 0.6300\n",
      "Training loss (for one batch) at step 20: 253.3746, Accuracy: 0.6722\n",
      "Training loss (for one batch) at step 30: 228.6980, Accuracy: 0.7160\n",
      "Training loss (for one batch) at step 40: 244.3271, Accuracy: 0.7321\n",
      "Training loss (for one batch) at step 50: 243.7433, Accuracy: 0.7512\n",
      "Training loss (for one batch) at step 60: 257.5095, Accuracy: 0.7613\n",
      "Training loss (for one batch) at step 70: 279.3127, Accuracy: 0.7482\n",
      "Training loss (for one batch) at step 80: 263.1853, Accuracy: 0.7295\n",
      "Training loss (for one batch) at step 90: 237.5216, Accuracy: 0.7260\n",
      "Training loss (for one batch) at step 100: 227.7883, Accuracy: 0.7315\n",
      "Training loss (for one batch) at step 110: 256.4085, Accuracy: 0.7358\n",
      "---- Training ----\n",
      "Training loss: 84.6968\n",
      "Training acc over epoch: 0.7341\n",
      "---- Validation ----\n",
      "Validation loss: 32.1842\n",
      "Validation acc: 0.6701\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 275.7284, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 284.3518, Accuracy: 0.6300\n",
      "Training loss (for one batch) at step 20: 235.8542, Accuracy: 0.6763\n",
      "Training loss (for one batch) at step 30: 237.2238, Accuracy: 0.7147\n",
      "Training loss (for one batch) at step 40: 231.0125, Accuracy: 0.7380\n",
      "Training loss (for one batch) at step 50: 240.5186, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 60: 249.8587, Accuracy: 0.7609\n",
      "Training loss (for one batch) at step 70: 262.1255, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 80: 260.6807, Accuracy: 0.7289\n",
      "Training loss (for one batch) at step 90: 223.2831, Accuracy: 0.7253\n",
      "Training loss (for one batch) at step 100: 249.1278, Accuracy: 0.7316\n",
      "Training loss (for one batch) at step 110: 268.6996, Accuracy: 0.7335\n",
      "---- Training ----\n",
      "Training loss: 92.8359\n",
      "Training acc over epoch: 0.7327\n",
      "---- Validation ----\n",
      "Validation loss: 43.6640\n",
      "Validation acc: 0.6386\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 300.8167, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 276.9740, Accuracy: 0.6293\n",
      "Training loss (for one batch) at step 20: 245.8815, Accuracy: 0.6763\n",
      "Training loss (for one batch) at step 30: 241.2047, Accuracy: 0.7099\n",
      "Training loss (for one batch) at step 40: 251.4805, Accuracy: 0.7334\n",
      "Training loss (for one batch) at step 50: 244.5701, Accuracy: 0.7503\n",
      "Training loss (for one batch) at step 60: 253.8779, Accuracy: 0.7573\n",
      "Training loss (for one batch) at step 70: 249.6427, Accuracy: 0.7444\n",
      "Training loss (for one batch) at step 80: 251.6646, Accuracy: 0.7242\n",
      "Training loss (for one batch) at step 90: 258.5276, Accuracy: 0.7211\n",
      "Training loss (for one batch) at step 100: 244.7861, Accuracy: 0.7287\n",
      "Training loss (for one batch) at step 110: 234.4369, Accuracy: 0.7308\n",
      "---- Training ----\n",
      "Training loss: 73.6377\n",
      "Training acc over epoch: 0.7300\n",
      "---- Validation ----\n",
      "Validation loss: 35.7160\n",
      "Validation acc: 0.6491\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 300.9686, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 245.4866, Accuracy: 0.6349\n",
      "Training loss (for one batch) at step 20: 267.6062, Accuracy: 0.6745\n",
      "Training loss (for one batch) at step 30: 269.1158, Accuracy: 0.7089\n",
      "Training loss (for one batch) at step 40: 220.0534, Accuracy: 0.7357\n",
      "Training loss (for one batch) at step 50: 229.0455, Accuracy: 0.7509\n",
      "Training loss (for one batch) at step 60: 244.0394, Accuracy: 0.7599\n",
      "Training loss (for one batch) at step 70: 267.9663, Accuracy: 0.7436\n",
      "Training loss (for one batch) at step 80: 259.6389, Accuracy: 0.7271\n",
      "Training loss (for one batch) at step 90: 249.3568, Accuracy: 0.7243\n",
      "Training loss (for one batch) at step 100: 235.5813, Accuracy: 0.7309\n",
      "Training loss (for one batch) at step 110: 252.3000, Accuracy: 0.7335\n",
      "---- Training ----\n",
      "Training loss: 85.1070\n",
      "Training acc over epoch: 0.7320\n",
      "---- Validation ----\n",
      "Validation loss: 54.5662\n",
      "Validation acc: 0.6693\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 284.0314, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 282.5535, Accuracy: 0.6264\n",
      "Training loss (for one batch) at step 20: 255.6914, Accuracy: 0.6730\n",
      "Training loss (for one batch) at step 30: 217.1235, Accuracy: 0.7092\n",
      "Training loss (for one batch) at step 40: 238.4186, Accuracy: 0.7346\n",
      "Training loss (for one batch) at step 50: 237.6125, Accuracy: 0.7506\n",
      "Training loss (for one batch) at step 60: 238.4793, Accuracy: 0.7604\n",
      "Training loss (for one batch) at step 70: 276.1636, Accuracy: 0.7436\n",
      "Training loss (for one batch) at step 80: 268.8155, Accuracy: 0.7269\n",
      "Training loss (for one batch) at step 90: 252.4590, Accuracy: 0.7240\n",
      "Training loss (for one batch) at step 100: 244.4440, Accuracy: 0.7287\n",
      "Training loss (for one batch) at step 110: 230.2705, Accuracy: 0.7322\n",
      "---- Training ----\n",
      "Training loss: 91.7620\n",
      "Training acc over epoch: 0.7301\n",
      "---- Validation ----\n",
      "Validation loss: 48.5259\n",
      "Validation acc: 0.6679\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 286.5124, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 256.0102, Accuracy: 0.6286\n",
      "Training loss (for one batch) at step 20: 239.1698, Accuracy: 0.6745\n",
      "Training loss (for one batch) at step 30: 225.0746, Accuracy: 0.7145\n",
      "Training loss (for one batch) at step 40: 226.2247, Accuracy: 0.7355\n",
      "Training loss (for one batch) at step 50: 220.7017, Accuracy: 0.7503\n",
      "Training loss (for one batch) at step 60: 242.8078, Accuracy: 0.7573\n",
      "Training loss (for one batch) at step 70: 276.8190, Accuracy: 0.7421\n",
      "Training loss (for one batch) at step 80: 252.7070, Accuracy: 0.7249\n",
      "Training loss (for one batch) at step 90: 240.4081, Accuracy: 0.7231\n",
      "Training loss (for one batch) at step 100: 239.0366, Accuracy: 0.7294\n",
      "Training loss (for one batch) at step 110: 259.9868, Accuracy: 0.7317\n",
      "---- Training ----\n",
      "Training loss: 95.8440\n",
      "Training acc over epoch: 0.7308\n",
      "---- Validation ----\n",
      "Validation loss: 56.7288\n",
      "Validation acc: 0.6628\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 265.9460, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 252.9162, Accuracy: 0.6293\n",
      "Training loss (for one batch) at step 20: 232.7346, Accuracy: 0.6763\n",
      "Training loss (for one batch) at step 30: 242.3588, Accuracy: 0.7135\n",
      "Training loss (for one batch) at step 40: 222.1960, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 50: 243.4637, Accuracy: 0.7526\n",
      "Training loss (for one batch) at step 60: 255.4815, Accuracy: 0.7582\n",
      "Training loss (for one batch) at step 70: 250.1177, Accuracy: 0.7463\n",
      "Training loss (for one batch) at step 80: 268.7905, Accuracy: 0.7279\n",
      "Training loss (for one batch) at step 90: 246.9126, Accuracy: 0.7234\n",
      "Training loss (for one batch) at step 100: 235.7755, Accuracy: 0.7293\n",
      "Training loss (for one batch) at step 110: 248.6275, Accuracy: 0.7330\n",
      "---- Training ----\n",
      "Training loss: 81.5870\n",
      "Training acc over epoch: 0.7314\n",
      "---- Validation ----\n",
      "Validation loss: 43.3898\n",
      "Validation acc: 0.6776\n",
      "Time taken: 18.21s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 295.5214, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 270.9792, Accuracy: 0.6286\n",
      "Training loss (for one batch) at step 20: 244.3033, Accuracy: 0.6708\n",
      "Training loss (for one batch) at step 30: 216.4242, Accuracy: 0.7200\n",
      "Training loss (for one batch) at step 40: 226.1773, Accuracy: 0.7414\n",
      "Training loss (for one batch) at step 50: 221.8760, Accuracy: 0.7569\n",
      "Training loss (for one batch) at step 60: 244.1104, Accuracy: 0.7632\n",
      "Training loss (for one batch) at step 70: 279.2275, Accuracy: 0.7494\n",
      "Training loss (for one batch) at step 80: 255.7914, Accuracy: 0.7297\n",
      "Training loss (for one batch) at step 90: 236.9979, Accuracy: 0.7258\n",
      "Training loss (for one batch) at step 100: 239.3309, Accuracy: 0.7326\n",
      "Training loss (for one batch) at step 110: 259.8428, Accuracy: 0.7342\n",
      "---- Training ----\n",
      "Training loss: 83.1243\n",
      "Training acc over epoch: 0.7332\n",
      "---- Validation ----\n",
      "Validation loss: 52.9031\n",
      "Validation acc: 0.6736\n",
      "Time taken: 18.01s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 255.4656, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 253.8448, Accuracy: 0.6207\n",
      "Training loss (for one batch) at step 20: 237.5667, Accuracy: 0.6763\n",
      "Training loss (for one batch) at step 30: 237.5199, Accuracy: 0.7127\n",
      "Training loss (for one batch) at step 40: 236.9335, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 50: 220.6056, Accuracy: 0.7534\n",
      "Training loss (for one batch) at step 60: 240.4432, Accuracy: 0.7605\n",
      "Training loss (for one batch) at step 70: 243.6088, Accuracy: 0.7471\n",
      "Training loss (for one batch) at step 80: 256.9975, Accuracy: 0.7294\n",
      "Training loss (for one batch) at step 90: 229.6416, Accuracy: 0.7258\n",
      "Training loss (for one batch) at step 100: 256.0902, Accuracy: 0.7315\n",
      "Training loss (for one batch) at step 110: 258.0950, Accuracy: 0.7336\n",
      "---- Training ----\n",
      "Training loss: 80.3842\n",
      "Training acc over epoch: 0.7333\n",
      "---- Validation ----\n",
      "Validation loss: 47.2337\n",
      "Validation acc: 0.6668\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 278.7393, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 240.4793, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 20: 254.5616, Accuracy: 0.6756\n",
      "Training loss (for one batch) at step 30: 242.8815, Accuracy: 0.7107\n",
      "Training loss (for one batch) at step 40: 229.6297, Accuracy: 0.7365\n",
      "Training loss (for one batch) at step 50: 218.8743, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 60: 250.3670, Accuracy: 0.7597\n",
      "Training loss (for one batch) at step 70: 271.1956, Accuracy: 0.7461\n",
      "Training loss (for one batch) at step 80: 257.5021, Accuracy: 0.7263\n",
      "Training loss (for one batch) at step 90: 219.6667, Accuracy: 0.7238\n",
      "Training loss (for one batch) at step 100: 236.6393, Accuracy: 0.7299\n",
      "Training loss (for one batch) at step 110: 240.8331, Accuracy: 0.7322\n",
      "---- Training ----\n",
      "Training loss: 76.5462\n",
      "Training acc over epoch: 0.7315\n",
      "---- Validation ----\n",
      "Validation loss: 57.6024\n",
      "Validation acc: 0.6762\n",
      "Time taken: 18.08s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 250.5365, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 248.5230, Accuracy: 0.6222\n",
      "Training loss (for one batch) at step 20: 229.9518, Accuracy: 0.6767\n",
      "Training loss (for one batch) at step 30: 231.9921, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 40: 222.9786, Accuracy: 0.7372\n",
      "Training loss (for one batch) at step 50: 240.9843, Accuracy: 0.7534\n",
      "Training loss (for one batch) at step 60: 235.9996, Accuracy: 0.7628\n",
      "Training loss (for one batch) at step 70: 242.7824, Accuracy: 0.7488\n",
      "Training loss (for one batch) at step 80: 222.1483, Accuracy: 0.7282\n",
      "Training loss (for one batch) at step 90: 238.4035, Accuracy: 0.7242\n",
      "Training loss (for one batch) at step 100: 240.4674, Accuracy: 0.7313\n",
      "Training loss (for one batch) at step 110: 254.7252, Accuracy: 0.7324\n",
      "---- Training ----\n",
      "Training loss: 83.7804\n",
      "Training acc over epoch: 0.7318\n",
      "---- Validation ----\n",
      "Validation loss: 73.5069\n",
      "Validation acc: 0.6687\n",
      "Time taken: 18.11s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 263.5165, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 244.6272, Accuracy: 0.6278\n",
      "Training loss (for one batch) at step 20: 237.6569, Accuracy: 0.6711\n",
      "Training loss (for one batch) at step 30: 229.5714, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 40: 219.4683, Accuracy: 0.7376\n",
      "Training loss (for one batch) at step 50: 224.7771, Accuracy: 0.7541\n",
      "Training loss (for one batch) at step 60: 237.1477, Accuracy: 0.7609\n",
      "Training loss (for one batch) at step 70: 249.6894, Accuracy: 0.7469\n",
      "Training loss (for one batch) at step 80: 254.2575, Accuracy: 0.7277\n",
      "Training loss (for one batch) at step 90: 243.2227, Accuracy: 0.7248\n",
      "Training loss (for one batch) at step 100: 232.8040, Accuracy: 0.7310\n",
      "Training loss (for one batch) at step 110: 241.5923, Accuracy: 0.7335\n",
      "---- Training ----\n",
      "Training loss: 82.3483\n",
      "Training acc over epoch: 0.7320\n",
      "---- Validation ----\n",
      "Validation loss: 62.1258\n",
      "Validation acc: 0.6703\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 260.3711, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 246.4216, Accuracy: 0.6179\n",
      "Training loss (for one batch) at step 20: 230.3820, Accuracy: 0.6663\n",
      "Training loss (for one batch) at step 30: 234.9410, Accuracy: 0.7137\n",
      "Training loss (for one batch) at step 40: 234.2666, Accuracy: 0.7353\n",
      "Training loss (for one batch) at step 50: 228.3549, Accuracy: 0.7537\n",
      "Training loss (for one batch) at step 60: 228.0850, Accuracy: 0.7593\n",
      "Training loss (for one batch) at step 70: 257.6623, Accuracy: 0.7456\n",
      "Training loss (for one batch) at step 80: 270.1076, Accuracy: 0.7280\n",
      "Training loss (for one batch) at step 90: 238.4056, Accuracy: 0.7254\n",
      "Training loss (for one batch) at step 100: 233.0397, Accuracy: 0.7304\n",
      "Training loss (for one batch) at step 110: 245.0392, Accuracy: 0.7341\n",
      "---- Training ----\n",
      "Training loss: 78.5919\n",
      "Training acc over epoch: 0.7321\n",
      "---- Validation ----\n",
      "Validation loss: 43.7537\n",
      "Validation acc: 0.6698\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 251.1308, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 254.3543, Accuracy: 0.6364\n",
      "Training loss (for one batch) at step 20: 233.8594, Accuracy: 0.6696\n",
      "Training loss (for one batch) at step 30: 252.6022, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 40: 218.1259, Accuracy: 0.7380\n",
      "Training loss (for one batch) at step 50: 218.6711, Accuracy: 0.7511\n",
      "Training loss (for one batch) at step 60: 239.8845, Accuracy: 0.7574\n",
      "Training loss (for one batch) at step 70: 249.3430, Accuracy: 0.7452\n",
      "Training loss (for one batch) at step 80: 245.9227, Accuracy: 0.7291\n",
      "Training loss (for one batch) at step 90: 229.8108, Accuracy: 0.7245\n",
      "Training loss (for one batch) at step 100: 238.3540, Accuracy: 0.7294\n",
      "Training loss (for one batch) at step 110: 226.0252, Accuracy: 0.7316\n",
      "---- Training ----\n",
      "Training loss: 78.0108\n",
      "Training acc over epoch: 0.7306\n",
      "---- Validation ----\n",
      "Validation loss: 52.9006\n",
      "Validation acc: 0.6674\n",
      "Time taken: 18.10s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 257.5487, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 249.1975, Accuracy: 0.6392\n",
      "Training loss (for one batch) at step 20: 235.8337, Accuracy: 0.6745\n",
      "Training loss (for one batch) at step 30: 225.1318, Accuracy: 0.7157\n",
      "Training loss (for one batch) at step 40: 221.4979, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 50: 211.5620, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 60: 221.5833, Accuracy: 0.7636\n",
      "Training loss (for one batch) at step 70: 248.9301, Accuracy: 0.7501\n",
      "Training loss (for one batch) at step 80: 255.5520, Accuracy: 0.7313\n",
      "Training loss (for one batch) at step 90: 222.5979, Accuracy: 0.7272\n",
      "Training loss (for one batch) at step 100: 235.8802, Accuracy: 0.7320\n",
      "Training loss (for one batch) at step 110: 234.4321, Accuracy: 0.7342\n",
      "---- Training ----\n",
      "Training loss: 89.8941\n",
      "Training acc over epoch: 0.7334\n",
      "---- Validation ----\n",
      "Validation loss: 40.8871\n",
      "Validation acc: 0.6612\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 261.8181, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 254.7839, Accuracy: 0.6349\n",
      "Training loss (for one batch) at step 20: 242.0153, Accuracy: 0.6737\n",
      "Training loss (for one batch) at step 30: 231.9627, Accuracy: 0.7160\n",
      "Training loss (for one batch) at step 40: 222.4599, Accuracy: 0.7378\n",
      "Training loss (for one batch) at step 50: 233.8620, Accuracy: 0.7544\n",
      "Training loss (for one batch) at step 60: 221.6581, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 70: 247.4131, Accuracy: 0.7453\n",
      "Training loss (for one batch) at step 80: 258.0905, Accuracy: 0.7279\n",
      "Training loss (for one batch) at step 90: 220.3012, Accuracy: 0.7264\n",
      "Training loss (for one batch) at step 100: 244.7955, Accuracy: 0.7330\n",
      "Training loss (for one batch) at step 110: 240.2176, Accuracy: 0.7353\n",
      "---- Training ----\n",
      "Training loss: 85.4214\n",
      "Training acc over epoch: 0.7328\n",
      "---- Validation ----\n",
      "Validation loss: 57.6267\n",
      "Validation acc: 0.6709\n",
      "Time taken: 18.04s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 253.5089, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 257.6012, Accuracy: 0.6392\n",
      "Training loss (for one batch) at step 20: 235.2233, Accuracy: 0.6782\n",
      "Training loss (for one batch) at step 30: 231.9858, Accuracy: 0.7137\n",
      "Training loss (for one batch) at step 40: 229.1714, Accuracy: 0.7384\n",
      "Training loss (for one batch) at step 50: 227.1404, Accuracy: 0.7560\n",
      "Training loss (for one batch) at step 60: 240.8199, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 70: 248.7010, Accuracy: 0.7448\n",
      "Training loss (for one batch) at step 80: 257.7169, Accuracy: 0.7284\n",
      "Training loss (for one batch) at step 90: 231.8427, Accuracy: 0.7252\n",
      "Training loss (for one batch) at step 100: 246.5692, Accuracy: 0.7335\n",
      "Training loss (for one batch) at step 110: 242.7706, Accuracy: 0.7350\n",
      "---- Training ----\n",
      "Training loss: 86.6157\n",
      "Training acc over epoch: 0.7331\n",
      "---- Validation ----\n",
      "Validation loss: 42.3762\n",
      "Validation acc: 0.6599\n",
      "Time taken: 17.90s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 252.2740, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 240.9800, Accuracy: 0.6214\n",
      "Training loss (for one batch) at step 20: 237.5821, Accuracy: 0.6752\n",
      "Training loss (for one batch) at step 30: 245.4144, Accuracy: 0.7160\n",
      "Training loss (for one batch) at step 40: 213.0869, Accuracy: 0.7369\n",
      "Training loss (for one batch) at step 50: 226.0546, Accuracy: 0.7531\n",
      "Training loss (for one batch) at step 60: 225.6070, Accuracy: 0.7611\n",
      "Training loss (for one batch) at step 70: 247.9147, Accuracy: 0.7455\n",
      "Training loss (for one batch) at step 80: 259.5421, Accuracy: 0.7251\n",
      "Training loss (for one batch) at step 90: 224.6693, Accuracy: 0.7234\n",
      "Training loss (for one batch) at step 100: 235.7280, Accuracy: 0.7295\n",
      "Training loss (for one batch) at step 110: 236.2591, Accuracy: 0.7329\n",
      "---- Training ----\n",
      "Training loss: 75.8060\n",
      "Training acc over epoch: 0.7300\n",
      "---- Validation ----\n",
      "Validation loss: 49.4962\n",
      "Validation acc: 0.6690\n",
      "Time taken: 18.13s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 242.8283, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 269.7356, Accuracy: 0.6207\n",
      "Training loss (for one batch) at step 20: 259.9406, Accuracy: 0.6674\n",
      "Training loss (for one batch) at step 30: 221.7243, Accuracy: 0.7132\n",
      "Training loss (for one batch) at step 40: 237.6890, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 50: 226.3120, Accuracy: 0.7547\n",
      "Training loss (for one batch) at step 60: 238.0488, Accuracy: 0.7609\n",
      "Training loss (for one batch) at step 70: 263.9034, Accuracy: 0.7455\n",
      "Training loss (for one batch) at step 80: 231.6712, Accuracy: 0.7280\n",
      "Training loss (for one batch) at step 90: 229.9610, Accuracy: 0.7239\n",
      "Training loss (for one batch) at step 100: 238.1413, Accuracy: 0.7298\n",
      "Training loss (for one batch) at step 110: 229.2983, Accuracy: 0.7325\n",
      "---- Training ----\n",
      "Training loss: 72.5073\n",
      "Training acc over epoch: 0.7313\n",
      "---- Validation ----\n",
      "Validation loss: 32.4702\n",
      "Validation acc: 0.6768\n",
      "Time taken: 18.22s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 262.2547, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 244.3636, Accuracy: 0.6229\n",
      "Training loss (for one batch) at step 20: 236.5649, Accuracy: 0.6674\n",
      "Training loss (for one batch) at step 30: 225.6725, Accuracy: 0.7087\n",
      "Training loss (for one batch) at step 40: 211.2729, Accuracy: 0.7340\n",
      "Training loss (for one batch) at step 50: 226.6083, Accuracy: 0.7511\n",
      "Training loss (for one batch) at step 60: 224.7005, Accuracy: 0.7579\n",
      "Training loss (for one batch) at step 70: 240.1102, Accuracy: 0.7444\n",
      "Training loss (for one batch) at step 80: 242.8496, Accuracy: 0.7259\n",
      "Training loss (for one batch) at step 90: 235.6624, Accuracy: 0.7236\n",
      "Training loss (for one batch) at step 100: 234.0335, Accuracy: 0.7296\n",
      "Training loss (for one batch) at step 110: 220.4924, Accuracy: 0.7322\n",
      "---- Training ----\n",
      "Training loss: 92.9383\n",
      "Training acc over epoch: 0.7313\n",
      "---- Validation ----\n",
      "Validation loss: 72.1562\n",
      "Validation acc: 0.6625\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 255.0590, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 244.2754, Accuracy: 0.6378\n",
      "Training loss (for one batch) at step 20: 213.2928, Accuracy: 0.6823\n",
      "Training loss (for one batch) at step 30: 219.6344, Accuracy: 0.7165\n",
      "Training loss (for one batch) at step 40: 237.7887, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 50: 234.1381, Accuracy: 0.7552\n",
      "Training loss (for one batch) at step 60: 241.6090, Accuracy: 0.7627\n",
      "Training loss (for one batch) at step 70: 250.4821, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 80: 239.8211, Accuracy: 0.7267\n",
      "Training loss (for one batch) at step 90: 220.9405, Accuracy: 0.7226\n",
      "Training loss (for one batch) at step 100: 225.2688, Accuracy: 0.7293\n",
      "Training loss (for one batch) at step 110: 235.8567, Accuracy: 0.7326\n",
      "---- Training ----\n",
      "Training loss: 87.1845\n",
      "Training acc over epoch: 0.7313\n",
      "---- Validation ----\n",
      "Validation loss: 48.9565\n",
      "Validation acc: 0.6819\n",
      "Time taken: 18.11s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 239.3409, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 237.6349, Accuracy: 0.6122\n",
      "Training loss (for one batch) at step 20: 231.3270, Accuracy: 0.6663\n",
      "Training loss (for one batch) at step 30: 238.2186, Accuracy: 0.7102\n",
      "Training loss (for one batch) at step 40: 217.8534, Accuracy: 0.7372\n",
      "Training loss (for one batch) at step 50: 229.4549, Accuracy: 0.7523\n",
      "Training loss (for one batch) at step 60: 240.2278, Accuracy: 0.7579\n",
      "Training loss (for one batch) at step 70: 233.9737, Accuracy: 0.7439\n",
      "Training loss (for one batch) at step 80: 243.5345, Accuracy: 0.7253\n",
      "Training loss (for one batch) at step 90: 234.6728, Accuracy: 0.7204\n",
      "Training loss (for one batch) at step 100: 253.4485, Accuracy: 0.7271\n",
      "Training loss (for one batch) at step 110: 231.1891, Accuracy: 0.7294\n",
      "---- Training ----\n",
      "Training loss: 77.9334\n",
      "Training acc over epoch: 0.7286\n",
      "---- Validation ----\n",
      "Validation loss: 40.7395\n",
      "Validation acc: 0.6781\n",
      "Time taken: 17.95s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 253.9685, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 235.2101, Accuracy: 0.6335\n",
      "Training loss (for one batch) at step 20: 232.8491, Accuracy: 0.6752\n",
      "Training loss (for one batch) at step 30: 227.8366, Accuracy: 0.7160\n",
      "Training loss (for one batch) at step 40: 210.8979, Accuracy: 0.7391\n",
      "Training loss (for one batch) at step 50: 231.2071, Accuracy: 0.7540\n",
      "Training loss (for one batch) at step 60: 230.0654, Accuracy: 0.7627\n",
      "Training loss (for one batch) at step 70: 244.2787, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 80: 244.5470, Accuracy: 0.7283\n",
      "Training loss (for one batch) at step 90: 225.2182, Accuracy: 0.7257\n",
      "Training loss (for one batch) at step 100: 228.6758, Accuracy: 0.7299\n",
      "Training loss (for one batch) at step 110: 234.2062, Accuracy: 0.7313\n",
      "---- Training ----\n",
      "Training loss: 71.3409\n",
      "Training acc over epoch: 0.7306\n",
      "---- Validation ----\n",
      "Validation loss: 38.7874\n",
      "Validation acc: 0.6738\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 252.4199, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 227.1496, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 20: 225.8530, Accuracy: 0.6689\n",
      "Training loss (for one batch) at step 30: 240.3926, Accuracy: 0.7132\n",
      "Training loss (for one batch) at step 40: 218.6956, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 50: 233.3128, Accuracy: 0.7520\n",
      "Training loss (for one batch) at step 60: 230.5779, Accuracy: 0.7608\n",
      "Training loss (for one batch) at step 70: 242.4268, Accuracy: 0.7465\n",
      "Training loss (for one batch) at step 80: 257.8019, Accuracy: 0.7268\n",
      "Training loss (for one batch) at step 90: 230.4675, Accuracy: 0.7236\n",
      "Training loss (for one batch) at step 100: 249.6237, Accuracy: 0.7300\n",
      "Training loss (for one batch) at step 110: 245.1082, Accuracy: 0.7336\n",
      "---- Training ----\n",
      "Training loss: 85.1242\n",
      "Training acc over epoch: 0.7315\n",
      "---- Validation ----\n",
      "Validation loss: 60.2951\n",
      "Validation acc: 0.6730\n",
      "Time taken: 17.87s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 258.7342, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 254.3227, Accuracy: 0.6243\n",
      "Training loss (for one batch) at step 20: 218.8265, Accuracy: 0.6708\n",
      "Training loss (for one batch) at step 30: 211.8339, Accuracy: 0.7180\n",
      "Training loss (for one batch) at step 40: 226.4769, Accuracy: 0.7414\n",
      "Training loss (for one batch) at step 50: 224.6130, Accuracy: 0.7572\n",
      "Training loss (for one batch) at step 60: 211.2079, Accuracy: 0.7655\n",
      "Training loss (for one batch) at step 70: 232.8597, Accuracy: 0.7480\n",
      "Training loss (for one batch) at step 80: 229.4329, Accuracy: 0.7283\n",
      "Training loss (for one batch) at step 90: 221.6819, Accuracy: 0.7239\n",
      "Training loss (for one batch) at step 100: 233.1702, Accuracy: 0.7310\n",
      "Training loss (for one batch) at step 110: 249.6596, Accuracy: 0.7323\n",
      "---- Training ----\n",
      "Training loss: 76.7134\n",
      "Training acc over epoch: 0.7317\n",
      "---- Validation ----\n",
      "Validation loss: 62.8628\n",
      "Validation acc: 0.6728\n",
      "Time taken: 18.12s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 242.2649, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 249.0592, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 20: 218.7419, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 30: 215.1878, Accuracy: 0.7162\n",
      "Training loss (for one batch) at step 40: 234.6906, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 50: 224.3027, Accuracy: 0.7517\n",
      "Training loss (for one batch) at step 60: 238.1091, Accuracy: 0.7606\n",
      "Training loss (for one batch) at step 70: 256.0180, Accuracy: 0.7449\n",
      "Training loss (for one batch) at step 80: 248.7139, Accuracy: 0.7252\n",
      "Training loss (for one batch) at step 90: 222.0206, Accuracy: 0.7219\n",
      "Training loss (for one batch) at step 100: 240.2920, Accuracy: 0.7291\n",
      "Training loss (for one batch) at step 110: 225.7059, Accuracy: 0.7320\n",
      "---- Training ----\n",
      "Training loss: 85.0157\n",
      "Training acc over epoch: 0.7305\n",
      "---- Validation ----\n",
      "Validation loss: 38.7285\n",
      "Validation acc: 0.6814\n",
      "Time taken: 17.92s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 251.1801, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 254.9698, Accuracy: 0.6357\n",
      "Training loss (for one batch) at step 20: 219.2324, Accuracy: 0.6752\n",
      "Training loss (for one batch) at step 30: 226.4354, Accuracy: 0.7142\n",
      "Training loss (for one batch) at step 40: 211.0334, Accuracy: 0.7372\n",
      "Training loss (for one batch) at step 50: 212.7254, Accuracy: 0.7531\n",
      "Training loss (for one batch) at step 60: 219.3351, Accuracy: 0.7626\n",
      "Training loss (for one batch) at step 70: 240.6270, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 80: 231.2095, Accuracy: 0.7273\n",
      "Training loss (for one batch) at step 90: 248.3152, Accuracy: 0.7242\n",
      "Training loss (for one batch) at step 100: 225.0723, Accuracy: 0.7299\n",
      "Training loss (for one batch) at step 110: 226.3528, Accuracy: 0.7318\n",
      "---- Training ----\n",
      "Training loss: 74.9725\n",
      "Training acc over epoch: 0.7309\n",
      "---- Validation ----\n",
      "Validation loss: 44.1211\n",
      "Validation acc: 0.6910\n",
      "Time taken: 17.98s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 234.6904, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 234.2033, Accuracy: 0.6186\n",
      "Training loss (for one batch) at step 20: 212.5080, Accuracy: 0.6659\n",
      "Training loss (for one batch) at step 30: 210.5731, Accuracy: 0.7127\n",
      "Training loss (for one batch) at step 40: 232.2347, Accuracy: 0.7376\n",
      "Training loss (for one batch) at step 50: 219.4964, Accuracy: 0.7544\n",
      "Training loss (for one batch) at step 60: 230.1429, Accuracy: 0.7627\n",
      "Training loss (for one batch) at step 70: 242.8070, Accuracy: 0.7463\n",
      "Training loss (for one batch) at step 80: 245.6870, Accuracy: 0.7254\n",
      "Training loss (for one batch) at step 90: 231.9730, Accuracy: 0.7227\n",
      "Training loss (for one batch) at step 100: 228.2549, Accuracy: 0.7289\n",
      "Training loss (for one batch) at step 110: 227.4865, Accuracy: 0.7313\n",
      "---- Training ----\n",
      "Training loss: 73.3815\n",
      "Training acc over epoch: 0.7315\n",
      "---- Validation ----\n",
      "Validation loss: 59.7303\n",
      "Validation acc: 0.6599\n",
      "Time taken: 18.32s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 268.1878, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 260.6443, Accuracy: 0.6108\n",
      "Training loss (for one batch) at step 20: 235.2707, Accuracy: 0.6700\n",
      "Training loss (for one batch) at step 30: 223.1514, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 40: 229.7003, Accuracy: 0.7330\n",
      "Training loss (for one batch) at step 50: 215.0521, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 60: 231.0959, Accuracy: 0.7595\n",
      "Training loss (for one batch) at step 70: 232.9957, Accuracy: 0.7416\n",
      "Training loss (for one batch) at step 80: 238.8297, Accuracy: 0.7255\n",
      "Training loss (for one batch) at step 90: 220.2429, Accuracy: 0.7251\n",
      "Training loss (for one batch) at step 100: 222.6415, Accuracy: 0.7311\n",
      "Training loss (for one batch) at step 110: 232.4642, Accuracy: 0.7333\n",
      "---- Training ----\n",
      "Training loss: 91.6348\n",
      "Training acc over epoch: 0.7315\n",
      "---- Validation ----\n",
      "Validation loss: 47.7246\n",
      "Validation acc: 0.6647\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 253.5853, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 247.5913, Accuracy: 0.6222\n",
      "Training loss (for one batch) at step 20: 248.7052, Accuracy: 0.6685\n",
      "Training loss (for one batch) at step 30: 231.5830, Accuracy: 0.7122\n",
      "Training loss (for one batch) at step 40: 217.4616, Accuracy: 0.7391\n",
      "Training loss (for one batch) at step 50: 217.1283, Accuracy: 0.7538\n",
      "Training loss (for one batch) at step 60: 225.8854, Accuracy: 0.7604\n",
      "Training loss (for one batch) at step 70: 249.9147, Accuracy: 0.7445\n",
      "Training loss (for one batch) at step 80: 234.8518, Accuracy: 0.7267\n",
      "Training loss (for one batch) at step 90: 224.1717, Accuracy: 0.7232\n",
      "Training loss (for one batch) at step 100: 222.8668, Accuracy: 0.7288\n",
      "Training loss (for one batch) at step 110: 223.5787, Accuracy: 0.7316\n",
      "---- Training ----\n",
      "Training loss: 86.2004\n",
      "Training acc over epoch: 0.7307\n",
      "---- Validation ----\n",
      "Validation loss: 44.8596\n",
      "Validation acc: 0.6762\n",
      "Time taken: 18.14s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 253.2027, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 236.0451, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 20: 220.9430, Accuracy: 0.6715\n",
      "Training loss (for one batch) at step 30: 215.7328, Accuracy: 0.7104\n",
      "Training loss (for one batch) at step 40: 214.0647, Accuracy: 0.7332\n",
      "Training loss (for one batch) at step 50: 229.3876, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 60: 224.3808, Accuracy: 0.7624\n",
      "Training loss (for one batch) at step 70: 235.3882, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 80: 255.1250, Accuracy: 0.7291\n",
      "Training loss (for one batch) at step 90: 227.3145, Accuracy: 0.7248\n",
      "Training loss (for one batch) at step 100: 238.9406, Accuracy: 0.7326\n",
      "Training loss (for one batch) at step 110: 235.1653, Accuracy: 0.7337\n",
      "---- Training ----\n",
      "Training loss: 88.4425\n",
      "Training acc over epoch: 0.7333\n",
      "---- Validation ----\n",
      "Validation loss: 57.3301\n",
      "Validation acc: 0.6789\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 229.3423, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 271.4144, Accuracy: 0.6243\n",
      "Training loss (for one batch) at step 20: 230.9054, Accuracy: 0.6637\n",
      "Training loss (for one batch) at step 30: 224.2231, Accuracy: 0.7104\n",
      "Training loss (for one batch) at step 40: 231.6387, Accuracy: 0.7369\n",
      "Training loss (for one batch) at step 50: 216.1450, Accuracy: 0.7531\n",
      "Training loss (for one batch) at step 60: 230.7360, Accuracy: 0.7610\n",
      "Training loss (for one batch) at step 70: 233.4146, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 80: 273.3968, Accuracy: 0.7251\n",
      "Training loss (for one batch) at step 90: 225.6297, Accuracy: 0.7224\n",
      "Training loss (for one batch) at step 100: 224.5742, Accuracy: 0.7290\n",
      "Training loss (for one batch) at step 110: 229.9035, Accuracy: 0.7316\n",
      "---- Training ----\n",
      "Training loss: 79.5963\n",
      "Training acc over epoch: 0.7301\n",
      "---- Validation ----\n",
      "Validation loss: 41.8764\n",
      "Validation acc: 0.6932\n",
      "Time taken: 18.09s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 242.8954, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 231.6641, Accuracy: 0.6136\n",
      "Training loss (for one batch) at step 20: 228.5266, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 30: 223.4229, Accuracy: 0.7084\n",
      "Training loss (for one batch) at step 40: 217.5463, Accuracy: 0.7359\n",
      "Training loss (for one batch) at step 50: 204.8312, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 60: 221.4799, Accuracy: 0.7608\n",
      "Training loss (for one batch) at step 70: 263.7350, Accuracy: 0.7456\n",
      "Training loss (for one batch) at step 80: 222.9355, Accuracy: 0.7249\n",
      "Training loss (for one batch) at step 90: 222.0518, Accuracy: 0.7227\n",
      "Training loss (for one batch) at step 100: 228.0436, Accuracy: 0.7283\n",
      "Training loss (for one batch) at step 110: 232.3649, Accuracy: 0.7313\n",
      "---- Training ----\n",
      "Training loss: 79.1625\n",
      "Training acc over epoch: 0.7301\n",
      "---- Validation ----\n",
      "Validation loss: 39.7001\n",
      "Validation acc: 0.6814\n",
      "Time taken: 17.99s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 246.4917, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 226.8358, Accuracy: 0.6151\n",
      "Training loss (for one batch) at step 20: 228.4291, Accuracy: 0.6722\n",
      "Training loss (for one batch) at step 30: 210.1068, Accuracy: 0.7140\n",
      "Training loss (for one batch) at step 40: 234.6917, Accuracy: 0.7353\n",
      "Training loss (for one batch) at step 50: 233.7333, Accuracy: 0.7511\n",
      "Training loss (for one batch) at step 60: 227.5862, Accuracy: 0.7585\n",
      "Training loss (for one batch) at step 70: 231.1373, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 80: 233.6299, Accuracy: 0.7263\n",
      "Training loss (for one batch) at step 90: 230.8956, Accuracy: 0.7217\n",
      "Training loss (for one batch) at step 100: 219.1798, Accuracy: 0.7285\n",
      "Training loss (for one batch) at step 110: 248.9324, Accuracy: 0.7311\n",
      "---- Training ----\n",
      "Training loss: 76.5206\n",
      "Training acc over epoch: 0.7294\n",
      "---- Validation ----\n",
      "Validation loss: 50.8207\n",
      "Validation acc: 0.6784\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 242.6481, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 240.4661, Accuracy: 0.6378\n",
      "Training loss (for one batch) at step 20: 227.9482, Accuracy: 0.6741\n",
      "Training loss (for one batch) at step 30: 220.5311, Accuracy: 0.7142\n",
      "Training loss (for one batch) at step 40: 218.6467, Accuracy: 0.7367\n",
      "Training loss (for one batch) at step 50: 212.9061, Accuracy: 0.7523\n",
      "Training loss (for one batch) at step 60: 212.2514, Accuracy: 0.7615\n",
      "Training loss (for one batch) at step 70: 240.0105, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 80: 219.4086, Accuracy: 0.7293\n",
      "Training loss (for one batch) at step 90: 236.6530, Accuracy: 0.7275\n",
      "Training loss (for one batch) at step 100: 224.7305, Accuracy: 0.7317\n",
      "Training loss (for one batch) at step 110: 221.2630, Accuracy: 0.7342\n",
      "---- Training ----\n",
      "Training loss: 78.2022\n",
      "Training acc over epoch: 0.7330\n",
      "---- Validation ----\n",
      "Validation loss: 48.2083\n",
      "Validation acc: 0.6642\n",
      "Time taken: 18.13s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 262.4678, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 222.7094, Accuracy: 0.6257\n",
      "Training loss (for one batch) at step 20: 230.2204, Accuracy: 0.6737\n",
      "Training loss (for one batch) at step 30: 219.0668, Accuracy: 0.7130\n",
      "Training loss (for one batch) at step 40: 207.7947, Accuracy: 0.7348\n",
      "Training loss (for one batch) at step 50: 224.2217, Accuracy: 0.7534\n",
      "Training loss (for one batch) at step 60: 217.6266, Accuracy: 0.7604\n",
      "Training loss (for one batch) at step 70: 232.0130, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 80: 238.5969, Accuracy: 0.7262\n",
      "Training loss (for one batch) at step 90: 224.5404, Accuracy: 0.7235\n",
      "Training loss (for one batch) at step 100: 227.8474, Accuracy: 0.7290\n",
      "Training loss (for one batch) at step 110: 230.0577, Accuracy: 0.7323\n",
      "---- Training ----\n",
      "Training loss: 72.7268\n",
      "Training acc over epoch: 0.7307\n",
      "---- Validation ----\n",
      "Validation loss: 54.3717\n",
      "Validation acc: 0.6999\n",
      "Time taken: 18.13s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 231.6360, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 235.0018, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 20: 215.5408, Accuracy: 0.6741\n",
      "Training loss (for one batch) at step 30: 225.6934, Accuracy: 0.7087\n",
      "Training loss (for one batch) at step 40: 213.6836, Accuracy: 0.7323\n",
      "Training loss (for one batch) at step 50: 221.2470, Accuracy: 0.7492\n",
      "Training loss (for one batch) at step 60: 228.0471, Accuracy: 0.7586\n",
      "Training loss (for one batch) at step 70: 235.4015, Accuracy: 0.7444\n",
      "Training loss (for one batch) at step 80: 240.9068, Accuracy: 0.7251\n",
      "Training loss (for one batch) at step 90: 223.4795, Accuracy: 0.7227\n",
      "Training loss (for one batch) at step 100: 229.0870, Accuracy: 0.7292\n",
      "Training loss (for one batch) at step 110: 227.1591, Accuracy: 0.7314\n",
      "---- Training ----\n",
      "Training loss: 68.3195\n",
      "Training acc over epoch: 0.7293\n",
      "---- Validation ----\n",
      "Validation loss: 67.0270\n",
      "Validation acc: 0.6814\n",
      "Time taken: 18.05s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 248.4479, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 236.9787, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 20: 217.1707, Accuracy: 0.6752\n",
      "Training loss (for one batch) at step 30: 222.5978, Accuracy: 0.7132\n",
      "Training loss (for one batch) at step 40: 223.6655, Accuracy: 0.7365\n",
      "Training loss (for one batch) at step 50: 229.8393, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 60: 233.3759, Accuracy: 0.7618\n",
      "Training loss (for one batch) at step 70: 235.0767, Accuracy: 0.7438\n",
      "Training loss (for one batch) at step 80: 251.4174, Accuracy: 0.7283\n",
      "Training loss (for one batch) at step 90: 221.6875, Accuracy: 0.7235\n",
      "Training loss (for one batch) at step 100: 238.1596, Accuracy: 0.7294\n",
      "Training loss (for one batch) at step 110: 224.3497, Accuracy: 0.7315\n",
      "---- Training ----\n",
      "Training loss: 81.6571\n",
      "Training acc over epoch: 0.7297\n",
      "---- Validation ----\n",
      "Validation loss: 72.3771\n",
      "Validation acc: 0.6671\n",
      "Time taken: 17.88s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 244.0843, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 238.5942, Accuracy: 0.6065\n",
      "Training loss (for one batch) at step 20: 206.6101, Accuracy: 0.6685\n",
      "Training loss (for one batch) at step 30: 218.8264, Accuracy: 0.7127\n",
      "Training loss (for one batch) at step 40: 208.2935, Accuracy: 0.7369\n",
      "Training loss (for one batch) at step 50: 216.8669, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 60: 228.0065, Accuracy: 0.7609\n",
      "Training loss (for one batch) at step 70: 222.0930, Accuracy: 0.7444\n",
      "Training loss (for one batch) at step 80: 218.1937, Accuracy: 0.7241\n",
      "Training loss (for one batch) at step 90: 220.9435, Accuracy: 0.7229\n",
      "Training loss (for one batch) at step 100: 210.8039, Accuracy: 0.7289\n",
      "Training loss (for one batch) at step 110: 243.5126, Accuracy: 0.7314\n",
      "---- Training ----\n",
      "Training loss: 79.9739\n",
      "Training acc over epoch: 0.7305\n",
      "---- Validation ----\n",
      "Validation loss: 52.5659\n",
      "Validation acc: 0.6805\n",
      "Time taken: 20.18s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 247.3918, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 234.8470, Accuracy: 0.6335\n",
      "Training loss (for one batch) at step 20: 225.4023, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 30: 224.0423, Accuracy: 0.7099\n",
      "Training loss (for one batch) at step 40: 220.4700, Accuracy: 0.7353\n",
      "Training loss (for one batch) at step 50: 248.4070, Accuracy: 0.7547\n",
      "Training loss (for one batch) at step 60: 227.6899, Accuracy: 0.7637\n",
      "Training loss (for one batch) at step 70: 243.6720, Accuracy: 0.7464\n",
      "Training loss (for one batch) at step 80: 237.5714, Accuracy: 0.7278\n",
      "Training loss (for one batch) at step 90: 215.4023, Accuracy: 0.7241\n",
      "Training loss (for one batch) at step 100: 242.2237, Accuracy: 0.7292\n",
      "Training loss (for one batch) at step 110: 223.4509, Accuracy: 0.7311\n",
      "---- Training ----\n",
      "Training loss: 79.6791\n",
      "Training acc over epoch: 0.7307\n",
      "---- Validation ----\n",
      "Validation loss: 46.7646\n",
      "Validation acc: 0.6701\n",
      "Time taken: 20.11s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 229.5238, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 226.5451, Accuracy: 0.6222\n",
      "Training loss (for one batch) at step 20: 220.4794, Accuracy: 0.6771\n",
      "Training loss (for one batch) at step 30: 211.9801, Accuracy: 0.7127\n",
      "Training loss (for one batch) at step 40: 207.3974, Accuracy: 0.7332\n",
      "Training loss (for one batch) at step 50: 208.2362, Accuracy: 0.7512\n",
      "Training loss (for one batch) at step 60: 230.0162, Accuracy: 0.7581\n",
      "Training loss (for one batch) at step 70: 240.4108, Accuracy: 0.7444\n",
      "Training loss (for one batch) at step 80: 231.0485, Accuracy: 0.7251\n",
      "Training loss (for one batch) at step 90: 214.3092, Accuracy: 0.7224\n",
      "Training loss (for one batch) at step 100: 222.4142, Accuracy: 0.7290\n",
      "Training loss (for one batch) at step 110: 233.6472, Accuracy: 0.7304\n",
      "---- Training ----\n",
      "Training loss: 82.8354\n",
      "Training acc over epoch: 0.7299\n",
      "---- Validation ----\n",
      "Validation loss: 57.0344\n",
      "Validation acc: 0.6574\n",
      "Time taken: 17.93s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 232.2084, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 217.6201, Accuracy: 0.6115\n",
      "Training loss (for one batch) at step 20: 227.8740, Accuracy: 0.6711\n",
      "Training loss (for one batch) at step 30: 229.5839, Accuracy: 0.7135\n",
      "Training loss (for one batch) at step 40: 219.4283, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 50: 213.1450, Accuracy: 0.7535\n",
      "Training loss (for one batch) at step 60: 213.3892, Accuracy: 0.7608\n",
      "Training loss (for one batch) at step 70: 251.3916, Accuracy: 0.7449\n",
      "Training loss (for one batch) at step 80: 236.3594, Accuracy: 0.7264\n",
      "Training loss (for one batch) at step 90: 227.6247, Accuracy: 0.7236\n",
      "Training loss (for one batch) at step 100: 235.2567, Accuracy: 0.7292\n",
      "Training loss (for one batch) at step 110: 223.5202, Accuracy: 0.7328\n",
      "---- Training ----\n",
      "Training loss: 72.0875\n",
      "Training acc over epoch: 0.7313\n",
      "---- Validation ----\n",
      "Validation loss: 68.4101\n",
      "Validation acc: 0.6722\n",
      "Time taken: 17.96s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 276.1019, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 234.6767, Accuracy: 0.6158\n",
      "Training loss (for one batch) at step 20: 224.5891, Accuracy: 0.6708\n",
      "Training loss (for one batch) at step 30: 218.0734, Accuracy: 0.7142\n",
      "Training loss (for one batch) at step 40: 214.2256, Accuracy: 0.7315\n",
      "Training loss (for one batch) at step 50: 211.8354, Accuracy: 0.7502\n",
      "Training loss (for one batch) at step 60: 216.2023, Accuracy: 0.7595\n",
      "Training loss (for one batch) at step 70: 240.9589, Accuracy: 0.7421\n",
      "Training loss (for one batch) at step 80: 212.5605, Accuracy: 0.7246\n",
      "Training loss (for one batch) at step 90: 213.1164, Accuracy: 0.7224\n",
      "Training loss (for one batch) at step 100: 215.1395, Accuracy: 0.7283\n",
      "Training loss (for one batch) at step 110: 229.2162, Accuracy: 0.7295\n",
      "---- Training ----\n",
      "Training loss: 72.2779\n",
      "Training acc over epoch: 0.7292\n",
      "---- Validation ----\n",
      "Validation loss: 46.6433\n",
      "Validation acc: 0.6762\n",
      "Time taken: 18.03s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 245.0657, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 223.3658, Accuracy: 0.6193\n",
      "Training loss (for one batch) at step 20: 215.2995, Accuracy: 0.6711\n",
      "Training loss (for one batch) at step 30: 222.0234, Accuracy: 0.7094\n",
      "Training loss (for one batch) at step 40: 224.7790, Accuracy: 0.7349\n",
      "Training loss (for one batch) at step 50: 240.2737, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 60: 213.4064, Accuracy: 0.7597\n",
      "Training loss (for one batch) at step 70: 239.6972, Accuracy: 0.7442\n",
      "Training loss (for one batch) at step 80: 251.8223, Accuracy: 0.7279\n",
      "Training loss (for one batch) at step 90: 214.2916, Accuracy: 0.7225\n",
      "Training loss (for one batch) at step 100: 225.2272, Accuracy: 0.7287\n",
      "Training loss (for one batch) at step 110: 226.5963, Accuracy: 0.7316\n",
      "---- Training ----\n",
      "Training loss: 71.9656\n",
      "Training acc over epoch: 0.7303\n",
      "---- Validation ----\n",
      "Validation loss: 61.4273\n",
      "Validation acc: 0.6883\n",
      "Time taken: 18.14s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 241.5718, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 260.4241, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 20: 203.9622, Accuracy: 0.6689\n",
      "Training loss (for one batch) at step 30: 206.1552, Accuracy: 0.7099\n",
      "Training loss (for one batch) at step 40: 212.9621, Accuracy: 0.7359\n",
      "Training loss (for one batch) at step 50: 212.5432, Accuracy: 0.7526\n",
      "Training loss (for one batch) at step 60: 232.5842, Accuracy: 0.7605\n",
      "Training loss (for one batch) at step 70: 221.4266, Accuracy: 0.7455\n",
      "Training loss (for one batch) at step 80: 245.1347, Accuracy: 0.7251\n",
      "Training loss (for one batch) at step 90: 231.8699, Accuracy: 0.7232\n",
      "Training loss (for one batch) at step 100: 230.6316, Accuracy: 0.7299\n",
      "Training loss (for one batch) at step 110: 227.8121, Accuracy: 0.7320\n",
      "---- Training ----\n",
      "Training loss: 78.9449\n",
      "Training acc over epoch: 0.7308\n",
      "---- Validation ----\n",
      "Validation loss: 103.4853\n",
      "Validation acc: 0.6741\n",
      "Time taken: 17.97s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 241.9821, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 238.1775, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 20: 213.8119, Accuracy: 0.6696\n",
      "Training loss (for one batch) at step 30: 221.5887, Accuracy: 0.7079\n",
      "Training loss (for one batch) at step 40: 199.6521, Accuracy: 0.7332\n",
      "Training loss (for one batch) at step 50: 212.3739, Accuracy: 0.7512\n",
      "Training loss (for one batch) at step 60: 221.7993, Accuracy: 0.7614\n",
      "Training loss (for one batch) at step 70: 218.7854, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 80: 254.0448, Accuracy: 0.7229\n",
      "Training loss (for one batch) at step 90: 234.4755, Accuracy: 0.7236\n",
      "Training loss (for one batch) at step 100: 218.1674, Accuracy: 0.7300\n",
      "Training loss (for one batch) at step 110: 240.4354, Accuracy: 0.7316\n",
      "---- Training ----\n",
      "Training loss: 84.1496\n",
      "Training acc over epoch: 0.7301\n",
      "---- Validation ----\n",
      "Validation loss: 49.3783\n",
      "Validation acc: 0.6762\n",
      "Time taken: 17.93s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.6896\n",
      "Validation AUC: 0.6901\n"
     ]
    }
   ],
   "source": [
    "#model = create_model()\n",
    "K=2\n",
    "R=5\n",
    "NUM_RUNS = 10\n",
    "N_EPOCHS = 100\n",
    "val_acc = np.zeros(NUM_RUNS)\n",
    "AUC= np.zeros(NUM_RUNS)\n",
    "\n",
    "for i in range(NUM_RUNS):\n",
    "  MA = MultipleAnnotators_Classification(2, 5, 0.1)\n",
    "  model =  create_model()\n",
    "  model = MA.fit(model, train_batches_MA, val_batches_MA, N_EPOCHS)\n",
    "  #model = MA.fit(model, Data_train_MA, N_EPOCHS)\n",
    "  val_acc[i] = MA.eval_model(test_batches_MA)\n",
    "  print(\"Validation acc: %.4f\" % (float(val_acc[i]),))\n",
    "    \n",
    " #AUC =======================\n",
    "  val_AUC_metric = tf.keras.metrics.AUC( from_logits = True)\n",
    "  for x_batch_val, y_batch_val in test_batches_MA:\n",
    "      val_logits = model(x_batch_val.numpy(), training=False)\n",
    "      # tf.print(y_batch_val)\n",
    "      val_AUC_metric.update_state(y_batch_val, val_logits[:,:K].numpy().argmax(axis=1).astype('float'))\n",
    "\n",
    "  val_AUC = val_AUC_metric.result()\n",
    "  val_AUC_metric.reset_states()\n",
    "  val_AUC = val_AUC.numpy()\n",
    "  print(\"Validation AUC: %.4f\" % (float(val_AUC),))\n",
    "  AUC[i] = val_AUC\n",
    "  #===================================================\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(val_acc)\n",
    "#df.to_csv('/content/CatDogs_MA_InceptionV3.csv',index=False) # save to notebook output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fd8392f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T05:19:56.095738Z",
     "iopub.status.busy": "2023-01-03T05:19:56.095360Z",
     "iopub.status.idle": "2023-01-03T05:19:56.102685Z",
     "shell.execute_reply": "2023-01-03T05:19:56.101627Z"
    },
    "papermill": {
     "duration": 0.673279,
     "end_time": "2023-01-03T05:19:56.105685",
     "exception": false,
     "start_time": "2023-01-03T05:19:55.432406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6938951 , 0.65348238, 0.70872742, 0.58684438, 0.68744624,\n",
       "       0.67863286, 0.67712814, 0.62596732, 0.54471195, 0.68959588])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49c4e999",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T05:19:57.562636Z",
     "iopub.status.busy": "2023-01-03T05:19:57.562268Z",
     "iopub.status.idle": "2023-01-03T05:19:57.568443Z",
     "shell.execute_reply": "2023-01-03T05:19:57.567562Z"
    },
    "id": "Uq3Ki4uQxcVj",
    "papermill": {
     "duration": 0.744445,
     "end_time": "2023-01-03T05:19:57.570726",
     "exception": false,
     "start_time": "2023-01-03T05:19:56.826281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69611442, 0.65454608, 0.71057403, 0.58999997, 0.68717086,\n",
       "       0.68096101, 0.67741001, 0.62581247, 0.5452823 , 0.69012088])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2213569",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T05:19:58.914160Z",
     "iopub.status.busy": "2023-01-03T05:19:58.913788Z",
     "iopub.status.idle": "2023-01-03T05:19:58.923468Z",
     "shell.execute_reply": "2023-01-03T05:19:58.922368Z"
    },
    "papermill": {
     "duration": 0.690954,
     "end_time": "2023-01-03T05:19:58.926226",
     "exception": false,
     "start_time": "2023-01-03T05:19:58.235272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy:  65.46\n",
      "Average std:  5.029999999999999\n"
     ]
    }
   ],
   "source": [
    "print('Average Accuracy: ', np.round(val_acc.mean(),4)*100) \n",
    "print('Average std: ',np.round(np.std(val_acc),4)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc3a5a58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T05:20:00.558133Z",
     "iopub.status.busy": "2023-01-03T05:20:00.557762Z",
     "iopub.status.idle": "2023-01-03T05:20:00.564106Z",
     "shell.execute_reply": "2023-01-03T05:20:00.563129Z"
    },
    "papermill": {
     "duration": 0.692527,
     "end_time": "2023-01-03T05:20:00.567664",
     "exception": false,
     "start_time": "2023-01-03T05:19:59.875137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy:  65.58\n",
      "Average std:  5.029999999999999\n"
     ]
    }
   ],
   "source": [
    "print('Average Accuracy: ', np.round( AUC.mean(),4)*100) \n",
    "print('Average std: ',np.round(np.std( AUC),4)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7b5d77f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T05:20:01.983208Z",
     "iopub.status.busy": "2023-01-03T05:20:01.982140Z",
     "iopub.status.idle": "2023-01-03T05:20:01.987169Z",
     "shell.execute_reply": "2023-01-03T05:20:01.986276Z"
    },
    "id": "cxSh9vktxcVj",
    "papermill": {
     "duration": 0.737046,
     "end_time": "2023-01-03T05:20:01.989101",
     "exception": false,
     "start_time": "2023-01-03T05:20:01.252055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # accuracy\n",
    "# val_acc_GCCE  = np.zeros(NUM_RUNS)\n",
    "\n",
    "# for i in range(len(classification_report_r)):\n",
    "   \n",
    "#   val_acc_GCCE[i] = classification_report_r[i]['accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7d4a2f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T05:20:03.754780Z",
     "iopub.status.busy": "2023-01-03T05:20:03.754414Z",
     "iopub.status.idle": "2023-01-03T05:20:03.760678Z",
     "shell.execute_reply": "2023-01-03T05:20:03.759763Z"
    },
    "id": "Ak1z-BteyMF6",
    "outputId": "8b14abfb-4940-45fb-b50a-df0a4f7a731b",
    "papermill": {
     "duration": 0.957128,
     "end_time": "2023-01-03T05:20:03.762626",
     "exception": false,
     "start_time": "2023-01-03T05:20:02.805498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6938951 , 0.65348238, 0.70872742, 0.58684438, 0.68744624,\n",
       "       0.67863286, 0.67712814, 0.62596732, 0.54471195, 0.68959588])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77be2a33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T05:20:05.149213Z",
     "iopub.status.busy": "2023-01-03T05:20:05.148831Z",
     "iopub.status.idle": "2023-01-03T05:20:05.154401Z",
     "shell.execute_reply": "2023-01-03T05:20:05.153433Z"
    },
    "id": "K-EeM9bqyI-w",
    "outputId": "b59f0070-7e4c-4480-cd00-da90044724ed",
    "papermill": {
     "duration": 0.735965,
     "end_time": "2023-01-03T05:20:05.156902",
     "exception": false,
     "start_time": "2023-01-03T05:20:04.420937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy:  65.46\n"
     ]
    }
   ],
   "source": [
    "print('Average Accuracy: ', np.round(val_acc.mean(),4)*100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b74efd68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T05:20:06.475343Z",
     "iopub.status.busy": "2023-01-03T05:20:06.474952Z",
     "iopub.status.idle": "2023-01-03T05:20:06.479428Z",
     "shell.execute_reply": "2023-01-03T05:20:06.478479Z"
    },
    "id": "I0E-qXsr1RFC",
    "papermill": {
     "duration": 0.668465,
     "end_time": "2023-01-03T05:20:06.481518",
     "exception": false,
     "start_time": "2023-01-03T05:20:05.813053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y_test = np.asarray([aux[1].numpy() for aux  in validation_data])\n",
    "# X_test = np.asarray([aux[0].numpy() for aux  in validation_data])\n",
    "# # N = len(y_true)\n",
    "# # #test_batches_MA\n",
    "# # aux1 = [test_batches_MA[i][0] for i in range(N)]\n",
    "# # aux2 = [test_batches_MA[i][1] for i in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b86252c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T05:20:07.896988Z",
     "iopub.status.busy": "2023-01-03T05:20:07.896213Z",
     "iopub.status.idle": "2023-01-03T05:20:07.900732Z",
     "shell.execute_reply": "2023-01-03T05:20:07.899748Z"
    },
    "id": "9P-KeFKp2TEr",
    "outputId": "1998d331-5546-4bc4-c007-8180b58dd574",
    "papermill": {
     "duration": 0.676882,
     "end_time": "2023-01-03T05:20:07.902630",
     "exception": false,
     "start_time": "2023-01-03T05:20:07.225748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b344ddc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T05:20:09.305889Z",
     "iopub.status.busy": "2023-01-03T05:20:09.304807Z",
     "iopub.status.idle": "2023-01-03T05:20:09.309664Z",
     "shell.execute_reply": "2023-01-03T05:20:09.308724Z"
    },
    "id": "6pWXVQnK1GXo",
    "outputId": "89294013-e100-4cc0-9a9e-3f7bad8f7729",
    "papermill": {
     "duration": 0.75112,
     "end_time": "2023-01-03T05:20:09.311725",
     "exception": false,
     "start_time": "2023-01-03T05:20:08.560605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pred = model.predict(X_test)\n",
    "# pred[:, :2].argmax(axis=1)\n",
    "# print(classification_report(pred[:, :2].argmax(axis=1), y_test ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083fe7c1",
   "metadata": {
    "id": "STPfoIdfxcVj",
    "papermill": {
     "duration": 0.861234,
     "end_time": "2023-01-03T05:20:10.842787",
     "exception": false,
     "start_time": "2023-01-03T05:20:09.981553",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f478b93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T05:20:12.308549Z",
     "iopub.status.busy": "2023-01-03T05:20:12.308164Z",
     "iopub.status.idle": "2023-01-03T05:20:12.314212Z",
     "shell.execute_reply": "2023-01-03T05:20:12.313301Z"
    },
    "id": "z1b6tNcdxcVj",
    "outputId": "fcf6fada-a56b-45e9-ab8b-e3562b97621e",
    "papermill": {
     "duration": 0.745218,
     "end_time": "2023-01-03T05:20:12.316319",
     "exception": false,
     "start_time": "2023-01-03T05:20:11.571101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6546431660652161"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b84180de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T05:20:13.635183Z",
     "iopub.status.busy": "2023-01-03T05:20:13.634073Z",
     "iopub.status.idle": "2023-01-03T05:20:13.640787Z",
     "shell.execute_reply": "2023-01-03T05:20:13.639726Z"
    },
    "papermill": {
     "duration": 0.666297,
     "end_time": "2023-01-03T05:20:13.642798",
     "exception": false,
     "start_time": "2023-01-03T05:20:12.976501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD Accuracy:  5.029999999999999\n"
     ]
    }
   ],
   "source": [
    "print('STD Accuracy: ', np.round(np.std(val_acc),4)*100) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d6121f",
   "metadata": {
    "id": "xxyE_WnFxcVj",
    "papermill": {
     "duration": 0.66263,
     "end_time": "2023-01-03T05:20:15.046648",
     "exception": false,
     "start_time": "2023-01-03T05:20:14.384018",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "MC droput run this in a loop with training layer set to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aee06e5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T05:20:16.450125Z",
     "iopub.status.busy": "2023-01-03T05:20:16.449300Z",
     "iopub.status.idle": "2023-01-03T05:20:16.454160Z",
     "shell.execute_reply": "2023-01-03T05:20:16.453203Z"
    },
    "id": "I6R3im8_xcVk",
    "papermill": {
     "duration": 0.750249,
     "end_time": "2023-01-03T05:20:16.456518",
     "exception": false,
     "start_time": "2023-01-03T05:20:15.706269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y_probas = np.stack([MA.eval_model((test_batches,training=True) # se activa training en True para que el Dropout se aplique\n",
    "#                    for sample in range(100)])\n",
    "\n",
    "# y_proba = y_probas.mean(axis=0)\n",
    "# y_std = y_probas.std(axis=0)\n",
    "# y_probas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99a809b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T05:20:17.792465Z",
     "iopub.status.busy": "2023-01-03T05:20:17.792066Z",
     "iopub.status.idle": "2023-01-03T05:20:17.796256Z",
     "shell.execute_reply": "2023-01-03T05:20:17.795301Z"
    },
    "id": "uLKmvJlpxcVk",
    "papermill": {
     "duration": 0.655799,
     "end_time": "2023-01-03T05:20:17.798336",
     "exception": false,
     "start_time": "2023-01-03T05:20:17.142537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y_ped =np.argmax(y_proba,axis=1)\n",
    "# accuracy=np.sum(y_pred==test_label)/len(test_label)\n",
    "# accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "acb34f23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T05:20:19.183755Z",
     "iopub.status.busy": "2023-01-03T05:20:19.182532Z",
     "iopub.status.idle": "2023-01-03T05:20:19.187693Z",
     "shell.execute_reply": "2023-01-03T05:20:19.186756Z"
    },
    "id": "Jl1sFNOf1KgC",
    "papermill": {
     "duration": 0.734092,
     "end_time": "2023-01-03T05:20:19.189695",
     "exception": false,
     "start_time": "2023-01-03T05:20:18.455603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "   \n",
    "# r1 = np.mean(val_acc)\n",
    "# print(\"\\nMean: \", r1)\n",
    "  \n",
    "# r2 = np.std(val_acc)\n",
    "# print(\"\\nstd: \", r2)\n",
    "  \n",
    "# r3 = np.var(val_acc)\n",
    "# print(\"\\nvariance: \", r3)\n",
    "# #MA.eval_model(test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "667eab94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T05:20:20.500264Z",
     "iopub.status.busy": "2023-01-03T05:20:20.499892Z",
     "iopub.status.idle": "2023-01-03T05:20:20.504115Z",
     "shell.execute_reply": "2023-01-03T05:20:20.503131Z"
    },
    "id": "KqeZpkxuxcVk",
    "papermill": {
     "duration": 0.662183,
     "end_time": "2023-01-03T05:20:20.506145",
     "exception": false,
     "start_time": "2023-01-03T05:20:19.843962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de55bda6",
   "metadata": {
    "id": "6cjM2VJJkuKK",
    "papermill": {
     "duration": 0.880312,
     "end_time": "2023-01-03T05:20:22.156333",
     "exception": false,
     "start_time": "2023-01-03T05:20:21.276021",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "VGG19 --> acc:0.8613  --> 0.894454 --> 0.772356"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18370.634954,
   "end_time": "2023-01-03T05:20:25.891966",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-03T00:14:15.257012",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
