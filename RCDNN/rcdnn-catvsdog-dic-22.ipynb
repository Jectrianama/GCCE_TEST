{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dae962d",
   "metadata": {
    "id": "oAuRT75GdLFw",
    "papermill": {
     "duration": 0.010056,
     "end_time": "2023-01-03T14:52:59.238117",
     "exception": false,
     "start_time": "2023-01-03T14:52:59.228061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cats vs. Dogs Class dataset for multiple annotators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af158d2a",
   "metadata": {
    "id": "9rK94t33nwDC",
    "papermill": {
     "duration": 0.008111,
     "end_time": "2023-01-03T14:52:59.254862",
     "exception": false,
     "start_time": "2023-01-03T14:52:59.246751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa469631",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T14:52:59.274342Z",
     "iopub.status.busy": "2023-01-03T14:52:59.273099Z",
     "iopub.status.idle": "2023-01-03T14:53:06.077440Z",
     "shell.execute_reply": "2023-01-03T14:53:06.076499Z"
    },
    "id": "zSyMHuCVys-O",
    "papermill": {
     "duration": 6.816815,
     "end_time": "2023-01-03T14:53:06.080168",
     "exception": false,
     "start_time": "2023-01-03T14:52:59.263353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense,Conv2D,Flatten,MaxPooling2D,GlobalAveragePooling2D\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5708179",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T14:53:06.099948Z",
     "iopub.status.busy": "2023-01-03T14:53:06.099142Z",
     "iopub.status.idle": "2023-01-03T14:53:06.103443Z",
     "shell.execute_reply": "2023-01-03T14:53:06.102482Z"
    },
    "id": "-E1MJt8cxlwg",
    "outputId": "ea43c1c9-075f-44de-d2d8-e135799b6630",
    "papermill": {
     "duration": 0.015655,
     "end_time": "2023-01-03T14:53:06.105529",
     "exception": false,
     "start_time": "2023-01-03T14:53:06.089874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fa3829c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T14:53:06.124009Z",
     "iopub.status.busy": "2023-01-03T14:53:06.123111Z",
     "iopub.status.idle": "2023-01-03T14:53:06.127898Z",
     "shell.execute_reply": "2023-01-03T14:53:06.127085Z"
    },
    "id": "QJPvjdZ-f8ca",
    "papermill": {
     "duration": 0.015817,
     "end_time": "2023-01-03T14:53:06.129877",
     "exception": false,
     "start_time": "2023-01-03T14:53:06.114060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# os.chdir('/content/drive/Shareddrives/Multiple Anotators/CrowdLayer/Notebooks')\n",
    "# cwd = os.getcwd()\n",
    "# sys.path.append(\"../Models\")\n",
    "\n",
    "\n",
    "# from Multiple_Annotators_C import MultipleAnnotators_Classification\n",
    "\n",
    "#import sys\n",
    "#sys.path.insert(1, '../input/multiple-annotators-c/')\n",
    "#os.chdir('/Multiple Anotators-c/')\n",
    "#cwd = os.getcwd()\n",
    "#sys.path.append('/input/multiple-annotators-c')\n",
    "#from Multiple_Annotators_C import MultipleAnnotators_Classification\n",
    "\n",
    "# seed_value= 12321 \n",
    "# from numpy.random import seed\n",
    "# seed(seed_value)\n",
    "# tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a2b3ac",
   "metadata": {
    "id": "6Un5nFWgnyem",
    "papermill": {
     "duration": 0.008078,
     "end_time": "2023-01-03T14:53:06.146314",
     "exception": false,
     "start_time": "2023-01-03T14:53:06.138236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Download and Prepare the Dataset\n",
    "\n",
    "We will use the [Cats vs Dogs](https://www.tensorflow.org/datasets/catalog/cats_vs_dogs) dataset and we can load it via Tensorflow Datasets. The images are labeled 0 for cats and 1 for dogs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda2de13",
   "metadata": {
    "id": "Gw6K2Uey06kh",
    "papermill": {
     "duration": 0.008222,
     "end_time": "2023-01-03T14:53:06.162587",
     "exception": false,
     "start_time": "2023-01-03T14:53:06.154365",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Multiple annotators model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a66243d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T14:53:06.180551Z",
     "iopub.status.busy": "2023-01-03T14:53:06.179696Z",
     "iopub.status.idle": "2023-01-03T14:53:08.953357Z",
     "shell.execute_reply": "2023-01-03T14:53:08.952315Z"
    },
    "id": "xam4REp209Sd",
    "papermill": {
     "duration": 2.785374,
     "end_time": "2023-01-03T14:53:08.956057",
     "exception": false,
     "start_time": "2023-01-03T14:53:06.170683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 14:53:06.276756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-03 14:53:06.362089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-03 14:53:06.362908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-03 14:53:06.364520: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-03 14:53:06.371332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-03 14:53:06.372036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-03 14:53:06.372735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-03 14:53:08.521815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-03 14:53:08.522784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-03 14:53:08.523575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-03 14:53:08.524242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "validation_data = tf.data.experimental.load('/kaggle/input/cat-vs-dog-ma-sin/cats_dogs_Te')\n",
    "train_data_MA = tf.data.experimental.load('/kaggle/input/cat-vs-dog-ma-sin/cats_dogs_MA_sin_Tr_1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b5e8ab2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T14:53:08.977137Z",
     "iopub.status.busy": "2023-01-03T14:53:08.976821Z",
     "iopub.status.idle": "2023-01-03T14:53:08.986930Z",
     "shell.execute_reply": "2023-01-03T14:53:08.985753Z"
    },
    "id": "D_S0EJ3mFdfK",
    "outputId": "9ed3c2c7-50b4-4445-a01e-c9a3d780c403",
    "papermill": {
     "duration": 0.02345,
     "end_time": "2023-01-03T14:53:08.989802",
     "exception": false,
     "start_time": "2023-01-03T14:53:08.966352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18610"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_count = tf.data.experimental.cardinality(train_data_MA).numpy() # los datos de training son 18610 usar subconjunto de 5000\n",
    "image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfe3a6c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T14:53:09.010682Z",
     "iopub.status.busy": "2023-01-03T14:53:09.009828Z",
     "iopub.status.idle": "2023-01-03T14:53:09.017210Z",
     "shell.execute_reply": "2023-01-03T14:53:09.016181Z"
    },
    "id": "ctjLei0TxcVh",
    "outputId": "6f578b73-ebdf-4465-91c7-2adb7d127174",
    "papermill": {
     "duration": 0.019826,
     "end_time": "2023-01-03T14:53:09.019301",
     "exception": false,
     "start_time": "2023-01-03T14:53:08.999475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4652"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_count1 = tf.data.experimental.cardinality(validation_data).numpy() # los datos de training son 18610\n",
    "image_count1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "783f7102",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T14:53:09.040359Z",
     "iopub.status.busy": "2023-01-03T14:53:09.039490Z",
     "iopub.status.idle": "2023-01-03T14:53:26.319077Z",
     "shell.execute_reply": "2023-01-03T14:53:26.318051Z"
    },
    "id": "opk5MXl4IwjC",
    "papermill": {
     "duration": 17.292629,
     "end_time": "2023-01-03T14:53:26.321606",
     "exception": false,
     "start_time": "2023-01-03T14:53:09.028977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 14:53:09.071429: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "#X_test = [validation_data[i][0] for i in range(image_count1)]\n",
    "#Y_true_test = [validation_data[i][1] for i in range(image_count1)]\n",
    "Y_true_test = np.asarray([aux[1].numpy() for aux  in validation_data])\n",
    "X_test = np.asarray([aux[0].numpy() for aux  in validation_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5747299a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T14:53:26.341868Z",
     "iopub.status.busy": "2023-01-03T14:53:26.341182Z",
     "iopub.status.idle": "2023-01-03T14:53:26.347896Z",
     "shell.execute_reply": "2023-01-03T14:53:26.346866Z"
    },
    "id": "-BydcVOQxcVh",
    "outputId": "8c1b4ed2-7c43-4675-f055-f9e4e3f5b3dd",
    "papermill": {
     "duration": 0.020209,
     "end_time": "2023-01-03T14:53:26.351261",
     "exception": false,
     "start_time": "2023-01-03T14:53:26.331052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18610"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4606320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T14:53:26.370100Z",
     "iopub.status.busy": "2023-01-03T14:53:26.369824Z",
     "iopub.status.idle": "2023-01-03T14:53:26.377127Z",
     "shell.execute_reply": "2023-01-03T14:53:26.376206Z"
    },
    "id": "HdFme6fdxcVh",
    "papermill": {
     "duration": 0.019067,
     "end_time": "2023-01-03T14:53:26.379249",
     "exception": false,
     "start_time": "2023-01-03T14:53:26.360182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_size = int(image_count * 0.2)\n",
    "train_ds_MA = train_data_MA.skip(val_size)\n",
    "val_ds_MA = train_data_MA.take(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e309c10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T14:53:26.397993Z",
     "iopub.status.busy": "2023-01-03T14:53:26.397705Z",
     "iopub.status.idle": "2023-01-03T14:53:26.407958Z",
     "shell.execute_reply": "2023-01-03T14:53:26.407088Z"
    },
    "id": "aVHIlFpgxcVi",
    "papermill": {
     "duration": 0.02201,
     "end_time": "2023-01-03T14:53:26.410014",
     "exception": false,
     "start_time": "2023-01-03T14:53:26.388004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_batches_MA = train_ds_MA.shuffle(1024).batch(batch_size)\n",
    "val_batches_MA = val_ds_MA.shuffle(1024).batch(batch_size)\n",
    "test_batches_MA = validation_data.shuffle(1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a219e54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T14:53:26.428424Z",
     "iopub.status.busy": "2023-01-03T14:53:26.428142Z",
     "iopub.status.idle": "2023-01-03T14:53:26.436102Z",
     "shell.execute_reply": "2023-01-03T14:53:26.435244Z"
    },
    "id": "GsB4EA2-xcVi",
    "outputId": "2d45809e-a9cc-408f-9a8b-745e8fe850e9",
    "papermill": {
     "duration": 0.019519,
     "end_time": "2023-01-03T14:53:26.438131",
     "exception": false,
     "start_time": "2023-01-03T14:53:26.418612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14888"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_count = tf.data.experimental.cardinality(train_ds_MA).numpy() # los datos de training son 18610 usar subconjunto de 5000\n",
    "image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6c4c517",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T14:53:26.456790Z",
     "iopub.status.busy": "2023-01-03T14:53:26.456529Z",
     "iopub.status.idle": "2023-01-03T14:53:26.463240Z",
     "shell.execute_reply": "2023-01-03T14:53:26.462226Z"
    },
    "id": "Hk33DzwkxcVi",
    "outputId": "aad91eec-842c-4995-de90-5bb715539b6a",
    "papermill": {
     "duration": 0.018692,
     "end_time": "2023-01-03T14:53:26.465584",
     "exception": false,
     "start_time": "2023-01-03T14:53:26.446892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3722"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_count_val = tf.data.experimental.cardinality(val_ds_MA).numpy() # los datos de training son 18610 usar subconjunto de 5000\n",
    "image_count_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed7ad05",
   "metadata": {
    "id": "UMeK3NG3xcVi",
    "papermill": {
     "duration": 0.008506,
     "end_time": "2023-01-03T14:53:26.482917",
     "exception": false,
     "start_time": "2023-01-03T14:53:26.474411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbcb7162",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T14:53:26.501724Z",
     "iopub.status.busy": "2023-01-03T14:53:26.501433Z",
     "iopub.status.idle": "2023-01-03T14:53:44.256145Z",
     "shell.execute_reply": "2023-01-03T14:53:44.255185Z"
    },
    "id": "uvwc7eixxcVi",
    "outputId": "d7766078-8c40-41ed-fb01-66b5f62a07f1",
    "papermill": {
     "duration": 17.766752,
     "end_time": "2023-01-03T14:53:44.258484",
     "exception": false,
     "start_time": "2023-01-03T14:53:26.491732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 14:53:38.834532: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 1 of 1024\n",
      "2023-01-03 14:53:42.181053: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:228] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotator 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.88      0.85        60\n",
      "         1.0       0.89      0.84      0.86        68\n",
      "\n",
      "    accuracy                           0.86       128\n",
      "   macro avg       0.86      0.86      0.86       128\n",
      "weighted avg       0.86      0.86      0.86       128\n",
      "\n",
      "annotator 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.62      0.57        60\n",
      "         1.0       0.61      0.53      0.57        68\n",
      "\n",
      "    accuracy                           0.57       128\n",
      "   macro avg       0.57      0.57      0.57       128\n",
      "weighted avg       0.58      0.57      0.57       128\n",
      "\n",
      "annotator 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.45      0.48        60\n",
      "         1.0       0.57      0.63      0.60        68\n",
      "\n",
      "    accuracy                           0.55       128\n",
      "   macro avg       0.54      0.54      0.54       128\n",
      "weighted avg       0.54      0.55      0.54       128\n",
      "\n",
      "annotator 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      0.45      0.45        60\n",
      "         1.0       0.51      0.51      0.51        68\n",
      "\n",
      "    accuracy                           0.48       128\n",
      "   macro avg       0.48      0.48      0.48       128\n",
      "weighted avg       0.48      0.48      0.48       128\n",
      "\n",
      "annotator 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.18      0.18      0.18        60\n",
      "         1.0       0.27      0.26      0.27        68\n",
      "\n",
      "    accuracy                           0.23       128\n",
      "   macro avg       0.22      0.22      0.22       128\n",
      "weighted avg       0.23      0.23      0.23       128\n",
      "\n",
      "annotator 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.86      0.85        65\n",
      "         1.0       0.85      0.83      0.84        63\n",
      "\n",
      "    accuracy                           0.84       128\n",
      "   macro avg       0.84      0.84      0.84       128\n",
      "weighted avg       0.84      0.84      0.84       128\n",
      "\n",
      "annotator 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.48      0.51        65\n",
      "         1.0       0.53      0.60      0.56        63\n",
      "\n",
      "    accuracy                           0.54       128\n",
      "   macro avg       0.54      0.54      0.54       128\n",
      "weighted avg       0.54      0.54      0.54       128\n",
      "\n",
      "annotator 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.69      0.65        65\n",
      "         1.0       0.64      0.56      0.59        63\n",
      "\n",
      "    accuracy                           0.62       128\n",
      "   macro avg       0.63      0.62      0.62       128\n",
      "weighted avg       0.63      0.62      0.62       128\n",
      "\n",
      "annotator 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.48      0.50        65\n",
      "         1.0       0.50      0.54      0.52        63\n",
      "\n",
      "    accuracy                           0.51       128\n",
      "   macro avg       0.51      0.51      0.51       128\n",
      "weighted avg       0.51      0.51      0.51       128\n",
      "\n",
      "annotator 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.27      0.31      0.29        65\n",
      "         1.0       0.15      0.13      0.14        63\n",
      "\n",
      "    accuracy                           0.22       128\n",
      "   macro avg       0.21      0.22      0.21       128\n",
      "weighted avg       0.21      0.22      0.21       128\n",
      "\n",
      "annotator 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.89      0.92        70\n",
      "         1.0       0.87      0.95      0.91        58\n",
      "\n",
      "    accuracy                           0.91       128\n",
      "   macro avg       0.91      0.92      0.91       128\n",
      "weighted avg       0.92      0.91      0.91       128\n",
      "\n",
      "annotator 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.57      0.59        70\n",
      "         1.0       0.52      0.55      0.53        58\n",
      "\n",
      "    accuracy                           0.56       128\n",
      "   macro avg       0.56      0.56      0.56       128\n",
      "weighted avg       0.57      0.56      0.56       128\n",
      "\n",
      "annotator 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.61      0.62        70\n",
      "         1.0       0.55      0.57      0.56        58\n",
      "\n",
      "    accuracy                           0.59       128\n",
      "   macro avg       0.59      0.59      0.59       128\n",
      "weighted avg       0.60      0.59      0.59       128\n",
      "\n",
      "annotator 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.51      0.50      0.51        70\n",
      "         1.0       0.42      0.43      0.42        58\n",
      "\n",
      "    accuracy                           0.47       128\n",
      "   macro avg       0.47      0.47      0.47       128\n",
      "weighted avg       0.47      0.47      0.47       128\n",
      "\n",
      "annotator 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.27      0.24      0.26        70\n",
      "         1.0       0.18      0.21      0.20        58\n",
      "\n",
      "    accuracy                           0.23       128\n",
      "   macro avg       0.23      0.22      0.23       128\n",
      "weighted avg       0.23      0.23      0.23       128\n",
      "\n",
      "annotator 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.75      0.83        69\n",
      "         1.0       0.76      0.92      0.83        59\n",
      "\n",
      "    accuracy                           0.83       128\n",
      "   macro avg       0.84      0.83      0.83       128\n",
      "weighted avg       0.84      0.83      0.83       128\n",
      "\n",
      "annotator 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.62      0.60        69\n",
      "         1.0       0.51      0.46      0.48        59\n",
      "\n",
      "    accuracy                           0.55       128\n",
      "   macro avg       0.54      0.54      0.54       128\n",
      "weighted avg       0.54      0.55      0.54       128\n",
      "\n",
      "annotator 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.65      0.66        69\n",
      "         1.0       0.61      0.63      0.62        59\n",
      "\n",
      "    accuracy                           0.64       128\n",
      "   macro avg       0.64      0.64      0.64       128\n",
      "weighted avg       0.64      0.64      0.64       128\n",
      "\n",
      "annotator 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.57      0.56        69\n",
      "         1.0       0.48      0.47      0.48        59\n",
      "\n",
      "    accuracy                           0.52       128\n",
      "   macro avg       0.52      0.52      0.52       128\n",
      "weighted avg       0.52      0.52      0.52       128\n",
      "\n",
      "annotator 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.31      0.28      0.29        69\n",
      "         1.0       0.25      0.29      0.27        59\n",
      "\n",
      "    accuracy                           0.28       128\n",
      "   macro avg       0.28      0.28      0.28       128\n",
      "weighted avg       0.28      0.28      0.28       128\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "i = 0\n",
    "fig, ax = plt.subplots(1, 4)\n",
    "for image, label, label2 in train_batches_MA.take(4):\n",
    "   # predictedLabel = int(predictions[i] >= 0.5)\n",
    "   # print(label2)\n",
    "    ax[i].axis('off')\n",
    "   # ax[i].set_title(classNames[label[i]])\n",
    "    ax[i].imshow(image[0])\n",
    "    i += 1\n",
    "    for j in range(label2.shape[1]):\n",
    "      print('annotator',j+1)\n",
    "      print(classification_report(label ,label2[:,j]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836df0dc",
   "metadata": {
    "id": "9AgOHREc1bmd",
    "papermill": {
     "duration": 0.01017,
     "end_time": "2023-01-03T14:53:44.279148",
     "exception": false,
     "start_time": "2023-01-03T14:53:44.268978",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Build the classifier from multiple annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5b81b2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T14:53:44.301206Z",
     "iopub.status.busy": "2023-01-03T14:53:44.300895Z",
     "iopub.status.idle": "2023-01-03T14:53:44.326925Z",
     "shell.execute_reply": "2023-01-03T14:53:44.325825Z"
    },
    "id": "k-ePr0-fxcVi",
    "papermill": {
     "duration": 0.039667,
     "end_time": "2023-01-03T14:53:44.329055",
     "exception": false,
     "start_time": "2023-01-03T14:53:44.289388",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense,Conv2D,Flatten,MaxPooling2D,GlobalAveragePooling2D\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "class MultipleAnnotators_Classification():\n",
    "    def __init__(self, output_dim, num_annotators, q= 0.0001):\n",
    "        self.K = output_dim\n",
    "        self.R = num_annotators\n",
    "        self.q = q\n",
    "        #self.callbacks #=callbacks\n",
    "        #self.l1_param=l1_param \n",
    "        #self.l2_param=l1_param\n",
    "\n",
    "    def CrowdLayer(self, input):\n",
    "       #x = keras.layers.Dense(self.R + self.K, kernel_regularizer=regularizers.L1L2(l1= 1e-2, l2=1e-3),  activation='tanh')(input)\n",
    "        output_cla = keras.layers.Dense(self.K,  activation='softmax')(input)\n",
    "        output_ann = keras.layers.Dense(self.R,  activation='sigmoid')(input)\n",
    "        output = keras.layers.Concatenate()([output_cla, output_ann])\n",
    "        \n",
    "        return output\n",
    "#RCDNN   \n",
    "    def loss(self):\n",
    "        def custom_loss(y_true, y_pred):\n",
    "            # print(y_true,y_pred)\n",
    "            pred = y_pred[:, :self.K]\n",
    "            pred = tf.clip_by_value(pred, clip_value_min=1e-9, clip_value_max=1-1e-9) #estabilidad numerica de la funcion de costo\n",
    "            ann_ = y_pred[:, self.K:]\n",
    "            Y_true = tf.one_hot(tf.cast(y_true, dtype=tf.int32), depth=self.K, axis=1)\n",
    "            Y_hat = tf.repeat(tf.expand_dims(pred,-1), self.R, axis = -1)\n",
    "            p_logreg = tf.math.reduce_prod(tf.math.pow(Y_hat, Y_true), axis=1)\n",
    "            temp1 = ann_*tf.math.log(p_logreg)  \n",
    "            temp2 = (1 - ann_)*tf.math.log(1/self.K)*tf.reduce_sum(Y_true,axis=1)\n",
    "            # temp2 = (tf.ones(tf.shape(ann_)) - ann_)*tf.math.log(1/K)\n",
    "            # print(tf.reduce_mean(Y_true,axis=1).numpy())\n",
    "            return -tf.math.reduce_sum((temp1 + temp2))\n",
    "        return custom_loss\n",
    "    \n",
    "#     def loss(self):\n",
    "#         def custom_loss(y_true, y_pred):\n",
    "#                # print(y_true,y_pred)\n",
    "#            # q = 0.1\n",
    "#             pred = y_pred[:, :self.K]\n",
    "#             pred = tf.clip_by_value(pred, clip_value_min=1e-9, clip_value_max=1)\n",
    "#             ann_ = y_pred[:, self.K:]\n",
    "#             # ann_ = tf.clip_by_value(ann_, clip_value_min=1e-9, clip_value_max=1-1e-9)\n",
    "#             Y_true = tf.one_hot(tf.cast(y_true, dtype=tf.int32), depth=self.K, axis=1)\n",
    "#             Y_hat = tf.repeat(tf.expand_dims(pred,-1), self.R, axis = -1)\n",
    "\n",
    "#             p_gcce = Y_true*(1 - Y_hat**self.q)/self.q\n",
    "#             temp1 = ann_*tf.math.reduce_sum(p_gcce, axis=1)\n",
    "#             temp2 = (1 - ann_)*(1-(1/self.K)**self.q)/self.q*tf.reduce_sum(Y_true,axis=1)\n",
    "#             return tf.math.reduce_sum((temp1 + temp2))\n",
    "#         return custom_loss\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, x, Y, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.model(x, training=True)\n",
    "            loss_value = self.loss_fn(Y, logits)\n",
    "        grads = tape.gradient(loss_value, self.model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "        self.train_acc_metric.update_state(y, logits[:, :self.K])\n",
    "        return loss_value\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, x, y):\n",
    "        val_logits = self.model(x, training=False)\n",
    "        self.val_acc_metric.update_state(y, val_logits[:,:self.K])\n",
    "\n",
    "    def fit(self, model, Data_tr, Data_Val, epochs):\n",
    "        self.model = model\n",
    "        #++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        # Instantiate an optimizer.\n",
    "        #self.optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-4)\n",
    "        self.optimizer =  tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        #self.optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4, clipnorm=1.0)\n",
    "        #++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        # Instantiate a loss function.\n",
    "        self.loss_fn = self.loss()\n",
    "        self.train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "        self.val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "        train_loss = np.zeros(epochs)\n",
    "        train_accur = np.zeros(epochs)\n",
    "        val_accur = np.zeros(epochs)\n",
    "        val_loss = np.zeros(epochs)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Iterate over the batches of the dataset.\n",
    "            for step, (x_batch_train, y_batch_train, Y_batch_train) in enumerate(Data_tr):\n",
    "                # print(y_batch_train, Y_batch_train)\n",
    "                loss_value = self.train_step(x_batch_train, Y_batch_train, y_batch_train)\n",
    "\n",
    "                # Log every 200 batches.\n",
    "                if step % 10 == 0:\n",
    "                    train_acc = self.train_acc_metric.result()\n",
    "                    print(\n",
    "                      \"Training loss (for one batch) at step %d: %.4f, Accuracy: %.4f\"\n",
    "                      % (step, float(loss_value), float(train_acc))\n",
    "                            )\n",
    "                # print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "\n",
    "\n",
    "\n",
    "            # Run a validation loop at the end of each epoch.\n",
    "            for x_batch_val, y_batch_val,Y_batch_val in Data_Val:\n",
    "\n",
    "                val_logits = model(x_batch_val, training=False)\n",
    "\n",
    "                val_loss_value = self.loss_fn(Y_batch_val, val_logits)\n",
    "\n",
    "                self.val_acc_metric.update_state(y_batch_val, val_logits[:,:self.K])\n",
    "                \n",
    "               # np.round(np.mean([model(x_batch_val, training= True) for sample in range(100)]), 2)\n",
    "\n",
    "\n",
    "             # Display metrics at the end of each epoch.\n",
    "            train_acc = self.train_acc_metric.result()\n",
    "            val_acc = self.val_acc_metric.result()\n",
    "\n",
    "\n",
    "            print('---- Training ----')\n",
    "            print(\"Training loss: %.4f\" % (float(loss_value),))\n",
    "            print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "            # Reset training metrics at the end of each epoch\n",
    "            self.train_acc_metric.reset_states()\n",
    "            self.val_acc_metric.reset_states()\n",
    "\n",
    "\n",
    "            train_loss[epoch] = float(loss_value)\n",
    "            train_accur[epoch] = float(train_acc)\n",
    "\n",
    "            val_accur[epoch] = float(val_acc)\n",
    "            val_loss[epoch] = float(val_loss_value) \n",
    "\n",
    "\n",
    "            print('---- Validation ----')\n",
    "            print(\"Validation loss: %.4f\" % (float(val_loss_value),))\n",
    "            print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "\n",
    "            print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.suptitle('Loss and accuracy')\n",
    "        ax1.plot(range(1,epochs+1),train_loss)\n",
    "        ax1.plot(range(1,epochs+1), val_loss)\n",
    "        ax2.plot(range(1,epochs+1),train_accur)\n",
    "        ax2.plot(range(1,epochs+1),val_accur)\n",
    "        #plt.figure(figsize=(16,9))\n",
    "        ax1.set(xlabel= 'Epoch', ylabel=\"Loss\")\n",
    "        ax2.set(xlabel= 'Epoch',ylabel=\"Accuracy\")\n",
    "        ax1.legend(['Training_loss', 'Validation_loss'])\n",
    "        ax2.legend(['Training', 'Validation'])\n",
    "        ax1.grid()\n",
    "        ax2.grid()\n",
    "        plt.show()\n",
    "        return self.model\n",
    "\n",
    "    def eval_model(self, Data):\n",
    "        self.val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "        for x_batch_val, y_batch_val in Data:\n",
    "            self.test_step(x_batch_val, y_batch_val)\n",
    "\n",
    "        val_acc = self.val_acc_metric.result()\n",
    "        self.val_acc_metric.reset_states()\n",
    "        return val_acc\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "474ba549",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T14:53:44.350668Z",
     "iopub.status.busy": "2023-01-03T14:53:44.350379Z",
     "iopub.status.idle": "2023-01-03T14:53:44.357048Z",
     "shell.execute_reply": "2023-01-03T14:53:44.356157Z"
    },
    "id": "4l-_pkpaBkSv",
    "papermill": {
     "duration": 0.019928,
     "end_time": "2023-01-03T14:53:44.359069",
     "exception": false,
     "start_time": "2023-01-03T14:53:44.339141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "  # print(y_true,y_pred)\n",
    "  K = 2 #len(np.unique(y_true))\n",
    "  R = 5\n",
    "  q = 0.1\n",
    "  pred = y_pred[:, K]\n",
    "  pred = tf.clip_by_value(pred, clip_value_min=1e-9, clip_value_max=1)\n",
    "  ann_ = y_pred[:,  K:]\n",
    "  # ann_ = tf.clip_by_value(ann_, clip_value_min=1e-9, clip_value_max=1-1e-9)\n",
    "  Y_true = tf.one_hot(tf.cast(y_true, dtype=tf.int32), depth=K, axis=1)\n",
    "  Y_hat = tf.repeat(tf.expand_dims(pred,-1), R, axis = -1)\n",
    "\n",
    "  p_gcce = Y_true*(1 - Y_hat**q)/q\n",
    "  temp1 = ann_*tf.math.reduce_sum(p_gcce, axis=1)\n",
    "  temp2 = (1 - ann_)*(1-(1/K)**q)/q*tf.reduce_sum(Y_true,axis=1)\n",
    "  return tf.math.reduce_sum((temp1 + temp2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd50170b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T14:53:44.380263Z",
     "iopub.status.busy": "2023-01-03T14:53:44.379998Z",
     "iopub.status.idle": "2023-01-03T14:53:44.391500Z",
     "shell.execute_reply": "2023-01-03T14:53:44.390554Z"
    },
    "id": "0I4Rrc5TxcVj",
    "papermill": {
     "duration": 0.024446,
     "end_time": "2023-01-03T14:53:44.393540",
     "exception": false,
     "start_time": "2023-01-03T14:53:44.369094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MA = MultipleAnnotators_Classification(2, 5, 0.001)\n",
    " \n",
    "def create_model():\n",
    "   \n",
    "    l1 = 1e-2\n",
    "    # Block 1\n",
    "    inputs = keras.layers.Input(shape=(150, 150, 3), name='entrada')\n",
    "    x = keras.layers.BatchNormalization()(inputs)\n",
    "    x = keras.layers.Conv2D(32, (3, 3), activation=\"relu\" , name=\"block1_conv1\")(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name=\"block1_pool\")(x)\n",
    "\n",
    "\n",
    "    # Block 2\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Conv2D(32, (3, 3), activation=\"relu\", name=\"block2_conv1\")(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    #x = keras.layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name=\"block2_pool\")(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Conv2D(64, (3, 3), activation=\"relu\", name=\"block3_conv1\" )(x)             \n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "   # x = keras.layers.Dropout(0.2)(x)\n",
    "   \n",
    "    x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name=\"block3_pool\")(x)\n",
    "    \n",
    "    # Block 4\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Conv2D(64, (3, 3), activation=\"relu\", name=\"block4_conv1\")(x)            \n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name=\"block4_pool\")(x)\n",
    "    #x = keras.layers.Dropout(0.2)(x)\n",
    "    \n",
    "    #x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "   \n",
    "    x = keras.layers.Flatten()(x)\n",
    "    #x = keras.layers.Dropout(0.5)(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    output = MA.CrowdLayer(x)\n",
    "    model = keras.Model(inputs=inputs,outputs=output)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5337eecb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T14:53:44.415148Z",
     "iopub.status.busy": "2023-01-03T14:53:44.414847Z",
     "iopub.status.idle": "2023-01-03T14:53:44.420182Z",
     "shell.execute_reply": "2023-01-03T14:53:44.419333Z"
    },
    "id": "iZAxrNF3_hE_",
    "papermill": {
     "duration": 0.018506,
     "end_time": "2023-01-03T14:53:44.422205",
     "exception": false,
     "start_time": "2023-01-03T14:53:44.403699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "# callbacks = [\n",
    "#     EarlyStopping(patience=10, verbose=1),\n",
    "#     ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00001, verbose=1),\n",
    "#     ModelCheckpoint('model1.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b87140",
   "metadata": {
    "id": "Z-fV95n3GEqa",
    "papermill": {
     "duration": 0.010317,
     "end_time": "2023-01-03T14:53:44.442881",
     "exception": false,
     "start_time": "2023-01-03T14:53:44.432564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3401735c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T14:53:44.465573Z",
     "iopub.status.busy": "2023-01-03T14:53:44.465177Z",
     "iopub.status.idle": "2023-01-03T14:53:44.470324Z",
     "shell.execute_reply": "2023-01-03T14:53:44.469357Z"
    },
    "id": "_H_sb1cl1FC_",
    "outputId": "59d957da-9223-4a01-e4d9-33933f7a2f4a",
    "papermill": {
     "duration": 0.019004,
     "end_time": "2023-01-03T14:53:44.472463",
     "exception": false,
     "start_time": "2023-01-03T14:53:44.453459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# classification_report_r= []\n",
    "# model = create_model()\n",
    "# K=2\n",
    "# R=5\n",
    "# NUM_RUNS = 5\n",
    "# N_EPOCHS = 30\n",
    "# val_acc = np.zeros(NUM_RUNS)\n",
    "# for i in range(NUM_RUNS):\n",
    "#   MA = MultipleAnnotators_Classification(K, R, 0.1)\n",
    "#   model = create_model()\n",
    "#   optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, clipnorm=1.0)\n",
    "#   model.compile(optimizer=optimizer, loss= MA.loss())\n",
    "#   history_model = model.fit(train_batches_MA, validation_data=val_batches_MA, epochs= N_EPOCHS, callbacks=callbacks, verbose=0)\n",
    "#   #model = MA.fit(model, Data_train_MA, N_EPOCHS)\n",
    "#   pred_2 = model.predict(X_test)\n",
    "\n",
    "#   lambda_R_ = pred_2[:, K:] #annotators reliability prediction N x R   \n",
    "#   classification_report_r += [classification_report( pred_2[:,:K].argmax(axis=1),Y_true_test.ravel(),output_dict=True)]\n",
    "#   print(classification_report( pred_2[:,:K].argmax(axis=1),Y_true_test.ravel()))\n",
    "#   #val_acc[i] = MA.eval_model(test_batches_MA)\n",
    "#   #print(\"Validation acc: %.4f\" % (float(val_acc[i]),))\n",
    "#   # Create the history figure\n",
    "#   plt.figure(figsize=(16,9))\n",
    "#   for i in  history_model.history:\n",
    "#       plt.plot(history_model.history[i],label=i)\n",
    "#   plt.title('Model history')\n",
    "#   plt.legend()\n",
    "#   plt.grid()\n",
    "\n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame(val_acc)\n",
    "# #df.to_csimport pandas as pddf = pd.DataFrame(val_acc)#df.to_csv('/kaggle/working/CatDogs_MA_InceptionV3.csv',index=False) # save to notebook output​v('/kaggle/working/CatDogs_MA_InceptionV3.csv',index=False) # save to notebook output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13e34167",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T14:53:44.495326Z",
     "iopub.status.busy": "2023-01-03T14:53:44.494924Z",
     "iopub.status.idle": "2023-01-03T17:52:33.080827Z",
     "shell.execute_reply": "2023-01-03T17:52:33.079470Z"
    },
    "id": "Mu0lyAUIGSTB",
    "outputId": "cb82872d-c3ba-4d76-a28c-237eb266e78b",
    "papermill": {
     "duration": 10728.602012,
     "end_time": "2023-01-03T17:52:33.085080",
     "exception": false,
     "start_time": "2023-01-03T14:53:44.483068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 14:53:47.888254: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 0: 644.5963, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 580.3276, Accuracy: 0.5298\n",
      "Training loss (for one batch) at step 20: 548.0957, Accuracy: 0.5212\n",
      "Training loss (for one batch) at step 30: 514.1437, Accuracy: 0.5144\n",
      "Training loss (for one batch) at step 40: 498.6020, Accuracy: 0.5139\n",
      "Training loss (for one batch) at step 50: 485.4148, Accuracy: 0.5115\n",
      "Training loss (for one batch) at step 60: 487.1341, Accuracy: 0.5120\n",
      "Training loss (for one batch) at step 70: 484.5093, Accuracy: 0.5151\n",
      "Training loss (for one batch) at step 80: 474.6805, Accuracy: 0.5146\n",
      "Training loss (for one batch) at step 90: 467.6777, Accuracy: 0.5155\n",
      "Training loss (for one batch) at step 100: 472.0956, Accuracy: 0.5122\n",
      "Training loss (for one batch) at step 110: 464.2170, Accuracy: 0.5106\n",
      "---- Training ----\n",
      "Training loss: 142.5707\n",
      "Training acc over epoch: 0.5117\n",
      "---- Validation ----\n",
      "Validation loss: 34.6058\n",
      "Validation acc: 0.5032\n",
      "Time taken: 69.29s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 462.9399, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 460.7133, Accuracy: 0.5320\n",
      "Training loss (for one batch) at step 20: 451.8907, Accuracy: 0.5409\n",
      "Training loss (for one batch) at step 30: 458.6358, Accuracy: 0.5305\n",
      "Training loss (for one batch) at step 40: 451.1185, Accuracy: 0.5257\n",
      "Training loss (for one batch) at step 50: 452.0685, Accuracy: 0.5257\n",
      "Training loss (for one batch) at step 60: 448.4707, Accuracy: 0.5229\n",
      "Training loss (for one batch) at step 70: 451.2502, Accuracy: 0.5245\n",
      "Training loss (for one batch) at step 80: 448.5619, Accuracy: 0.5271\n",
      "Training loss (for one batch) at step 90: 450.6821, Accuracy: 0.5294\n",
      "Training loss (for one batch) at step 100: 451.3426, Accuracy: 0.5282\n",
      "Training loss (for one batch) at step 110: 451.0725, Accuracy: 0.5277\n",
      "---- Training ----\n",
      "Training loss: 139.5918\n",
      "Training acc over epoch: 0.5283\n",
      "---- Validation ----\n",
      "Validation loss: 34.8322\n",
      "Validation acc: 0.5134\n",
      "Time taken: 14.51s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 446.0131, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 444.5132, Accuracy: 0.5369\n",
      "Training loss (for one batch) at step 20: 449.7751, Accuracy: 0.5350\n",
      "Training loss (for one batch) at step 30: 449.1885, Accuracy: 0.5300\n",
      "Training loss (for one batch) at step 40: 448.3175, Accuracy: 0.5353\n",
      "Training loss (for one batch) at step 50: 444.9465, Accuracy: 0.5346\n",
      "Training loss (for one batch) at step 60: 449.7882, Accuracy: 0.5357\n",
      "Training loss (for one batch) at step 70: 448.3349, Accuracy: 0.5343\n",
      "Training loss (for one batch) at step 80: 447.6956, Accuracy: 0.5353\n",
      "Training loss (for one batch) at step 90: 446.5228, Accuracy: 0.5371\n",
      "Training loss (for one batch) at step 100: 444.9279, Accuracy: 0.5370\n",
      "Training loss (for one batch) at step 110: 444.1744, Accuracy: 0.5381\n",
      "---- Training ----\n",
      "Training loss: 139.6539\n",
      "Training acc over epoch: 0.5394\n",
      "---- Validation ----\n",
      "Validation loss: 34.7300\n",
      "Validation acc: 0.5336\n",
      "Time taken: 10.79s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 444.7161, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 442.5683, Accuracy: 0.5653\n",
      "Training loss (for one batch) at step 20: 445.9454, Accuracy: 0.5584\n",
      "Training loss (for one batch) at step 30: 443.7864, Accuracy: 0.5491\n",
      "Training loss (for one batch) at step 40: 446.0045, Accuracy: 0.5467\n",
      "Training loss (for one batch) at step 50: 442.4002, Accuracy: 0.5538\n",
      "Training loss (for one batch) at step 60: 443.0298, Accuracy: 0.5569\n",
      "Training loss (for one batch) at step 70: 443.2916, Accuracy: 0.5639\n",
      "Training loss (for one batch) at step 80: 444.2552, Accuracy: 0.5668\n",
      "Training loss (for one batch) at step 90: 446.4545, Accuracy: 0.5679\n",
      "Training loss (for one batch) at step 100: 444.7538, Accuracy: 0.5642\n",
      "Training loss (for one batch) at step 110: 441.9763, Accuracy: 0.5639\n",
      "---- Training ----\n",
      "Training loss: 138.9508\n",
      "Training acc over epoch: 0.5639\n",
      "---- Validation ----\n",
      "Validation loss: 34.7458\n",
      "Validation acc: 0.5712\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 443.5904, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 443.9901, Accuracy: 0.5888\n",
      "Training loss (for one batch) at step 20: 443.8852, Accuracy: 0.5740\n",
      "Training loss (for one batch) at step 30: 447.4104, Accuracy: 0.5743\n",
      "Training loss (for one batch) at step 40: 442.2647, Accuracy: 0.5755\n",
      "Training loss (for one batch) at step 50: 444.9301, Accuracy: 0.5728\n",
      "Training loss (for one batch) at step 60: 444.9377, Accuracy: 0.5765\n",
      "Training loss (for one batch) at step 70: 441.7375, Accuracy: 0.5785\n",
      "Training loss (for one batch) at step 80: 445.0666, Accuracy: 0.5770\n",
      "Training loss (for one batch) at step 90: 444.4104, Accuracy: 0.5747\n",
      "Training loss (for one batch) at step 100: 444.6234, Accuracy: 0.5738\n",
      "Training loss (for one batch) at step 110: 444.9907, Accuracy: 0.5737\n",
      "---- Training ----\n",
      "Training loss: 139.8518\n",
      "Training acc over epoch: 0.5729\n",
      "---- Validation ----\n",
      "Validation loss: 34.6151\n",
      "Validation acc: 0.5951\n",
      "Time taken: 11.24s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 442.1843, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 440.8871, Accuracy: 0.5795\n",
      "Training loss (for one batch) at step 20: 442.7656, Accuracy: 0.5874\n",
      "Training loss (for one batch) at step 30: 441.1606, Accuracy: 0.5927\n",
      "Training loss (for one batch) at step 40: 442.2639, Accuracy: 0.5926\n",
      "Training loss (for one batch) at step 50: 442.0319, Accuracy: 0.5907\n",
      "Training loss (for one batch) at step 60: 439.4305, Accuracy: 0.5917\n",
      "Training loss (for one batch) at step 70: 444.4564, Accuracy: 0.5967\n",
      "Training loss (for one batch) at step 80: 441.4801, Accuracy: 0.5988\n",
      "Training loss (for one batch) at step 90: 445.1162, Accuracy: 0.5980\n",
      "Training loss (for one batch) at step 100: 445.5418, Accuracy: 0.5931\n",
      "Training loss (for one batch) at step 110: 440.1514, Accuracy: 0.5896\n",
      "---- Training ----\n",
      "Training loss: 139.6916\n",
      "Training acc over epoch: 0.5899\n",
      "---- Validation ----\n",
      "Validation loss: 34.8296\n",
      "Validation acc: 0.6128\n",
      "Time taken: 10.68s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 440.6038, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 444.1661, Accuracy: 0.6030\n",
      "Training loss (for one batch) at step 20: 443.2512, Accuracy: 0.6068\n",
      "Training loss (for one batch) at step 30: 439.1767, Accuracy: 0.6147\n",
      "Training loss (for one batch) at step 40: 440.3792, Accuracy: 0.6107\n",
      "Training loss (for one batch) at step 50: 436.9886, Accuracy: 0.6138\n",
      "Training loss (for one batch) at step 60: 439.3309, Accuracy: 0.6136\n",
      "Training loss (for one batch) at step 70: 439.3676, Accuracy: 0.6202\n",
      "Training loss (for one batch) at step 80: 445.8980, Accuracy: 0.6191\n",
      "Training loss (for one batch) at step 90: 444.3709, Accuracy: 0.6196\n",
      "Training loss (for one batch) at step 100: 443.9145, Accuracy: 0.6148\n",
      "Training loss (for one batch) at step 110: 441.4257, Accuracy: 0.6118\n",
      "---- Training ----\n",
      "Training loss: 138.3066\n",
      "Training acc over epoch: 0.6132\n",
      "---- Validation ----\n",
      "Validation loss: 34.8238\n",
      "Validation acc: 0.6413\n",
      "Time taken: 10.82s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 441.7482, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 442.7325, Accuracy: 0.6214\n",
      "Training loss (for one batch) at step 20: 440.3361, Accuracy: 0.6298\n",
      "Training loss (for one batch) at step 30: 433.8498, Accuracy: 0.6348\n",
      "Training loss (for one batch) at step 40: 438.8477, Accuracy: 0.6282\n",
      "Training loss (for one batch) at step 50: 440.0989, Accuracy: 0.6314\n",
      "Training loss (for one batch) at step 60: 438.4657, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 70: 443.3924, Accuracy: 0.6336\n",
      "Training loss (for one batch) at step 80: 440.4554, Accuracy: 0.6330\n",
      "Training loss (for one batch) at step 90: 440.8037, Accuracy: 0.6314\n",
      "Training loss (for one batch) at step 100: 441.5705, Accuracy: 0.6287\n",
      "Training loss (for one batch) at step 110: 444.0055, Accuracy: 0.6283\n",
      "---- Training ----\n",
      "Training loss: 136.7685\n",
      "Training acc over epoch: 0.6289\n",
      "---- Validation ----\n",
      "Validation loss: 34.8601\n",
      "Validation acc: 0.6472\n",
      "Time taken: 10.90s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 440.7512, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 440.4313, Accuracy: 0.6548\n",
      "Training loss (for one batch) at step 20: 439.0636, Accuracy: 0.6496\n",
      "Training loss (for one batch) at step 30: 435.2459, Accuracy: 0.6467\n",
      "Training loss (for one batch) at step 40: 436.9688, Accuracy: 0.6509\n",
      "Training loss (for one batch) at step 50: 429.2420, Accuracy: 0.6530\n",
      "Training loss (for one batch) at step 60: 435.7002, Accuracy: 0.6556\n",
      "Training loss (for one batch) at step 70: 444.7416, Accuracy: 0.6544\n",
      "Training loss (for one batch) at step 80: 442.7644, Accuracy: 0.6527\n",
      "Training loss (for one batch) at step 90: 438.5749, Accuracy: 0.6512\n",
      "Training loss (for one batch) at step 100: 434.8281, Accuracy: 0.6508\n",
      "Training loss (for one batch) at step 110: 434.4201, Accuracy: 0.6478\n",
      "---- Training ----\n",
      "Training loss: 136.6347\n",
      "Training acc over epoch: 0.6485\n",
      "---- Validation ----\n",
      "Validation loss: 33.3334\n",
      "Validation acc: 0.6198\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 439.6429, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 441.5074, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 20: 435.9855, Accuracy: 0.6488\n",
      "Training loss (for one batch) at step 30: 436.2480, Accuracy: 0.6555\n",
      "Training loss (for one batch) at step 40: 437.5352, Accuracy: 0.6597\n",
      "Training loss (for one batch) at step 50: 427.5140, Accuracy: 0.6690\n",
      "Training loss (for one batch) at step 60: 436.3992, Accuracy: 0.6702\n",
      "Training loss (for one batch) at step 70: 446.0526, Accuracy: 0.6707\n",
      "Training loss (for one batch) at step 80: 439.8616, Accuracy: 0.6723\n",
      "Training loss (for one batch) at step 90: 435.7660, Accuracy: 0.6657\n",
      "Training loss (for one batch) at step 100: 435.3003, Accuracy: 0.6621\n",
      "Training loss (for one batch) at step 110: 435.6246, Accuracy: 0.6632\n",
      "---- Training ----\n",
      "Training loss: 136.9218\n",
      "Training acc over epoch: 0.6640\n",
      "---- Validation ----\n",
      "Validation loss: 35.1759\n",
      "Validation acc: 0.6867\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 439.2333, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 441.3304, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 20: 442.1333, Accuracy: 0.6737\n",
      "Training loss (for one batch) at step 30: 431.2725, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 40: 422.7488, Accuracy: 0.6785\n",
      "Training loss (for one batch) at step 50: 432.0403, Accuracy: 0.6837\n",
      "Training loss (for one batch) at step 60: 427.5970, Accuracy: 0.6902\n",
      "Training loss (for one batch) at step 70: 442.1567, Accuracy: 0.6883\n",
      "Training loss (for one batch) at step 80: 440.2323, Accuracy: 0.6871\n",
      "Training loss (for one batch) at step 90: 435.9780, Accuracy: 0.6869\n",
      "Training loss (for one batch) at step 100: 434.8007, Accuracy: 0.6860\n",
      "Training loss (for one batch) at step 110: 434.7552, Accuracy: 0.6866\n",
      "---- Training ----\n",
      "Training loss: 137.7971\n",
      "Training acc over epoch: 0.6873\n",
      "---- Validation ----\n",
      "Validation loss: 35.0632\n",
      "Validation acc: 0.6905\n",
      "Time taken: 10.72s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 439.9923, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 443.2683, Accuracy: 0.6946\n",
      "Training loss (for one batch) at step 20: 436.8956, Accuracy: 0.6961\n",
      "Training loss (for one batch) at step 30: 431.6363, Accuracy: 0.6988\n",
      "Training loss (for one batch) at step 40: 426.6308, Accuracy: 0.7016\n",
      "Training loss (for one batch) at step 50: 428.5641, Accuracy: 0.7019\n",
      "Training loss (for one batch) at step 60: 427.6174, Accuracy: 0.7054\n",
      "Training loss (for one batch) at step 70: 432.6183, Accuracy: 0.7009\n",
      "Training loss (for one batch) at step 80: 443.7714, Accuracy: 0.6965\n",
      "Training loss (for one batch) at step 90: 435.1808, Accuracy: 0.6956\n",
      "Training loss (for one batch) at step 100: 433.2344, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 110: 440.9315, Accuracy: 0.6954\n",
      "---- Training ----\n",
      "Training loss: 134.6490\n",
      "Training acc over epoch: 0.6963\n",
      "---- Validation ----\n",
      "Validation loss: 34.0282\n",
      "Validation acc: 0.7020\n",
      "Time taken: 10.78s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 438.0074, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 432.3091, Accuracy: 0.7074\n",
      "Training loss (for one batch) at step 20: 434.9218, Accuracy: 0.7001\n",
      "Training loss (for one batch) at step 30: 437.0071, Accuracy: 0.7046\n",
      "Training loss (for one batch) at step 40: 425.4399, Accuracy: 0.7138\n",
      "Training loss (for one batch) at step 50: 424.7494, Accuracy: 0.7181\n",
      "Training loss (for one batch) at step 60: 425.5937, Accuracy: 0.7234\n",
      "Training loss (for one batch) at step 70: 436.8115, Accuracy: 0.7243\n",
      "Training loss (for one batch) at step 80: 432.1421, Accuracy: 0.7233\n",
      "Training loss (for one batch) at step 90: 436.4934, Accuracy: 0.7182\n",
      "Training loss (for one batch) at step 100: 429.4952, Accuracy: 0.7172\n",
      "Training loss (for one batch) at step 110: 431.4023, Accuracy: 0.7162\n",
      "---- Training ----\n",
      "Training loss: 137.2286\n",
      "Training acc over epoch: 0.7161\n",
      "---- Validation ----\n",
      "Validation loss: 34.1726\n",
      "Validation acc: 0.7023\n",
      "Time taken: 19.33s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 444.2475, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 435.1274, Accuracy: 0.7259\n",
      "Training loss (for one batch) at step 20: 432.4087, Accuracy: 0.7202\n",
      "Training loss (for one batch) at step 30: 420.4882, Accuracy: 0.7240\n",
      "Training loss (for one batch) at step 40: 413.9495, Accuracy: 0.7268\n",
      "Training loss (for one batch) at step 50: 424.7308, Accuracy: 0.7358\n",
      "Training loss (for one batch) at step 60: 425.4380, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 70: 438.3793, Accuracy: 0.7421\n",
      "Training loss (for one batch) at step 80: 436.6960, Accuracy: 0.7399\n",
      "Training loss (for one batch) at step 90: 425.5444, Accuracy: 0.7408\n",
      "Training loss (for one batch) at step 100: 432.3726, Accuracy: 0.7415\n",
      "Training loss (for one batch) at step 110: 427.5663, Accuracy: 0.7417\n",
      "---- Training ----\n",
      "Training loss: 132.0945\n",
      "Training acc over epoch: 0.7417\n",
      "---- Validation ----\n",
      "Validation loss: 35.5997\n",
      "Validation acc: 0.7176\n",
      "Time taken: 10.59s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 433.4063, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 433.8250, Accuracy: 0.7315\n",
      "Training loss (for one batch) at step 20: 422.8215, Accuracy: 0.7366\n",
      "Training loss (for one batch) at step 30: 419.9908, Accuracy: 0.7404\n",
      "Training loss (for one batch) at step 40: 411.4472, Accuracy: 0.7475\n",
      "Training loss (for one batch) at step 50: 421.0537, Accuracy: 0.7546\n",
      "Training loss (for one batch) at step 60: 415.9276, Accuracy: 0.7613\n",
      "Training loss (for one batch) at step 70: 428.5769, Accuracy: 0.7625\n",
      "Training loss (for one batch) at step 80: 427.7402, Accuracy: 0.7602\n",
      "Training loss (for one batch) at step 90: 439.3925, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 100: 428.4184, Accuracy: 0.7523\n",
      "Training loss (for one batch) at step 110: 428.8998, Accuracy: 0.7529\n",
      "---- Training ----\n",
      "Training loss: 134.6422\n",
      "Training acc over epoch: 0.7536\n",
      "---- Validation ----\n",
      "Validation loss: 37.0456\n",
      "Validation acc: 0.7319\n",
      "Time taken: 11.83s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 437.2837, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 431.8943, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 20: 428.9602, Accuracy: 0.7455\n",
      "Training loss (for one batch) at step 30: 427.1613, Accuracy: 0.7462\n",
      "Training loss (for one batch) at step 40: 414.7323, Accuracy: 0.7508\n",
      "Training loss (for one batch) at step 50: 416.0905, Accuracy: 0.7577\n",
      "Training loss (for one batch) at step 60: 429.9780, Accuracy: 0.7634\n",
      "Training loss (for one batch) at step 70: 421.5670, Accuracy: 0.7619\n",
      "Training loss (for one batch) at step 80: 424.5942, Accuracy: 0.7619\n",
      "Training loss (for one batch) at step 90: 432.0172, Accuracy: 0.7600\n",
      "Training loss (for one batch) at step 100: 434.2305, Accuracy: 0.7562\n",
      "Training loss (for one batch) at step 110: 418.7418, Accuracy: 0.7562\n",
      "---- Training ----\n",
      "Training loss: 133.5039\n",
      "Training acc over epoch: 0.7561\n",
      "---- Validation ----\n",
      "Validation loss: 33.6496\n",
      "Validation acc: 0.7123\n",
      "Time taken: 11.25s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 438.2170, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 429.3463, Accuracy: 0.7614\n",
      "Training loss (for one batch) at step 20: 422.1669, Accuracy: 0.7608\n",
      "Training loss (for one batch) at step 30: 418.1816, Accuracy: 0.7674\n",
      "Training loss (for one batch) at step 40: 408.2328, Accuracy: 0.7708\n",
      "Training loss (for one batch) at step 50: 406.0995, Accuracy: 0.7793\n",
      "Training loss (for one batch) at step 60: 430.3306, Accuracy: 0.7833\n",
      "Training loss (for one batch) at step 70: 440.3226, Accuracy: 0.7800\n",
      "Training loss (for one batch) at step 80: 438.2401, Accuracy: 0.7773\n",
      "Training loss (for one batch) at step 90: 419.9826, Accuracy: 0.7747\n",
      "Training loss (for one batch) at step 100: 421.7888, Accuracy: 0.7727\n",
      "Training loss (for one batch) at step 110: 424.4159, Accuracy: 0.7722\n",
      "---- Training ----\n",
      "Training loss: 133.7506\n",
      "Training acc over epoch: 0.7722\n",
      "---- Validation ----\n",
      "Validation loss: 36.3691\n",
      "Validation acc: 0.7386\n",
      "Time taken: 13.53s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 437.5343, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 429.4619, Accuracy: 0.7777\n",
      "Training loss (for one batch) at step 20: 428.2731, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 30: 419.0100, Accuracy: 0.7684\n",
      "Training loss (for one batch) at step 40: 407.1850, Accuracy: 0.7746\n",
      "Training loss (for one batch) at step 50: 402.9398, Accuracy: 0.7826\n",
      "Training loss (for one batch) at step 60: 406.6894, Accuracy: 0.7875\n",
      "Training loss (for one batch) at step 70: 435.4704, Accuracy: 0.7853\n",
      "Training loss (for one batch) at step 80: 422.9910, Accuracy: 0.7809\n",
      "Training loss (for one batch) at step 90: 418.3226, Accuracy: 0.7771\n",
      "Training loss (for one batch) at step 100: 432.2151, Accuracy: 0.7739\n",
      "Training loss (for one batch) at step 110: 421.4022, Accuracy: 0.7746\n",
      "---- Training ----\n",
      "Training loss: 131.6126\n",
      "Training acc over epoch: 0.7744\n",
      "---- Validation ----\n",
      "Validation loss: 36.0224\n",
      "Validation acc: 0.7305\n",
      "Time taken: 18.86s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 431.2459, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 428.3369, Accuracy: 0.7713\n",
      "Training loss (for one batch) at step 20: 421.9424, Accuracy: 0.7645\n",
      "Training loss (for one batch) at step 30: 409.6511, Accuracy: 0.7729\n",
      "Training loss (for one batch) at step 40: 401.0296, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 50: 386.9423, Accuracy: 0.7921\n",
      "Training loss (for one batch) at step 60: 424.3724, Accuracy: 0.7943\n",
      "Training loss (for one batch) at step 70: 418.6724, Accuracy: 0.7931\n",
      "Training loss (for one batch) at step 80: 419.1172, Accuracy: 0.7906\n",
      "Training loss (for one batch) at step 90: 414.2363, Accuracy: 0.7872\n",
      "Training loss (for one batch) at step 100: 412.4989, Accuracy: 0.7869\n",
      "Training loss (for one batch) at step 110: 409.3362, Accuracy: 0.7858\n",
      "---- Training ----\n",
      "Training loss: 129.6815\n",
      "Training acc over epoch: 0.7852\n",
      "---- Validation ----\n",
      "Validation loss: 37.8053\n",
      "Validation acc: 0.7303\n",
      "Time taken: 17.40s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 435.4153, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 415.4453, Accuracy: 0.7670\n",
      "Training loss (for one batch) at step 20: 424.3017, Accuracy: 0.7783\n",
      "Training loss (for one batch) at step 30: 405.3028, Accuracy: 0.7820\n",
      "Training loss (for one batch) at step 40: 401.3598, Accuracy: 0.7919\n",
      "Training loss (for one batch) at step 50: 384.2159, Accuracy: 0.8013\n",
      "Training loss (for one batch) at step 60: 419.3807, Accuracy: 0.8087\n",
      "Training loss (for one batch) at step 70: 409.5292, Accuracy: 0.8066\n",
      "Training loss (for one batch) at step 80: 426.7231, Accuracy: 0.8061\n",
      "Training loss (for one batch) at step 90: 424.1833, Accuracy: 0.8034\n",
      "Training loss (for one batch) at step 100: 415.8732, Accuracy: 0.8022\n",
      "Training loss (for one batch) at step 110: 422.0462, Accuracy: 0.8016\n",
      "---- Training ----\n",
      "Training loss: 127.5318\n",
      "Training acc over epoch: 0.8016\n",
      "---- Validation ----\n",
      "Validation loss: 34.5377\n",
      "Validation acc: 0.7294\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 416.5140, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 411.4087, Accuracy: 0.7756\n",
      "Training loss (for one batch) at step 20: 420.0004, Accuracy: 0.7719\n",
      "Training loss (for one batch) at step 30: 418.4902, Accuracy: 0.7828\n",
      "Training loss (for one batch) at step 40: 400.0406, Accuracy: 0.7919\n",
      "Training loss (for one batch) at step 50: 405.6281, Accuracy: 0.8001\n",
      "Training loss (for one batch) at step 60: 395.8508, Accuracy: 0.8074\n",
      "Training loss (for one batch) at step 70: 418.2652, Accuracy: 0.8049\n",
      "Training loss (for one batch) at step 80: 414.3616, Accuracy: 0.8019\n",
      "Training loss (for one batch) at step 90: 410.9102, Accuracy: 0.8011\n",
      "Training loss (for one batch) at step 100: 415.2865, Accuracy: 0.7993\n",
      "Training loss (for one batch) at step 110: 417.8525, Accuracy: 0.7980\n",
      "---- Training ----\n",
      "Training loss: 130.8719\n",
      "Training acc over epoch: 0.7981\n",
      "---- Validation ----\n",
      "Validation loss: 37.8041\n",
      "Validation acc: 0.7386\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 419.8763, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 431.1681, Accuracy: 0.7876\n",
      "Training loss (for one batch) at step 20: 409.1569, Accuracy: 0.7738\n",
      "Training loss (for one batch) at step 30: 401.0722, Accuracy: 0.7762\n",
      "Training loss (for one batch) at step 40: 407.6285, Accuracy: 0.7894\n",
      "Training loss (for one batch) at step 50: 399.5101, Accuracy: 0.7972\n",
      "Training loss (for one batch) at step 60: 395.7080, Accuracy: 0.8012\n",
      "Training loss (for one batch) at step 70: 411.9377, Accuracy: 0.8012\n",
      "Training loss (for one batch) at step 80: 414.3236, Accuracy: 0.8004\n",
      "Training loss (for one batch) at step 90: 403.1052, Accuracy: 0.7976\n",
      "Training loss (for one batch) at step 100: 399.9108, Accuracy: 0.7960\n",
      "Training loss (for one batch) at step 110: 418.6441, Accuracy: 0.7960\n",
      "---- Training ----\n",
      "Training loss: 127.1641\n",
      "Training acc over epoch: 0.7965\n",
      "---- Validation ----\n",
      "Validation loss: 36.7212\n",
      "Validation acc: 0.7270\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 428.3471, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 409.8249, Accuracy: 0.7820\n",
      "Training loss (for one batch) at step 20: 407.9437, Accuracy: 0.7920\n",
      "Training loss (for one batch) at step 30: 397.6909, Accuracy: 0.8052\n",
      "Training loss (for one batch) at step 40: 389.1283, Accuracy: 0.8157\n",
      "Training loss (for one batch) at step 50: 399.9446, Accuracy: 0.8222\n",
      "Training loss (for one batch) at step 60: 374.7995, Accuracy: 0.8279\n",
      "Training loss (for one batch) at step 70: 410.5955, Accuracy: 0.8253\n",
      "Training loss (for one batch) at step 80: 407.6296, Accuracy: 0.8208\n",
      "Training loss (for one batch) at step 90: 404.0517, Accuracy: 0.8193\n",
      "Training loss (for one batch) at step 100: 407.7339, Accuracy: 0.8154\n",
      "Training loss (for one batch) at step 110: 403.4621, Accuracy: 0.8137\n",
      "---- Training ----\n",
      "Training loss: 123.7901\n",
      "Training acc over epoch: 0.8125\n",
      "---- Validation ----\n",
      "Validation loss: 38.9057\n",
      "Validation acc: 0.7630\n",
      "Time taken: 10.76s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 413.8207, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 409.4803, Accuracy: 0.7983\n",
      "Training loss (for one batch) at step 20: 400.2607, Accuracy: 0.7961\n",
      "Training loss (for one batch) at step 30: 418.4233, Accuracy: 0.7971\n",
      "Training loss (for one batch) at step 40: 382.8506, Accuracy: 0.8072\n",
      "Training loss (for one batch) at step 50: 380.8985, Accuracy: 0.8172\n",
      "Training loss (for one batch) at step 60: 394.5975, Accuracy: 0.8220\n",
      "Training loss (for one batch) at step 70: 397.8962, Accuracy: 0.8194\n",
      "Training loss (for one batch) at step 80: 400.2482, Accuracy: 0.8147\n",
      "Training loss (for one batch) at step 90: 408.3599, Accuracy: 0.8156\n",
      "Training loss (for one batch) at step 100: 406.9445, Accuracy: 0.8168\n",
      "Training loss (for one batch) at step 110: 412.9666, Accuracy: 0.8152\n",
      "---- Training ----\n",
      "Training loss: 124.1344\n",
      "Training acc over epoch: 0.8164\n",
      "---- Validation ----\n",
      "Validation loss: 36.1021\n",
      "Validation acc: 0.7673\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 414.6544, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 10: 406.5697, Accuracy: 0.8139\n",
      "Training loss (for one batch) at step 20: 411.2269, Accuracy: 0.8010\n",
      "Training loss (for one batch) at step 30: 400.5718, Accuracy: 0.8054\n",
      "Training loss (for one batch) at step 40: 378.1506, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 50: 378.3220, Accuracy: 0.8231\n",
      "Training loss (for one batch) at step 60: 382.9066, Accuracy: 0.8289\n",
      "Training loss (for one batch) at step 70: 403.1826, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 80: 411.7202, Accuracy: 0.8254\n",
      "Training loss (for one batch) at step 90: 414.8046, Accuracy: 0.8197\n",
      "Training loss (for one batch) at step 100: 398.0046, Accuracy: 0.8168\n",
      "Training loss (for one batch) at step 110: 421.6801, Accuracy: 0.8152\n",
      "---- Training ----\n",
      "Training loss: 131.9021\n",
      "Training acc over epoch: 0.8145\n",
      "---- Validation ----\n",
      "Validation loss: 32.0660\n",
      "Validation acc: 0.7512\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 415.0219, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 406.4164, Accuracy: 0.8153\n",
      "Training loss (for one batch) at step 20: 395.4117, Accuracy: 0.8069\n",
      "Training loss (for one batch) at step 30: 369.2707, Accuracy: 0.8158\n",
      "Training loss (for one batch) at step 40: 374.8956, Accuracy: 0.8230\n",
      "Training loss (for one batch) at step 50: 371.9364, Accuracy: 0.8290\n",
      "Training loss (for one batch) at step 60: 386.8807, Accuracy: 0.8320\n",
      "Training loss (for one batch) at step 70: 405.0992, Accuracy: 0.8314\n",
      "Training loss (for one batch) at step 80: 391.7899, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 90: 402.3034, Accuracy: 0.8244\n",
      "Training loss (for one batch) at step 100: 398.0177, Accuracy: 0.8208\n",
      "Training loss (for one batch) at step 110: 387.1539, Accuracy: 0.8191\n",
      "---- Training ----\n",
      "Training loss: 124.0836\n",
      "Training acc over epoch: 0.8181\n",
      "---- Validation ----\n",
      "Validation loss: 37.1761\n",
      "Validation acc: 0.7649\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 417.2802, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 396.8427, Accuracy: 0.8061\n",
      "Training loss (for one batch) at step 20: 391.2594, Accuracy: 0.7995\n",
      "Training loss (for one batch) at step 30: 379.3203, Accuracy: 0.8082\n",
      "Training loss (for one batch) at step 40: 371.6287, Accuracy: 0.8163\n",
      "Training loss (for one batch) at step 50: 370.7516, Accuracy: 0.8289\n",
      "Training loss (for one batch) at step 60: 382.4072, Accuracy: 0.8330\n",
      "Training loss (for one batch) at step 70: 385.4685, Accuracy: 0.8275\n",
      "Training loss (for one batch) at step 80: 403.3896, Accuracy: 0.8246\n",
      "Training loss (for one batch) at step 90: 406.8409, Accuracy: 0.8213\n",
      "Training loss (for one batch) at step 100: 389.9928, Accuracy: 0.8210\n",
      "Training loss (for one batch) at step 110: 416.7127, Accuracy: 0.8207\n",
      "---- Training ----\n",
      "Training loss: 121.0242\n",
      "Training acc over epoch: 0.8190\n",
      "---- Validation ----\n",
      "Validation loss: 32.6302\n",
      "Validation acc: 0.7394\n",
      "Time taken: 10.16s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 407.8267, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 392.7865, Accuracy: 0.7926\n",
      "Training loss (for one batch) at step 20: 377.1421, Accuracy: 0.8106\n",
      "Training loss (for one batch) at step 30: 385.2029, Accuracy: 0.8193\n",
      "Training loss (for one batch) at step 40: 375.3975, Accuracy: 0.8277\n",
      "Training loss (for one batch) at step 50: 377.7633, Accuracy: 0.8323\n",
      "Training loss (for one batch) at step 60: 369.2387, Accuracy: 0.8343\n",
      "Training loss (for one batch) at step 70: 394.3008, Accuracy: 0.8305\n",
      "Training loss (for one batch) at step 80: 397.2101, Accuracy: 0.8283\n",
      "Training loss (for one batch) at step 90: 399.6485, Accuracy: 0.8220\n",
      "Training loss (for one batch) at step 100: 404.7967, Accuracy: 0.8235\n",
      "Training loss (for one batch) at step 110: 392.5679, Accuracy: 0.8236\n",
      "---- Training ----\n",
      "Training loss: 116.7879\n",
      "Training acc over epoch: 0.8229\n",
      "---- Validation ----\n",
      "Validation loss: 35.9183\n",
      "Validation acc: 0.7461\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 398.0972, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 384.0282, Accuracy: 0.8217\n",
      "Training loss (for one batch) at step 20: 375.4745, Accuracy: 0.8147\n",
      "Training loss (for one batch) at step 30: 377.9064, Accuracy: 0.8158\n",
      "Training loss (for one batch) at step 40: 367.9811, Accuracy: 0.8230\n",
      "Training loss (for one batch) at step 50: 366.1536, Accuracy: 0.8324\n",
      "Training loss (for one batch) at step 60: 385.7453, Accuracy: 0.8385\n",
      "Training loss (for one batch) at step 70: 382.1343, Accuracy: 0.8348\n",
      "Training loss (for one batch) at step 80: 380.8701, Accuracy: 0.8306\n",
      "Training loss (for one batch) at step 90: 376.9126, Accuracy: 0.8286\n",
      "Training loss (for one batch) at step 100: 385.4136, Accuracy: 0.8247\n",
      "Training loss (for one batch) at step 110: 379.7318, Accuracy: 0.8247\n",
      "---- Training ----\n",
      "Training loss: 121.2470\n",
      "Training acc over epoch: 0.8246\n",
      "---- Validation ----\n",
      "Validation loss: 38.2709\n",
      "Validation acc: 0.7582\n",
      "Time taken: 10.58s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 419.2516, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 395.8547, Accuracy: 0.8189\n",
      "Training loss (for one batch) at step 20: 377.7155, Accuracy: 0.8106\n",
      "Training loss (for one batch) at step 30: 372.5443, Accuracy: 0.8153\n",
      "Training loss (for one batch) at step 40: 364.9468, Accuracy: 0.8245\n",
      "Training loss (for one batch) at step 50: 352.7408, Accuracy: 0.8318\n",
      "Training loss (for one batch) at step 60: 368.7807, Accuracy: 0.8344\n",
      "Training loss (for one batch) at step 70: 384.0918, Accuracy: 0.8319\n",
      "Training loss (for one batch) at step 80: 383.5527, Accuracy: 0.8283\n",
      "Training loss (for one batch) at step 90: 368.7428, Accuracy: 0.8259\n",
      "Training loss (for one batch) at step 100: 401.9667, Accuracy: 0.8256\n",
      "Training loss (for one batch) at step 110: 377.9626, Accuracy: 0.8254\n",
      "---- Training ----\n",
      "Training loss: 114.7362\n",
      "Training acc over epoch: 0.8241\n",
      "---- Validation ----\n",
      "Validation loss: 40.3895\n",
      "Validation acc: 0.7593\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 406.4184, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 411.4245, Accuracy: 0.8089\n",
      "Training loss (for one batch) at step 20: 384.0088, Accuracy: 0.8166\n",
      "Training loss (for one batch) at step 30: 378.0059, Accuracy: 0.8246\n",
      "Training loss (for one batch) at step 40: 358.8369, Accuracy: 0.8310\n",
      "Training loss (for one batch) at step 50: 351.2603, Accuracy: 0.8407\n",
      "Training loss (for one batch) at step 60: 365.0672, Accuracy: 0.8427\n",
      "Training loss (for one batch) at step 70: 382.0862, Accuracy: 0.8384\n",
      "Training loss (for one batch) at step 80: 394.1227, Accuracy: 0.8346\n",
      "Training loss (for one batch) at step 90: 378.6335, Accuracy: 0.8306\n",
      "Training loss (for one batch) at step 100: 372.2897, Accuracy: 0.8302\n",
      "Training loss (for one batch) at step 110: 393.2906, Accuracy: 0.8296\n",
      "---- Training ----\n",
      "Training loss: 124.6775\n",
      "Training acc over epoch: 0.8285\n",
      "---- Validation ----\n",
      "Validation loss: 37.5280\n",
      "Validation acc: 0.7603\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 406.0429, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 10: 394.1709, Accuracy: 0.8182\n",
      "Training loss (for one batch) at step 20: 381.1860, Accuracy: 0.8162\n",
      "Training loss (for one batch) at step 30: 370.0096, Accuracy: 0.8201\n",
      "Training loss (for one batch) at step 40: 372.3378, Accuracy: 0.8283\n",
      "Training loss (for one batch) at step 50: 357.5229, Accuracy: 0.8341\n",
      "Training loss (for one batch) at step 60: 355.2532, Accuracy: 0.8380\n",
      "Training loss (for one batch) at step 70: 378.7926, Accuracy: 0.8356\n",
      "Training loss (for one batch) at step 80: 388.3400, Accuracy: 0.8331\n",
      "Training loss (for one batch) at step 90: 383.3225, Accuracy: 0.8294\n",
      "Training loss (for one batch) at step 100: 362.6149, Accuracy: 0.8297\n",
      "Training loss (for one batch) at step 110: 361.8075, Accuracy: 0.8283\n",
      "---- Training ----\n",
      "Training loss: 116.3426\n",
      "Training acc over epoch: 0.8269\n",
      "---- Validation ----\n",
      "Validation loss: 35.6203\n",
      "Validation acc: 0.7582\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 386.2269, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 377.8034, Accuracy: 0.8040\n",
      "Training loss (for one batch) at step 20: 379.2094, Accuracy: 0.8110\n",
      "Training loss (for one batch) at step 30: 362.9257, Accuracy: 0.8223\n",
      "Training loss (for one batch) at step 40: 361.9850, Accuracy: 0.8325\n",
      "Training loss (for one batch) at step 50: 351.5207, Accuracy: 0.8402\n",
      "Training loss (for one batch) at step 60: 351.7516, Accuracy: 0.8431\n",
      "Training loss (for one batch) at step 70: 383.8794, Accuracy: 0.8382\n",
      "Training loss (for one batch) at step 80: 383.1635, Accuracy: 0.8369\n",
      "Training loss (for one batch) at step 90: 379.1631, Accuracy: 0.8341\n",
      "Training loss (for one batch) at step 100: 363.2632, Accuracy: 0.8336\n",
      "Training loss (for one batch) at step 110: 363.9861, Accuracy: 0.8331\n",
      "---- Training ----\n",
      "Training loss: 120.0655\n",
      "Training acc over epoch: 0.8314\n",
      "---- Validation ----\n",
      "Validation loss: 34.0530\n",
      "Validation acc: 0.7552\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 386.5440, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 10: 373.9343, Accuracy: 0.8196\n",
      "Training loss (for one batch) at step 20: 362.0239, Accuracy: 0.8181\n",
      "Training loss (for one batch) at step 30: 373.5486, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 40: 367.1069, Accuracy: 0.8308\n",
      "Training loss (for one batch) at step 50: 352.6821, Accuracy: 0.8390\n",
      "Training loss (for one batch) at step 60: 347.2000, Accuracy: 0.8439\n",
      "Training loss (for one batch) at step 70: 385.5127, Accuracy: 0.8417\n",
      "Training loss (for one batch) at step 80: 375.7629, Accuracy: 0.8378\n",
      "Training loss (for one batch) at step 90: 356.5181, Accuracy: 0.8345\n",
      "Training loss (for one batch) at step 100: 365.8121, Accuracy: 0.8346\n",
      "Training loss (for one batch) at step 110: 366.9769, Accuracy: 0.8337\n",
      "---- Training ----\n",
      "Training loss: 121.2434\n",
      "Training acc over epoch: 0.8323\n",
      "---- Validation ----\n",
      "Validation loss: 33.6463\n",
      "Validation acc: 0.7571\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 385.8177, Accuracy: 0.8516\n",
      "Training loss (for one batch) at step 10: 371.0399, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 20: 381.7061, Accuracy: 0.8240\n",
      "Training loss (for one batch) at step 30: 336.1306, Accuracy: 0.8337\n",
      "Training loss (for one batch) at step 40: 350.3344, Accuracy: 0.8396\n",
      "Training loss (for one batch) at step 50: 344.6770, Accuracy: 0.8473\n",
      "Training loss (for one batch) at step 60: 353.5747, Accuracy: 0.8477\n",
      "Training loss (for one batch) at step 70: 372.6112, Accuracy: 0.8441\n",
      "Training loss (for one batch) at step 80: 369.4947, Accuracy: 0.8402\n",
      "Training loss (for one batch) at step 90: 359.2625, Accuracy: 0.8370\n",
      "Training loss (for one batch) at step 100: 355.9453, Accuracy: 0.8361\n",
      "Training loss (for one batch) at step 110: 349.1409, Accuracy: 0.8347\n",
      "---- Training ----\n",
      "Training loss: 112.9605\n",
      "Training acc over epoch: 0.8335\n",
      "---- Validation ----\n",
      "Validation loss: 35.1636\n",
      "Validation acc: 0.7515\n",
      "Time taken: 10.51s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 379.9648, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 362.4269, Accuracy: 0.8075\n",
      "Training loss (for one batch) at step 20: 357.3028, Accuracy: 0.8099\n",
      "Training loss (for one batch) at step 30: 360.1047, Accuracy: 0.8254\n",
      "Training loss (for one batch) at step 40: 333.0535, Accuracy: 0.8335\n",
      "Training loss (for one batch) at step 50: 336.2375, Accuracy: 0.8427\n",
      "Training loss (for one batch) at step 60: 355.9827, Accuracy: 0.8414\n",
      "Training loss (for one batch) at step 70: 362.4946, Accuracy: 0.8396\n",
      "Training loss (for one batch) at step 80: 370.5359, Accuracy: 0.8362\n",
      "Training loss (for one batch) at step 90: 342.1892, Accuracy: 0.8353\n",
      "Training loss (for one batch) at step 100: 349.7118, Accuracy: 0.8356\n",
      "Training loss (for one batch) at step 110: 364.8822, Accuracy: 0.8345\n",
      "---- Training ----\n",
      "Training loss: 114.2869\n",
      "Training acc over epoch: 0.8332\n",
      "---- Validation ----\n",
      "Validation loss: 46.6849\n",
      "Validation acc: 0.7407\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 395.9534, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 373.9183, Accuracy: 0.8089\n",
      "Training loss (for one batch) at step 20: 380.6699, Accuracy: 0.8132\n",
      "Training loss (for one batch) at step 30: 360.7998, Accuracy: 0.8301\n",
      "Training loss (for one batch) at step 40: 349.4612, Accuracy: 0.8348\n",
      "Training loss (for one batch) at step 50: 331.3474, Accuracy: 0.8444\n",
      "Training loss (for one batch) at step 60: 341.1159, Accuracy: 0.8449\n",
      "Training loss (for one batch) at step 70: 358.3751, Accuracy: 0.8458\n",
      "Training loss (for one batch) at step 80: 387.7303, Accuracy: 0.8427\n",
      "Training loss (for one batch) at step 90: 352.3976, Accuracy: 0.8379\n",
      "Training loss (for one batch) at step 100: 365.6902, Accuracy: 0.8362\n",
      "Training loss (for one batch) at step 110: 356.5837, Accuracy: 0.8335\n",
      "---- Training ----\n",
      "Training loss: 123.9665\n",
      "Training acc over epoch: 0.8342\n",
      "---- Validation ----\n",
      "Validation loss: 36.4961\n",
      "Validation acc: 0.7633\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 383.2793, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 371.1243, Accuracy: 0.8224\n",
      "Training loss (for one batch) at step 20: 361.3238, Accuracy: 0.8270\n",
      "Training loss (for one batch) at step 30: 356.6070, Accuracy: 0.8337\n",
      "Training loss (for one batch) at step 40: 354.3062, Accuracy: 0.8396\n",
      "Training loss (for one batch) at step 50: 337.4252, Accuracy: 0.8459\n",
      "Training loss (for one batch) at step 60: 360.7763, Accuracy: 0.8485\n",
      "Training loss (for one batch) at step 70: 362.2757, Accuracy: 0.8446\n",
      "Training loss (for one batch) at step 80: 369.8195, Accuracy: 0.8399\n",
      "Training loss (for one batch) at step 90: 356.0046, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 100: 357.4329, Accuracy: 0.8379\n",
      "Training loss (for one batch) at step 110: 352.6396, Accuracy: 0.8371\n",
      "---- Training ----\n",
      "Training loss: 124.1730\n",
      "Training acc over epoch: 0.8347\n",
      "---- Validation ----\n",
      "Validation loss: 37.5453\n",
      "Validation acc: 0.7550\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 371.6276, Accuracy: 0.8516\n",
      "Training loss (for one batch) at step 10: 363.7741, Accuracy: 0.8175\n",
      "Training loss (for one batch) at step 20: 348.2035, Accuracy: 0.8192\n",
      "Training loss (for one batch) at step 30: 363.9190, Accuracy: 0.8271\n",
      "Training loss (for one batch) at step 40: 344.2705, Accuracy: 0.8377\n",
      "Training loss (for one batch) at step 50: 331.6465, Accuracy: 0.8428\n",
      "Training loss (for one batch) at step 60: 341.8799, Accuracy: 0.8440\n",
      "Training loss (for one batch) at step 70: 354.9703, Accuracy: 0.8429\n",
      "Training loss (for one batch) at step 80: 370.4744, Accuracy: 0.8392\n",
      "Training loss (for one batch) at step 90: 360.7007, Accuracy: 0.8365\n",
      "Training loss (for one batch) at step 100: 343.7380, Accuracy: 0.8362\n",
      "Training loss (for one batch) at step 110: 364.5863, Accuracy: 0.8366\n",
      "---- Training ----\n",
      "Training loss: 112.4355\n",
      "Training acc over epoch: 0.8366\n",
      "---- Validation ----\n",
      "Validation loss: 36.8774\n",
      "Validation acc: 0.7636\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 368.3803, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 360.2628, Accuracy: 0.8118\n",
      "Training loss (for one batch) at step 20: 345.7266, Accuracy: 0.8225\n",
      "Training loss (for one batch) at step 30: 337.8887, Accuracy: 0.8380\n",
      "Training loss (for one batch) at step 40: 353.4722, Accuracy: 0.8434\n",
      "Training loss (for one batch) at step 50: 345.8389, Accuracy: 0.8502\n",
      "Training loss (for one batch) at step 60: 349.7182, Accuracy: 0.8511\n",
      "Training loss (for one batch) at step 70: 376.7066, Accuracy: 0.8458\n",
      "Training loss (for one batch) at step 80: 362.5318, Accuracy: 0.8410\n",
      "Training loss (for one batch) at step 90: 348.4218, Accuracy: 0.8374\n",
      "Training loss (for one batch) at step 100: 337.7753, Accuracy: 0.8384\n",
      "Training loss (for one batch) at step 110: 353.4622, Accuracy: 0.8378\n",
      "---- Training ----\n",
      "Training loss: 114.7545\n",
      "Training acc over epoch: 0.8353\n",
      "---- Validation ----\n",
      "Validation loss: 42.2932\n",
      "Validation acc: 0.7431\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 370.6947, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 373.3711, Accuracy: 0.8189\n",
      "Training loss (for one batch) at step 20: 354.4062, Accuracy: 0.8259\n",
      "Training loss (for one batch) at step 30: 357.0462, Accuracy: 0.8317\n",
      "Training loss (for one batch) at step 40: 354.8965, Accuracy: 0.8401\n",
      "Training loss (for one batch) at step 50: 336.8023, Accuracy: 0.8482\n",
      "Training loss (for one batch) at step 60: 336.5706, Accuracy: 0.8496\n",
      "Training loss (for one batch) at step 70: 350.9879, Accuracy: 0.8451\n",
      "Training loss (for one batch) at step 80: 371.7592, Accuracy: 0.8402\n",
      "Training loss (for one batch) at step 90: 346.3395, Accuracy: 0.8375\n",
      "Training loss (for one batch) at step 100: 350.2733, Accuracy: 0.8372\n",
      "Training loss (for one batch) at step 110: 348.0389, Accuracy: 0.8373\n",
      "---- Training ----\n",
      "Training loss: 109.4163\n",
      "Training acc over epoch: 0.8370\n",
      "---- Validation ----\n",
      "Validation loss: 38.2517\n",
      "Validation acc: 0.7483\n",
      "Time taken: 10.54s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 355.3019, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 363.5020, Accuracy: 0.8239\n",
      "Training loss (for one batch) at step 20: 345.8543, Accuracy: 0.8296\n",
      "Training loss (for one batch) at step 30: 345.1290, Accuracy: 0.8377\n",
      "Training loss (for one batch) at step 40: 328.7235, Accuracy: 0.8457\n",
      "Training loss (for one batch) at step 50: 343.1115, Accuracy: 0.8490\n",
      "Training loss (for one batch) at step 60: 349.8507, Accuracy: 0.8522\n",
      "Training loss (for one batch) at step 70: 367.1619, Accuracy: 0.8496\n",
      "Training loss (for one batch) at step 80: 366.8938, Accuracy: 0.8454\n",
      "Training loss (for one batch) at step 90: 342.1861, Accuracy: 0.8420\n",
      "Training loss (for one batch) at step 100: 343.8985, Accuracy: 0.8410\n",
      "Training loss (for one batch) at step 110: 366.3770, Accuracy: 0.8407\n",
      "---- Training ----\n",
      "Training loss: 131.4784\n",
      "Training acc over epoch: 0.8401\n",
      "---- Validation ----\n",
      "Validation loss: 34.9151\n",
      "Validation acc: 0.7504\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 393.8024, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 369.2683, Accuracy: 0.8253\n",
      "Training loss (for one batch) at step 20: 336.8391, Accuracy: 0.8300\n",
      "Training loss (for one batch) at step 30: 352.2792, Accuracy: 0.8400\n",
      "Training loss (for one batch) at step 40: 334.4936, Accuracy: 0.8447\n",
      "Training loss (for one batch) at step 50: 336.8804, Accuracy: 0.8506\n",
      "Training loss (for one batch) at step 60: 352.7505, Accuracy: 0.8521\n",
      "Training loss (for one batch) at step 70: 356.6952, Accuracy: 0.8498\n",
      "Training loss (for one batch) at step 80: 391.4207, Accuracy: 0.8452\n",
      "Training loss (for one batch) at step 90: 332.3409, Accuracy: 0.8394\n",
      "Training loss (for one batch) at step 100: 333.2652, Accuracy: 0.8407\n",
      "Training loss (for one batch) at step 110: 349.6972, Accuracy: 0.8407\n",
      "---- Training ----\n",
      "Training loss: 118.2536\n",
      "Training acc over epoch: 0.8394\n",
      "---- Validation ----\n",
      "Validation loss: 36.5877\n",
      "Validation acc: 0.7566\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 357.4742, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 354.7709, Accuracy: 0.8210\n",
      "Training loss (for one batch) at step 20: 349.4044, Accuracy: 0.8207\n",
      "Training loss (for one batch) at step 30: 332.4646, Accuracy: 0.8324\n",
      "Training loss (for one batch) at step 40: 341.9766, Accuracy: 0.8413\n",
      "Training loss (for one batch) at step 50: 321.5654, Accuracy: 0.8493\n",
      "Training loss (for one batch) at step 60: 339.8126, Accuracy: 0.8513\n",
      "Training loss (for one batch) at step 70: 356.0041, Accuracy: 0.8479\n",
      "Training loss (for one batch) at step 80: 372.5847, Accuracy: 0.8435\n",
      "Training loss (for one batch) at step 90: 334.6768, Accuracy: 0.8406\n",
      "Training loss (for one batch) at step 100: 341.8508, Accuracy: 0.8414\n",
      "Training loss (for one batch) at step 110: 363.0265, Accuracy: 0.8397\n",
      "---- Training ----\n",
      "Training loss: 110.0555\n",
      "Training acc over epoch: 0.8383\n",
      "---- Validation ----\n",
      "Validation loss: 49.8301\n",
      "Validation acc: 0.7509\n",
      "Time taken: 10.67s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 368.5100, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 350.3340, Accuracy: 0.8239\n",
      "Training loss (for one batch) at step 20: 356.5568, Accuracy: 0.8266\n",
      "Training loss (for one batch) at step 30: 336.9693, Accuracy: 0.8357\n",
      "Training loss (for one batch) at step 40: 335.9730, Accuracy: 0.8453\n",
      "Training loss (for one batch) at step 50: 324.2921, Accuracy: 0.8534\n",
      "Training loss (for one batch) at step 60: 330.2386, Accuracy: 0.8522\n",
      "Training loss (for one batch) at step 70: 377.4230, Accuracy: 0.8471\n",
      "Training loss (for one batch) at step 80: 363.6545, Accuracy: 0.8451\n",
      "Training loss (for one batch) at step 90: 338.8524, Accuracy: 0.8434\n",
      "Training loss (for one batch) at step 100: 341.3689, Accuracy: 0.8426\n",
      "Training loss (for one batch) at step 110: 355.6366, Accuracy: 0.8413\n",
      "---- Training ----\n",
      "Training loss: 101.9656\n",
      "Training acc over epoch: 0.8410\n",
      "---- Validation ----\n",
      "Validation loss: 47.3899\n",
      "Validation acc: 0.7571\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 365.6559, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 355.4160, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 20: 338.0791, Accuracy: 0.8237\n",
      "Training loss (for one batch) at step 30: 326.6826, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 40: 330.0343, Accuracy: 0.8434\n",
      "Training loss (for one batch) at step 50: 309.3141, Accuracy: 0.8503\n",
      "Training loss (for one batch) at step 60: 330.4380, Accuracy: 0.8521\n",
      "Training loss (for one batch) at step 70: 360.2167, Accuracy: 0.8485\n",
      "Training loss (for one batch) at step 80: 369.0353, Accuracy: 0.8435\n",
      "Training loss (for one batch) at step 90: 354.4507, Accuracy: 0.8413\n",
      "Training loss (for one batch) at step 100: 344.3562, Accuracy: 0.8422\n",
      "Training loss (for one batch) at step 110: 352.3239, Accuracy: 0.8418\n",
      "---- Training ----\n",
      "Training loss: 106.3749\n",
      "Training acc over epoch: 0.8399\n",
      "---- Validation ----\n",
      "Validation loss: 63.0974\n",
      "Validation acc: 0.7477\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 368.7591, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 358.3098, Accuracy: 0.8153\n",
      "Training loss (for one batch) at step 20: 336.3994, Accuracy: 0.8300\n",
      "Training loss (for one batch) at step 30: 331.5155, Accuracy: 0.8430\n",
      "Training loss (for one batch) at step 40: 347.4822, Accuracy: 0.8495\n",
      "Training loss (for one batch) at step 50: 326.7798, Accuracy: 0.8562\n",
      "Training loss (for one batch) at step 60: 326.0155, Accuracy: 0.8553\n",
      "Training loss (for one batch) at step 70: 347.7899, Accuracy: 0.8519\n",
      "Training loss (for one batch) at step 80: 341.7420, Accuracy: 0.8459\n",
      "Training loss (for one batch) at step 90: 327.4431, Accuracy: 0.8431\n",
      "Training loss (for one batch) at step 100: 340.7573, Accuracy: 0.8429\n",
      "Training loss (for one batch) at step 110: 339.9975, Accuracy: 0.8431\n",
      "---- Training ----\n",
      "Training loss: 116.9054\n",
      "Training acc over epoch: 0.8425\n",
      "---- Validation ----\n",
      "Validation loss: 43.9961\n",
      "Validation acc: 0.7625\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 364.4979, Accuracy: 0.8984\n",
      "Training loss (for one batch) at step 10: 335.5785, Accuracy: 0.8224\n",
      "Training loss (for one batch) at step 20: 340.1976, Accuracy: 0.8352\n",
      "Training loss (for one batch) at step 30: 330.4914, Accuracy: 0.8422\n",
      "Training loss (for one batch) at step 40: 344.9908, Accuracy: 0.8462\n",
      "Training loss (for one batch) at step 50: 334.8217, Accuracy: 0.8528\n",
      "Training loss (for one batch) at step 60: 333.0540, Accuracy: 0.8535\n",
      "Training loss (for one batch) at step 70: 349.3609, Accuracy: 0.8489\n",
      "Training loss (for one batch) at step 80: 364.8148, Accuracy: 0.8439\n",
      "Training loss (for one batch) at step 90: 352.9256, Accuracy: 0.8408\n",
      "Training loss (for one batch) at step 100: 341.2365, Accuracy: 0.8426\n",
      "Training loss (for one batch) at step 110: 332.3968, Accuracy: 0.8418\n",
      "---- Training ----\n",
      "Training loss: 108.6279\n",
      "Training acc over epoch: 0.8407\n",
      "---- Validation ----\n",
      "Validation loss: 39.9259\n",
      "Validation acc: 0.7539\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 365.1876, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 10: 355.0036, Accuracy: 0.8324\n",
      "Training loss (for one batch) at step 20: 338.1155, Accuracy: 0.8326\n",
      "Training loss (for one batch) at step 30: 339.4745, Accuracy: 0.8400\n",
      "Training loss (for one batch) at step 40: 333.4914, Accuracy: 0.8457\n",
      "Training loss (for one batch) at step 50: 320.5693, Accuracy: 0.8523\n",
      "Training loss (for one batch) at step 60: 337.0791, Accuracy: 0.8558\n",
      "Training loss (for one batch) at step 70: 340.7564, Accuracy: 0.8511\n",
      "Training loss (for one batch) at step 80: 347.9417, Accuracy: 0.8464\n",
      "Training loss (for one batch) at step 90: 324.6150, Accuracy: 0.8419\n",
      "Training loss (for one batch) at step 100: 345.0322, Accuracy: 0.8415\n",
      "Training loss (for one batch) at step 110: 343.6081, Accuracy: 0.8404\n",
      "---- Training ----\n",
      "Training loss: 114.1891\n",
      "Training acc over epoch: 0.8410\n",
      "---- Validation ----\n",
      "Validation loss: 31.3642\n",
      "Validation acc: 0.7483\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 382.2737, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 348.1291, Accuracy: 0.8097\n",
      "Training loss (for one batch) at step 20: 337.3637, Accuracy: 0.8255\n",
      "Training loss (for one batch) at step 30: 333.0197, Accuracy: 0.8319\n",
      "Training loss (for one batch) at step 40: 330.3672, Accuracy: 0.8432\n",
      "Training loss (for one batch) at step 50: 315.4788, Accuracy: 0.8508\n",
      "Training loss (for one batch) at step 60: 326.4910, Accuracy: 0.8528\n",
      "Training loss (for one batch) at step 70: 343.1520, Accuracy: 0.8533\n",
      "Training loss (for one batch) at step 80: 345.1460, Accuracy: 0.8462\n",
      "Training loss (for one batch) at step 90: 330.5876, Accuracy: 0.8444\n",
      "Training loss (for one batch) at step 100: 333.6997, Accuracy: 0.8451\n",
      "Training loss (for one batch) at step 110: 338.4907, Accuracy: 0.8440\n",
      "---- Training ----\n",
      "Training loss: 102.2398\n",
      "Training acc over epoch: 0.8434\n",
      "---- Validation ----\n",
      "Validation loss: 35.7159\n",
      "Validation acc: 0.7512\n",
      "Time taken: 10.65s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 366.9188, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 10: 345.2095, Accuracy: 0.8104\n",
      "Training loss (for one batch) at step 20: 340.1493, Accuracy: 0.8296\n",
      "Training loss (for one batch) at step 30: 337.2043, Accuracy: 0.8334\n",
      "Training loss (for one batch) at step 40: 323.2359, Accuracy: 0.8443\n",
      "Training loss (for one batch) at step 50: 319.2988, Accuracy: 0.8488\n",
      "Training loss (for one batch) at step 60: 325.5819, Accuracy: 0.8539\n",
      "Training loss (for one batch) at step 70: 338.6698, Accuracy: 0.8496\n",
      "Training loss (for one batch) at step 80: 350.4957, Accuracy: 0.8449\n",
      "Training loss (for one batch) at step 90: 335.0170, Accuracy: 0.8434\n",
      "Training loss (for one batch) at step 100: 329.0255, Accuracy: 0.8420\n",
      "Training loss (for one batch) at step 110: 323.9514, Accuracy: 0.8418\n",
      "---- Training ----\n",
      "Training loss: 101.9824\n",
      "Training acc over epoch: 0.8411\n",
      "---- Validation ----\n",
      "Validation loss: 42.9897\n",
      "Validation acc: 0.7515\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 367.9337, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 359.1352, Accuracy: 0.8161\n",
      "Training loss (for one batch) at step 20: 344.9278, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 30: 330.9678, Accuracy: 0.8387\n",
      "Training loss (for one batch) at step 40: 311.3123, Accuracy: 0.8479\n",
      "Training loss (for one batch) at step 50: 323.1971, Accuracy: 0.8531\n",
      "Training loss (for one batch) at step 60: 323.9807, Accuracy: 0.8557\n",
      "Training loss (for one batch) at step 70: 333.9311, Accuracy: 0.8534\n",
      "Training loss (for one batch) at step 80: 350.8418, Accuracy: 0.8475\n",
      "Training loss (for one batch) at step 90: 331.8088, Accuracy: 0.8456\n",
      "Training loss (for one batch) at step 100: 310.7497, Accuracy: 0.8461\n",
      "Training loss (for one batch) at step 110: 333.5043, Accuracy: 0.8451\n",
      "---- Training ----\n",
      "Training loss: 111.3739\n",
      "Training acc over epoch: 0.8444\n",
      "---- Validation ----\n",
      "Validation loss: 31.4971\n",
      "Validation acc: 0.7512\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 350.4199, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 366.3141, Accuracy: 0.8139\n",
      "Training loss (for one batch) at step 20: 346.3557, Accuracy: 0.8240\n",
      "Training loss (for one batch) at step 30: 348.5819, Accuracy: 0.8372\n",
      "Training loss (for one batch) at step 40: 320.8908, Accuracy: 0.8479\n",
      "Training loss (for one batch) at step 50: 318.8291, Accuracy: 0.8537\n",
      "Training loss (for one batch) at step 60: 313.7443, Accuracy: 0.8560\n",
      "Training loss (for one batch) at step 70: 336.8254, Accuracy: 0.8508\n",
      "Training loss (for one batch) at step 80: 350.8706, Accuracy: 0.8481\n",
      "Training loss (for one batch) at step 90: 348.2073, Accuracy: 0.8454\n",
      "Training loss (for one batch) at step 100: 318.6028, Accuracy: 0.8458\n",
      "Training loss (for one batch) at step 110: 350.0662, Accuracy: 0.8449\n",
      "---- Training ----\n",
      "Training loss: 114.7418\n",
      "Training acc over epoch: 0.8438\n",
      "---- Validation ----\n",
      "Validation loss: 44.4231\n",
      "Validation acc: 0.7552\n",
      "Time taken: 10.56s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 351.7489, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 10: 352.4608, Accuracy: 0.8132\n",
      "Training loss (for one batch) at step 20: 328.8093, Accuracy: 0.8296\n",
      "Training loss (for one batch) at step 30: 322.6688, Accuracy: 0.8425\n",
      "Training loss (for one batch) at step 40: 317.3691, Accuracy: 0.8457\n",
      "Training loss (for one batch) at step 50: 313.7617, Accuracy: 0.8517\n",
      "Training loss (for one batch) at step 60: 317.7443, Accuracy: 0.8558\n",
      "Training loss (for one batch) at step 70: 349.6117, Accuracy: 0.8513\n",
      "Training loss (for one batch) at step 80: 351.4989, Accuracy: 0.8457\n",
      "Training loss (for one batch) at step 90: 334.4990, Accuracy: 0.8445\n",
      "Training loss (for one batch) at step 100: 312.3745, Accuracy: 0.8441\n",
      "Training loss (for one batch) at step 110: 332.0027, Accuracy: 0.8444\n",
      "---- Training ----\n",
      "Training loss: 105.3443\n",
      "Training acc over epoch: 0.8433\n",
      "---- Validation ----\n",
      "Validation loss: 48.7214\n",
      "Validation acc: 0.7552\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 356.1041, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 341.1327, Accuracy: 0.8288\n",
      "Training loss (for one batch) at step 20: 332.0305, Accuracy: 0.8304\n",
      "Training loss (for one batch) at step 30: 337.9052, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 40: 329.1948, Accuracy: 0.8430\n",
      "Training loss (for one batch) at step 50: 305.0886, Accuracy: 0.8488\n",
      "Training loss (for one batch) at step 60: 322.1964, Accuracy: 0.8523\n",
      "Training loss (for one batch) at step 70: 330.3240, Accuracy: 0.8484\n",
      "Training loss (for one batch) at step 80: 339.0109, Accuracy: 0.8448\n",
      "Training loss (for one batch) at step 90: 333.4104, Accuracy: 0.8430\n",
      "Training loss (for one batch) at step 100: 328.7029, Accuracy: 0.8424\n",
      "Training loss (for one batch) at step 110: 343.3120, Accuracy: 0.8418\n",
      "---- Training ----\n",
      "Training loss: 99.2871\n",
      "Training acc over epoch: 0.8422\n",
      "---- Validation ----\n",
      "Validation loss: 37.7736\n",
      "Validation acc: 0.7526\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 355.2733, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 350.7562, Accuracy: 0.8210\n",
      "Training loss (for one batch) at step 20: 319.1567, Accuracy: 0.8233\n",
      "Training loss (for one batch) at step 30: 338.1985, Accuracy: 0.8390\n",
      "Training loss (for one batch) at step 40: 314.1980, Accuracy: 0.8453\n",
      "Training loss (for one batch) at step 50: 305.1018, Accuracy: 0.8514\n",
      "Training loss (for one batch) at step 60: 321.6045, Accuracy: 0.8549\n",
      "Training loss (for one batch) at step 70: 326.4052, Accuracy: 0.8513\n",
      "Training loss (for one batch) at step 80: 347.9388, Accuracy: 0.8461\n",
      "Training loss (for one batch) at step 90: 316.9256, Accuracy: 0.8458\n",
      "Training loss (for one batch) at step 100: 322.8000, Accuracy: 0.8461\n",
      "Training loss (for one batch) at step 110: 325.5638, Accuracy: 0.8445\n",
      "---- Training ----\n",
      "Training loss: 110.6031\n",
      "Training acc over epoch: 0.8442\n",
      "---- Validation ----\n",
      "Validation loss: 50.2122\n",
      "Validation acc: 0.7542\n",
      "Time taken: 10.60s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 360.3745, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 324.2533, Accuracy: 0.8317\n",
      "Training loss (for one batch) at step 20: 308.4228, Accuracy: 0.8356\n",
      "Training loss (for one batch) at step 30: 323.6626, Accuracy: 0.8448\n",
      "Training loss (for one batch) at step 40: 301.6542, Accuracy: 0.8493\n",
      "Training loss (for one batch) at step 50: 311.8647, Accuracy: 0.8526\n",
      "Training loss (for one batch) at step 60: 316.3593, Accuracy: 0.8534\n",
      "Training loss (for one batch) at step 70: 330.5524, Accuracy: 0.8484\n",
      "Training loss (for one batch) at step 80: 335.1250, Accuracy: 0.8436\n",
      "Training loss (for one batch) at step 90: 337.8992, Accuracy: 0.8415\n",
      "Training loss (for one batch) at step 100: 323.3082, Accuracy: 0.8419\n",
      "Training loss (for one batch) at step 110: 345.8127, Accuracy: 0.8413\n",
      "---- Training ----\n",
      "Training loss: 101.8249\n",
      "Training acc over epoch: 0.8406\n",
      "---- Validation ----\n",
      "Validation loss: 50.4642\n",
      "Validation acc: 0.7480\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 357.9044, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 350.0088, Accuracy: 0.8253\n",
      "Training loss (for one batch) at step 20: 318.3276, Accuracy: 0.8255\n",
      "Training loss (for one batch) at step 30: 321.4189, Accuracy: 0.8377\n",
      "Training loss (for one batch) at step 40: 309.2737, Accuracy: 0.8466\n",
      "Training loss (for one batch) at step 50: 312.7482, Accuracy: 0.8522\n",
      "Training loss (for one batch) at step 60: 332.0701, Accuracy: 0.8560\n",
      "Training loss (for one batch) at step 70: 348.4000, Accuracy: 0.8521\n",
      "Training loss (for one batch) at step 80: 354.3419, Accuracy: 0.8456\n",
      "Training loss (for one batch) at step 90: 303.7471, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 100: 328.8613, Accuracy: 0.8441\n",
      "Training loss (for one batch) at step 110: 330.8683, Accuracy: 0.8430\n",
      "---- Training ----\n",
      "Training loss: 104.8154\n",
      "Training acc over epoch: 0.8430\n",
      "---- Validation ----\n",
      "Validation loss: 49.3125\n",
      "Validation acc: 0.7560\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 341.2289, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 10: 339.7435, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 20: 333.2315, Accuracy: 0.8341\n",
      "Training loss (for one batch) at step 30: 314.0043, Accuracy: 0.8453\n",
      "Training loss (for one batch) at step 40: 350.6426, Accuracy: 0.8552\n",
      "Training loss (for one batch) at step 50: 315.3059, Accuracy: 0.8562\n",
      "Training loss (for one batch) at step 60: 316.3054, Accuracy: 0.8566\n",
      "Training loss (for one batch) at step 70: 336.3073, Accuracy: 0.8529\n",
      "Training loss (for one batch) at step 80: 326.7676, Accuracy: 0.8491\n",
      "Training loss (for one batch) at step 90: 313.9325, Accuracy: 0.8445\n",
      "Training loss (for one batch) at step 100: 323.2266, Accuracy: 0.8441\n",
      "Training loss (for one batch) at step 110: 324.2298, Accuracy: 0.8454\n",
      "---- Training ----\n",
      "Training loss: 104.6797\n",
      "Training acc over epoch: 0.8444\n",
      "---- Validation ----\n",
      "Validation loss: 33.6893\n",
      "Validation acc: 0.7560\n",
      "Time taken: 10.60s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 364.5030, Accuracy: 0.8984\n",
      "Training loss (for one batch) at step 10: 342.8224, Accuracy: 0.8182\n",
      "Training loss (for one batch) at step 20: 350.0244, Accuracy: 0.8300\n",
      "Training loss (for one batch) at step 30: 324.3018, Accuracy: 0.8450\n",
      "Training loss (for one batch) at step 40: 317.0855, Accuracy: 0.8516\n",
      "Training loss (for one batch) at step 50: 313.6706, Accuracy: 0.8549\n",
      "Training loss (for one batch) at step 60: 335.0804, Accuracy: 0.8581\n",
      "Training loss (for one batch) at step 70: 340.3069, Accuracy: 0.8541\n",
      "Training loss (for one batch) at step 80: 325.2953, Accuracy: 0.8482\n",
      "Training loss (for one batch) at step 90: 319.6114, Accuracy: 0.8451\n",
      "Training loss (for one batch) at step 100: 305.6381, Accuracy: 0.8452\n",
      "Training loss (for one batch) at step 110: 321.8393, Accuracy: 0.8451\n",
      "---- Training ----\n",
      "Training loss: 124.8876\n",
      "Training acc over epoch: 0.8442\n",
      "---- Validation ----\n",
      "Validation loss: 41.3989\n",
      "Validation acc: 0.7636\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 343.8947, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 331.1997, Accuracy: 0.8175\n",
      "Training loss (for one batch) at step 20: 323.5225, Accuracy: 0.8315\n",
      "Training loss (for one batch) at step 30: 324.0611, Accuracy: 0.8415\n",
      "Training loss (for one batch) at step 40: 319.6122, Accuracy: 0.8455\n",
      "Training loss (for one batch) at step 50: 304.1105, Accuracy: 0.8528\n",
      "Training loss (for one batch) at step 60: 315.9866, Accuracy: 0.8590\n",
      "Training loss (for one batch) at step 70: 338.5817, Accuracy: 0.8542\n",
      "Training loss (for one batch) at step 80: 331.1748, Accuracy: 0.8484\n",
      "Training loss (for one batch) at step 90: 316.1779, Accuracy: 0.8462\n",
      "Training loss (for one batch) at step 100: 321.3929, Accuracy: 0.8468\n",
      "Training loss (for one batch) at step 110: 324.2761, Accuracy: 0.8454\n",
      "---- Training ----\n",
      "Training loss: 102.5451\n",
      "Training acc over epoch: 0.8444\n",
      "---- Validation ----\n",
      "Validation loss: 67.1598\n",
      "Validation acc: 0.7458\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 351.8513, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 340.5483, Accuracy: 0.8217\n",
      "Training loss (for one batch) at step 20: 324.6847, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 30: 319.2220, Accuracy: 0.8392\n",
      "Training loss (for one batch) at step 40: 309.4907, Accuracy: 0.8457\n",
      "Training loss (for one batch) at step 50: 312.2786, Accuracy: 0.8545\n",
      "Training loss (for one batch) at step 60: 294.6255, Accuracy: 0.8591\n",
      "Training loss (for one batch) at step 70: 333.4003, Accuracy: 0.8562\n",
      "Training loss (for one batch) at step 80: 330.5887, Accuracy: 0.8490\n",
      "Training loss (for one batch) at step 90: 325.9704, Accuracy: 0.8468\n",
      "Training loss (for one batch) at step 100: 317.3268, Accuracy: 0.8459\n",
      "Training loss (for one batch) at step 110: 323.0306, Accuracy: 0.8460\n",
      "---- Training ----\n",
      "Training loss: 108.1835\n",
      "Training acc over epoch: 0.8450\n",
      "---- Validation ----\n",
      "Validation loss: 40.5203\n",
      "Validation acc: 0.7542\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 338.9854, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 337.1121, Accuracy: 0.8168\n",
      "Training loss (for one batch) at step 20: 312.9534, Accuracy: 0.8285\n",
      "Training loss (for one batch) at step 30: 328.8785, Accuracy: 0.8390\n",
      "Training loss (for one batch) at step 40: 313.1546, Accuracy: 0.8458\n",
      "Training loss (for one batch) at step 50: 311.1785, Accuracy: 0.8526\n",
      "Training loss (for one batch) at step 60: 314.8244, Accuracy: 0.8560\n",
      "Training loss (for one batch) at step 70: 338.0340, Accuracy: 0.8508\n",
      "Training loss (for one batch) at step 80: 335.6312, Accuracy: 0.8459\n",
      "Training loss (for one batch) at step 90: 323.4955, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 100: 312.9881, Accuracy: 0.8426\n",
      "Training loss (for one batch) at step 110: 332.9247, Accuracy: 0.8412\n",
      "---- Training ----\n",
      "Training loss: 107.4015\n",
      "Training acc over epoch: 0.8415\n",
      "---- Validation ----\n",
      "Validation loss: 37.2333\n",
      "Validation acc: 0.7520\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 340.6140, Accuracy: 0.8516\n",
      "Training loss (for one batch) at step 10: 349.2760, Accuracy: 0.8224\n",
      "Training loss (for one batch) at step 20: 321.5433, Accuracy: 0.8311\n",
      "Training loss (for one batch) at step 30: 323.2400, Accuracy: 0.8422\n",
      "Training loss (for one batch) at step 40: 305.3005, Accuracy: 0.8498\n",
      "Training loss (for one batch) at step 50: 312.2933, Accuracy: 0.8557\n",
      "Training loss (for one batch) at step 60: 322.8116, Accuracy: 0.8580\n",
      "Training loss (for one batch) at step 70: 345.8282, Accuracy: 0.8526\n",
      "Training loss (for one batch) at step 80: 350.5677, Accuracy: 0.8487\n",
      "Training loss (for one batch) at step 90: 319.1437, Accuracy: 0.8458\n",
      "Training loss (for one batch) at step 100: 302.9085, Accuracy: 0.8447\n",
      "Training loss (for one batch) at step 110: 330.6087, Accuracy: 0.8430\n",
      "---- Training ----\n",
      "Training loss: 110.3069\n",
      "Training acc over epoch: 0.8440\n",
      "---- Validation ----\n",
      "Validation loss: 42.8034\n",
      "Validation acc: 0.7515\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 347.7157, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 330.4344, Accuracy: 0.8267\n",
      "Training loss (for one batch) at step 20: 316.9556, Accuracy: 0.8311\n",
      "Training loss (for one batch) at step 30: 315.5268, Accuracy: 0.8422\n",
      "Training loss (for one batch) at step 40: 322.8044, Accuracy: 0.8487\n",
      "Training loss (for one batch) at step 50: 301.7773, Accuracy: 0.8552\n",
      "Training loss (for one batch) at step 60: 324.3004, Accuracy: 0.8586\n",
      "Training loss (for one batch) at step 70: 329.9104, Accuracy: 0.8518\n",
      "Training loss (for one batch) at step 80: 342.9855, Accuracy: 0.8481\n",
      "Training loss (for one batch) at step 90: 331.7708, Accuracy: 0.8451\n",
      "Training loss (for one batch) at step 100: 308.7961, Accuracy: 0.8441\n",
      "Training loss (for one batch) at step 110: 316.0389, Accuracy: 0.8440\n",
      "---- Training ----\n",
      "Training loss: 102.4803\n",
      "Training acc over epoch: 0.8439\n",
      "---- Validation ----\n",
      "Validation loss: 42.7803\n",
      "Validation acc: 0.7448\n",
      "Time taken: 10.74s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 358.6392, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 325.1285, Accuracy: 0.8182\n",
      "Training loss (for one batch) at step 20: 316.0695, Accuracy: 0.8263\n",
      "Training loss (for one batch) at step 30: 318.1812, Accuracy: 0.8417\n",
      "Training loss (for one batch) at step 40: 317.9122, Accuracy: 0.8476\n",
      "Training loss (for one batch) at step 50: 317.3237, Accuracy: 0.8548\n",
      "Training loss (for one batch) at step 60: 303.5562, Accuracy: 0.8563\n",
      "Training loss (for one batch) at step 70: 319.8517, Accuracy: 0.8531\n",
      "Training loss (for one batch) at step 80: 345.0020, Accuracy: 0.8458\n",
      "Training loss (for one batch) at step 90: 313.2031, Accuracy: 0.8437\n",
      "Training loss (for one batch) at step 100: 313.2183, Accuracy: 0.8446\n",
      "Training loss (for one batch) at step 110: 301.5129, Accuracy: 0.8429\n",
      "---- Training ----\n",
      "Training loss: 121.6826\n",
      "Training acc over epoch: 0.8420\n",
      "---- Validation ----\n",
      "Validation loss: 47.7359\n",
      "Validation acc: 0.7528\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 350.6948, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 324.5417, Accuracy: 0.8288\n",
      "Training loss (for one batch) at step 20: 309.2910, Accuracy: 0.8318\n",
      "Training loss (for one batch) at step 30: 312.7982, Accuracy: 0.8443\n",
      "Training loss (for one batch) at step 40: 329.1637, Accuracy: 0.8504\n",
      "Training loss (for one batch) at step 50: 308.2677, Accuracy: 0.8591\n",
      "Training loss (for one batch) at step 60: 305.8117, Accuracy: 0.8613\n",
      "Training loss (for one batch) at step 70: 315.2562, Accuracy: 0.8566\n",
      "Training loss (for one batch) at step 80: 322.8655, Accuracy: 0.8519\n",
      "Training loss (for one batch) at step 90: 319.9034, Accuracy: 0.8481\n",
      "Training loss (for one batch) at step 100: 303.1787, Accuracy: 0.8488\n",
      "Training loss (for one batch) at step 110: 328.0923, Accuracy: 0.8490\n",
      "---- Training ----\n",
      "Training loss: 110.5441\n",
      "Training acc over epoch: 0.8470\n",
      "---- Validation ----\n",
      "Validation loss: 49.1925\n",
      "Validation acc: 0.7585\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 357.7313, Accuracy: 0.8828\n",
      "Training loss (for one batch) at step 10: 351.1030, Accuracy: 0.8324\n",
      "Training loss (for one batch) at step 20: 336.1411, Accuracy: 0.8292\n",
      "Training loss (for one batch) at step 30: 308.2949, Accuracy: 0.8422\n",
      "Training loss (for one batch) at step 40: 307.3124, Accuracy: 0.8510\n",
      "Training loss (for one batch) at step 50: 305.0449, Accuracy: 0.8580\n",
      "Training loss (for one batch) at step 60: 297.2154, Accuracy: 0.8601\n",
      "Training loss (for one batch) at step 70: 323.5174, Accuracy: 0.8572\n",
      "Training loss (for one batch) at step 80: 333.9934, Accuracy: 0.8519\n",
      "Training loss (for one batch) at step 90: 330.3394, Accuracy: 0.8478\n",
      "Training loss (for one batch) at step 100: 312.8418, Accuracy: 0.8483\n",
      "Training loss (for one batch) at step 110: 324.8395, Accuracy: 0.8478\n",
      "---- Training ----\n",
      "Training loss: 108.0059\n",
      "Training acc over epoch: 0.8472\n",
      "---- Validation ----\n",
      "Validation loss: 44.8710\n",
      "Validation acc: 0.7555\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 336.4036, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 341.3553, Accuracy: 0.8295\n",
      "Training loss (for one batch) at step 20: 318.6311, Accuracy: 0.8344\n",
      "Training loss (for one batch) at step 30: 313.7839, Accuracy: 0.8475\n",
      "Training loss (for one batch) at step 40: 306.7634, Accuracy: 0.8533\n",
      "Training loss (for one batch) at step 50: 300.1560, Accuracy: 0.8559\n",
      "Training loss (for one batch) at step 60: 318.7335, Accuracy: 0.8575\n",
      "Training loss (for one batch) at step 70: 329.1912, Accuracy: 0.8544\n",
      "Training loss (for one batch) at step 80: 322.6651, Accuracy: 0.8474\n",
      "Training loss (for one batch) at step 90: 314.2988, Accuracy: 0.8467\n",
      "Training loss (for one batch) at step 100: 323.7372, Accuracy: 0.8490\n",
      "Training loss (for one batch) at step 110: 315.8039, Accuracy: 0.8472\n",
      "---- Training ----\n",
      "Training loss: 108.0154\n",
      "Training acc over epoch: 0.8458\n",
      "---- Validation ----\n",
      "Validation loss: 41.7327\n",
      "Validation acc: 0.7544\n",
      "Time taken: 10.60s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 337.1746, Accuracy: 0.8672\n",
      "Training loss (for one batch) at step 10: 329.7505, Accuracy: 0.8267\n",
      "Training loss (for one batch) at step 20: 313.0742, Accuracy: 0.8348\n",
      "Training loss (for one batch) at step 30: 297.7552, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 40: 309.5454, Accuracy: 0.8506\n",
      "Training loss (for one batch) at step 50: 316.4094, Accuracy: 0.8571\n",
      "Training loss (for one batch) at step 60: 313.3215, Accuracy: 0.8576\n",
      "Training loss (for one batch) at step 70: 331.5657, Accuracy: 0.8545\n",
      "Training loss (for one batch) at step 80: 317.8428, Accuracy: 0.8497\n",
      "Training loss (for one batch) at step 90: 304.5739, Accuracy: 0.8474\n",
      "Training loss (for one batch) at step 100: 315.3586, Accuracy: 0.8469\n",
      "Training loss (for one batch) at step 110: 345.7984, Accuracy: 0.8478\n",
      "---- Training ----\n",
      "Training loss: 112.4932\n",
      "Training acc over epoch: 0.8463\n",
      "---- Validation ----\n",
      "Validation loss: 49.9990\n",
      "Validation acc: 0.7509\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 358.3785, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 339.4116, Accuracy: 0.8146\n",
      "Training loss (for one batch) at step 20: 319.3591, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 30: 296.8907, Accuracy: 0.8450\n",
      "Training loss (for one batch) at step 40: 310.7338, Accuracy: 0.8510\n",
      "Training loss (for one batch) at step 50: 299.9108, Accuracy: 0.8534\n",
      "Training loss (for one batch) at step 60: 320.4720, Accuracy: 0.8591\n",
      "Training loss (for one batch) at step 70: 337.8041, Accuracy: 0.8556\n",
      "Training loss (for one batch) at step 80: 316.1733, Accuracy: 0.8489\n",
      "Training loss (for one batch) at step 90: 315.0349, Accuracy: 0.8474\n",
      "Training loss (for one batch) at step 100: 316.9846, Accuracy: 0.8462\n",
      "Training loss (for one batch) at step 110: 319.6551, Accuracy: 0.8457\n",
      "---- Training ----\n",
      "Training loss: 99.6959\n",
      "Training acc over epoch: 0.8456\n",
      "---- Validation ----\n",
      "Validation loss: 55.4241\n",
      "Validation acc: 0.7579\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 325.4776, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 329.6666, Accuracy: 0.8317\n",
      "Training loss (for one batch) at step 20: 325.1728, Accuracy: 0.8318\n",
      "Training loss (for one batch) at step 30: 306.7163, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 40: 314.4809, Accuracy: 0.8529\n",
      "Training loss (for one batch) at step 50: 285.0917, Accuracy: 0.8566\n",
      "Training loss (for one batch) at step 60: 310.7036, Accuracy: 0.8612\n",
      "Training loss (for one batch) at step 70: 344.3093, Accuracy: 0.8562\n",
      "Training loss (for one batch) at step 80: 339.3348, Accuracy: 0.8527\n",
      "Training loss (for one batch) at step 90: 316.3003, Accuracy: 0.8503\n",
      "Training loss (for one batch) at step 100: 321.6567, Accuracy: 0.8495\n",
      "Training loss (for one batch) at step 110: 316.9079, Accuracy: 0.8471\n",
      "---- Training ----\n",
      "Training loss: 107.0106\n",
      "Training acc over epoch: 0.8472\n",
      "---- Validation ----\n",
      "Validation loss: 43.5573\n",
      "Validation acc: 0.7466\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 337.7021, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 318.5460, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 20: 322.6180, Accuracy: 0.8337\n",
      "Training loss (for one batch) at step 30: 311.7835, Accuracy: 0.8443\n",
      "Training loss (for one batch) at step 40: 303.6205, Accuracy: 0.8512\n",
      "Training loss (for one batch) at step 50: 306.6718, Accuracy: 0.8591\n",
      "Training loss (for one batch) at step 60: 312.9877, Accuracy: 0.8637\n",
      "Training loss (for one batch) at step 70: 322.4095, Accuracy: 0.8568\n",
      "Training loss (for one batch) at step 80: 333.4215, Accuracy: 0.8507\n",
      "Training loss (for one batch) at step 90: 309.2078, Accuracy: 0.8488\n",
      "Training loss (for one batch) at step 100: 310.1557, Accuracy: 0.8482\n",
      "Training loss (for one batch) at step 110: 325.5457, Accuracy: 0.8480\n",
      "---- Training ----\n",
      "Training loss: 111.4737\n",
      "Training acc over epoch: 0.8479\n",
      "---- Validation ----\n",
      "Validation loss: 55.6585\n",
      "Validation acc: 0.7611\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 343.2551, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 10: 318.3094, Accuracy: 0.8288\n",
      "Training loss (for one batch) at step 20: 313.5735, Accuracy: 0.8426\n",
      "Training loss (for one batch) at step 30: 323.4932, Accuracy: 0.8513\n",
      "Training loss (for one batch) at step 40: 306.8649, Accuracy: 0.8575\n",
      "Training loss (for one batch) at step 50: 309.9569, Accuracy: 0.8621\n",
      "Training loss (for one batch) at step 60: 318.7414, Accuracy: 0.8640\n",
      "Training loss (for one batch) at step 70: 328.6189, Accuracy: 0.8604\n",
      "Training loss (for one batch) at step 80: 320.7946, Accuracy: 0.8542\n",
      "Training loss (for one batch) at step 90: 310.7484, Accuracy: 0.8513\n",
      "Training loss (for one batch) at step 100: 304.7722, Accuracy: 0.8502\n",
      "Training loss (for one batch) at step 110: 346.3310, Accuracy: 0.8494\n",
      "---- Training ----\n",
      "Training loss: 96.6232\n",
      "Training acc over epoch: 0.8496\n",
      "---- Validation ----\n",
      "Validation loss: 41.3882\n",
      "Validation acc: 0.7501\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 346.2612, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 10: 332.9279, Accuracy: 0.8189\n",
      "Training loss (for one batch) at step 20: 308.4048, Accuracy: 0.8330\n",
      "Training loss (for one batch) at step 30: 315.4297, Accuracy: 0.8445\n",
      "Training loss (for one batch) at step 40: 328.5301, Accuracy: 0.8504\n",
      "Training loss (for one batch) at step 50: 292.9678, Accuracy: 0.8559\n",
      "Training loss (for one batch) at step 60: 321.4792, Accuracy: 0.8612\n",
      "Training loss (for one batch) at step 70: 311.6201, Accuracy: 0.8582\n",
      "Training loss (for one batch) at step 80: 326.2246, Accuracy: 0.8508\n",
      "Training loss (for one batch) at step 90: 298.9763, Accuracy: 0.8480\n",
      "Training loss (for one batch) at step 100: 294.0202, Accuracy: 0.8482\n",
      "Training loss (for one batch) at step 110: 301.3263, Accuracy: 0.8476\n",
      "---- Training ----\n",
      "Training loss: 94.7291\n",
      "Training acc over epoch: 0.8478\n",
      "---- Validation ----\n",
      "Validation loss: 56.0450\n",
      "Validation acc: 0.7571\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 333.1024, Accuracy: 0.8516\n",
      "Training loss (for one batch) at step 10: 320.8073, Accuracy: 0.8224\n",
      "Training loss (for one batch) at step 20: 312.2105, Accuracy: 0.8296\n",
      "Training loss (for one batch) at step 30: 315.8290, Accuracy: 0.8435\n",
      "Training loss (for one batch) at step 40: 310.8497, Accuracy: 0.8519\n",
      "Training loss (for one batch) at step 50: 296.6581, Accuracy: 0.8594\n",
      "Training loss (for one batch) at step 60: 324.2803, Accuracy: 0.8627\n",
      "Training loss (for one batch) at step 70: 313.0095, Accuracy: 0.8592\n",
      "Training loss (for one batch) at step 80: 361.5676, Accuracy: 0.8525\n",
      "Training loss (for one batch) at step 90: 309.0725, Accuracy: 0.8492\n",
      "Training loss (for one batch) at step 100: 322.4066, Accuracy: 0.8489\n",
      "Training loss (for one batch) at step 110: 318.9281, Accuracy: 0.8473\n",
      "---- Training ----\n",
      "Training loss: 93.9217\n",
      "Training acc over epoch: 0.8475\n",
      "---- Validation ----\n",
      "Validation loss: 42.9184\n",
      "Validation acc: 0.7609\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 331.9584, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 324.2074, Accuracy: 0.8168\n",
      "Training loss (for one batch) at step 20: 315.3904, Accuracy: 0.8356\n",
      "Training loss (for one batch) at step 30: 304.1920, Accuracy: 0.8480\n",
      "Training loss (for one batch) at step 40: 292.8177, Accuracy: 0.8582\n",
      "Training loss (for one batch) at step 50: 301.5409, Accuracy: 0.8606\n",
      "Training loss (for one batch) at step 60: 320.5461, Accuracy: 0.8630\n",
      "Training loss (for one batch) at step 70: 330.6267, Accuracy: 0.8592\n",
      "Training loss (for one batch) at step 80: 328.9120, Accuracy: 0.8550\n",
      "Training loss (for one batch) at step 90: 322.2251, Accuracy: 0.8522\n",
      "Training loss (for one batch) at step 100: 297.2621, Accuracy: 0.8510\n",
      "Training loss (for one batch) at step 110: 325.9461, Accuracy: 0.8516\n",
      "---- Training ----\n",
      "Training loss: 104.3753\n",
      "Training acc over epoch: 0.8512\n",
      "---- Validation ----\n",
      "Validation loss: 84.5961\n",
      "Validation acc: 0.7590\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 341.4542, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 327.8875, Accuracy: 0.8189\n",
      "Training loss (for one batch) at step 20: 304.2455, Accuracy: 0.8326\n",
      "Training loss (for one batch) at step 30: 310.4634, Accuracy: 0.8420\n",
      "Training loss (for one batch) at step 40: 289.7553, Accuracy: 0.8506\n",
      "Training loss (for one batch) at step 50: 310.1974, Accuracy: 0.8572\n",
      "Training loss (for one batch) at step 60: 316.0845, Accuracy: 0.8589\n",
      "Training loss (for one batch) at step 70: 341.1192, Accuracy: 0.8552\n",
      "Training loss (for one batch) at step 80: 342.4269, Accuracy: 0.8498\n",
      "Training loss (for one batch) at step 90: 302.7808, Accuracy: 0.8484\n",
      "Training loss (for one batch) at step 100: 296.1168, Accuracy: 0.8489\n",
      "Training loss (for one batch) at step 110: 306.6459, Accuracy: 0.8479\n",
      "---- Training ----\n",
      "Training loss: 95.8507\n",
      "Training acc over epoch: 0.8476\n",
      "---- Validation ----\n",
      "Validation loss: 56.3750\n",
      "Validation acc: 0.7638\n",
      "Time taken: 10.58s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 325.6451, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 10: 324.8319, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 20: 299.9957, Accuracy: 0.8344\n",
      "Training loss (for one batch) at step 30: 308.0931, Accuracy: 0.8432\n",
      "Training loss (for one batch) at step 40: 322.2757, Accuracy: 0.8531\n",
      "Training loss (for one batch) at step 50: 317.5311, Accuracy: 0.8569\n",
      "Training loss (for one batch) at step 60: 305.9803, Accuracy: 0.8590\n",
      "Training loss (for one batch) at step 70: 333.4370, Accuracy: 0.8527\n",
      "Training loss (for one batch) at step 80: 342.0375, Accuracy: 0.8472\n",
      "Training loss (for one batch) at step 90: 301.5725, Accuracy: 0.8460\n",
      "Training loss (for one batch) at step 100: 300.2256, Accuracy: 0.8454\n",
      "Training loss (for one batch) at step 110: 318.6771, Accuracy: 0.8450\n",
      "---- Training ----\n",
      "Training loss: 101.5885\n",
      "Training acc over epoch: 0.8456\n",
      "---- Validation ----\n",
      "Validation loss: 47.5847\n",
      "Validation acc: 0.7641\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 330.5524, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 10: 330.8597, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 20: 305.6240, Accuracy: 0.8333\n",
      "Training loss (for one batch) at step 30: 302.8397, Accuracy: 0.8463\n",
      "Training loss (for one batch) at step 40: 292.4154, Accuracy: 0.8542\n",
      "Training loss (for one batch) at step 50: 291.5797, Accuracy: 0.8586\n",
      "Training loss (for one batch) at step 60: 306.5909, Accuracy: 0.8577\n",
      "Training loss (for one batch) at step 70: 312.3186, Accuracy: 0.8562\n",
      "Training loss (for one batch) at step 80: 324.6921, Accuracy: 0.8508\n",
      "Training loss (for one batch) at step 90: 316.6927, Accuracy: 0.8498\n",
      "Training loss (for one batch) at step 100: 303.6998, Accuracy: 0.8510\n",
      "Training loss (for one batch) at step 110: 308.1261, Accuracy: 0.8482\n",
      "---- Training ----\n",
      "Training loss: 103.0905\n",
      "Training acc over epoch: 0.8477\n",
      "---- Validation ----\n",
      "Validation loss: 40.3216\n",
      "Validation acc: 0.7636\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 332.7203, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 10: 327.9429, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 20: 326.0055, Accuracy: 0.8337\n",
      "Training loss (for one batch) at step 30: 293.5442, Accuracy: 0.8485\n",
      "Training loss (for one batch) at step 40: 314.6995, Accuracy: 0.8533\n",
      "Training loss (for one batch) at step 50: 308.4013, Accuracy: 0.8586\n",
      "Training loss (for one batch) at step 60: 318.6812, Accuracy: 0.8607\n",
      "Training loss (for one batch) at step 70: 327.9566, Accuracy: 0.8572\n",
      "Training loss (for one batch) at step 80: 322.1151, Accuracy: 0.8503\n",
      "Training loss (for one batch) at step 90: 302.3317, Accuracy: 0.8511\n",
      "Training loss (for one batch) at step 100: 296.6070, Accuracy: 0.8493\n",
      "Training loss (for one batch) at step 110: 314.2914, Accuracy: 0.8483\n",
      "---- Training ----\n",
      "Training loss: 91.3574\n",
      "Training acc over epoch: 0.8485\n",
      "---- Validation ----\n",
      "Validation loss: 45.8471\n",
      "Validation acc: 0.7563\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 332.1903, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 10: 314.6381, Accuracy: 0.8452\n",
      "Training loss (for one batch) at step 20: 304.8238, Accuracy: 0.8456\n",
      "Training loss (for one batch) at step 30: 312.4362, Accuracy: 0.8473\n",
      "Training loss (for one batch) at step 40: 298.7365, Accuracy: 0.8548\n",
      "Training loss (for one batch) at step 50: 290.9512, Accuracy: 0.8594\n",
      "Training loss (for one batch) at step 60: 320.8016, Accuracy: 0.8604\n",
      "Training loss (for one batch) at step 70: 302.4517, Accuracy: 0.8581\n",
      "Training loss (for one batch) at step 80: 330.3998, Accuracy: 0.8525\n",
      "Training loss (for one batch) at step 90: 316.8452, Accuracy: 0.8493\n",
      "Training loss (for one batch) at step 100: 325.7034, Accuracy: 0.8491\n",
      "Training loss (for one batch) at step 110: 302.9375, Accuracy: 0.8491\n",
      "---- Training ----\n",
      "Training loss: 109.6041\n",
      "Training acc over epoch: 0.8485\n",
      "---- Validation ----\n",
      "Validation loss: 65.4154\n",
      "Validation acc: 0.7550\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 333.3190, Accuracy: 0.8828\n",
      "Training loss (for one batch) at step 10: 337.7328, Accuracy: 0.8303\n",
      "Training loss (for one batch) at step 20: 307.9893, Accuracy: 0.8307\n",
      "Training loss (for one batch) at step 30: 304.6583, Accuracy: 0.8430\n",
      "Training loss (for one batch) at step 40: 287.2282, Accuracy: 0.8510\n",
      "Training loss (for one batch) at step 50: 282.6283, Accuracy: 0.8580\n",
      "Training loss (for one batch) at step 60: 316.6355, Accuracy: 0.8613\n",
      "Training loss (for one batch) at step 70: 301.2233, Accuracy: 0.8557\n",
      "Training loss (for one batch) at step 80: 317.1469, Accuracy: 0.8500\n",
      "Training loss (for one batch) at step 90: 294.7892, Accuracy: 0.8476\n",
      "Training loss (for one batch) at step 100: 308.5067, Accuracy: 0.8472\n",
      "Training loss (for one batch) at step 110: 299.2330, Accuracy: 0.8470\n",
      "---- Training ----\n",
      "Training loss: 113.1994\n",
      "Training acc over epoch: 0.8461\n",
      "---- Validation ----\n",
      "Validation loss: 60.1012\n",
      "Validation acc: 0.7622\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 326.5955, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 10: 320.1821, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 20: 309.5551, Accuracy: 0.8363\n",
      "Training loss (for one batch) at step 30: 295.9982, Accuracy: 0.8453\n",
      "Training loss (for one batch) at step 40: 298.0733, Accuracy: 0.8546\n",
      "Training loss (for one batch) at step 50: 311.9573, Accuracy: 0.8598\n",
      "Training loss (for one batch) at step 60: 299.6128, Accuracy: 0.8616\n",
      "Training loss (for one batch) at step 70: 317.9423, Accuracy: 0.8588\n",
      "Training loss (for one batch) at step 80: 331.5280, Accuracy: 0.8519\n",
      "Training loss (for one batch) at step 90: 310.3843, Accuracy: 0.8489\n",
      "Training loss (for one batch) at step 100: 288.4612, Accuracy: 0.8495\n",
      "Training loss (for one batch) at step 110: 319.7510, Accuracy: 0.8479\n",
      "---- Training ----\n",
      "Training loss: 107.7797\n",
      "Training acc over epoch: 0.8481\n",
      "---- Validation ----\n",
      "Validation loss: 62.8809\n",
      "Validation acc: 0.7601\n",
      "Time taken: 10.70s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 317.9564, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 329.6347, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 20: 292.3374, Accuracy: 0.8311\n",
      "Training loss (for one batch) at step 30: 295.5517, Accuracy: 0.8440\n",
      "Training loss (for one batch) at step 40: 311.9806, Accuracy: 0.8535\n",
      "Training loss (for one batch) at step 50: 286.4988, Accuracy: 0.8571\n",
      "Training loss (for one batch) at step 60: 297.0454, Accuracy: 0.8599\n",
      "Training loss (for one batch) at step 70: 330.1742, Accuracy: 0.8553\n",
      "Training loss (for one batch) at step 80: 336.6313, Accuracy: 0.8509\n",
      "Training loss (for one batch) at step 90: 303.4512, Accuracy: 0.8482\n",
      "Training loss (for one batch) at step 100: 288.2973, Accuracy: 0.8485\n",
      "Training loss (for one batch) at step 110: 306.5009, Accuracy: 0.8486\n",
      "---- Training ----\n",
      "Training loss: 109.2027\n",
      "Training acc over epoch: 0.8481\n",
      "---- Validation ----\n",
      "Validation loss: 45.1004\n",
      "Validation acc: 0.7550\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 321.4384, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 10: 321.2458, Accuracy: 0.8381\n",
      "Training loss (for one batch) at step 20: 293.3259, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 30: 283.7925, Accuracy: 0.8533\n",
      "Training loss (for one batch) at step 40: 294.8980, Accuracy: 0.8586\n",
      "Training loss (for one batch) at step 50: 293.1461, Accuracy: 0.8629\n",
      "Training loss (for one batch) at step 60: 293.1343, Accuracy: 0.8658\n",
      "Training loss (for one batch) at step 70: 301.1335, Accuracy: 0.8618\n",
      "Training loss (for one batch) at step 80: 321.4637, Accuracy: 0.8567\n",
      "Training loss (for one batch) at step 90: 301.5613, Accuracy: 0.8534\n",
      "Training loss (for one batch) at step 100: 312.8918, Accuracy: 0.8518\n",
      "Training loss (for one batch) at step 110: 304.0641, Accuracy: 0.8519\n",
      "---- Training ----\n",
      "Training loss: 94.4740\n",
      "Training acc over epoch: 0.8504\n",
      "---- Validation ----\n",
      "Validation loss: 63.6182\n",
      "Validation acc: 0.7595\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 326.8582, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 326.1984, Accuracy: 0.8317\n",
      "Training loss (for one batch) at step 20: 310.0681, Accuracy: 0.8445\n",
      "Training loss (for one batch) at step 30: 309.7631, Accuracy: 0.8498\n",
      "Training loss (for one batch) at step 40: 295.7910, Accuracy: 0.8571\n",
      "Training loss (for one batch) at step 50: 289.4317, Accuracy: 0.8624\n",
      "Training loss (for one batch) at step 60: 320.9555, Accuracy: 0.8645\n",
      "Training loss (for one batch) at step 70: 316.9924, Accuracy: 0.8588\n",
      "Training loss (for one batch) at step 80: 337.5455, Accuracy: 0.8535\n",
      "Training loss (for one batch) at step 90: 307.9915, Accuracy: 0.8516\n",
      "Training loss (for one batch) at step 100: 295.6635, Accuracy: 0.8505\n",
      "Training loss (for one batch) at step 110: 322.4587, Accuracy: 0.8497\n",
      "---- Training ----\n",
      "Training loss: 107.4258\n",
      "Training acc over epoch: 0.8487\n",
      "---- Validation ----\n",
      "Validation loss: 60.4617\n",
      "Validation acc: 0.7520\n",
      "Time taken: 10.81s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 327.1305, Accuracy: 0.8516\n",
      "Training loss (for one batch) at step 10: 323.6132, Accuracy: 0.8459\n",
      "Training loss (for one batch) at step 20: 284.6332, Accuracy: 0.8423\n",
      "Training loss (for one batch) at step 30: 309.1421, Accuracy: 0.8503\n",
      "Training loss (for one batch) at step 40: 295.8295, Accuracy: 0.8542\n",
      "Training loss (for one batch) at step 50: 288.7945, Accuracy: 0.8620\n",
      "Training loss (for one batch) at step 60: 306.3718, Accuracy: 0.8621\n",
      "Training loss (for one batch) at step 70: 321.8559, Accuracy: 0.8583\n",
      "Training loss (for one batch) at step 80: 307.3825, Accuracy: 0.8532\n",
      "Training loss (for one batch) at step 90: 311.3987, Accuracy: 0.8515\n",
      "Training loss (for one batch) at step 100: 292.2316, Accuracy: 0.8513\n",
      "Training loss (for one batch) at step 110: 308.3813, Accuracy: 0.8514\n",
      "---- Training ----\n",
      "Training loss: 101.8787\n",
      "Training acc over epoch: 0.8499\n",
      "---- Validation ----\n",
      "Validation loss: 56.2408\n",
      "Validation acc: 0.7550\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 309.6951, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 10: 309.0313, Accuracy: 0.8253\n",
      "Training loss (for one batch) at step 20: 289.5272, Accuracy: 0.8348\n",
      "Training loss (for one batch) at step 30: 287.0830, Accuracy: 0.8453\n",
      "Training loss (for one batch) at step 40: 285.1507, Accuracy: 0.8525\n",
      "Training loss (for one batch) at step 50: 290.7462, Accuracy: 0.8586\n",
      "Training loss (for one batch) at step 60: 302.7756, Accuracy: 0.8623\n",
      "Training loss (for one batch) at step 70: 302.8822, Accuracy: 0.8600\n",
      "Training loss (for one batch) at step 80: 323.1474, Accuracy: 0.8545\n",
      "Training loss (for one batch) at step 90: 288.0546, Accuracy: 0.8520\n",
      "Training loss (for one batch) at step 100: 281.3014, Accuracy: 0.8518\n",
      "Training loss (for one batch) at step 110: 299.3563, Accuracy: 0.8516\n",
      "---- Training ----\n",
      "Training loss: 95.3513\n",
      "Training acc over epoch: 0.8508\n",
      "---- Validation ----\n",
      "Validation loss: 39.7404\n",
      "Validation acc: 0.7598\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 329.0100, Accuracy: 0.8828\n",
      "Training loss (for one batch) at step 10: 325.2205, Accuracy: 0.8388\n",
      "Training loss (for one batch) at step 20: 300.8761, Accuracy: 0.8378\n",
      "Training loss (for one batch) at step 30: 292.2486, Accuracy: 0.8488\n",
      "Training loss (for one batch) at step 40: 285.7457, Accuracy: 0.8577\n",
      "Training loss (for one batch) at step 50: 287.4585, Accuracy: 0.8627\n",
      "Training loss (for one batch) at step 60: 299.1454, Accuracy: 0.8654\n",
      "Training loss (for one batch) at step 70: 312.8930, Accuracy: 0.8598\n",
      "Training loss (for one batch) at step 80: 332.7148, Accuracy: 0.8544\n",
      "Training loss (for one batch) at step 90: 299.7173, Accuracy: 0.8522\n",
      "Training loss (for one batch) at step 100: 337.9977, Accuracy: 0.8519\n",
      "Training loss (for one batch) at step 110: 320.7798, Accuracy: 0.8513\n",
      "---- Training ----\n",
      "Training loss: 93.1821\n",
      "Training acc over epoch: 0.8501\n",
      "---- Validation ----\n",
      "Validation loss: 55.8383\n",
      "Validation acc: 0.7582\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 330.2322, Accuracy: 0.8594\n",
      "Training loss (for one batch) at step 10: 322.8129, Accuracy: 0.8409\n",
      "Training loss (for one batch) at step 20: 303.3462, Accuracy: 0.8490\n",
      "Training loss (for one batch) at step 30: 294.9678, Accuracy: 0.8501\n",
      "Training loss (for one batch) at step 40: 302.1770, Accuracy: 0.8582\n",
      "Training loss (for one batch) at step 50: 286.0098, Accuracy: 0.8612\n",
      "Training loss (for one batch) at step 60: 299.9684, Accuracy: 0.8637\n",
      "Training loss (for one batch) at step 70: 312.7535, Accuracy: 0.8594\n",
      "Training loss (for one batch) at step 80: 310.1407, Accuracy: 0.8539\n",
      "Training loss (for one batch) at step 90: 296.4341, Accuracy: 0.8498\n",
      "Training loss (for one batch) at step 100: 286.4485, Accuracy: 0.8491\n",
      "Training loss (for one batch) at step 110: 325.3542, Accuracy: 0.8485\n",
      "---- Training ----\n",
      "Training loss: 94.6267\n",
      "Training acc over epoch: 0.8481\n",
      "---- Validation ----\n",
      "Validation loss: 55.7369\n",
      "Validation acc: 0.7601\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 354.0841, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 320.4105, Accuracy: 0.8182\n",
      "Training loss (for one batch) at step 20: 292.2325, Accuracy: 0.8374\n",
      "Training loss (for one batch) at step 30: 319.1082, Accuracy: 0.8480\n",
      "Training loss (for one batch) at step 40: 292.7402, Accuracy: 0.8519\n",
      "Training loss (for one batch) at step 50: 290.8015, Accuracy: 0.8568\n",
      "Training loss (for one batch) at step 60: 315.1576, Accuracy: 0.8585\n",
      "Training loss (for one batch) at step 70: 309.7995, Accuracy: 0.8555\n",
      "Training loss (for one batch) at step 80: 341.1907, Accuracy: 0.8517\n",
      "Training loss (for one batch) at step 90: 297.3074, Accuracy: 0.8489\n",
      "Training loss (for one batch) at step 100: 306.7117, Accuracy: 0.8495\n",
      "Training loss (for one batch) at step 110: 302.7685, Accuracy: 0.8488\n",
      "---- Training ----\n",
      "Training loss: 112.9781\n",
      "Training acc over epoch: 0.8487\n",
      "---- Validation ----\n",
      "Validation loss: 50.7650\n",
      "Validation acc: 0.7598\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 320.2381, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 10: 310.5571, Accuracy: 0.8288\n",
      "Training loss (for one batch) at step 20: 305.6169, Accuracy: 0.8344\n",
      "Training loss (for one batch) at step 30: 287.9526, Accuracy: 0.8465\n",
      "Training loss (for one batch) at step 40: 290.1305, Accuracy: 0.8558\n",
      "Training loss (for one batch) at step 50: 291.6548, Accuracy: 0.8598\n",
      "Training loss (for one batch) at step 60: 289.6492, Accuracy: 0.8639\n",
      "Training loss (for one batch) at step 70: 314.4043, Accuracy: 0.8583\n",
      "Training loss (for one batch) at step 80: 317.2488, Accuracy: 0.8523\n",
      "Training loss (for one batch) at step 90: 300.0404, Accuracy: 0.8504\n",
      "Training loss (for one batch) at step 100: 285.3814, Accuracy: 0.8513\n",
      "Training loss (for one batch) at step 110: 295.2539, Accuracy: 0.8504\n",
      "---- Training ----\n",
      "Training loss: 100.2094\n",
      "Training acc over epoch: 0.8489\n",
      "---- Validation ----\n",
      "Validation loss: 37.5959\n",
      "Validation acc: 0.7547\n",
      "Time taken: 10.56s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 318.6261, Accuracy: 0.8672\n",
      "Training loss (for one batch) at step 10: 310.2971, Accuracy: 0.8459\n",
      "Training loss (for one batch) at step 20: 286.5910, Accuracy: 0.8423\n",
      "Training loss (for one batch) at step 30: 287.9756, Accuracy: 0.8506\n",
      "Training loss (for one batch) at step 40: 308.7027, Accuracy: 0.8558\n",
      "Training loss (for one batch) at step 50: 286.5817, Accuracy: 0.8588\n",
      "Training loss (for one batch) at step 60: 294.5942, Accuracy: 0.8614\n",
      "Training loss (for one batch) at step 70: 317.1424, Accuracy: 0.8596\n",
      "Training loss (for one batch) at step 80: 316.0254, Accuracy: 0.8530\n",
      "Training loss (for one batch) at step 90: 296.3306, Accuracy: 0.8510\n",
      "Training loss (for one batch) at step 100: 298.3430, Accuracy: 0.8506\n",
      "Training loss (for one batch) at step 110: 309.5724, Accuracy: 0.8506\n",
      "---- Training ----\n",
      "Training loss: 102.0001\n",
      "Training acc over epoch: 0.8503\n",
      "---- Validation ----\n",
      "Validation loss: 40.0095\n",
      "Validation acc: 0.7531\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 329.3246, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 323.7346, Accuracy: 0.8139\n",
      "Training loss (for one batch) at step 20: 290.9403, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 30: 287.4630, Accuracy: 0.8402\n",
      "Training loss (for one batch) at step 40: 278.3060, Accuracy: 0.8500\n",
      "Training loss (for one batch) at step 50: 275.2133, Accuracy: 0.8571\n",
      "Training loss (for one batch) at step 60: 310.2661, Accuracy: 0.8603\n",
      "Training loss (for one batch) at step 70: 316.6667, Accuracy: 0.8574\n",
      "Training loss (for one batch) at step 80: 343.5590, Accuracy: 0.8534\n",
      "Training loss (for one batch) at step 90: 279.2096, Accuracy: 0.8518\n",
      "Training loss (for one batch) at step 100: 282.1172, Accuracy: 0.8512\n",
      "Training loss (for one batch) at step 110: 285.5727, Accuracy: 0.8492\n",
      "---- Training ----\n",
      "Training loss: 90.0127\n",
      "Training acc over epoch: 0.8491\n",
      "---- Validation ----\n",
      "Validation loss: 61.4969\n",
      "Validation acc: 0.7515\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 322.1895, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 326.0737, Accuracy: 0.8317\n",
      "Training loss (for one batch) at step 20: 315.9828, Accuracy: 0.8371\n",
      "Training loss (for one batch) at step 30: 294.6167, Accuracy: 0.8473\n",
      "Training loss (for one batch) at step 40: 296.8274, Accuracy: 0.8523\n",
      "Training loss (for one batch) at step 50: 294.6783, Accuracy: 0.8577\n",
      "Training loss (for one batch) at step 60: 302.5324, Accuracy: 0.8622\n",
      "Training loss (for one batch) at step 70: 312.3960, Accuracy: 0.8577\n",
      "Training loss (for one batch) at step 80: 312.9957, Accuracy: 0.8543\n",
      "Training loss (for one batch) at step 90: 294.8952, Accuracy: 0.8505\n",
      "Training loss (for one batch) at step 100: 308.9188, Accuracy: 0.8499\n",
      "Training loss (for one batch) at step 110: 305.7948, Accuracy: 0.8490\n",
      "---- Training ----\n",
      "Training loss: 95.9586\n",
      "Training acc over epoch: 0.8487\n",
      "---- Validation ----\n",
      "Validation loss: 52.9215\n",
      "Validation acc: 0.7531\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 331.9319, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 315.5541, Accuracy: 0.8196\n",
      "Training loss (for one batch) at step 20: 305.7640, Accuracy: 0.8430\n",
      "Training loss (for one batch) at step 30: 290.7998, Accuracy: 0.8468\n",
      "Training loss (for one batch) at step 40: 283.9551, Accuracy: 0.8546\n",
      "Training loss (for one batch) at step 50: 282.9743, Accuracy: 0.8603\n",
      "Training loss (for one batch) at step 60: 299.0443, Accuracy: 0.8630\n",
      "Training loss (for one batch) at step 70: 303.3995, Accuracy: 0.8596\n",
      "Training loss (for one batch) at step 80: 315.1670, Accuracy: 0.8538\n",
      "Training loss (for one batch) at step 90: 303.0803, Accuracy: 0.8505\n",
      "Training loss (for one batch) at step 100: 290.6257, Accuracy: 0.8511\n",
      "Training loss (for one batch) at step 110: 293.1624, Accuracy: 0.8519\n",
      "---- Training ----\n",
      "Training loss: 102.3136\n",
      "Training acc over epoch: 0.8510\n",
      "---- Validation ----\n",
      "Validation loss: 73.3968\n",
      "Validation acc: 0.7577\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 345.6603, Accuracy: 0.8906\n",
      "Training loss (for one batch) at step 10: 333.4640, Accuracy: 0.8345\n",
      "Training loss (for one batch) at step 20: 294.0264, Accuracy: 0.8326\n",
      "Training loss (for one batch) at step 30: 300.3954, Accuracy: 0.8475\n",
      "Training loss (for one batch) at step 40: 314.0963, Accuracy: 0.8523\n",
      "Training loss (for one batch) at step 50: 293.4705, Accuracy: 0.8594\n",
      "Training loss (for one batch) at step 60: 320.6749, Accuracy: 0.8618\n",
      "Training loss (for one batch) at step 70: 312.5097, Accuracy: 0.8574\n",
      "Training loss (for one batch) at step 80: 341.8275, Accuracy: 0.8521\n",
      "Training loss (for one batch) at step 90: 290.7200, Accuracy: 0.8486\n",
      "Training loss (for one batch) at step 100: 288.0551, Accuracy: 0.8479\n",
      "Training loss (for one batch) at step 110: 300.1978, Accuracy: 0.8468\n",
      "---- Training ----\n",
      "Training loss: 96.5510\n",
      "Training acc over epoch: 0.8473\n",
      "---- Validation ----\n",
      "Validation loss: 48.1367\n",
      "Validation acc: 0.7593\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 331.7213, Accuracy: 0.8516\n",
      "Training loss (for one batch) at step 10: 302.0415, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 20: 317.0771, Accuracy: 0.8404\n",
      "Training loss (for one batch) at step 30: 296.8251, Accuracy: 0.8506\n",
      "Training loss (for one batch) at step 40: 298.1567, Accuracy: 0.8554\n",
      "Training loss (for one batch) at step 50: 268.9482, Accuracy: 0.8595\n",
      "Training loss (for one batch) at step 60: 268.0599, Accuracy: 0.8628\n",
      "Training loss (for one batch) at step 70: 319.0234, Accuracy: 0.8592\n",
      "Training loss (for one batch) at step 80: 306.6129, Accuracy: 0.8547\n",
      "Training loss (for one batch) at step 90: 285.1863, Accuracy: 0.8529\n",
      "Training loss (for one batch) at step 100: 287.5071, Accuracy: 0.8517\n",
      "Training loss (for one batch) at step 110: 286.1659, Accuracy: 0.8500\n",
      "---- Training ----\n",
      "Training loss: 100.5621\n",
      "Training acc over epoch: 0.8494\n",
      "---- Validation ----\n",
      "Validation loss: 48.1881\n",
      "Validation acc: 0.7603\n",
      "Time taken: 10.40s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 314.1478, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 307.1674, Accuracy: 0.8253\n",
      "Training loss (for one batch) at step 20: 285.9619, Accuracy: 0.8330\n",
      "Training loss (for one batch) at step 30: 284.8326, Accuracy: 0.8475\n",
      "Training loss (for one batch) at step 40: 269.1124, Accuracy: 0.8540\n",
      "Training loss (for one batch) at step 50: 285.1618, Accuracy: 0.8597\n",
      "Training loss (for one batch) at step 60: 286.9162, Accuracy: 0.8635\n",
      "Training loss (for one batch) at step 70: 307.3063, Accuracy: 0.8585\n",
      "Training loss (for one batch) at step 80: 324.6485, Accuracy: 0.8544\n",
      "Training loss (for one batch) at step 90: 303.7975, Accuracy: 0.8525\n",
      "Training loss (for one batch) at step 100: 288.2944, Accuracy: 0.8529\n",
      "Training loss (for one batch) at step 110: 322.5585, Accuracy: 0.8518\n",
      "---- Training ----\n",
      "Training loss: 94.2467\n",
      "Training acc over epoch: 0.8516\n",
      "---- Validation ----\n",
      "Validation loss: 39.3911\n",
      "Validation acc: 0.7509\n",
      "Time taken: 10.24s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEjCAYAAADZk82GAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAB+Y0lEQVR4nO2dd3gc1dW437tFWvUu2Zbce6/YYFNk7NDBQGiGEBsI7aMEkkDKlw9Myy8EEkIN3ZSADaE4Bkw1limm2Lj3Isu23NS7Vtvu7487s7uSVs1eFcv3fZ59dvdOOzsazZlT7jlCSolGo9FoNMFYOlsAjUaj0XQ9tHLQaDQaTSO0ctBoNBpNI7Ry0Gg0Gk0jtHLQaDQaTSO0ctBoNBpNI7Ry0GjagBAiWwiR39lyaDTtjVYOmg5DCJEnhJjZ2XJoNJqW0cpBo+kmCCFsnS2DpvuglYOm0xFCRAoh/imEOGC8/imEiDSWpQohPhRClAkhSoQQXwshLMay3wsh9gshKoUQ24QQM5rY/7lCiDVCiAohxD4hxLygZf2EEFIIMUcIsVcIUSSE+N+g5VFCiFeEEKVCiM3ACS38lseNY1QIIX4SQpwStMwqhPiTEGKXIfNPQojexrKRQojPjd94WAjxJ2P8FSHEg0H7qOfWMqyx3wsh1gPVQgibEOIPQcfYLIS4qIGM1wshtgQtnyCEuEsI8W6D9Z4QQjze3O/VdGOklPqlXx3yAvKAmSHG7we+B9KBNGAF8ICx7P8BzwJ243UKIIChwD6gl7FeP2BgE8fNBkajHobGAIeBC4O2k8ALQBQwFqgDhhvL/wp8DSQDvYGNQH4zv/EXQApgA34LHAIcxrK7gA2G7MI4VgoQBxw01ncY36cY27wCPNjgt+Q3OKdrDdmijLFLgV7G770cqAZ6Bi3bj1JyAhgE9AV6GuslGuvZgAJgYmdfN/rVOa9OF0C/jp9XM8phF3BO0PczgTzj8/3Af4FBDbYZZNy8ZgL2NsrxT+Ax47OpHLKClv8IXGF8zgXOClp2Q3PKIcSxSoGxxudtwKwQ68wG1jSxfWuUw7UtyLDWPC7wKfDrJtb7GLje+HwesLmzrxn96ryXditpugK9gD1B3/cYYwCPADuBz4QQuUKIPwBIKXcCdwDzgAIhxEIhRC9CIISYIoRYJoQoFEKUAzcBqQ1WOxT0uQaIDZJtXwPZmkQI8TvDZVMuhCgDEoKO1RulCBvS1HhrCZYPIcQvhRBrDVdcGTCqFTIAvIqyfDDeXz8KmTTHOFo5aLoCB1CuDZM+xhhSykop5W+llAOAC4DfmLEFKeWbUsqTjW0l8HAT+38TWAz0llImoNxUopWyHUTdUINlC4kRX7gbuAxIklImAuVBx9oHDAyx6T5gQBO7rQaig773CLGOv7SyEKIvykV2K5BiyLCxFTIALALGCCFGoSyHN5pYT3McoJWDpqOxCyEcQS8bsAD4sxAiTQiRCtwD/BtACHGeEGKQEEKgbrRewCeEGCqEON0IXDuBWsDXxDHjgBIppVMIMRm4sg3yvg38UQiRJITIAm5rZt04wAMUAjYhxD1AfNDyF4EHhBCDhWKMECIF+BDoKYS4wwjOxwkhphjbrAXOEUIkCyF6oKyl5ohBKYtCACHENSjLIViG3wkhJhoyDDIUClJKJ/AOSpn+KKXc28KxNN0YrRw0Hc0S1I3cfM0DHgRWAetRAdvVxhjAYOALoAr4DnhGSrkMiEQFi4tQLqF04I9NHPN/gPuFEJUoxfN2G+S9D+VK2g18RvOulk+BT4DtxjZO6rt8/mEc+zOgAngJFUSuBH4GnG/8lh3AdGOb14F1qNjCZ8BbzQkrpdwM/B11rg6jAvHfBi3/D/AQSgFUoqyF5KBdvGpso11KxzlCSt3sR6PRKIQQfYCtQA8pZUVny6PpPLTloNFoADDmj/wGWKgVg0bPqNRoNAghYlBuqD3AWZ0sjqYLoN1KGo1Go2mEditpNBqNphFaOWg0Go2mEVo5aDQajaYRWjloNBqNphFaOWg0Go2mEVo5aDQajaYRWjloNBqNphFaOWg0Go2mEVo5aDQajaYRWjloNBqNphFaOWg0Go2mEVo5aDQajaYRWjloNBqNphFaOWg0Go2mEcd0P4fU1FTZr1+/RuPV1dXExMR0vEAh0LKEpqvI0pwcP/30U5GUMq2DRQJCX9td5ZyBlqUpjhVZWnVtSymP2dfEiRNlKJYtWxZyvDPQsoSmq8jSnBzAKtmFru2ucs6k1LI0xbEiS2uube1W0mg0Gk0jtHLQaDQaTSO0ctBoNBpNI47pgHRXxO12k5+fj9PpBCAhIYEtW7Z0slQKLUtoOXbv3k1WVhZ2u72zxdFougxaOYSZ/Px84uLi6NevH0IIKisriYuL62yxALQsIaioqMDlcpGfn0///v07WxyNpsug3Uphxul0kpKSghCis0XRtAIhBCkpKX5LT6PRKLRyaAe0Yji2aM3fSwhxlhBimxBipxDiDyGW9xFCLBNCrBFCrBdCnGOM9xNC1Aoh1hqvZ9vhJ2g0YadbKofVhz28+HVuZ4uh6SYIIazA08DZwAhgthBiRIPV/gy8LaUcD1wBPBO0bJeUcpzxuqlDhNZ0KgfKaimpdh3VPrw+2ep1y2vd1Hm8/u9f7yjky73uozp+t4w5rC/ysmrzDuZM7Yfd2i31n6ZjmQzslFLmAgghFgKzgM1B60gg3vicABzoUAk1nYaUkpzthYzoqf78hZV1nPvE1yRE2Vny61OIjmj5Nrsyr4S/fryVa6f1Jz7Kxqsr8vhqexE9EhxcOK4Xt80Y7L+XrdhVxEtf72ZUZgLXTuvPrqIq5r78IyN6xfPmr07kndX5/Om9DfSKEbg8PiJsR3YPbDflIIR4GTgPKJBSjmqw7LfAo0CalLJIKLv+ceAcoAaYK6VcfaTHHpFiJWdfHevzy5jYN/nIf8QxSHFxMTNmzADg0KFDWK1W0tLULPmlS5c2u+2qVat47bXXeOKJJ5pdb+rUqaxYsSI8AgOvvPIKq1at4qmnngrbPsNMJrAv6Hs+MKXBOvOAz4QQtwExwMygZf2FEGuACuDPUsqvQx1ECHEDcANARkYGOTk59ZZXVVU1GussjidZpJRIwNLA/eiTkrwKHx/ucrO6wEuMHc7vK3nsp2VU1HoprXFz24tLuXpEJBV1kq/y3awv8uKTcEIPGzP62Fi2z0NBjY/l+zx4JdyypxSAxEjByZlWimrqeOLLnfx3VS4COFzjo9YDsXZYurWAx5fuwCLAYYXvc0s4/++fsKnYx8gUC3MHe1nxzVdH/Lvb03J4BXgKeC14UAjRGzgD2Bs0fDYw2HhNAf5F43++VjM82YoQ8O3O4uNOOaSkpLB27VoA5s2bR2xsLL/73e8AlSHk8Xiw2UL/2SdNmsSkSZNaPEY4FUM3YjbwipTy70KIk4DXhRCjgINAHyllsRBiIrBICDFSSlnRcAdSyueB5wEmTZoks7Oz6y3Pycmh4VhncbzIcs9/N/LWyn1YLYLrTxlAalwkTpeX3slR/O2TbeQWOYmwWrhz5hA+23yIhTsrAC+/P2sYhyucvLIij7SMnny5tYCCSjdjsxLwSViwtZyvD1vJL3URHWFlXN9knpw9nkVr9hMdYeWyE3oTabMCsGjNfh77Yju9EqM4LT2WQRlxXDoxi50FVeRsK6Csxs2vThnA7/6zjm92FnHJxCz+38Wj+fbrr47qvLSbcpBSfiWE6Bdi0WPA3cB/g8ZmAa8ZNT++F0IkCiF6SikPHsmx4yIEI3rG883OIm6fMfhIdhEW7vtgExv2lWK1WsO2zxG94rn3/JFt2mbu3Lk4HA5WrVrFqaeeyhVXXMGvf/1rnE4nUVFRzJ8/n6FDh5KTk8Ojjz7Khx9+yLx589i7dy+5ubns3buXO+64g9tvvx2A2NhY/9PavHnzSE1NZePGjUycOJF///vfCCFYsmQJv/nNb4iJiWHatGnk5uby4YcftihrXl4e1157LUVFRaSlpTF//nz69OnDf/7zH+677z6sVisJCQl89dVXbNq0iWuuuQaXy4XP5+Pdd99l8OB2+XvvB3oHfc8yxoK5DjgLQEr5nRDCAaRKKQuAOmP8JyHELmAIsKo9BD2eWb23lLyiak7ol0xWUhRCCKSUrMsvZ3RmAlaLwOP18eH6g9S6vURHWDlc4eSH3BLG9k7k9hmDOVBWS3GVi97JUewrqeW17/Ywc3gGNovg8aU76h0vKymKf1w2ltOGpJESG8kt0weycEkOQ0aNY1LfJDw+iU9KXvtuD72To/jo9pMZ2SsBn09y/4ebeWvlPh67fCwXjc/y7/PG0wY2+l0Xjs/kwvGZjcZHZSYwKjPB//3J2eNZtaeUmcPTw5IU06ExByHELGC/lHJdA+FDme2ZqKeuI+LkQam8/O1uvth8mDG9E4h32Hl86Q6khHNH92R0VkLLO+lG5Ofn88UXX5CYmEhFRQVff/01NpuNL774gj/96U+8++67jbbZunUry5Yto7KykqFDh3LzzTc3mii2Zs0aNm3aRK9evZg2bRrffvstkyZN4sYbb+Srr76if//+zJ49u9Vy3nbbbcyZM4c5c+bw8ssvc/vtt7No0SLuv/9+Pv30UzIzMykrKwPg2Wef5de//jVXXXUVLpcLr9fb/M6PnJXAYCFEf5RSuAK4ssE6e4EZwCtCiOGAAygUQqQBJVJKrxBiAMo61tkSR8juomo8Xh+DM9QcmRe/zmXxT07i+pfwy5d+pNqlroG0uEjOGtmDkhoXH60/yK9O7s8lk7K47c017CioqrfP5JgIlm4toNbt5cWvc3F7JTERVvqmxBDvsPHY5WOJc9jZWVBJpM2KzSrYkF/OtEGpxEQGbqE2q4WsOAuT+ytvRYRFcP+sUcwa14tBaXEkRKv/HYtFMO+CkfzvucPDGhNNiongZyMywra/DlMOQoho4E8ol9LR7KdZvywoH2RCnRe3V/Kr11YRFwF9461sLPJiFTD/m108fEoUiY7wB6sTEhKorKwE4DfZffB6M8NqOQD+/bdEXV0ddrsdt9vNeeed5992//793H333ezatQshBG63m8rKSmpqavB4PFRWVlJXV8fMmTNxuVxERkaSmprKrl27yMzM9O+npqaGiRMnkpCQQHV1NSNHjmTLli0IIejbty+pqalUVlZy4YUXMn/+/Hpye71e/3en04nL5aKyspIVK1bw6quv+re76667qKysZPLkyVx99dVcdNFFnH/++VitVsaNG8eDDz7Irl27OP/88xk0aFCrz01DOZxOZ5N+aymlRwhxK/ApYAVellJuEkLcj6puuRj4LfCCEOJOVHB6rpRSCiFOBe4XQrgBH3CTlLKkTUIe5+wsqOTbncUM7RHH9a+totbl5cbTBlBS7WLBj+qZ8uf/+o6EKDtv/moSOwur+HF3CW+t3IfH52NsVgIvfbubt1ftw2G38tzVExmVmUCty0NKTCRREVYueOob/pWzi5G94rl9xmD+lbOLtfvKuGPmYOIc6qY+KD0wabNnQlSr5W/Ktd3Vk2U60nIYCPQHTKshC1gthJhM68x2oGW/LCgf5M3nnsbkiaXUuLz86f0NbCyq5Q9nD+OskT342WPLWVGVwt/OGhvO3wfAli1b6s387cyZwJGRkURGRmK320lNTcVqtRIXF8fDDz/Mz372Mz744APy8vLIzs4mLi6O6OhobDYbcXFxREZGEhsb65fdbrfjcDj83831o6Oj/WMOhwO73U5MTIz/WABRUVH+/ZoEnxeHw0FERARxcXEIIYiLi/MrNfP7Sy+9xA8//MBHH31EdnY2P/30E9dddx3Z2dl89NFHXHbZZTz33HOcfvrpbTpHphwOh4Px48c3uZ6UcgmwpMHYPUGfNwPTQmz3LtDYLNOExOP1cd2rq9i4v5wZw9M5c2QPfv/uBoqq6gDoleDgxAEpPL1sFwC/OLEPfWQB7+TZ+MPZw5g6KJWpg1L55Un9KKqqo9blJSU2gvOe/Aany8uCG06kb0rjHgdPXzmBV1bk8dszhpIcE0H20DQ+33yYmcPD9yR+rNFhykFKuQFIN78LIfKASUa20mLgViNFcApQfqTxhqD9M6mf0tiLbpnGun1lnD5M+eLmnNSPl77dzdyp/RnRK96/jQp5dP9JbOXl5X4L4JVXXgn7/ocOHUpubi55eXn069ePt956q9XbTp06lYULF3L11VfzxhtvcMoppwCwa9cupkyZwpQpU/j444/Zt28f5eXlDBgwgNtvv529e/eyfv36NisHTefj80ksFvU/99ePt7J8eyGnDUljyYZDvL0qn4QoO8/+YiI/7Snhyil96ZcSzcFyJ8kxETjsVnJycvjsotMa7Tc1NtL/efGtJ2MRNJlWOjgjjocuGu3/Hmmzct6YXmH+pccW7ZnKugDIBlKFEPnAvVLKl5pYfQkqjXUnKpX1mnDKkhobyYygJ4DbTh/MW6v28dgX23nhlyo7x+P1cduCNVTVeXj9uiNOlDomuPvuu5kzZw4PPvgg5557btj3HxUVxTPPPMNZZ51FTEwMJ5xwQqu3ffLJJ7nmmmt45JFH/AFpgLvuuosdO3YgpWTGjBmMHTuWhx9+mNdffx273U6PHj3405/+FPbfomlfftpTyq1vriY93kFWYhQfbTjInJP6ct+sUZTXunl/dT6T+iUzKjOBs0b18G/XK7H1bh2A2MhuOaWrfWmpG1BXfh1NJ7jHv9gu+/7+Q/lDbrE8XFEr71y4Rvb9/Yey3x8+lKXVdfIXL34vH/9ie4v7acjmzZvrfa+oqGjzPtqLjpSlsrJSSimlz+eTN998s/zHP/7RabI0hylHw7+blLoTXHMciSxVTrf8aU+J3HKwXD65dLu89NkVcuAfP5LT/rpUnviXL+SAP34kH/9iu3R7vO0uS3txrMjSmmv7uFWn10zrxysr8rjsue/8YzOHp/PFlgLe+Smfr3cU4XR7OzUV9ljmhRde4NVXX8XlcjF+/HhuvPHGzhZJ00nsKa7m3dX7ee27PMpqAiUdxmYlcO3J/fmf7IE47FZKql1ttgg07cdxqxziHHYW/c80crYXUOn0cObIHmQlRTH2vs948sudAOQWVneylMcud955J3feeWe9sfnz5/P4448D4PP5sFgsTJs2jaeffrozRNR0AMu2FXDdKyuRwOlD07lkYhY1Li8n9EumT0p0vXW1YuhaHLfKAaBPSjS/PKlfvbEJfZL4LrcYgOJqF2U1LhKjIzpBuu7HNddcwzXXqHBSV+nnoAk/Ukq+3VnMkB6xPPjhZvqlxvDv66bom/8xxnGtHEJx4oAUvsstZliPOLYeqmRXYTUVzjIm9EkiIUp3CtNoWiJnWyHXvLKSCJsFl8fH81dP1IrhGKRrz8LoBGYMT8dht/hjDV9sOcw181dy51tr/amuGo2mMWU1LgoqnTy0ZAt9U6KZ2CeJmcMzwjprV9NxaMuhAaMyE9hy/1l4fRK7VfDv7/cA8OXWAm58/ScqnG4ev2I8GfGOTpZUo+ka+HySl7/dzaOfbcPp9gHw/NUTOWNkjxa21HRltHIIgRACm1XQJzmaXYXVjMqMJzkmkpzthbg8Pt5auU9nMWk0Bq+syOPBj7YwY1g6UwYkIxDaWugGaLdSMwxIiwXgjBE9eHnOJNbfewYnDkjmvdX5XdbFNH36dD799NN6Y//85z+5+eabQ66fnZ3NqlWqQOg555zjL2oXzLx583j00UebPe6iRYvYvDnQ++aee+7hiy++aKP0TfPKK69w6623hm1/mvCwv6yWRz/bxvShabw4ZxI3nDqQ608d0O2rDBwPaOXQDAPSVA2Wn43IwGa14LBb+fmELPKKa1i9t6zZbaWUlFTXtanVXziYPXs2CxcurDe2cOHCVlVGXbJkCYmJiUd03IbK4f7772fmzJnNbKE5lvFJyVNf7uDnz6xASrh/1iitELoZ2q3UDLNP6ENqTCTDegRSLs8e3ZN7/ruJd37ax8S+SU1u6/b68C75AzGlm8EextPcYzSc/VdcHh+HymvJSor216UBuOSSS/jzn/+My+UiIiKCvLw8Dhw4wIIFC7jjjjuoq6vjkksu4b777mu06379+rFq1SpSU1N56KGHePXVV0lPT6d3795MnDgRUJPbnn/+eVwuF4MGDeL1119n7dq1LF68mOXLl/Pggw/y7rvv8sADD3DeeedxySWXsHTpUn73u9/h8Xg44YQT+Ne//uU/3pw5c/jggw9wu9385z//YdiwYS2egi7a8+G44JONh0iJjeDLvR7+vWU7pwxO5Zbpg+idHN3yxppjCm05NEO/1JhGJnJspI1zx/Rk8doDVNV5APh4w0F/4NrE7VUWQ3vZDdUuD2UNmooDJCcnM3nyZD7++GNAWQ2XXXYZDz30EMuXL2f9+vX+96b46aefWLhwIWvXrmXJkiWsXLnSv+ziiy9m5cqVrFu3juHDh/PSSy8xdepULrjgAh555BHWrl3LwIGBhiVOp5O5c+fy1ltvsWHDBjwej185AKSmprJ69WpuvvnmFl1XJmbPh/Xr13PVVVf5mxCZPR/WrVvH4sWLgUDPh7Vr17Jq1SqysrKa27WmGdbuK+N/3viJy577joVbXWQPTeO1aydz4oCUzhZN0w5oy+EIuGpKH975KZ9Fa/bzixP78vjSHRwsd3LVlD7+ddxeHwdPupdKu2BARvgbC/mMmEcot5XpWpo1axYLFy7kpZde4u233+bZZ5/F5/Nx8OBBNm/ezJgxY0Lu++uvv+aiiy4iOlo9DV5wwQX+ZRs3buTPf/4zZWVlVFVVceaZZzYr57Zt2+jfvz9DhgwBYM6cOTz99NNcd911gFI2ABMnTuS9995r1W//7rvv/OteffXV3H333QBMmzaNuXPnctlll/n3e9JJJ/HQQw+Rn5/PxRdfrK2GI8Tl8fH7d9aTEe/gpAEp5Gw5wMM/H6NdSd0YbTkcAeN6JzKiZzz//n4P5bVuth2upLzWzZ7iGv86HsNy8LST6WDGw70h9j9r1iyWLl3K6tWrqampITk5mUcffZTFixezfv16zj33XJxO5xEdd+7cuTz11FNs2LCBe++994j3YxIZqcoqW61WPB7PUe3r2Wef5cEHH2Tfvn1MnDiR4uJirrzyShYvXkxUVBTnnHMOX3755VEd43jl251FbDtcyf+dN4J/XD6Ov58WpdO5uzlaORwBQghmT+nD1kOVvP5dnv9GvXZfmX8dt0/le3t97SNDc5ZDbGws06dP59prr2X27NlUVFQQExNDQkIChw8f9rucmuLUU09l0aJF1NbWUllZyQcffOBfVllZSc+ePXG73bzxxhv+8bi4uJBd2IYOHUpeXh47d6p6Va+//jqnnda49n5bMHs+ACF7Ptx///2kpaWxb98+cnNz/T0fZs2a1aw7TdM0OwrU33bawFSg+/c80WjlcMScP6YnEVYLTy3bic0icNgt9ZSDaTl42ynl1dQJviayoWbPns26deuYPXs2Y8eOZfz48UycOJErr7ySadMaNSyrx4QJE7j88ssZO3YsZ599dr1+DA888ABTpkxh2rRp9YLHV1xxBY888gjjx49n165d/nGHw8H8+fO59NJLGT16NBaLhZtuuukofrnq+TB//nzGjBnD66+/7i/md9dddzF69GhGjRrF1KlTGTt2LG+//TajRo1i3LhxbNy4kV/+8pdHdezjldzCalJiIvx9kDXdH9FV8/Vbw6RJk6SZox9MTk4OodqHhpub//0TH288xNjeiURaLbh9Pv4yPZnhw4eTW1jlD1iP7JWA1RLeJ62D5bUUVtaREe9otXnflYrddRVZTDm2bNnC8OHD6y0TQvwkpZzUGXKFurY76roOxaXPrgDgPzdN7XRZGqJlCU1zsrTm2taWw1Fw8QSV+XJC3yTG9k5g04EK/+Q4d1AwwNMOviXTYOjoeRTHK0KIs4QQ24QQO4UQfwixvI8QYpkQYo0QYr0Q4pygZX80ttsmhGg+gt9FyS2sZqAxKVRzfKCVw1GQPTSNX5zYh8tO6M243km4PD6/UvB4fThsVgDc7XADl8Y+m3IrHavMnz+fcePG1XvdcsstnSqTEMIKPA2cDYwAZgshRjRY7c/A21LK8cAVwDPGtiOM7yOBs4BnjP11aaSUVDpVY56yGhfF1S7/pFDN8YFOZT0K7FYLD16ompLHOdSprPP48Hh9eKUkPsKK0+NtH8vBeG+vmEZnEdzzoaNohWt1MrBTSpkLIIRYCMwCNgetI4F443MCcMD4PAtYKKWsA3YLIXYa+/uOLsybP+7loY+2sOiWaVQ6lXt0QKq2HI4ntOUQJnomRJGZGMWhag+FRUVIKYmyG5ZDqHzTo0Q2k62kaT1SSoqLi3E4mo3bZAL7gr7nG2PBzAN+IYTIB5YAt7Vh2y7H+n3l1Li8/HrhWrYdUplK2nI4vtCWQxg5oV8S//yuhEeToikqr8JXGklxlYsah42iMDcKKqqqw+n2UWoT1BW1LiDtdDpbugl2GF1FFqfTSWJiYjhmTs8GXpFS/l0IcRLwuhBiVFt2IIS4AbgBICMjg5ycnHrLq6qqGo21F2t21RJrhy0HK5i3eANWAbkbVrLXSKzoSFlaQssSmqOVRSuHMDKxXzKL1h7guwIrj3x6iM/vPJUb3v+aqUN78vgVjWcjf7LxENV1Hn4+se03psue+44fd5cwIDWGL383vlXb5OTkMH5869YFKKhwcvrfl7Pg+hMZnRXeWd5tlaW9aKUc+4HeQd+zjLFgrkPFFJBSfieEcACprdwWY7vngedBZSs1zDTpyEyY333zOeeOzWDqoBTuXbyJ3knRzDz95E6RpSW0LKE5Wlm0cggjJ/RThfjeXqW8COnxDpIcgsMVoWcRz/92N8XVriaVQ63Ly8vf7ubH3SVcfWJfZgbVyK9zq5pKFUbQsD3IL6ulqs5DblFV2JXDMcZKYLAQoj/qxn4FcGWDdfYCM4BXhBDDAQdQCCwG3hRC/APoBQwGfuwowY+E8lo3RVUqAD1rXCbTh6X75+1ojh+0cggjQ9LjSIy2s6e4hovGZxLvsJERbWHjoUp8PlmveipAaY2Lwsq6Jvf35dYCHvl0GwA94h31lIPZcavCeXQlJ5rD6VIKqM7dTtO8jxGklB4hxK3Ap4AVeFlKuUkIcT+wSkq5GPgt8IIQ4k5UcHquVIGhTUKIt1HBaw9wi5TSG/pIXYPdRdUA9E9VMYZ4h574djyilUMYsVgEC64/EZtFMDhDTfAalmzh6/0uth6qZESv+Hrrl1S7KTcqq0baGmc3FlYqiyM1NpLy2voWgtOoxury+Jrc/mipNawTp6dL38s6BCnlElSgOXjsnqDPm4GQU8+llA8BD7WrgGFkd1EVoAPQxzs6WynMDO8Z71cMAMNT1E17xa6ieuv5fJLSGhcARVWukPsqqXYhBPRNiaastv46Tnfghl3ZTtaDXzm4tXI4nsgtrMYioE+yVg7HM1o5tDPJDgv9U2P4bldxvfFKp8efhlrQREyiuNpFUnQESdERlNfWVwBOt48EIwOqorZ94g61LlM5NO1W2l1Uzf6y2nY5vqZzyC2qpndyNBE2fXs4nmm3v74Q4mUhRIEQYmPQ2CNCiK1GeYH3hRCJQcuO+RIDTXHSwBR+2F2Cx+sjt7CKf3+/h5KagCVgxh22Hqrg9e/y/OOlNS6SYyJIiLI3UgBOt5e0OFXuur0sB2crLIffvr2W+xZvapfjazqH3MJqf7xBc/zSno8Gr2Ck9gXxOTBKSjkG2A78EY7dEgOt5aQBKVTVedhysJL53+bx50UbyS8N9H4orKqjxuXhhtd+4v/+u8nf3a24KqAcgmMOPp+kzuMjvZ2VQ8Ct1LTlUFbjpqCZoLrm2KLW5WXH4UpG9IxveWVNt6bdlIOU8iugpMHYZ1JK8072PSrnG4JKDEgpdwNmiYFuwRgjDXTTgXL/bNON+yv8ywsr6/jbJ9vYW6IURrERgyipdpEcrZRDVZ0Ht1GGo86j3gOWQ3u5ldRxmgtI17q9jYLlmmOXtfvK8Pgkk/o13R9dc3zQmdlK1wJvGZ8zUcrCpMkSAy3NIoWuN0tx1/ofcVjh81Vb2HhA6cYv1+7wr7N6626+O+AhxSEodko+Wb6CAQlWDpVVkxnppMCndOzHS5cTHyGocqlYhbOsAIAf120kqnhbq2Rpy3nZtkspqT37DpCTUxxynYoaJ5XQ5vPdVf5GXUWOrsLqvaUATOijlcPxTqcoByHE/6Jyvt9oad2GtDSLFLrmLMVRW1ewrcJJjdEK85ArAqild3IUu6rA7fNwyeT+PPdVLr0Hj+LUYelUf7qE0YP7MSAthje2rGPU+BMYkBbLwfJa+PJLJgwfxKd5W8nsO5DsUwa0WpZWy16xCXbnkZiSRnb2hJDruL/4GI/Xx6mnntZoHkc4ZWkvuoocXYVVeSUMSo8lMTqis0XRdDIdno4ghJgLnAdcJQPlMFtdYuBYZXjPePJLA1k9+0pqibRZ6JMc7c/2MSe5FVbWUVbrxifxxxwAv/vGjAGkxiq3UntNhAtkK4V2K3l9EpfHh09CZV37TcbTdAw+n+SnPaVM6qutBk0HKwchxFnA3cAFUsqaoEWLgSuEEJFGiYIuX2KgrQRPgEsyWi0mx0SQHqeKz6XERDA2KxFQyqGk2uVfJyFKPcUFlIO6WUdHWIlz2NotldWMNTQVc6gNUhrlNTrucKyzem8pFU4PE7Vy0NC+qawLUDXrhwoh8oUQ1wFPAXHA50KItUKIZwGklJsAs8TAJxwDJQbaynAj+6NngoOhPdQkOaUc1NP/2N6JRNgsJMdEUFDpbKAcGloO6tQ47FbSYiMprGqfbKGW5jnUuALWQsNJeppji/X5ZVz7ykoy4iOZPiy9s8XRdAHaLeYgpZwdYvilZtY/pkoMtJWhGXFYBAztEUdKTCRQQnJMhD/jyLQa0mIjDctB3fCDlUNFA7dSpN1CWlwkhRX1lcOOw5X876KNvDz3BGIjj/xP3NIMaacroDTKtOVwTPPE0h1E2Ky8c9NUv7tSc3yjp0B2EFERVuZM7celE3uTmRQFQFJ0kHLordJd0+KUJVBsWA4pMZF+5WDegE03j8NuJS0ukoLK+jOsf8wr4cfdJew4XHlUMrc0Ca7GHWw5aOVwLLOzoIrJ/ZPonRzd2aJougi68F4Hcu/5IwGoXqluqskxEUwfls6dM4cwdWAqoJRDXl41JcZch6QYOxE2C1F2a8CtZLh7HDYr6XEOCisL6h3HVCLNVXxtDS1NgjPdTgDlNdqtdKzi8vjYV1rLeWN6tbzy13+HHmMAXam1u6Mth06gV2LAcoh32Pn1zMH+OjbpcZEUVCrLITbS5q+2as6SllIGWQ7KrVTt8lIdlC1UalgdRxuLMG/+dU0FpIOUg3YrHbvsLanG65MtV2Hd/RUsvR9+fEF9P7gO3KHrgmmOfbRy6AT6GKa76VIKJi0uEpfHR15xNckxgVzzhCg72w5XMva+z1i6RVkKURFWf0A72EooNW7UBRWhlYPPJznzsa9484e9zcppWgxNWg5B7qZSrRyOWXYVqv4NA9Nim17J54VP/qg+F20noq4Ens+GdW8e2UGLdsJhoybXhnfg0Mbm1w83hzfB+rfb9xgFW+C1WfDkRPW+4Z32PV6Y0cqhE+iTEs2zv5jABeMam/Gmwli9p5SM+IDySIiysz6/nAqnh293qvLfDpvVv35BPeUQsBxe/mY35z35db1j7CioYtvhSjbsL2tWzpYC0jXBloPOVjpmyTWUQ7OWw55v4fBGSBsGZXuIr9gO0gdlzT9gNMnHd8OCK8BZDu/dACuebN12n/0ZPrzzyI4ZzOf3wn9vUUrvaPD5oKqg8ZiU8NHv4MAayBgJpXnw/k1QW6bW2b8aFl6lxrsoWjl0EmeN6hkykygtaGLbdScHZj0nRAd8vOZTusNuJT0+lOXg8o/9sLuYTQcq8PkCbR5X5qlyHMVN9JEwMd1GHp/E421sPZjKIzHa3m7zHHYWVOl+Eu3MrsIq0uIiiWuu45t5Axx+PkgfqUVGtZvKw/XX83lh60fw9hzYubTp/ZXtVa/vnwXphfJ9rRN2ywew9k1w1bS8blPUlkJuDnhdUHGUc203vgOPjYLKQwDEl2+Bh/vBO9fCnm8g+09w2Wvw85fA51bnBmDFE7D1Q3hxprIwpISD69W7yfbPoGwfVBxQymz535pXJj4fbPsE3rkODm04ut+FVg5dDtMSOGVwKmeODLQFNTOWgom0WfzKJDhjyfT/F1TWsae4BtlgBvMqUzlUN60cpJTUur047OoScXpCKAdDefRMiGqXbKXSahfnPP41//5+T6u3+XZnEXNe/tHfK0PTMrmFVQxsKd5Qq2oukXUCAKlFxhzVqkOBdTa9D09OgIVXqpvgG5eqG3lDpFQ3PIBvHlPvrVEOrhoo3QMeJ+R90/L6TbHtE3WjBijJbX7ddQsbu4OkVMrF54X8leCtg73fgbuWYVufUPve9B7E9YKJc9U2mRMhsY8ad1XD9k9h8JnK+vr0f9UxnjsFtn2s1t/zHbx5KTxzIjx7srKslj0EC2YrJdCQ4l3wyjmw4HKlsFa9fOTnx0Arhy7GwLRYbp8xmP938WiECNQqSjSUw+nGBKUImwWLRZAUHYHNIkJbDhVO8oqVyyB4FvWqPeofvaQZ5WBWfk0yauyEeno3LYeeCQ7KgrKV9pXUMP7+z/wVaI+UtfvKcHl9bdrPj7tLWL69UFeKbQO5RdUMaC7eAOAsU++GcrB51XXltxx8XvjvrWCLgktfgbt2QP9Tletm1Xx45Tz44Xm1bl0FuI3tPUZJmYoD9V081UWw7C8q4L3uLXj9IijcimrPDez8vO0/1OuGHV/Amtch0qhY0JxyqC6GD34N7/4q8MQPyiJ6bRZsXqSe+gH2rYRvHye69gBc8SZcsQAu/zfYVQUEhICRFymlsuplcNfA1NvUa9dS+OT3ar1N76n3nL9ATDr0nQoJWfA/38NFz0PBZtj+cX0566pg/jlq2QVPwqCZkLu87eenAVo5dDEsFsFvfjaErKT6+eZXndiXxy4fy8mDVMqrw8husliEMddBKQevT/pvjAfKnf5gsjl2sLyW/FJV16momWwm0ypIbEY5mDGHjPj6Pa5X5pVQWuNme9A8i4IKJ64Q1kdzrDEqhJoKrjWYCqu9yph3N176ZjdlNW5GNuhv3ojaMrDHQHQyxPYIjFcZyqFoB7iqYNqv1U0wKkndHNNHwod3QN7XkPP/1NO/aTWkDFLvWSeAzwOVBwP7/e5pWP4wrH1Dve/6Ut2MAZIHwo4mlMP+n+DZU+CpE2Dli2qsZLd62l/zOrzxcxU/mTgHrJFqWW0p1IV4APlpvrJSUgcrN9GSu6CqEDYYgex9PxoKC9j3Pax+neLkCTBwOgw7B7Im1t/f2NkgLCpuEpuhbvyTrgNHAtQUQ/oIZdVs+0Rlhp3yG7jqP3DjV5A2BEb9HJL6qfPhCvqf+P4ZZcFd+R+Y8EsYOANKdhHpLGz679kKtHI4RuifGsNF47P8Hboc9kAvpLS4SL/lUFHrRkrIMibamZg377V7ywA4ZXAalU5Pkzds8yZr1oEKlbHkdHuJsltJio6grEal2QL+J/0K4wZd6/Jy+t+Xs+DHtgUv1+xTsu4uar1/2VRqFbXhLQQohDjL6FK4UwjxhxDLHzNKwqwVQmwXQpQFLfMGLVscVsGOgi+3HuaBDzdz9qgeXD6pd/Mr15ZBVKL6nDrYeB8KNUXqifzAGjXWa3xgm8hYuHIhnHgLXPQc1JaoG6vp5z/1LhhyNky5SX0vM1xLXjes+bf6/MV9ULJLfV79GljsMPkGKN0dyHYy8bph0S1KYUXGwUe/hZfPhifGKSWz5UNI6q9utqffo260Jbnw5uXwxAQVJA7e18oXYeDpMPcjpfBWzYeFs9V+ALZ/AtWFEJmglFJFPoczTmv6HKYPh5tXwJgrIPuPYLGCIx7OeEidg589AK5KeOsXSgGaLikTqw1O/z+VQvyvqUqBFGyFbx+HYedBnylqvf6nApBUur5pWVqBVg7HGKGUQ3qQ5WC6lIZmxNXbLmA5qNjE+D6JQMC1tHjdAT7ZGPAfB5RDY8vB7fVR4/JQ4/IQFWElMdqOxyepNm7MWw3lYHao21FQSVWdhwPlre817fNJ1u4tw2YRFFXVtdoSMK2ZijBaDkZXwqeBs4ERwGyje6EfKeWdUspxUspxwJPAe0GLa81lUsoLwibYUfL55sMkRNl5/Irx2Kwt3ApqS5U1AIEn/kEz1HtVgVIO9piA4jBJyIKz/gJjLleT575/FsoN5dB3qlIePcao72bcYdvHUF2gnpTrytWTdUIfJUPqYBh9Cdgc8OPzcGgjQ7Y9o3zx//45FG6B8/4J13wMA6arm3ZMGqx4St1Mh50LPceCLQKS+6uYwb4f1JP7Sz9T6bn7VsKuZcqSOeF6iE2Hi56FC59R67urIXNSIDg85lL1bo+mOGVK8+cxdTBc/BxMuiYwNuFqOPthdVN3JEBENFz5FtijGm8/+hKYu0RZIK+er+IRVjvMvC+wTvoIiE4lsUwrh+OKrKQobBZBVCPLQd30zUymwU0oh+LqOqwWwQBDyRQbNZyeW76LF78O+F8DbiVlOQRPhPvbJ1uZ9dS31Lp8RNmt9DZcYFsOqu52fsvBOKapLKrbUNZ7V2EVlXUesoeqGEteK62HWqOkR5gr1U4Gdkopc6WULmAhqnthU8wGFoRTgPZg7b5yxmQl+CdgNouzDByJ6vOQMylLGKFu7qBcGgfWqJuupYnuvkLA+KvVzXvfD4AIuKcSjIaQZXuV9bD0fojPhFlPq6f7idfAoNPVOmlDISYVxlymgsWvX0h6wXIVqC7eqVw3w84BWyRc9Q78Zgucerc6rs+tlINJ8oCAW+zq92Dq7cbx74NtSyAiNqAAQR1z7Gz1VD/t9sD4hF+q96Hn4LWFuKG3FluEkvnaTxsr2WD6TYObvoWT71Qy3fIjpA4KLLdYoP+pJJRvPnJZ0OUzjjlsVgt9UqL9WUQAaXEOiqtdeLw+f2B4SIYKMPaId3Cowkl5rZueGK1HYyJINbKizHTWsho31dbAzdvZyHJQbiUpJR9vPMSBsloGpMUQFWHl5MGp2K2CzzcfZkh6HIcqlKIyn95NZVHVhr4Tpkvp4gmZfLHlMLuLq+mR4Ag5cTAY03IIc1/tTCA4nSYfCPmIKIToC/QHvgwadgghVqEaXP1VSrmoiW2b7XIYzq51dV7JtkM1DBpgb9U+JxXlUxvVg005OUAUVYP/l547DjIR2LjiM4YfWMuBXmexq5l9xVbamAS4Ny7CF5HId9+s8C+bak+gbMNSEr55Gqu3jg2j/0T5tz8gRv8dKSyk1a5gJLC72sGenBxiLJM4wfMaHmnhm2H3K6Vh0kAGm7sXJ1ki8FkiWbGrFrlbLe9V7GEI4IxM4/s9Emyn0Tf9IP3zFuDOX0NZ4mg2ffNd/R+ReBki4RLs+yqYCrhtMXy7tYQeQ2+nLGZkmP5GNbD5cMur2bIhEVi1GVXQOkBE/HmUDT+P6KOQRSuHY5DzRvfEG5QPnR4XiZQqNdV0Ew0xLIfhPeMoqqpTT9IOKKpykRIT4Z99ba7fMLsneA4DBJTFnuIaf9OifSW1Rk8JOycNTOWzTYeYEVTu2bxB+5VDXevnK5gxlKkDUwB4+Zvd3L5gDR/dfjIjeyU0uV17uJXayBXAOw1KzveVUu4XQgwAvhRCbJBS7mq4YUtdDsPZte6nPSX4Pv+OC6aNJXtERis2cBPbe7D/+Dk5OUwcfw6s/h2joovB56L35AvoPaYZ+XynwIb/w+6qgl5D6v+Wbf1JP2goi+u/ZHxmg2Bu7QQoW0r/n11Pf3NZT4ktfSTsqm75vMTdj9Vi47TJQZbADg/seB7HmFlkT5+uxkr6whMLsHuqSDt5Dtnjmtnv5nuxJ2Qa2073n5eu0lnwaGXRbqVjkN+cMZS7zhzm/+6fJV1R55/j0CclmoQoO0N6xPnrMkGQ5RCjtimqqsPt9VFV56GqzuNXAqZbybQcSmvcfLW9kK93BDIg9hRX+2MfZ4zIIK+4hv+uO+CXqaFbqaqu9TfsWpcXIdT8jp4JDtYalsTBsuZr+QQC0mFVDm3pVHgFDVxKUsr9xnsukAOMb7xZx7J2XzkAY7OaVrT1qC0NuJVMYtLUu5m90+fE5vdhsULWJPU5vkGL+ETj9A4/X80JaEhUItywrP6yiXOh9wmtEB448WaYfH39sZ5j1dyDMVcExpL7Q5+pyqc/+Izm93nh03Bmt+0yoC2H7kC6v4SGk9IaFzaLIC7Sxrs3TyU9PpLPNh1WyiEJiqvqGJ2VSHyUDZtFUFLtqmc1FFbW0Ts5OhCQjlGWw4If9/LTnlLiHDYsAnwSql1eoiOUcvjZiAz+vGgjb/6wl8RoO0MyYqlweiiqqvOnzFa3wXKocXmJtlsRQtAvJcYfSK9qIW5hNiAKc+vUlcBgo0vhfpQCuLLhSkKIYUASqsmVOZYE1Egp64QQqcA04G/hFO5IWJ9fRo94B+nxjpZXdjvVfAQzIG1ii4DoFBXM7XdK4AbfHL1PVLn+cT3rjyf2Ve+n/LZV8oeF2DS4I8RM4rP+omo9xaQ0v72RFdRd0ZZDN8D8By+orKO0xk1itB0hBIPSY4l32IkPshyKq5VbSQhBckwEJdWuehVVzRu50+9WUpaDGWyudHo4ZXCaf30zMJ4R7+DFX05i3vkjeP7qScQ77FQ63X6XUkpMRIs39mBq3R6iItSzy0XjMzl3jLqZVNZ5+GTjIU57ZFnIarG17eBWklJ6gFuBT4EtwNtSyk1CiPuFEMHZR1cAC4N6owMMB1YJIdYBy1Axh6OLFIaBTQcqGN1aq8GcAGemsgZjBpXHXtF4WSh6T1bv8Q3qik25ES6ZXz8VtrPoNV5lEB3naMuhG+AvoVFRR1mNy39DN0mIslNe48Ltk1Q6PaQY8YbkmAiKqupbDkVGgLqhW6nG5WVoRhy3nj6IiX2TmPbwl0hJvaypmUG+63d/yqei1sPOgipApc6uyy9v9W+qCbJKLjuhN+eP7cVH6w9SXeeh0CgLsre4plFWVo27feY5SCmXAEsajN3T4Pu8ENutAEaHVZgwcKjc6Z9QCagsnboKVSQumN1fg92YkNnQcgCIy1BzBYa3MkO3z4nKyhjQYD5AYh/10nQZtOXQDYiwWUiKtlNQ6aS4ykVyKOVQ66bKpR5ok2PV8pTYCEqq6ygPqqhqWg61brN8RqCmU7/UaM4f24teiVFGq1NVNjwU8VE2KpxuDpTXEmG10D81pk3ZSsHKAVTvCqtFUOX0+Oc85BU3Tm/tAgHpLk+NS8WX0oOq/vL5PaokRHAJi9I8ePU8NSMXGsccAE66Fc57TE3mag0RMTD3w9BxBU2XQiuHbkJ6nIOCyjq2F1QyML1+EbWEKBvltW4qDOVgWg4pMZEUN3QrVdZ3K8U77JglnvqmBPZrlhMPthyCiXPYqXF5yS+tJSMhkthIO7Vub8jqrqGodXnrKR4hBDERVqrqPH6rIK+oflkNr0/6Z3yHOSDdrTAzwdLjguINFfvVbN/8VYGxXUY27m6jTk8oy2HQDBgXql285lhHK4duQnp8JBv3l1NW42ZYj/pPcQlRdiqcHirqDOVguKFSY1XZDXPinNWYjfxDbjGbD1bgsKvifpHGJKm+KYF6TxlGnCO6KcvBoTyWOw9X0SPeQazxvdrVuqB0jcvTaN9xDjtVdcGWQ3WjbUzCPM+hW2Eqh3pzRsyS3NuCCsztWqbePUaGWKiYg6bbopVDNyEtLtKf0TOsR30/fEKUHa9PUlhruJUMyyEzKYoal5e8omqEgN5JUeQWVXPViz/w+ebDJEap9cx01X5BloOZIeVo0q2k3FG5RVX0SIgiNlKtFxyUPlheywtf5VI/fquocXmJstcPicVG2qhyevwuoz0N3EpmnCTCZtFupWYwS630EsWBlp9+5WBU/PR5VbkJe5AVGspy0HRbtHLoJgS7CBpaDvFGE5fD1crlYs5xMNuVbthfTlykjfQ4Byt2FePxSe67YCTzr1E55A6jj7W5PgQypKKbcCuZx3R7JT3ilVsJ6pfQWLz2AA8t2eKfUR1MrdvbyHKIibQaloPhVmpkOQSqxFbVeeo1ONIEKDDOd+bOBbDkd6oXgLsaEnpD0XZVHvvDO1SW0gnXGlsJVWBOc9yglUM3wXyS75ngqNc1DgKNgg7WSGwWQXyUeiLvnazqwGw5WEFidASpcRF4fZIou5UrJvdmeE+lZBx2C3aroFdioG6MP+bQhOUQ5wg89fdIiCLGsByC3T0lQR3rQJXmWL23FCllo4A0QKzDTmVdwHI4UFZbL53VrxziHI0aHGkCFFap+lpRpdvUQP5K9T7lJlV9tLpIVUW12FVFVWukKghn0beL4wmdytpNMP3HDV1KEFAOh6t9JBlzHAB/wbw6j4/EaDupRizihP7JRNqCM4Ws9E6OxmoJNB/KMCwVR1OWQ1Dnuh7xDr+yCLYcyqrVTd7MkHp39X5+9591vHXDidS6vERH1L884yJtHCirpaLWQ7zDRoXTw76SWgalqzpSZtG9jAQlW0WtO2QHveOdgoo6UmMjEAXGdAtTOaQPh6m3qs9VhWpyW3xP6DlGBas1xxX6UaCbYFoOQ3s0Tik0b9SFtdKfqQQQE2nzxx8Souz++RLTBtafGZqVFM243on1xvoZVV0zmphhW085JDiIMfplB8ccgi2HOo+Xxz7fDiifeKiAdEyklUqnm0qnmzFZSp49Qa4l03LoYci09VAlWw9V+Jd/sfkwlz674rhvIVpYVUefWAllRvtVUznEBtVYik2DdKNES/YfVP8BzXGFthy6Cf1TY4iwWpjcv3HQcFB6LOeM7sFnmw4xML1+O8jeSVGUVLtIiLLTw3jinhY8OQr41y8mhNzn8ruy68UhgqnvVnL4g87BysGsIFtU5eLtlfvYX6YK+hVW1uGTjV1WsZF2iqpc+CSMzkrgm51F9eY6NFQOty1YTb+UGD65Q5U5+HpHISvzSjlQVkvvJuQ+HiioqGOqI6j386GN6j22iQJ8g2a2v1CaLke7WQ5CiJeFEAVCiI1BY8lCiM+FEDuM9yRjXAghnjC6bK0XQjS+G2maJT3ewcr/ncnpwxr/gzvsVp65aiJPz4jmn5ePq7fMvEkmRts5f2wvXrnmBEZl1g882q0W7CGawfRNianX5zqY2AgbQqgy/ulxkcQZAengiXBmCm1hZR052wr9MZCDRlOgxjEHm/+pv09yNDERVvJLA8rBzFYy3UpOt88/4xvwK5+9Ja3vLNcdKaisY5glX32JTgHpBWFVLUA1GoP2dCu9ApzVYOwPwFIp5WBgqfEdVIetwcbrBuBf7ShXt6VhILohkVbR6CbvVw5RETjsVn9znaPFYhT/S42NxG61+APSwTGH0uqAW2lPSQ3De8QTabP4U3IbKYfIwPd4h51eiVEcKAt0lwsEpAP5+6ptqlIoZqnxvSU1/Pv7PX431vGE1ycpqa6jvy8PbFHQ72S1ICat6UY9muOSdlMOUsqvgJIGw7OAV43PrwIXBo2/JhXfA4lCiAZlGzXtgRmUTmxBsRwJ8Ua5bVBNihx2i9+tJKWkzJjFXFDpZG9JDX2NMuOHDOUQFdFwnkNAxjiHjcykKL81AIFJcFnJ0ditgqRoOy6vD6fbh5TSrxz2FNfwxg97WbiybT2tuwPFVcpl18uVp2IKSf3UgtjwPBRoug8dHXPIkFIeND4fAkwfSKhOW5nAQRrQUrcsCG/HrKOlq8tSUqSetg/u2UWON7w3yzhRR5zP7T9mhPCxbfdecnIOU1hejdenXFIb9pXi8kJd8X5s0s3uw8qi2LVtMzmlgaf7vQcDVsfOzesRNR7yCjz+/W/epbbbsvp75p3oYEuJl39vgU++XE6EVfgV03eb89he5MUrobTC2WX+Ph2BOQEuqXoX9D4jUOyuqXiD5ril0wLSUkophGhz2khL3bKge3VjCiehZBlX4+KTgz9wxc/GNapwerRMPNGNzWLxB5aTVy4jISWR7OzxvL3kS6CWOIfNP/dh5onj2Fy9w9/YZ8qEcUwNCo77th7m2XWq9s9p06bg3HSIZfu2MXnqyURH2PjBuRXbrlxmnq66cn2w7gD/3rKGkeNPUDWXln6D1SLYWirxGldenTW6y/x9OoJD5U7iqcbhLITUIYE+Clo5aBrQ0amsh013kfFuzNlvU6ctTRhJjI7gw9tOCbtiAFULKTjjKMYofwFQaRQBHBJ03H4pMSRE2fEYQedQ2Uom8Q47mcakvP2Gu6hhsT5zjkN5rdvvUhrXO5E6T6D4X1Ht8ZXWml9awyBh/GulDQuyHLRbSVOfjlYOi4E5xuc5wH+Dxn9pZC2dCJQHuZ803YTYSJvftVPlNpWDSq21WQS9Eh31Jq01nAQXGxn4Huew+ZVDvhF3aDg3wq8catz+rKaEwnVI6fNP6CuubV2V2O5Cfmktw+zGv1aaYTkk9ukaTXY0XYp2cysJIRYA2UCqECIfuBf4K/C2EOI6YA9wmbH6EuAcYCdQA1zTXnJpOo84hy3Q7tOwHAalK8shMykKm9VSb/Jc42wldblG2Cw47FYyk+pbDjUNZlUHWw77y2qJibCyd+VSDnzzLQNPmIGr/ykU1fZvj5/aZdlfVkt25GHwRSrFYLGGbpWpOe5pN+UgpWyqyPuMEOtK4Jb2kkXTNYiJtPm7zlUZRVNNy6FPcuOsqUZuJWNinVnULz3Ogc0i/BlLtS5vvf4SDd1KWUnRPPXifGb89ROGeTax9L+Ps+BTyWjuZPbs2cTFhd+11tXIL61lqPUgJA/WqauaZtHlMzQdxpisRPJLa9l8oIIql8QiYECaUg5mr4iEZiwHc66E2SvCahH0THT45zo0LNZnztKucCrlkJkURb/UGC6eMoi7bpzDoBPPoLq8hPfff58JEybw5JNPttMv7zrkl9bQx7dXBaM1mmbQykHTYVwyIQuH3cLr3++hyi1JjI4gPS6SfinRTOmv6jkFKweHrb5yiLRZibBaiAtaJzMxKuBWctcPSNusFmINayW/tIaspCg+/uhD9rx1PzfNvoD4SMHA6x7j448/Zt26dfz9739vUnYhxFlCiG3GLP4/hFj+mBBirfHaLoQoC1o2x6gKsEMIMafhth1FdZ2HmppqklyHIG1oZ4mhOUbQykHTYSRE25k1NpNFa/ZTUKMqwdqtFnLums75Y3updYwbf5TdisXSuDRHrMPmtxwAhmbEsfFAOVV1HmpDFOtLiLKzr6SGSqeHPsnRvPvuu9x5551s2LCBi+bcTLUtAZfHR3R0NC+99FJIuYUQVuBp1Ez+EcBsIcSI4HWklHdKKcdJKccBTwLvGdsmo+JtU4DJwL1m2ZiOpnT9El62P4JAastB0yJaOWg6lDlT++Hy+thU7CM5OqLRclM5NNV+NCHKTmLQdueP7YXT7ePtlfs4UOYkqcE+46PsbNhfDqhSIfPmzWPy5MmAsjq87jp+3LAVgBkzGoXDTCYDO6WUuVJKF7AQNau/KWYDC4zPZwKfSylLpJSlwOc0LivTIcT8+CQjLXmU9jsXBmR3hgiaYwhdlVXToYzoFc8/LhvLHQvX+suFB+O3HJpQDo9cMoakoO0m9k2iT3I0f1myBa+UXH1S3wb7s7HloCrb3Sc5mkvPP48VK1b4vydFWblx7i/YtG51c2KHmsE/JdSKQoi+QH/gy2a2zWxi22Zn/x/VbHvp5aSi9bzpPY2knr8i8cf1R7afcMgSZrQsoTlaWbRy0HQ4s8Zlsn/XVmZMa+z3bslymNSvfuVQIQQXjs/kiaU7uHh8JiN71a8oa2Y2gbIcPB4PERFKuUwZkMLjMxP49QdewsgVwDtSyjbvtKXZ/0c12/7wJljuZLMYxN/OmN5kNd3W0tVn/ncW3UkW7VbSdAojUqwMDdG1Lt5vObT+ueXKyX04Y0QGd53VtLJJiYkgNtJGWloaixcv9i//5ptvSE1NbbRdA9oyg/8KAi6ltm7bfuSrsiOF8aOOWjFojg+05aDpUjjsVhx2C9FNtB8NRY8EB8//clLIZaZyyDLmUTz77LNcddVV3HrrrUgpSUhIYNGiRS0dYiUwWAjRH3VjvwK4suFKQohhQBLwXdDwp8BfgoLQZwAd3lbNm7+KKhlD38GjO/rQmmMUrRw0XY6EKHuTbqUj2RcEJtkNHDiQ77//nqqqKgBWrVrFoEGDmt2HlNIjhLgVdaO3Ai9LKTcJIe4HVkkpTVPkCmChNBtIqG1LhBAPoBQMwP1Syoal7NsdZ96PrPMN4JQhuoaSpnW0SjkIIWKAWimlTwgxBBgGfCyldLerdJrjkol9k/xlNY4WswFSH6PLHMBHH33Epk2bcDqd7N69m6+++op77rmn2f1IKZegyrwEj93T4Pu8JrZ9GXj5SOQPCyW5RJVuZx0Xck2D/uAaTVO01nL4CjjFMI0/Qz0FXQ5c1V6CaY5fnrlqYtj21dByuOmmm6ipqWHZsmX86le/Yvny5Vgs3Tj05vPB4tupFQ429byoXvFCjaY5WvtfIaSUNcDFwDNSykuBke0nlkYTHlJiVMvQfikxAKxYsYLXXnuNpKQk7r33Xp5++mm2b+/G7UK3LIa8r3nAdSXDhgzrbGk0xxCtVg5CiJNQlsJHxpiu2qXp8kwdmMJLcyYxub9KgXU4VNvS6OhoDhw4gNVq5eDBblwdviQXgEXeaX4FqdG0htbamHegMizeNwJxA4Bl7SaVRhMmLBbBjOGBLmfnn38+ZWVl3HXXXUyYMAG3280tt3TjgsAeVSLdSQQZ8Y5OFkZzLNEq5SClXA4sBxBCWIAiKeXt7SmYRhNufD4fM2bMIDExkZ///Oecd955fP7555x33nmdLVr74a7BY3EAgh4JWjloWk+r3EpCiDeFEPFG1tJGYLMQ4q72FU2jCS8Wi6WelRAZGUlsbGwnStQBuJ14LCru0kNbDpo20NqYwwgpZQVwIfAxqnbM1e0llEbTXsyYMYN3332XoKkI3RtPLXUigoQoe5P1qjSaULQ25mAXQthRyuEpKaVbCHGc/HdpuhPPPfcc//jHP7DZbDgcDjweDzabjYqKis4WrX1w1+IkQlsNmjbTWsvhOSAPiAG+MipPdtP/Jk13prKyEp/Ph8vloqKigiVLlnRfxQDgdlLjiyBDxxs0baS1AekngCeChvYIIaa3j0gaTfvx1Vdf1fu+bt06LBYLp556aidJ1M64a6jy2ekRH9nZkmiOMVpbPiMB1c3K/A9aDtwPlLeTXJrjAWc57P0BhpzRYYd85JFHAod3Ovnuu++YPHkyX375ZTNbHbtIdy2VXpt2K2naTGtjDi+jspQuM75fDcxHzZjWaI6MDf+Bj34Lv8+DqI7pnPnBBx/U+/7222/z1ltvdcixOwNPXQ21MlK7lTRtprXKYaCU8udB3+8TQqxtB3k0xxOumsB7BymHhqSlpbFly5ZOOXZH4HHV4CSZnlo5aNpIa5VDrRDiZCnlNwBCiGlAbfuJpTku8BlFfb11HXbI2267zd/sxufzsXz5ciZMmNBhx+9opEtlK/XTbiVNG2mtcrgJeM2IPQCUAnPaRyTNcYPP6KTpcXXYISdNCjQFstlsDB06lNtuu63Djt/RCE8tThmpYw6aNtPabKV1wFghRLzxvUIIcQdwdF3KNcc33o63HC655BIcDgdWq5oQtnTpUmpqaoiOju4wGToS4XHis0WSHBPR2aJojjHaVMheSllhzJQG+M2RHlQIcacQYpMQYqMQYoEQwiGE6C+E+EEIsVMI8ZYQQl/N3R3TrdSBlsOMGTOorQ14RF0uFzNnzuyw43c0dp+TqOg43Tda02aOpsvJEV1tQohM4HZgkpRyFKr09xXAw8BjUspBKLfVdUchm+ZYoBMsB6fTWa+eUlRUFDU1NR12/A7F68aKj5juXj9K0y4cjXI4mvIZNiBKCGEDooGDwOnAO8byV1GlOjTdGZ9HvXs6TjnExMSwevVq//dt27YRFRXVzBbHLhWVyshPiEtoYU2NpjHNxhyEEJWEVgICOKL/KCnlfiHEo8BeVMbTZ8BPQJmU0rhbkA9kNiHTDcANABkZGeTk5DRap6qqKuR4Z6BlCU1VVRX7D+whE9iwdhXF+R1TFO7qq6/m/PPPJyUlBSklxcXF3HvvvS2eFyHEWcDjKEv3RSnlX0OscxkwD/U/s05KeaUx7gU2GKvtlVJeELYf1Ax5h4oYAyQlxHfE4TTdjGaVg5QyPF3egzD6UM9CVXYtA/4DnNXa7aWUzwPPA0yaNElmZ2c3WicnJ4dQ452BliU0OTk5ZPZIhwMwevgQGJndIcfNzs7muuuuY9u2bQAcOnSoxZiDEMIKPA38DPXgslIIsVhKuTloncGohljTpJSlQoj0oF3USinHhfeXtMy+ghLGAKlJ2nLQtJ3O6Kw+E9gtpSyUUrqB94BpQKLhZgLIAvZ3gmyajsTvVuq4gPTTTz9NdXU1o0aNYtSoUdTW1vLMM8+0tNlkYKeUMldK6QIWoh5wgrkeeFpKWQogpSwIu/Bt5EBhCQDJCVo5aNpOZyiHvcCJQohooVIoZgCbUW1HLzHWmQP8txNk03QknRCQfuGFF0hMTPR/j4uL44UXXmhps0xgX9D3UG7PIcAQIcS3QojvDTeUiUMIscoYv/CIhW8jh4pLAbBGds80XU370tpJcGFDSvmDEOIdYDXgAdag3EQfAQuFEA8aYy91tGyaDsafytpxysHr9SKl9Kd2er1eXK6wWC42YDCQjbJ8vxJCjJZSlgF9jVjbAOBLIcQGKeWuhjtoKZ7W2piRxVtHrwOfUHhQ9c5eu3k7ZQfDG3TvavErLUtjjlaWDlcOAFLKe1FVXoPJRZnvmuMFv+XQcW6ls846i8svv5wbb7wRgAceeICzzz67pc32A72Dvodye+YDPxiu0t1CiO0oZbFSSrkfQEqZK4TIAcYDjZRDS/G0VseMfngOdr3MJN9ZYIFxE0+EPlNa3q4NdLX4lZalMUcrS2e4lTQaRSeksj788MOcfvrpPPvsszz77LMMGDCg3qS4JlgJDDYmakag5uUsbrDOIpTVgBAiFeVmyhVCJAkhIoPGp6HcqO2DlLDyRQB6ycNqzN49U3U17YtWDprOw1QOHWg5WCwWpkyZQr9+/fjxxx9Zs2YNw4cPb3YbI8X6VuBTYAvwtpRykxDifiGEmZb6KVAshDDjZ3dJKYuB4cAqIcQ6Y/yvwVlOYWf3V1C0HYAsUaTGtHLQHAGd4lbSaICAW6kDLIft27ezYMECFixYQGpqKpdffjkAjz32WKtMbynlEmBJg7F7gj5LVEmZ3zRYZwUw+qh/QGvZ+A44EvB43PR2GwlTNl10T9N2tHLQdB4d6FYaNmwYp5xyCh9++CGDBg0ClGLodpTvh+SBVJUWkejZq8bsOltJ03a0W0nTeXRgKut7771Hz549mT59Otdffz1Lly5FPex3M6oLITadUpEYGLNry0HTdrTloOk8OjCV9cILL+TC6SdQveVL/ptr5Z///CcFBQU89thjuFwuzjij4/pYtyvVRdBzDIWyiv7mmE3HHDRtR1sOms7D28EB6bVvEvPJrVx56cV88MEH5OfnM2jQIB5++OGOOX57I6WyHGLSOOAxKt9YI8Gi/801bUdfNZrOo6NTWV1VxvGcACQlJXH++eezdOnSjjl+e+MsB58bX3Qqe5xGmW6dqaQ5QrRy0HQevg6eBOc25jN04LyKDqW6EIAKSyKHfUYlVq0cNEeIVg6azqMDU1nVcUzl4OyY43U0hnI47IujSBrKQaexao4QrRw0nUdHT4I7TiyHvNpoiqRRiVWnsWqOEK0cNJ2H33LooCd59/FhOWytdFBuTVJjOo1Vc4Ro5aDpPDq6Kqu7pmOP19FUq3IZm0ptxKb0UmPactAcIVo5aDoPn1e9d7hbqRtbDlFJbC9ykpWeCvYYHXPQHDFaOWg6j44OSHd7y6EQX0wae0tqGJgWA3E9IDK2s6XSHKPoGdKazqPDU1kNi6HbWg5FOCOS8UkYmB4LQ5+GqKTOlkpzjKKVg6ZlDq4HawSkDwvfPqXs+Elw3d2tVFVAuWMgAAPTYiHzpE4WSHMso91Kmpb56Lfw+T0tr9cGhPQGvnSY5dD93UqFPlU2Y0BaTCcLoznW0ZaDpmVcVcpyCCMB5SC05RAOPC5wlnHAHUvPBAfREfpfW3N0aMtB0zIeZ9hvqH7lEBGjSnY3LJ/tqoH9q8N3QCkDlkMHdp7rMMpU74bdnhR6JeqSGZqjRysHTct46sLec0FII94QYbg/Gt6w170JL86E6uLwHNBTBxgKqDtaDqW7Adhcl0qPeJ2+qjl6tHLQtIzHGXbXj8UXZDlA4/3XlIL0QvGO8BzQtBpCHas7UJILwNrqJDK0ctCEAa0cNC3jqQv7DbVFy8Fdrd6Ld9Yfd9UcmSzB1kJ3tBxKdiPtMexzxdIjIbKzpdF0A7Ry6G7UlqlXOGkX5WBaDrGBYwRjBo8bKofXL4Qlv2v7Ac39hTpWKxBCnCWE2CaE2CmE+EMT61wmhNgshNgkhHgzaHyOEGKH8ZrTduFbQUkudfF9AaEtB01Y0CkN3Y1F/6PeZ7/Z/HqtxedVk9XaK+Zg1v5puH/TDVS8E1bNh5JdcMaDULYPSvNUgFmI1h+wnlupbZaDEMIKPA38DMgHVgohFkspNwetMxj4IzBNSlkqhEg3xpOBe4FJqKDHT8a2pW0SoiVKd1MZrRqD6piDJhxoy6G7UZEPlQfCtz/zKTvsloNPffDHHBq4lVymcsiF756Cje+r7+5qqDoMxbvadsCjsxwmAzullLlSShewEJjVYJ3rgafNm76UssAYPxP4XEpZYiz7HDirrQI0i88LpXkUR2QC0CNBKwfN0aMth+5GXRVY7eHbn/mUHfaAtGE5RBq9jhtZDsbNvHALSF+gDISpNPK+htRBLR/I54PcZSCCnoPaHnPIBPYFfc8HpjRYZwiAEOJbwArMk1J+0sS2maEOIoS4AbgBICMjg5ycnHrLq6qqGo0BRDoLOcnrYn2J+nfetvZHdlvbYFUdAU3J0hloWUJztLJ0inIQQiQCLwKjUKb2tcA24C2gH5AHXBZ20/t4wFWlmsqHC1MpSC94PWANzyVTb54DNLYcTDeQaWG4qlWhPrMeU943MOmalg+05xv498Vwym8DY+2TrWQDBgPZQBbwlRBidFt2IKV8HngeYNKkSTI7O7ve8pycHBqOAZC7HL6HyqSRJJXbOWPG9CMQv200KUsnoGUJzdHK0llupceBT6SUw4CxwBbgD8BSKeVgYKnxXdNWXNX1/etHS/ATfRizfBplKzXct7umvpLzusBZEfie903jiXOhMIPzRUZKrD3mSH7HfqB30PcsYyyYfGCxlNItpdwNbEcpi9Zse3QYcxy2uVJ1MFoTNjpcOQghEoBTgZcApJQuKWUZyof7qrHaq8CFHS3bMY/PpyyHcCqH4KfsMM4sbmQ5hApIZ4xQn83SHUanMzJGQ9UhKN9Hi5iKoGyPeo9KOhLLYSUwWAjRXwgRAVwBLG6wziKU1YAQIhXlZsoFPgXOEEIkCSGSgDOMsfBRcQAQbKmO1fEGTdjoDLdSf6AQmC+EGAv8BPwayJBSHjTWOQRkhNq4Jb8sdC+/X1uwemo4BcBdQ86yZY2yeY5EltjKXCYZn1d8vQxXZEo4RMVRUwXAzj0HGQRsWPsTxfmBy3FyeTGVcQOpGHQDEa4S+u59h7Xffs44YL81i0w2sOGLBRSnNnT916fngbUMBdyFu7ADVV473pIC1hjnoTXnRErpEULcirqpW4GXpZSbhBD3A6uklIsJKIHNgBe4S0pZDCCEeAClYADul1KWtPY8tYqKAxCbzsFKNyOztHLQhIfOUA42YAJwm5TyByHE4zRwIUkppRAipM+gJb8sdC+/X5uoOAjfqI/Z06ZARP0WkUcky75opb6BqSdMgOT+Ry8nsP5dVTdp0IixsAtGDx8CI4NkWy2IzuxLxqxHYP3bsPcdxg3OhHWQOeUieP8TRqcJOK2F3/P9VtgOdo9SRrFpWeAs85+H1p4TKeUSYEmDsXuCPkvgN8ar4bYvAy+3eJAjpfIQvrieFBW5tFtJEzY6I+aQD+RLKX8wvr+DUhaHhRA9AYz3gia21zSFqyrwOTh182ioN7M4fIHcxjGHhqms1YE5EOa70SOZmFRIHgCH1rd8IE+D8xCV2P3KZ1QeojYyDYA+ybpntCY8dLhykFIeAvYJIYYaQzOAzSgfrjl7dA7w346W7ZinnnKoDs8+68UcwqkcjJhDc6msplIwFYgZc4iIgR6j4dCGlg8UrCStEWpGdncrn1F5gFJrKgD9UnUfB0146Kx5DrcBbxjBvVzgGpSielsIcR2wB7isk2Q7dqkLUg6uMAWlg5VDu1oOQfv2eZWyaEo52KOhxyjYvEhlMDnimz5QsHKwRYEtsntZDp46qCnmkEwEoL9WDpow0SnKQUq5FvxxzmBmdLAo3Yt6lkO4lEP7uJUaVWUNzoQyZbcbfQn8bqVgy2GM+nx4E/Rtph1msPz2KLA5upflUHkIgL2uBBKi7CRFh3ECpOa4RpfP6E7UtYdyCLPl4KqBj36L3V2uvkfEBcZNzKd9UzmEshwyRqnPhzeqFN6dS2HfyqYn05n7626Wg6Ecdjhj6Zcag2hLvSmNphm0cuhOuCoDn9slIN3GJ+6tS2DPCuUmemYqbHofDqyGlS+SVGoEk+1REBkPNUWB7cwbuqkUGimHKIhRAVhqy2DfD2oW9Esz4ZPf15fBHWw5RAcsh9ZMoDsWqFTZ35sqYuifooPRmvChlUN3wlUd+vPRcDQB6Y9/D1//A5zlULAJDq6HOqXAbB5DkVnt6kZfFZSc5mrCrVQV5FayRagAs6sqoFiikuHA2gbyN3QrGbOuu0urUEM5bKyM0sFoTVjRyqE7UdeFUlndTjWDubYUnGVqrK7SrxzsbqMUhsUGsekBqwCC3EoNUlldlWCxBwoLRsQqJWgqwsyJqkxGsFXgrgn0jDBjDg1/17FM5UF8lghKZJwORmvCilYO3Yn2SGUNfsJui3IozQOkUgxmfaO6SqhTSiHCZSgHq13NW6inHBpYDhZLUOZSkOvEVA6GwiFzglIglQeD9uWExL7G/qIDlkN3iTtUHMTpSAOEVg6asKKVQ3eirlIVloMwprIeoeVQYvRbqC1TbiWoZznYvIbystghJr2+W8lvOQTd7PxWRNBYRIxSBqbl0GuCei/cFiRzLcT3VMexO4Ish26iHCoPUmZNQQidxqoJL1o5dCdcVRBrBGrD5laqC/RCaEvMwWzG4ywPcitV1Hd9gbIcYtOhtkSV5IaA1WNaDhCwGOpZDjGGW6lKydhzrBo3K7BCYDJdYm+ITul+yqGqgL2uOMZkJRLn0GmsmvChlUN3wlWtMn+skWGcIe1U+4Qjsxx8blXzCQzlUFl/PWEJZB6Z5TEaprJCUNwghHKoq1LL43ooWYuCLAdTOVz5H5j+v0Fupe4Rc/DVlrKnJoJTB6d2tiiaboZWDt2JuipVjsIeFV7LISIWhLVtyiG4jWdpniFfZX3lYLGryrGx6ep7teFaapjKCo2D0xAUkK5S6woBqUOgaHuQ/E7lTkodpGIb3cxykLVllMoYThmc1tmiaLoZWjl0J1yV6oYZERPe8hm2yLbPLC7JDTztm70UggLSQCDrKMZQDmaqasNUVgjtVoqMVYrBVRU4VuoQKAxSDu5aVTbDpDtZDu5arD4XtZZYxvdJ7GxpNN0MrRy6E3VV6oZpj277DOmiHfDp/8KCK6EmqN2Ax2koh4jWzw1w1UDFfug1Xn0PthyCM6oshnIw4yR+y8GwemytdCu5qgNWRspA1QjIVDDu2vpKpjulshqB/uTUdOxW/a+sCS/6iuoOHFgDK54MPEHbo9quHBbfBj88B9s/gY/vDowfieVgtK0MKAfDcvA4oaY4sJ7Zj9pvOQS5lawR9ftVNyzCZ36uqwq400B1egN14/R6VMzDHspy6AZuJSNFOCo+PA2YNJpgtHLoDqx9Ez77s5orYLqV2qocqgth+PmQ/QfY8B/Y9rEa9ziVYrBGNH1DrSqEn14JTD4zA9A9Rhv7CIp/VBwIfDYth4gYZSWYcx3cNfUtBAi4kxrGHDy1ylVlKg1Hgnqvqwgc17QWgj93A8uhrkopWkdccidLoumOaOXQHXAafnzpM9xKUa2LOez9HlY8ZeyjXN1YT/6NulHnGS3l6lkOTSiHT34PH/w6EAiuUsXgSB3SeN1gy8FiWAZCKNdSs8rBcCs1tBwAqg4HlpvKwVkRqKvUTS2H8hKV3RUdrzOVNOFHK4fuQHCQN6INMYfv/wVfPqCe+E3lYLUpF40ZGzAtB1sTlsOBNbDxXfX58Eb1XnVYvacMAowqodbIwDbWCOM9yG0UPBGuYZwAms5WApUCayoKM+3WWR6wHELFHL59HL57uvHvOYaoKlfKIT5JZyppwo9WDt0BZ5ByaEtA+vBGdfOvLVXBZvOpOzI2MOvY61I3c5sj9CS4nIdVwTuLDQ6ZyqFA3aQjYwONeBKyAtvE91LvlqBJW8H1lVzNuZVCBKmRgZiDeby68qDAdpBbKToF+p+mSmz8+HyTp6YhQoizhBDbhBA7hRB/CLF8rhCiUAix1nj9KmiZN2h8casP2gI1FcoKS0rRykETfrRy6A7UlUPKYHWzTeijbqQtuZVcNYG5CGY2UVSieo+IDcxk9sccmuiDkP+jilWkDglYDpWHIDZDfXYY+0zsHdgm3lAU1mDlkBGoieSuqZ+yasoEod1KwZ/ruZUaFPAzjzlnMYy7KpA62wJCCCvwNHA2MAKYLYQYEWLVt6SU44zXi0HjtUHjF7TqoK3AVamyylK1ctC0A53VJlTTEh/eqW5wl7zU8rrOcuhzElz3mcrW2fVly5PgCrYARgDZzC7yWw7BbiUz5hAZqJFkUl2sYghpQ9Xx9nyrxqsKAsohKlHNcwi2HBIy1bsl6PKLz1T7cjvVvhoqh5BupWDlYCgP061UVxEIOtuDLAeT2DRwV2PxtiowPRnYKaXMBRBCLARmoXqfdxqemlKqpIPEuO7Rx8HtdpOfn4/T2bZkgYSEBLZs2dJOUrWNribL7t27ycrKwm5ve2kVrRy6KgfXN74ZN4WzQt0Uo42sFXuUKp8hpQr2huLwhsDnkgbKISI2EDfwxxwc4Cmovw8zAJ06BHwe2PC2miNRdQh6jjP2majeE/oEtos3lEOw5RDfU71XHlTKIaZBkDVkbaXYwOfIIMtCWNW581d3DXHzNNJnI1ytOseZwL6g7/nAlBDr/VwIcSqwHbhTSmlu4xBCrAI8wF+llItCHUQIcQNwA0BGRgY5OTn1lldVVdUbEyWHqBIxrFq+vDW/Iaw0lCUcxMbGkpGRQWZmZps62nm9XqxWa1hlOVK6kiwej4eqqirWrVtHVVVVyxs0QCuHroqzrP5ktKaQUk0uM33toG6g0qfiBbbI0NuZ8QEIshwSje1jQlgOEY1jDmYNo9QhASV0eKOyHOJ6qO+mqyrYrRTScjDiEJUHVfOenmPqH8usxhpclTUySDmYikIIZfkEZyvZQlgORj2nCFdZ42VHxgfAAillnRDiRuBV4HRjWV8p5X4hxADgSyHEBinlroY7kFI+DzwPMGnSJJmdnV1veU5ODsFjq77/K05bPA3X6wgayhIOtmzZQlZWVptbnVZWVhIXFxdWWY6UriZLVlYWVVVVTJo0qc3b65hDV8VskuPzNr+eqxqkN/DUD0HNcZopvnd4Y+Bp3pykFhyQrgtWDo7Qk+AKt6u014TekGHMadj3g1Ed1pjYZiqcuB7qiR4CMYfggLRpTRTtUAoiuX/9Y5mWhal0ILRbyfwdwfMcGmY+gX9Wtt1d1nhZY/YDQdqNLGPMj5SyWEppas8XgYlBy/Yb77lADjC+NQdtCbu7Apeta9yIwoXugR1ejuZ8auXQFZFSzX6VvpZdS2Yaa2SQ5WAqh+Yylgo2Q7+T1WczIO13KxkxB69HKR5bZOhJcEXbVUE7i0Upg/gs2Gwk4wTHHEApCTOjyLzRB6eyxhljZtwieUD9Y/UcC7evhV7jAmNNBacd8YZbqRnl4HcrlTVe1piVwGAhRH8hRARwBVAv60gI0TPo6wXAFmM8SQgRaXxOBaYRplhFlLcSb2RCyytqWkVxcTHjxo1j3Lhx9OjRg8zMTP93l6v50jGrVq3i9ttvb/EYU6dODZe47Y52K3VF6irVTRlUkDa6mRmwpvIIdiuZN+Ha0vqBYBOvW22X3F9lIZXnG/sITmWtCiiXpibBFW2DrMnqsxDQ/xRYt0B9b5itFJWoFJizDByJeKzR2IItB0e8Ukrm5LvkgY3lbmhNNOViikyon61kC6UcWu9WklJ6hBC3Ap8CVuBlKeUmIcT9wCop5WLgdiHEBai4Qgkw19h8OPCcEMKHehj7q5TyqJWD0+0lRlZTYZ5fzVGTkpLC2rVrAZg3bx6xsbH87ne/8y/3eDzYbKFvmZMmTWLSpElUVlaGXG6yYsWKsMnb3mjLoStiNseB+jOKQ65rWg5BT5Dpw9X74U31Vo2r2KEmrZnbOBIMxSPVDdSMT5gumloj5tFwEpzXrfZdtk9lKpmYlggElEPmREgbriwDU2lFxuGxRdcPSIOKO1QY3pqGlkMobBGBCXWh3Ep+yyFEzMEWAY7EVsccpJRLpJRDpJQDpZQPGWP3GIoBKeUfpZQjpZRjpZTTpZRbjfEVUsrRxvhoKWUr0s9apqCijgSqsUYnhWN3miaYO3cuN910E1OmTOHuu+/mxx9/5KSTTmL8+PFMnTqVbdtU3C0nJ4fzzjsPUIrl2muvJTs7mwEDBvDEE0/49xcbG+tfPzs7m0suuYRhw4Zx1VVXIY3yM0uWLGHYsGFMnDiR22+/3b/fjkZbDl2R2tLA54ZB6eJdkNRfuXIg4FYKthxSBqub/cF1MPYK//DgHc9B8SK4wLhYI+NV6mvlwfoxC/Mp3FRMpuXgrVMur6X3qUJ/AGnDAtuFUg4DToNbvjf2ayiHiFhqorNwJAS78VHKoWibeqoP/j3NERGrlFg95RCvFKAZIwllOQDEprc25tDl2FdUTh/h7LZ1le77YBObD1S0vCKtzxAa0Suee88f2WZZ8vPzWbFiBVarlYqKCr7++mtsNhtffPEFf/rTn3j33XcbbbN161aWLVtGZWUlQ4cO5eabb26UTrpmzRo2bdpEr169mDZtGt9++y2TJk3ixhtv5KuvvqJ///7Mnj27zfKGC205dEWMaptAfcuhbB88NSlQrgICbqXgmIPVBhkjVTpsEJF1RWp/dUGWg1nFNFg5RBg38Zogy8EaqWIgPg8c2qAU0KWvwNCzA9sl9lXBaWFVM5EbEhnnt0I2jL4Hzvp/9ZebGUutsRr8soaYHBcZFHOw2OvHNoKJSQ9ntlKHsnm3cgWmpWd0siTdn0svvdSvfMrLy7n00ksZNWoUd955J5s2bQq5zbnnnktkZCSpqamkp6dz+PDhRutMnjyZrKwsLBYL48aNIy8vj61btzJgwAD691cu1M5UDp1mORizTlcB+6WU5wkh+gMLgRTgJ+BqKWUrGwh0M+pZDkHKoWCLukHv+wHGXKrGgm/0wfQcq6qr+nzKyvC41I3Q6QiKUzShHEzLwWzbaY2oX7CuOBf6nAgjL6p/TCFg0EwVN7CEeO6IjPNbD9JiBUuDp70jUg6GUmhoOdRVGAX8mrAaAGLTiCjY3fpjdSF27VVTKKLiume57rY84bd3+mhMTODB4//+7/+YPn0677//Pnl5eU2m80ZGBlLIrVYrHo/niNbpTDrTcvg1RkaHwcPAY1LKQUApcF2nSNUVCI451Aa5lYp3qveD64LWDeFWAjVPoK4CyvLU96pDCKSRIhsUxA5pORj/DGagOjI+oBzqKqB8X9M38DP/Atd8HHrZsHNhzOWhl0GQcggRjG6KiBjDsgl6znEkAFIpt+aUQ0zaMelWklKSf9AoNWJmg2k6hPLycjIzVdr1K6+8Evb9Dx06lNzcXPLy8gB46623wn6M1tIpykEIkQWci8oHR6hk3NOBd4xVXgUu7AzZOgRnBbxyHuSvCr3ctBwcCfUthxJj3tThjYH5D85y5cZpOAu451j1bioSs8eCxxmofuoPSNNAORhP4cU71Ht8z8BN9tBGQDatHCKiA53dGjL6EjjzodDLAOJM5dC/6XUaHS+mvtUAARdb1eHQE+BMYtKxe6qPufLduUXVxNYZf8O4ns2vrAkrd999N3/84x8ZP358uzzpR0VF8cwzz3DWWWcxceJE4uLiSEjonHTlznIr/RO4GzBtwRSgTEppnu18VMmC7smm9yHva1j7BmSFmLlYW6ZcOQm96wekzUJ57hplRaQNVU/yjvjGZTLSRwQqpY68KJAFBIGezpFBlkPwE6gZOC4ylENcTxXDANho6O+2uH5aS9+TYOyVMPD0ltc1iYyrH2+AgBVVvKv5NGB/e9LC0Cm/XZQ1e8vIEkbRwMQ+za+sOSLmzZsXcvykk05i+/ZAj/IHH3wQgOzsbLKzs6msrGy07caNgWoEZhkLc32Tp556yv95+vTpbN26FSklt9xyyxHNbg4HHa4chBDnAQVSyp+EENlHsH2z9Wegfeq+tJaIulI8tih8VkeTsoxb8yyJQO3Gj/gh5oJGN/YhuZtJtURTXWfBcjCXNcb2Uw5swhvTl9jqPWz+cgEFGdkM37ODeBnBDyF+74n2REp3rGabNYesfV8zyBgv3L6KVATLv19Nz4OFDAX2HC5jt7GPiLpipgKew1uxCDtf/bAOIX2cbHHAxvexAt9sPYhnV+NjtpYm/0ZJl8OP6xuPN0GaZQSO5BT2Be0rqSSPsQCVB9iXMJFdTVwLKUWHGQ38tPxjKuMHt0H6zmXtvlKG24qRkfEI7Vbqdrzwwgu8+uqruFwuxo8fz4033tgpcnSG5TANuEAIcQ7gAOKBx4FEIYTNsB4alScwaan+DLRP3ZdW4XXDP0eroOysp0LLUpILOZsheSBRJbvIHtMHUhr42AteBncGERkD4dBGtb2nDpYXwcl3wndPMyLRzYjsbDjwL7BkhP6923vTM9pCz+xs+PRzMAyPNHsNRMaRPf102FwF25+m79Ax9J1m7KOuEr4Dm7cGEvuSPX26Gs8/CXKXgSOBk2ee33RRv1YQvr+R2ke9M5gfB4Z+6X3GrfTuc2LoTcsHs81VxsTTZwXKfRwD7Cyo4gJHGaJhKrCmW3DnnXdy5513drYYHR9zMCYLZUkp+6HKEHwppbwKWAZcYqw2B/hvR8vWJjx1AbdLdTGU71elsisPwvZPVJZQKNa/DQg4/3H1fdeXjdepLVNunqjkQMyhNE9lKqUNUy6eA2vUuLOicaaSSWxGIL4Q7FYq3RPYJlRAOnjmcbBPu68x9T95wFEphnbHdCvF9gjM4A5FQiYHe515TCkGgN1F1WRRWL+YoUYTZrrSPIffA78RQuxExSDCMpO03fjiPvjXNHUj/+8t8NwpgbaT1YVwSD26Cp+7fuOdje9B32lqwlhiX9i5tPG+a0vVTTs6RX1+7wZ1PFCZPFmTYf9q8LhUzCEyvvE+QE0m8yuHAzgjjZugsyygDBKyABEohgcqDdUM8gYXuutzkiFDO8Qbwon524afHzql9himus7D4Yo6UjyHdbxB06506n+OlDJHSnme8TlXSjlZSjlISnlpUIXLzqWmBJ47DXZ+ERhzlsPqV9WM4T0rVF5/TTHsXg4jZql1jPWHbnsK3rxMjRVsUTOAR16onryHnw87P4fKBhNkjPpDaiKZhPVvwbaP1LKUAWqOgadWKSBnedOziWMzlKLy+aDiIFWxfQPLTIWS3B9uXQmDZtTf1q8cgiyHrElqu4xRrThxnUhsOpz1V+WC62bsLqomnmoivVUqYUGjaSe612NVe/D13+HgWti0SLWVfPd6+Oi3qjCdsKg+xK5KGHyG+j7tDpVGalgE8RXbIX+lSj3d9L5aZ7jRKXLStWrG8erX6h+ztkxZDmbDm0nXwcx5MHa2Gjef4HcuVbn8pmuoIbHpqoBfTRFUHqAmujdguIOC3Uipgxu7iSJDWA72KLjlRzjpllafvk7jxJsDfSO6EbuLqskUxuRE7VbStCNaOTRH6Z5AE/q936sn+A1vq5nHfadB5iQVoAU4759w927InKBSMff9AM5yomoPqbkFpXmw+b9quzij5EHKQBgwHX6ar8pjg3qvq1Axh8E/U0/AZ/5FPQVf9KxaJy5DuXa+eUxZECMuDC2/6Us/vAl8HpyO1KAS2i3ULjLTQxvm0cf3bLqBkKbdqaccErRbKZxMnz6dTz/9tN7YP//5T26++eaQ62dnZ7NqlZqrdM4551BWVtZonXnz5vHoo482e9xFixaxeXOgUO8999zDF1980cwWHcPxqxykVJ3CpFRul5yHYdlf6k9MW/aQetI/4Xo1IWz9QlVhdM4HcPELgQBtYl/1lGreeDMnqSf2rR8hMALTucugcKu64Qdzwq9UsHjT++q7OXs5Kkk93Z94c+iqon1OUoqhz1ToE6pjJf6eBez7AYC6yPRACe2mgtgmZn2leD3Jqiuxu6iakdHGNaJjDmFl9uzZLFy4sN7YwoULW1XfaMmSJSQmJh7RcRsqh/vvv5+ZM2ce0b7CSbdUDhmHvoRXz4cHe8Bjo+DpKfCvk2HhVfD4OJiXCPclwUMZ8MHtkPsl5PwFlj8M889RBesOrFWWwok3w6iL1Y4PbVA39/6nKmXQd5oaN99NzNnJG94JjK18Wb33adDsY+g5asLa8oeV62nrh2o8dUjzP9KsgNqcX92sjGpkRFXF9gu4oJoKYptEhog5aDqd3KJqhjpKVaXZhn22NUfFJZdcwkcffeRv7JOXl8eBAwdYsGABkyZNYuTIkdx7770ht+3Xrx9FRcqie+ihhxgyZAgnn3yyv6Q3qPkLJ5xwAmPHjuXnP/85NTU1rFixgsWLF3PXXXcxbtw4du3axdy5c3nnHXXvWLp0KePHj2f06NFce+211NXV+Y937733MmHCBEaPHs3WrVvDfj66ZcnuuMpd4C6C8b8I1PX31Kkn97ShMOrnysd+aCOsfl2lhcakww058OJMWHCFKgoXnaJuvtYIVd3T51axBZM+J6qg4IgL6guQkKXSUHNz1PeYNCjYpEpcmIrDxGKB034P/5kDX/9DuZgyJ8GA7OZ/5OjLlMXSb1rT65gzgPNXgSOBushgt1JLlkOImIOmU5FSsruwiiGxeyBlUNdOJz5aPv6DehhrBVFeT9OVd4PpMRrO/muTi5OTk5k8eTIff/wxs2bNYuHChVx22WX86U9/Ijk5Ga/Xy4wZM1i/fj1jxowJuY81a9awcOFC1q5di8fjYcKECUycqDrGXnzxxVx//fUA/PnPf+all17itttu44ILLuC8887jkksuqbcvp9PJ3LlzWbp0KUOGDOGXv/wl//rXv7jjjjsASE1NZfXq1TzzzDM8+uijvPjii604W62nWyqHXQOvJev0GS2vWFMC/xyjLsLTfq+sgUvnw3vXq9TOU34TuIn2GgcFW5VCMHHEw50bG+9XCFX4LjeHuogkIrMmq2yjrEmqyUxDhl8A/U+DZWoqPhc+0/I/vtXWvGIAo2Ce0fs5Y5Tap39eQwuWg9mZzSyloel0nF7oGWejX81GGDGns8XplpiuJVM5vPTSS7z99ts8//zzeDweDh48yObNm5tUDitWrOCiiy4iOlrVOrvggsCD48aNG/nzn/9MWVkZVVVVnHnmmc3Ksm3bNvr378+QIcqLMGfOHJ5++mm/crj4YuXRmDhxIu+9997R/vRGdEvlIBuWgm6K6GSYdrsK7E68Ro31ORHuCPHEMnOeSgtt2L2sKXqOhdwcaqMyiUwfrpRDQ5eSicUCVy+CrR+oyXQtWQ2tRQhlEZXvDdRGam3M4cRb6ltJmk4nyib49PIEeNGp6lB1Z5p5wm9IbRhLds+aNYs777yT1atXU1NTQ3JyMo8++igrV64kKSmJuXPn4nQ6j2jfc+fOZdGiRYwdO5ZXXnnlqEv8mCW/26vcd7eMObSJU++COze1HHjtd3Lj/gXNYbiPaqIzoYcxL6C5J32LRc2ROOl/Wn+M1mBmLJnKIdSM6FCkDqrfyEfTNdhj9CBu6kFDc1TExsYyffp0rr32WmbPnk1FRQUxMTEkJCRw+PBhPv64iXL0BtOmTWPRokXU1tZSWVnJBx984F9WWVlJz549cbvdvPHGG/7xuLi4kL2nhw4dSl5eHjt3qlL9r7/+OqeddlqYfmnLaOUgRPOVO4+UXuMBqI7pDcPOh9lvQb9Twn+clvArh9Hq3R+Q7pwywMcqQoizhBDbhBA7hRB/CLF8rhCiUAix1nj9KmjZHCHEDuN1dP6gPStUvMFMh9aEndmzZ7Nu3Tpmz57N2LFjGT9+PMOGDePKK69k2rTmXbnjxo3j8ssvZ+zYsZx99tmccMIJ/mUPPPAAU6ZMYdq0aQwbFmive8UVV/DII48wfvx4du3a5R93OBzMnz+fSy+9lNGjR2OxWLjpppvC/4ObQkp5zL4mTpwoQ7Fs2bKQ4x3Oji/k8i8+7lwZFt8u5b0JUtZVqfOy7i0p742Xsji3U8XqKn+j5uQAVqk3rKiyhQOACGAdMEIGXYvAXOAp2eAaBZKBXOM9yfic1HC9hq9Q1/ayL5dK+f96S/nfW8N4Bo6M9vj7bd68+Yi2q6ioCLMkR05XlCXUeTWv7eZe3TLm0GUYNANffk7nyjDhl5A6NDCpbfgFcGVi2xrqaCYDO6WUuQBCiIXALGBzs1spzgQ+l1KWGNt+DpwFLGirEFZvrUp9HnJWWzfVaNqMVg7dncyJ6mVid8AQHWhuI5nAvqDv+UComYc/F0KcCmwH7pRS7mti2yOq6+G1xQRmyWs07YxWDhpNePgAWCClrBNC3IhqdduGlnYtN7LqzCZWDWkPWRISEkIGZlvC6/Ue0XbtQVeUxel0HtHfSisHjaZl9gPBVe4aNaOSUgY1++ZF4G9B22Y32DYn1EFkC42sOq2JVQjaQ5YtW7YQGxuLaOPkvsowprIeLV1NltjYWBwOB+PHj2/z9jpbSaNpmZXAYCFEfyFEBKpJ1eLgFYQQwbnQFwBbjM+fAmcIIZKEEEnAGcaYpgEOh4Pi4mIzkK85SqSUFBcX43CEqM3WCrTloNG0gJTSI4S4FXVTtwIvSyk3CSHuR2V9LAZuF0JcAHiAElT2ElLKEiHEAygFA3C/GZzW1CcrK4v8/HwKCwvbtJ3T6TziG2C46WqyJCYmkpWV1fLKIdDKQaNpBVLKJcCSBmP3BH3+I/DHJrZ9GXi5XQXsBtjtdvr3b3sWXU5OzhG5TdqD7iSLditpNBqNphFaOWg0Go2mEVo5aDQajaYR4ljODBBCFAJ7QixKBYo6WJym0LKEpqvI0pwcfaWUaR0pjEkT13ZXOWegZWmKY0WWFq/tY1o5NIUQYpWUclJnywFalqboKrJ0FTlaQ1eSVcsSmu4ki3YraTQajaYRWjloNBqNphHdVTk839kCBKFlCU1XkaWryNEaupKsWpbQdBtZumXMQaPRaDRHR3e1HDQajUZzFHQr5dBSK8d2PnZvIcQyIcRmIcQmIcSvjfF5Qoj9Qe0jz+kgefKEEBuMY64yxpKFEJ8b7So/NwrBtbccQ4N++1ohRIUQ4o6OOi9CiJeFEAVCiI1BYyHPg1A8YVw/64UQE9pDpiNBX9v15NHXNh1wbbfUKu5YedGKVo7tfPyewATjcxyq4csIYB7wu044H3lAaoOxvwF/MD7/AXi4E/5Gh4C+HXVegFOBCcDGls4DcA7wMSCAE4EfOvrv1sx509d2QB59bcv2v7a7k+Xgb+UopXQBZivHDkFKeVBKudr4XIkq2XxEHb/akVmoJjQY7xd28PFnALuklKEmLrYLUsqvUFVSg2nqPMwCXpOK74HEBqW4Owt9bbeMvrYVYbu2u5NyCFs7xqNFCNEPGA/8YAzdaphyL3eEuWsggc+EED8J1WEMIENKedD4fAjI6CBZTK6gfu/kzjgv0PR56DLXUAO6jFz62m6Sbndtdyfl0CUQQsQC7wJ3SCkrgH8BA4FxwEHg7x0kyslSygnA2cAtQvU29iOVrdlhqWpCNcm5APiPMdRZ56UeHX0ejmX0tR2a7nptdyfl0GIrx/ZGCGFH/fO8IaV8D0BKeVhK6ZVS+oAXUC6CdkdKud94LwDeN4572DQljfeCjpDF4GxgtZTysCFXp5wXg6bOQ6dfQ03Q6XLpa7tZuuW13Z2UQ4utHNsTIYQAXgK2SCn/ETQe7Ne7CNjYcNt2kCVGCBFnfka1ptyIOh9zjNXmAP9tb1mCmE2Q2d0Z5yWIps7DYuCXRmbHiUB5kInemehrO3BMfW03T/iu7Y6M6HdA9P4cVCbFLuB/O/jYJ6NMuPXAWuN1DvA6sMEYXwz07ABZBqAyWtYBm8xzAaQAS4EdwBdAcgedmxigGEgIGuuQ84L6pz0IuFF+1uuaOg+oTI6njetnAzCpI6+hFn6HvralvrYbHLtdr209Q1qj0Wg0jehObiWNRqPRhAmtHDQajUbTCK0cNBqNRtMIrRw0Go1G0witHDQajUbTCK0cjkGEEN4G1SDDVqVTCNEvuMqjRtOR6Gu762DrbAE0R0StlHJcZwuh0bQD+truImjLoRth1Ln/m1Hr/kchxCBjvJ8Q4kujENhSIUQfYzxDCPG+EGKd8Zpq7MoqhHhBqNr9nwkhojrtR2k06Gu7M9DK4dgkqoHpfXnQsnIp5WjgKeCfxtiTwKtSyjHAG8ATxvgTwHIp5VhUXfhNxvhg4Gkp5UigDPh5u/4ajSaAvra7CHqG9DGIEKJKShkbYjwPOF1KmWsUSjskpUwRQhShpvC7jfGDUspUIUQhkCWlrAvaRz/gcynlYOP77wG7lPLBDvhpmuMcfW13HbTl0P2QTXxuC3VBn73o2JSma6Cv7Q5EK4fux+VB798Zn1egKnkCXAV8bXxeCtwMIISwCiESOkpIjeYI0Nd2B6K15rFJlBBibdD3T6SUZspfkhBiPeoJabYxdhswXwhxF1AIXGOM/xp4XghxHeop6mZUlUeNprPQ13YXQcccuhGGX3aSlLKos2XRaMKJvrY7Hu1W0mg0Gk0jtOWg0Wg0mkZoy0Gj0Wg0jdDKQaPRaDSN0MpBo9FoNI3QykGj0Wg0jdDKQaPRaDSN0MpBo9FoNI34/2ZPrdYuOT5HAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.7582\n",
      "Validation AUC: 0.7579\n",
      "\n",
      "Start of epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 15:13:18.232967: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 1 of 1024\n",
      "2023-01-03 15:13:22.412206: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 948 of 1024\n",
      "2023-01-03 15:13:22.717810: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:228] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 0: 671.9108, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 569.5750, Accuracy: 0.5071\n",
      "Training loss (for one batch) at step 20: 512.8024, Accuracy: 0.5242\n",
      "Training loss (for one batch) at step 30: 508.6638, Accuracy: 0.5144\n",
      "Training loss (for one batch) at step 40: 496.6340, Accuracy: 0.5088\n",
      "Training loss (for one batch) at step 50: 485.4901, Accuracy: 0.5060\n",
      "Training loss (for one batch) at step 60: 482.8073, Accuracy: 0.5074\n",
      "Training loss (for one batch) at step 70: 489.3416, Accuracy: 0.5036\n",
      "Training loss (for one batch) at step 80: 490.7483, Accuracy: 0.5073\n",
      "Training loss (for one batch) at step 90: 479.4775, Accuracy: 0.5076\n",
      "Training loss (for one batch) at step 100: 466.0163, Accuracy: 0.5077\n",
      "Training loss (for one batch) at step 110: 455.5159, Accuracy: 0.5061\n",
      "---- Training ----\n",
      "Training loss: 150.8723\n",
      "Training acc over epoch: 0.5038\n",
      "---- Validation ----\n",
      "Validation loss: 34.4487\n",
      "Validation acc: 0.4906\n",
      "Time taken: 34.05s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 457.5515, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 459.9211, Accuracy: 0.5199\n",
      "Training loss (for one batch) at step 20: 453.1871, Accuracy: 0.5346\n",
      "Training loss (for one batch) at step 30: 456.4742, Accuracy: 0.5300\n",
      "Training loss (for one batch) at step 40: 457.3239, Accuracy: 0.5215\n",
      "Training loss (for one batch) at step 50: 458.1860, Accuracy: 0.5165\n",
      "Training loss (for one batch) at step 60: 455.1461, Accuracy: 0.5191\n",
      "Training loss (for one batch) at step 70: 453.0145, Accuracy: 0.5204\n",
      "Training loss (for one batch) at step 80: 452.2522, Accuracy: 0.5188\n",
      "Training loss (for one batch) at step 90: 452.4406, Accuracy: 0.5211\n",
      "Training loss (for one batch) at step 100: 445.8231, Accuracy: 0.5224\n",
      "Training loss (for one batch) at step 110: 447.4824, Accuracy: 0.5194\n",
      "---- Training ----\n",
      "Training loss: 140.0906\n",
      "Training acc over epoch: 0.5185\n",
      "---- Validation ----\n",
      "Validation loss: 34.6154\n",
      "Validation acc: 0.4887\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 446.5491, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 452.1780, Accuracy: 0.5227\n",
      "Training loss (for one batch) at step 20: 450.6766, Accuracy: 0.5324\n",
      "Training loss (for one batch) at step 30: 449.6185, Accuracy: 0.5209\n",
      "Training loss (for one batch) at step 40: 444.2926, Accuracy: 0.5164\n",
      "Training loss (for one batch) at step 50: 443.1187, Accuracy: 0.5190\n",
      "Training loss (for one batch) at step 60: 445.3666, Accuracy: 0.5146\n",
      "Training loss (for one batch) at step 70: 442.0145, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 80: 447.8519, Accuracy: 0.5170\n",
      "Training loss (for one batch) at step 90: 448.1612, Accuracy: 0.5197\n",
      "Training loss (for one batch) at step 100: 449.3654, Accuracy: 0.5191\n",
      "Training loss (for one batch) at step 110: 448.8686, Accuracy: 0.5189\n",
      "---- Training ----\n",
      "Training loss: 136.4874\n",
      "Training acc over epoch: 0.5182\n",
      "---- Validation ----\n",
      "Validation loss: 34.4619\n",
      "Validation acc: 0.5043\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 444.5902, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 446.1238, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 20: 443.9932, Accuracy: 0.5458\n",
      "Training loss (for one batch) at step 30: 442.6729, Accuracy: 0.5441\n",
      "Training loss (for one batch) at step 40: 443.5741, Accuracy: 0.5322\n",
      "Training loss (for one batch) at step 50: 444.1104, Accuracy: 0.5216\n",
      "Training loss (for one batch) at step 60: 443.6129, Accuracy: 0.5155\n",
      "Training loss (for one batch) at step 70: 444.5652, Accuracy: 0.5194\n",
      "Training loss (for one batch) at step 80: 441.7292, Accuracy: 0.5249\n",
      "Training loss (for one batch) at step 90: 442.7548, Accuracy: 0.5282\n",
      "Training loss (for one batch) at step 100: 445.0679, Accuracy: 0.5269\n",
      "Training loss (for one batch) at step 110: 445.6453, Accuracy: 0.5243\n",
      "---- Training ----\n",
      "Training loss: 138.0165\n",
      "Training acc over epoch: 0.5251\n",
      "---- Validation ----\n",
      "Validation loss: 34.4051\n",
      "Validation acc: 0.4868\n",
      "Time taken: 10.64s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 439.6837, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 443.8772, Accuracy: 0.5384\n",
      "Training loss (for one batch) at step 20: 443.7376, Accuracy: 0.5443\n",
      "Training loss (for one batch) at step 30: 440.4446, Accuracy: 0.5310\n",
      "Training loss (for one batch) at step 40: 443.0997, Accuracy: 0.5246\n",
      "Training loss (for one batch) at step 50: 442.9164, Accuracy: 0.5231\n",
      "Training loss (for one batch) at step 60: 446.8691, Accuracy: 0.5200\n",
      "Training loss (for one batch) at step 70: 441.5160, Accuracy: 0.5198\n",
      "Training loss (for one batch) at step 80: 441.2236, Accuracy: 0.5269\n",
      "Training loss (for one batch) at step 90: 443.7627, Accuracy: 0.5308\n",
      "Training loss (for one batch) at step 100: 444.6328, Accuracy: 0.5294\n",
      "Training loss (for one batch) at step 110: 441.6429, Accuracy: 0.5251\n",
      "---- Training ----\n",
      "Training loss: 137.9593\n",
      "Training acc over epoch: 0.5249\n",
      "---- Validation ----\n",
      "Validation loss: 35.1092\n",
      "Validation acc: 0.5078\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 444.1139, Accuracy: 0.4453\n",
      "Training loss (for one batch) at step 10: 443.4728, Accuracy: 0.5284\n",
      "Training loss (for one batch) at step 20: 441.4745, Accuracy: 0.5372\n",
      "Training loss (for one batch) at step 30: 441.8098, Accuracy: 0.5360\n",
      "Training loss (for one batch) at step 40: 441.7610, Accuracy: 0.5314\n",
      "Training loss (for one batch) at step 50: 441.4537, Accuracy: 0.5286\n",
      "Training loss (for one batch) at step 60: 442.8718, Accuracy: 0.5306\n",
      "Training loss (for one batch) at step 70: 442.2001, Accuracy: 0.5317\n",
      "Training loss (for one batch) at step 80: 438.1011, Accuracy: 0.5347\n",
      "Training loss (for one batch) at step 90: 445.9995, Accuracy: 0.5431\n",
      "Training loss (for one batch) at step 100: 443.9864, Accuracy: 0.5431\n",
      "Training loss (for one batch) at step 110: 442.5132, Accuracy: 0.5402\n",
      "---- Training ----\n",
      "Training loss: 137.9744\n",
      "Training acc over epoch: 0.5389\n",
      "---- Validation ----\n",
      "Validation loss: 34.1111\n",
      "Validation acc: 0.4979\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 442.8318, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 441.8427, Accuracy: 0.5263\n",
      "Training loss (for one batch) at step 20: 443.3705, Accuracy: 0.5417\n",
      "Training loss (for one batch) at step 30: 441.5789, Accuracy: 0.5393\n",
      "Training loss (for one batch) at step 40: 437.5413, Accuracy: 0.5316\n",
      "Training loss (for one batch) at step 50: 442.3629, Accuracy: 0.5311\n",
      "Training loss (for one batch) at step 60: 444.1323, Accuracy: 0.5301\n",
      "Training loss (for one batch) at step 70: 438.3521, Accuracy: 0.5343\n",
      "Training loss (for one batch) at step 80: 443.1801, Accuracy: 0.5415\n",
      "Training loss (for one batch) at step 90: 441.8263, Accuracy: 0.5472\n",
      "Training loss (for one batch) at step 100: 444.6479, Accuracy: 0.5483\n",
      "Training loss (for one batch) at step 110: 445.0368, Accuracy: 0.5432\n",
      "---- Training ----\n",
      "Training loss: 138.0983\n",
      "Training acc over epoch: 0.5436\n",
      "---- Validation ----\n",
      "Validation loss: 34.1821\n",
      "Validation acc: 0.5408\n",
      "Time taken: 10.76s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 442.3702, Accuracy: 0.4844\n",
      "Training loss (for one batch) at step 10: 441.2632, Accuracy: 0.5455\n",
      "Training loss (for one batch) at step 20: 441.4723, Accuracy: 0.5521\n",
      "Training loss (for one batch) at step 30: 442.3410, Accuracy: 0.5509\n",
      "Training loss (for one batch) at step 40: 442.1910, Accuracy: 0.5463\n",
      "Training loss (for one batch) at step 50: 439.0840, Accuracy: 0.5420\n",
      "Training loss (for one batch) at step 60: 439.2010, Accuracy: 0.5405\n",
      "Training loss (for one batch) at step 70: 443.9730, Accuracy: 0.5429\n",
      "Training loss (for one batch) at step 80: 438.0214, Accuracy: 0.5470\n",
      "Training loss (for one batch) at step 90: 444.7954, Accuracy: 0.5499\n",
      "Training loss (for one batch) at step 100: 436.3094, Accuracy: 0.5511\n",
      "Training loss (for one batch) at step 110: 440.6277, Accuracy: 0.5472\n",
      "---- Training ----\n",
      "Training loss: 138.8068\n",
      "Training acc over epoch: 0.5476\n",
      "---- Validation ----\n",
      "Validation loss: 35.1944\n",
      "Validation acc: 0.5430\n",
      "Time taken: 10.05s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 437.1607, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 437.6388, Accuracy: 0.5611\n",
      "Training loss (for one batch) at step 20: 439.3204, Accuracy: 0.5640\n",
      "Training loss (for one batch) at step 30: 438.5449, Accuracy: 0.5577\n",
      "Training loss (for one batch) at step 40: 438.9038, Accuracy: 0.5459\n",
      "Training loss (for one batch) at step 50: 439.1485, Accuracy: 0.5415\n",
      "Training loss (for one batch) at step 60: 443.4965, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 70: 434.8426, Accuracy: 0.5415\n",
      "Training loss (for one batch) at step 80: 439.1765, Accuracy: 0.5427\n",
      "Training loss (for one batch) at step 90: 439.7449, Accuracy: 0.5442\n",
      "Training loss (for one batch) at step 100: 443.0143, Accuracy: 0.5444\n",
      "Training loss (for one batch) at step 110: 443.6995, Accuracy: 0.5443\n",
      "---- Training ----\n",
      "Training loss: 137.8748\n",
      "Training acc over epoch: 0.5439\n",
      "---- Validation ----\n",
      "Validation loss: 34.1974\n",
      "Validation acc: 0.5145\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 437.2351, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 440.6713, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 20: 445.6145, Accuracy: 0.5688\n",
      "Training loss (for one batch) at step 30: 437.2633, Accuracy: 0.5592\n",
      "Training loss (for one batch) at step 40: 436.1892, Accuracy: 0.5554\n",
      "Training loss (for one batch) at step 50: 433.0657, Accuracy: 0.5479\n",
      "Training loss (for one batch) at step 60: 441.6338, Accuracy: 0.5475\n",
      "Training loss (for one batch) at step 70: 430.8254, Accuracy: 0.5456\n",
      "Training loss (for one batch) at step 80: 439.3212, Accuracy: 0.5468\n",
      "Training loss (for one batch) at step 90: 444.4702, Accuracy: 0.5507\n",
      "Training loss (for one batch) at step 100: 441.0251, Accuracy: 0.5511\n",
      "Training loss (for one batch) at step 110: 441.9539, Accuracy: 0.5482\n",
      "---- Training ----\n",
      "Training loss: 137.0339\n",
      "Training acc over epoch: 0.5488\n",
      "---- Validation ----\n",
      "Validation loss: 34.1596\n",
      "Validation acc: 0.5911\n",
      "Time taken: 10.67s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 438.4791, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 441.1671, Accuracy: 0.5810\n",
      "Training loss (for one batch) at step 20: 438.5639, Accuracy: 0.5692\n",
      "Training loss (for one batch) at step 30: 435.0696, Accuracy: 0.5585\n",
      "Training loss (for one batch) at step 40: 432.6107, Accuracy: 0.5526\n",
      "Training loss (for one batch) at step 50: 438.5538, Accuracy: 0.5470\n",
      "Training loss (for one batch) at step 60: 437.8793, Accuracy: 0.5484\n",
      "Training loss (for one batch) at step 70: 435.2589, Accuracy: 0.5492\n",
      "Training loss (for one batch) at step 80: 435.4086, Accuracy: 0.5527\n",
      "Training loss (for one batch) at step 90: 436.0189, Accuracy: 0.5544\n",
      "Training loss (for one batch) at step 100: 435.7304, Accuracy: 0.5538\n",
      "Training loss (for one batch) at step 110: 437.1544, Accuracy: 0.5529\n",
      "---- Training ----\n",
      "Training loss: 136.9247\n",
      "Training acc over epoch: 0.5523\n",
      "---- Validation ----\n",
      "Validation loss: 35.0383\n",
      "Validation acc: 0.5357\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 431.4180, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 433.6245, Accuracy: 0.5959\n",
      "Training loss (for one batch) at step 20: 435.6569, Accuracy: 0.5986\n",
      "Training loss (for one batch) at step 30: 439.9820, Accuracy: 0.5907\n",
      "Training loss (for one batch) at step 40: 436.3172, Accuracy: 0.5814\n",
      "Training loss (for one batch) at step 50: 432.3800, Accuracy: 0.5715\n",
      "Training loss (for one batch) at step 60: 433.1385, Accuracy: 0.5660\n",
      "Training loss (for one batch) at step 70: 431.2669, Accuracy: 0.5666\n",
      "Training loss (for one batch) at step 80: 432.4358, Accuracy: 0.5689\n",
      "Training loss (for one batch) at step 90: 437.6185, Accuracy: 0.5719\n",
      "Training loss (for one batch) at step 100: 436.0493, Accuracy: 0.5742\n",
      "Training loss (for one batch) at step 110: 433.0631, Accuracy: 0.5728\n",
      "---- Training ----\n",
      "Training loss: 134.9345\n",
      "Training acc over epoch: 0.5711\n",
      "---- Validation ----\n",
      "Validation loss: 34.3417\n",
      "Validation acc: 0.5253\n",
      "Time taken: 10.16s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 445.8593, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 430.1661, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 20: 437.1237, Accuracy: 0.6057\n",
      "Training loss (for one batch) at step 30: 439.7505, Accuracy: 0.6061\n",
      "Training loss (for one batch) at step 40: 436.1571, Accuracy: 0.5964\n",
      "Training loss (for one batch) at step 50: 437.3221, Accuracy: 0.5904\n",
      "Training loss (for one batch) at step 60: 438.4272, Accuracy: 0.5843\n",
      "Training loss (for one batch) at step 70: 441.0490, Accuracy: 0.5821\n",
      "Training loss (for one batch) at step 80: 430.7141, Accuracy: 0.5779\n",
      "Training loss (for one batch) at step 90: 435.1343, Accuracy: 0.5786\n",
      "Training loss (for one batch) at step 100: 432.8916, Accuracy: 0.5784\n",
      "Training loss (for one batch) at step 110: 431.5773, Accuracy: 0.5751\n",
      "---- Training ----\n",
      "Training loss: 132.9715\n",
      "Training acc over epoch: 0.5752\n",
      "---- Validation ----\n",
      "Validation loss: 34.3079\n",
      "Validation acc: 0.6107\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 431.7802, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 424.8893, Accuracy: 0.6520\n",
      "Training loss (for one batch) at step 20: 434.9691, Accuracy: 0.6499\n",
      "Training loss (for one batch) at step 30: 428.6154, Accuracy: 0.6331\n",
      "Training loss (for one batch) at step 40: 434.0783, Accuracy: 0.6233\n",
      "Training loss (for one batch) at step 50: 437.7003, Accuracy: 0.6207\n",
      "Training loss (for one batch) at step 60: 437.8465, Accuracy: 0.6212\n",
      "Training loss (for one batch) at step 70: 441.1580, Accuracy: 0.6206\n",
      "Training loss (for one batch) at step 80: 434.7624, Accuracy: 0.6195\n",
      "Training loss (for one batch) at step 90: 432.2947, Accuracy: 0.6210\n",
      "Training loss (for one batch) at step 100: 437.1812, Accuracy: 0.6194\n",
      "Training loss (for one batch) at step 110: 427.5468, Accuracy: 0.6164\n",
      "---- Training ----\n",
      "Training loss: 133.3761\n",
      "Training acc over epoch: 0.6157\n",
      "---- Validation ----\n",
      "Validation loss: 31.7247\n",
      "Validation acc: 0.5865\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 432.1275, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 427.1288, Accuracy: 0.6286\n",
      "Training loss (for one batch) at step 20: 426.1621, Accuracy: 0.6254\n",
      "Training loss (for one batch) at step 30: 430.8586, Accuracy: 0.6293\n",
      "Training loss (for one batch) at step 40: 425.3951, Accuracy: 0.6200\n",
      "Training loss (for one batch) at step 50: 429.5893, Accuracy: 0.6213\n",
      "Training loss (for one batch) at step 60: 416.9832, Accuracy: 0.6238\n",
      "Training loss (for one batch) at step 70: 429.5828, Accuracy: 0.6210\n",
      "Training loss (for one batch) at step 80: 434.8821, Accuracy: 0.6204\n",
      "Training loss (for one batch) at step 90: 433.8476, Accuracy: 0.6198\n",
      "Training loss (for one batch) at step 100: 439.1513, Accuracy: 0.6173\n",
      "Training loss (for one batch) at step 110: 434.3051, Accuracy: 0.6153\n",
      "---- Training ----\n",
      "Training loss: 134.8920\n",
      "Training acc over epoch: 0.6160\n",
      "---- Validation ----\n",
      "Validation loss: 33.9054\n",
      "Validation acc: 0.6069\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 428.8694, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 437.0563, Accuracy: 0.6548\n",
      "Training loss (for one batch) at step 20: 431.2197, Accuracy: 0.6559\n",
      "Training loss (for one batch) at step 30: 432.9335, Accuracy: 0.6472\n",
      "Training loss (for one batch) at step 40: 424.9988, Accuracy: 0.6393\n",
      "Training loss (for one batch) at step 50: 425.3021, Accuracy: 0.6396\n",
      "Training loss (for one batch) at step 60: 436.2070, Accuracy: 0.6386\n",
      "Training loss (for one batch) at step 70: 431.1250, Accuracy: 0.6407\n",
      "Training loss (for one batch) at step 80: 428.7186, Accuracy: 0.6370\n",
      "Training loss (for one batch) at step 90: 436.1005, Accuracy: 0.6368\n",
      "Training loss (for one batch) at step 100: 428.7453, Accuracy: 0.6356\n",
      "Training loss (for one batch) at step 110: 425.9357, Accuracy: 0.6316\n",
      "---- Training ----\n",
      "Training loss: 135.5435\n",
      "Training acc over epoch: 0.6302\n",
      "---- Validation ----\n",
      "Validation loss: 36.6963\n",
      "Validation acc: 0.5776\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 433.9054, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 423.1628, Accuracy: 0.6499\n",
      "Training loss (for one batch) at step 20: 433.6189, Accuracy: 0.6481\n",
      "Training loss (for one batch) at step 30: 428.5073, Accuracy: 0.6497\n",
      "Training loss (for one batch) at step 40: 423.3981, Accuracy: 0.6469\n",
      "Training loss (for one batch) at step 50: 422.1528, Accuracy: 0.6471\n",
      "Training loss (for one batch) at step 60: 431.6954, Accuracy: 0.6479\n",
      "Training loss (for one batch) at step 70: 428.1905, Accuracy: 0.6469\n",
      "Training loss (for one batch) at step 80: 442.2064, Accuracy: 0.6443\n",
      "Training loss (for one batch) at step 90: 436.0276, Accuracy: 0.6417\n",
      "Training loss (for one batch) at step 100: 425.7753, Accuracy: 0.6416\n",
      "Training loss (for one batch) at step 110: 430.5209, Accuracy: 0.6436\n",
      "---- Training ----\n",
      "Training loss: 132.9148\n",
      "Training acc over epoch: 0.6423\n",
      "---- Validation ----\n",
      "Validation loss: 33.8299\n",
      "Validation acc: 0.6316\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 441.3362, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 426.4974, Accuracy: 0.6570\n",
      "Training loss (for one batch) at step 20: 430.5577, Accuracy: 0.6544\n",
      "Training loss (for one batch) at step 30: 430.7066, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 40: 420.7390, Accuracy: 0.6618\n",
      "Training loss (for one batch) at step 50: 410.8922, Accuracy: 0.6631\n",
      "Training loss (for one batch) at step 60: 426.5452, Accuracy: 0.6629\n",
      "Training loss (for one batch) at step 70: 424.3491, Accuracy: 0.6618\n",
      "Training loss (for one batch) at step 80: 425.4177, Accuracy: 0.6568\n",
      "Training loss (for one batch) at step 90: 430.4366, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 100: 422.1755, Accuracy: 0.6548\n",
      "Training loss (for one batch) at step 110: 421.5420, Accuracy: 0.6545\n",
      "---- Training ----\n",
      "Training loss: 130.8037\n",
      "Training acc over epoch: 0.6535\n",
      "---- Validation ----\n",
      "Validation loss: 34.8894\n",
      "Validation acc: 0.5924\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 428.3917, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 426.1829, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 20: 418.7844, Accuracy: 0.6570\n",
      "Training loss (for one batch) at step 30: 424.6490, Accuracy: 0.6633\n",
      "Training loss (for one batch) at step 40: 429.6254, Accuracy: 0.6656\n",
      "Training loss (for one batch) at step 50: 413.6117, Accuracy: 0.6645\n",
      "Training loss (for one batch) at step 60: 431.3914, Accuracy: 0.6648\n",
      "Training loss (for one batch) at step 70: 427.7715, Accuracy: 0.6652\n",
      "Training loss (for one batch) at step 80: 428.7972, Accuracy: 0.6617\n",
      "Training loss (for one batch) at step 90: 425.9879, Accuracy: 0.6593\n",
      "Training loss (for one batch) at step 100: 424.3308, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 110: 434.5996, Accuracy: 0.6552\n",
      "---- Training ----\n",
      "Training loss: 136.7275\n",
      "Training acc over epoch: 0.6546\n",
      "---- Validation ----\n",
      "Validation loss: 34.1111\n",
      "Validation acc: 0.6548\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 415.2956, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 426.4906, Accuracy: 0.6861\n",
      "Training loss (for one batch) at step 20: 422.6877, Accuracy: 0.6856\n",
      "Training loss (for one batch) at step 30: 426.8690, Accuracy: 0.6878\n",
      "Training loss (for one batch) at step 40: 413.6985, Accuracy: 0.6860\n",
      "Training loss (for one batch) at step 50: 421.3755, Accuracy: 0.6869\n",
      "Training loss (for one batch) at step 60: 422.3945, Accuracy: 0.6865\n",
      "Training loss (for one batch) at step 70: 431.7835, Accuracy: 0.6864\n",
      "Training loss (for one batch) at step 80: 422.2182, Accuracy: 0.6814\n",
      "Training loss (for one batch) at step 90: 423.6456, Accuracy: 0.6764\n",
      "Training loss (for one batch) at step 100: 427.4301, Accuracy: 0.6700\n",
      "Training loss (for one batch) at step 110: 420.8317, Accuracy: 0.6652\n",
      "---- Training ----\n",
      "Training loss: 132.0534\n",
      "Training acc over epoch: 0.6636\n",
      "---- Validation ----\n",
      "Validation loss: 34.3101\n",
      "Validation acc: 0.5948\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 427.9070, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 416.2411, Accuracy: 0.6626\n",
      "Training loss (for one batch) at step 20: 413.3531, Accuracy: 0.6704\n",
      "Training loss (for one batch) at step 30: 418.5104, Accuracy: 0.6782\n",
      "Training loss (for one batch) at step 40: 411.0005, Accuracy: 0.6732\n",
      "Training loss (for one batch) at step 50: 414.7634, Accuracy: 0.6783\n",
      "Training loss (for one batch) at step 60: 431.3551, Accuracy: 0.6817\n",
      "Training loss (for one batch) at step 70: 428.7709, Accuracy: 0.6794\n",
      "Training loss (for one batch) at step 80: 425.7509, Accuracy: 0.6779\n",
      "Training loss (for one batch) at step 90: 421.1956, Accuracy: 0.6732\n",
      "Training loss (for one batch) at step 100: 416.3548, Accuracy: 0.6679\n",
      "Training loss (for one batch) at step 110: 422.0261, Accuracy: 0.6658\n",
      "---- Training ----\n",
      "Training loss: 129.5405\n",
      "Training acc over epoch: 0.6650\n",
      "---- Validation ----\n",
      "Validation loss: 35.4321\n",
      "Validation acc: 0.6470\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 430.6734, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 413.8307, Accuracy: 0.6612\n",
      "Training loss (for one batch) at step 20: 420.6314, Accuracy: 0.6700\n",
      "Training loss (for one batch) at step 30: 422.4425, Accuracy: 0.6762\n",
      "Training loss (for one batch) at step 40: 406.2585, Accuracy: 0.6839\n",
      "Training loss (for one batch) at step 50: 412.7274, Accuracy: 0.6904\n",
      "Training loss (for one batch) at step 60: 419.2090, Accuracy: 0.6915\n",
      "Training loss (for one batch) at step 70: 414.0476, Accuracy: 0.6912\n",
      "Training loss (for one batch) at step 80: 436.3008, Accuracy: 0.6874\n",
      "Training loss (for one batch) at step 90: 429.3535, Accuracy: 0.6826\n",
      "Training loss (for one batch) at step 100: 415.9328, Accuracy: 0.6805\n",
      "Training loss (for one batch) at step 110: 426.2835, Accuracy: 0.6788\n",
      "---- Training ----\n",
      "Training loss: 126.5015\n",
      "Training acc over epoch: 0.6777\n",
      "---- Validation ----\n",
      "Validation loss: 32.2154\n",
      "Validation acc: 0.6330\n",
      "Time taken: 10.75s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 416.8840, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 414.8559, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 20: 412.0283, Accuracy: 0.6815\n",
      "Training loss (for one batch) at step 30: 411.3337, Accuracy: 0.6847\n",
      "Training loss (for one batch) at step 40: 416.6560, Accuracy: 0.6854\n",
      "Training loss (for one batch) at step 50: 403.9959, Accuracy: 0.6903\n",
      "Training loss (for one batch) at step 60: 395.8092, Accuracy: 0.6924\n",
      "Training loss (for one batch) at step 70: 423.5532, Accuracy: 0.6923\n",
      "Training loss (for one batch) at step 80: 420.5385, Accuracy: 0.6913\n",
      "Training loss (for one batch) at step 90: 422.9696, Accuracy: 0.6900\n",
      "Training loss (for one batch) at step 100: 405.4055, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 110: 402.0326, Accuracy: 0.6854\n",
      "---- Training ----\n",
      "Training loss: 139.5931\n",
      "Training acc over epoch: 0.6842\n",
      "---- Validation ----\n",
      "Validation loss: 34.8870\n",
      "Validation acc: 0.6529\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 409.9604, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 409.0314, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 20: 402.5238, Accuracy: 0.6845\n",
      "Training loss (for one batch) at step 30: 412.5909, Accuracy: 0.6820\n",
      "Training loss (for one batch) at step 40: 412.3473, Accuracy: 0.6860\n",
      "Training loss (for one batch) at step 50: 404.9254, Accuracy: 0.6926\n",
      "Training loss (for one batch) at step 60: 416.3383, Accuracy: 0.6977\n",
      "Training loss (for one batch) at step 70: 410.4592, Accuracy: 0.6988\n",
      "Training loss (for one batch) at step 80: 413.2187, Accuracy: 0.6959\n",
      "Training loss (for one batch) at step 90: 419.4606, Accuracy: 0.6902\n",
      "Training loss (for one batch) at step 100: 415.0757, Accuracy: 0.6860\n",
      "Training loss (for one batch) at step 110: 420.8903, Accuracy: 0.6834\n",
      "---- Training ----\n",
      "Training loss: 130.1057\n",
      "Training acc over epoch: 0.6838\n",
      "---- Validation ----\n",
      "Validation loss: 33.3251\n",
      "Validation acc: 0.6445\n",
      "Time taken: 10.01s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 421.4832, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 419.5043, Accuracy: 0.6861\n",
      "Training loss (for one batch) at step 20: 398.7719, Accuracy: 0.6871\n",
      "Training loss (for one batch) at step 30: 411.6928, Accuracy: 0.6867\n",
      "Training loss (for one batch) at step 40: 398.2755, Accuracy: 0.6930\n",
      "Training loss (for one batch) at step 50: 399.5597, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 60: 426.0502, Accuracy: 0.7013\n",
      "Training loss (for one batch) at step 70: 405.6866, Accuracy: 0.7022\n",
      "Training loss (for one batch) at step 80: 407.2255, Accuracy: 0.7012\n",
      "Training loss (for one batch) at step 90: 409.1564, Accuracy: 0.6984\n",
      "Training loss (for one batch) at step 100: 409.0935, Accuracy: 0.6934\n",
      "Training loss (for one batch) at step 110: 394.8604, Accuracy: 0.6912\n",
      "---- Training ----\n",
      "Training loss: 131.1184\n",
      "Training acc over epoch: 0.6912\n",
      "---- Validation ----\n",
      "Validation loss: 31.4375\n",
      "Validation acc: 0.6515\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 406.5712, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 396.0405, Accuracy: 0.7095\n",
      "Training loss (for one batch) at step 20: 410.8713, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 30: 400.2289, Accuracy: 0.7077\n",
      "Training loss (for one batch) at step 40: 392.0514, Accuracy: 0.7100\n",
      "Training loss (for one batch) at step 50: 387.4343, Accuracy: 0.7103\n",
      "Training loss (for one batch) at step 60: 397.0384, Accuracy: 0.7120\n",
      "Training loss (for one batch) at step 70: 409.1796, Accuracy: 0.7090\n",
      "Training loss (for one batch) at step 80: 400.2532, Accuracy: 0.7052\n",
      "Training loss (for one batch) at step 90: 405.4301, Accuracy: 0.7018\n",
      "Training loss (for one batch) at step 100: 407.9394, Accuracy: 0.6956\n",
      "Training loss (for one batch) at step 110: 404.4989, Accuracy: 0.6943\n",
      "---- Training ----\n",
      "Training loss: 130.6086\n",
      "Training acc over epoch: 0.6939\n",
      "---- Validation ----\n",
      "Validation loss: 33.3832\n",
      "Validation acc: 0.6497\n",
      "Time taken: 10.15s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 417.9540, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 417.4221, Accuracy: 0.6996\n",
      "Training loss (for one batch) at step 20: 390.7235, Accuracy: 0.7065\n",
      "Training loss (for one batch) at step 30: 388.3158, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 40: 391.6335, Accuracy: 0.7058\n",
      "Training loss (for one batch) at step 50: 409.6762, Accuracy: 0.7037\n",
      "Training loss (for one batch) at step 60: 403.9149, Accuracy: 0.7041\n",
      "Training loss (for one batch) at step 70: 403.7034, Accuracy: 0.7054\n",
      "Training loss (for one batch) at step 80: 412.7690, Accuracy: 0.7041\n",
      "Training loss (for one batch) at step 90: 399.0771, Accuracy: 0.7010\n",
      "Training loss (for one batch) at step 100: 399.8121, Accuracy: 0.6989\n",
      "Training loss (for one batch) at step 110: 392.9313, Accuracy: 0.6973\n",
      "---- Training ----\n",
      "Training loss: 124.4761\n",
      "Training acc over epoch: 0.6973\n",
      "---- Validation ----\n",
      "Validation loss: 35.6791\n",
      "Validation acc: 0.6642\n",
      "Time taken: 10.15s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 422.9184, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 409.5216, Accuracy: 0.6939\n",
      "Training loss (for one batch) at step 20: 420.8495, Accuracy: 0.7087\n",
      "Training loss (for one batch) at step 30: 401.8279, Accuracy: 0.7119\n",
      "Training loss (for one batch) at step 40: 385.1916, Accuracy: 0.7130\n",
      "Training loss (for one batch) at step 50: 387.4068, Accuracy: 0.7180\n",
      "Training loss (for one batch) at step 60: 379.7731, Accuracy: 0.7186\n",
      "Training loss (for one batch) at step 70: 402.0657, Accuracy: 0.7175\n",
      "Training loss (for one batch) at step 80: 397.0358, Accuracy: 0.7136\n",
      "Training loss (for one batch) at step 90: 407.9311, Accuracy: 0.7121\n",
      "Training loss (for one batch) at step 100: 404.6624, Accuracy: 0.7054\n",
      "Training loss (for one batch) at step 110: 392.5371, Accuracy: 0.7027\n",
      "---- Training ----\n",
      "Training loss: 126.3832\n",
      "Training acc over epoch: 0.7022\n",
      "---- Validation ----\n",
      "Validation loss: 32.3108\n",
      "Validation acc: 0.6515\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 413.2417, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 400.8061, Accuracy: 0.7159\n",
      "Training loss (for one batch) at step 20: 408.7876, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 30: 400.3559, Accuracy: 0.7145\n",
      "Training loss (for one batch) at step 40: 385.1151, Accuracy: 0.7138\n",
      "Training loss (for one batch) at step 50: 373.3153, Accuracy: 0.7157\n",
      "Training loss (for one batch) at step 60: 384.9409, Accuracy: 0.7166\n",
      "Training loss (for one batch) at step 70: 393.8206, Accuracy: 0.7159\n",
      "Training loss (for one batch) at step 80: 405.0215, Accuracy: 0.7159\n",
      "Training loss (for one batch) at step 90: 391.2058, Accuracy: 0.7127\n",
      "Training loss (for one batch) at step 100: 399.1028, Accuracy: 0.7082\n",
      "Training loss (for one batch) at step 110: 391.5111, Accuracy: 0.7054\n",
      "---- Training ----\n",
      "Training loss: 120.2481\n",
      "Training acc over epoch: 0.7051\n",
      "---- Validation ----\n",
      "Validation loss: 41.3612\n",
      "Validation acc: 0.6499\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 393.5460, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 395.4418, Accuracy: 0.7138\n",
      "Training loss (for one batch) at step 20: 392.7150, Accuracy: 0.7169\n",
      "Training loss (for one batch) at step 30: 378.4845, Accuracy: 0.7190\n",
      "Training loss (for one batch) at step 40: 376.8434, Accuracy: 0.7146\n",
      "Training loss (for one batch) at step 50: 374.4552, Accuracy: 0.7160\n",
      "Training loss (for one batch) at step 60: 383.2923, Accuracy: 0.7144\n",
      "Training loss (for one batch) at step 70: 396.9922, Accuracy: 0.7148\n",
      "Training loss (for one batch) at step 80: 400.8112, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 90: 392.3398, Accuracy: 0.7111\n",
      "Training loss (for one batch) at step 100: 411.9004, Accuracy: 0.7082\n",
      "Training loss (for one batch) at step 110: 384.4438, Accuracy: 0.7065\n",
      "---- Training ----\n",
      "Training loss: 118.7562\n",
      "Training acc over epoch: 0.7067\n",
      "---- Validation ----\n",
      "Validation loss: 32.8600\n",
      "Validation acc: 0.6709\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 403.3182, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 403.5704, Accuracy: 0.7294\n",
      "Training loss (for one batch) at step 20: 391.2784, Accuracy: 0.7232\n",
      "Training loss (for one batch) at step 30: 359.7192, Accuracy: 0.7243\n",
      "Training loss (for one batch) at step 40: 395.6261, Accuracy: 0.7228\n",
      "Training loss (for one batch) at step 50: 374.8369, Accuracy: 0.7220\n",
      "Training loss (for one batch) at step 60: 380.0145, Accuracy: 0.7231\n",
      "Training loss (for one batch) at step 70: 382.6953, Accuracy: 0.7211\n",
      "Training loss (for one batch) at step 80: 397.5292, Accuracy: 0.7170\n",
      "Training loss (for one batch) at step 90: 380.3488, Accuracy: 0.7144\n",
      "Training loss (for one batch) at step 100: 386.0718, Accuracy: 0.7094\n",
      "Training loss (for one batch) at step 110: 390.5197, Accuracy: 0.7076\n",
      "---- Training ----\n",
      "Training loss: 119.5581\n",
      "Training acc over epoch: 0.7075\n",
      "---- Validation ----\n",
      "Validation loss: 35.7959\n",
      "Validation acc: 0.6566\n",
      "Time taken: 10.66s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 397.5302, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 390.2988, Accuracy: 0.7280\n",
      "Training loss (for one batch) at step 20: 378.0933, Accuracy: 0.7240\n",
      "Training loss (for one batch) at step 30: 384.6193, Accuracy: 0.7180\n",
      "Training loss (for one batch) at step 40: 378.4015, Accuracy: 0.7184\n",
      "Training loss (for one batch) at step 50: 376.6515, Accuracy: 0.7235\n",
      "Training loss (for one batch) at step 60: 376.3076, Accuracy: 0.7272\n",
      "Training loss (for one batch) at step 70: 396.3342, Accuracy: 0.7252\n",
      "Training loss (for one batch) at step 80: 395.6712, Accuracy: 0.7219\n",
      "Training loss (for one batch) at step 90: 383.4221, Accuracy: 0.7186\n",
      "Training loss (for one batch) at step 100: 382.6009, Accuracy: 0.7147\n",
      "Training loss (for one batch) at step 110: 395.7684, Accuracy: 0.7125\n",
      "---- Training ----\n",
      "Training loss: 125.7927\n",
      "Training acc over epoch: 0.7123\n",
      "---- Validation ----\n",
      "Validation loss: 37.1531\n",
      "Validation acc: 0.6421\n",
      "Time taken: 10.16s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 396.3531, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 386.8166, Accuracy: 0.7294\n",
      "Training loss (for one batch) at step 20: 383.1537, Accuracy: 0.7247\n",
      "Training loss (for one batch) at step 30: 370.6464, Accuracy: 0.7220\n",
      "Training loss (for one batch) at step 40: 365.1894, Accuracy: 0.7243\n",
      "Training loss (for one batch) at step 50: 379.4878, Accuracy: 0.7264\n",
      "Training loss (for one batch) at step 60: 379.5040, Accuracy: 0.7237\n",
      "Training loss (for one batch) at step 70: 391.8953, Accuracy: 0.7230\n",
      "Training loss (for one batch) at step 80: 378.9683, Accuracy: 0.7193\n",
      "Training loss (for one batch) at step 90: 387.4318, Accuracy: 0.7169\n",
      "Training loss (for one batch) at step 100: 383.9839, Accuracy: 0.7125\n",
      "Training loss (for one batch) at step 110: 390.2457, Accuracy: 0.7094\n",
      "---- Training ----\n",
      "Training loss: 123.3495\n",
      "Training acc over epoch: 0.7091\n",
      "---- Validation ----\n",
      "Validation loss: 36.2480\n",
      "Validation acc: 0.6523\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 392.2064, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 371.9263, Accuracy: 0.7180\n",
      "Training loss (for one batch) at step 20: 369.4872, Accuracy: 0.7240\n",
      "Training loss (for one batch) at step 30: 381.4799, Accuracy: 0.7220\n",
      "Training loss (for one batch) at step 40: 381.6497, Accuracy: 0.7243\n",
      "Training loss (for one batch) at step 50: 354.9082, Accuracy: 0.7272\n",
      "Training loss (for one batch) at step 60: 394.3798, Accuracy: 0.7281\n",
      "Training loss (for one batch) at step 70: 377.5982, Accuracy: 0.7265\n",
      "Training loss (for one batch) at step 80: 377.1487, Accuracy: 0.7243\n",
      "Training loss (for one batch) at step 90: 379.5701, Accuracy: 0.7230\n",
      "Training loss (for one batch) at step 100: 370.2055, Accuracy: 0.7194\n",
      "Training loss (for one batch) at step 110: 368.1608, Accuracy: 0.7165\n",
      "---- Training ----\n",
      "Training loss: 116.7675\n",
      "Training acc over epoch: 0.7171\n",
      "---- Validation ----\n",
      "Validation loss: 36.0205\n",
      "Validation acc: 0.6612\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 373.9481, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 380.8273, Accuracy: 0.7116\n",
      "Training loss (for one batch) at step 20: 364.5646, Accuracy: 0.7199\n",
      "Training loss (for one batch) at step 30: 370.2393, Accuracy: 0.7182\n",
      "Training loss (for one batch) at step 40: 360.7327, Accuracy: 0.7226\n",
      "Training loss (for one batch) at step 50: 360.5396, Accuracy: 0.7235\n",
      "Training loss (for one batch) at step 60: 370.9696, Accuracy: 0.7249\n",
      "Training loss (for one batch) at step 70: 384.7389, Accuracy: 0.7233\n",
      "Training loss (for one batch) at step 80: 403.7253, Accuracy: 0.7242\n",
      "Training loss (for one batch) at step 90: 383.2627, Accuracy: 0.7203\n",
      "Training loss (for one batch) at step 100: 375.4176, Accuracy: 0.7166\n",
      "Training loss (for one batch) at step 110: 368.6931, Accuracy: 0.7155\n",
      "---- Training ----\n",
      "Training loss: 101.8059\n",
      "Training acc over epoch: 0.7158\n",
      "---- Validation ----\n",
      "Validation loss: 32.3345\n",
      "Validation acc: 0.6591\n",
      "Time taken: 10.16s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 396.3966, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 368.7376, Accuracy: 0.7322\n",
      "Training loss (for one batch) at step 20: 389.7000, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 30: 375.9369, Accuracy: 0.7339\n",
      "Training loss (for one batch) at step 40: 364.3612, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 50: 361.3055, Accuracy: 0.7345\n",
      "Training loss (for one batch) at step 60: 350.0874, Accuracy: 0.7328\n",
      "Training loss (for one batch) at step 70: 363.3588, Accuracy: 0.7317\n",
      "Training loss (for one batch) at step 80: 362.4830, Accuracy: 0.7297\n",
      "Training loss (for one batch) at step 90: 375.5334, Accuracy: 0.7257\n",
      "Training loss (for one batch) at step 100: 359.6674, Accuracy: 0.7238\n",
      "Training loss (for one batch) at step 110: 364.4169, Accuracy: 0.7209\n",
      "---- Training ----\n",
      "Training loss: 118.1042\n",
      "Training acc over epoch: 0.7198\n",
      "---- Validation ----\n",
      "Validation loss: 39.0596\n",
      "Validation acc: 0.6746\n",
      "Time taken: 10.09s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 389.6398, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 390.6442, Accuracy: 0.7358\n",
      "Training loss (for one batch) at step 20: 368.2669, Accuracy: 0.7396\n",
      "Training loss (for one batch) at step 30: 368.0932, Accuracy: 0.7366\n",
      "Training loss (for one batch) at step 40: 347.0168, Accuracy: 0.7365\n",
      "Training loss (for one batch) at step 50: 358.9310, Accuracy: 0.7347\n",
      "Training loss (for one batch) at step 60: 343.2249, Accuracy: 0.7348\n",
      "Training loss (for one batch) at step 70: 386.0553, Accuracy: 0.7302\n",
      "Training loss (for one batch) at step 80: 374.6567, Accuracy: 0.7290\n",
      "Training loss (for one batch) at step 90: 380.2780, Accuracy: 0.7249\n",
      "Training loss (for one batch) at step 100: 358.6947, Accuracy: 0.7205\n",
      "Training loss (for one batch) at step 110: 364.9840, Accuracy: 0.7178\n",
      "---- Training ----\n",
      "Training loss: 129.8198\n",
      "Training acc over epoch: 0.7167\n",
      "---- Validation ----\n",
      "Validation loss: 36.5956\n",
      "Validation acc: 0.6478\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 381.3239, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 374.2004, Accuracy: 0.7230\n",
      "Training loss (for one batch) at step 20: 367.9326, Accuracy: 0.7314\n",
      "Training loss (for one batch) at step 30: 370.4240, Accuracy: 0.7341\n",
      "Training loss (for one batch) at step 40: 359.7255, Accuracy: 0.7348\n",
      "Training loss (for one batch) at step 50: 373.2304, Accuracy: 0.7341\n",
      "Training loss (for one batch) at step 60: 366.4746, Accuracy: 0.7319\n",
      "Training loss (for one batch) at step 70: 346.1322, Accuracy: 0.7291\n",
      "Training loss (for one batch) at step 80: 368.7747, Accuracy: 0.7295\n",
      "Training loss (for one batch) at step 90: 358.8007, Accuracy: 0.7261\n",
      "Training loss (for one batch) at step 100: 362.1115, Accuracy: 0.7220\n",
      "Training loss (for one batch) at step 110: 364.0024, Accuracy: 0.7202\n",
      "---- Training ----\n",
      "Training loss: 112.8732\n",
      "Training acc over epoch: 0.7196\n",
      "---- Validation ----\n",
      "Validation loss: 37.5693\n",
      "Validation acc: 0.6502\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 387.5629, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 360.9815, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 20: 349.4207, Accuracy: 0.7247\n",
      "Training loss (for one batch) at step 30: 354.8053, Accuracy: 0.7329\n",
      "Training loss (for one batch) at step 40: 346.1876, Accuracy: 0.7304\n",
      "Training loss (for one batch) at step 50: 350.5023, Accuracy: 0.7318\n",
      "Training loss (for one batch) at step 60: 339.8808, Accuracy: 0.7345\n",
      "Training loss (for one batch) at step 70: 364.1248, Accuracy: 0.7335\n",
      "Training loss (for one batch) at step 80: 352.3757, Accuracy: 0.7316\n",
      "Training loss (for one batch) at step 90: 356.5910, Accuracy: 0.7283\n",
      "Training loss (for one batch) at step 100: 360.8120, Accuracy: 0.7258\n",
      "Training loss (for one batch) at step 110: 363.2825, Accuracy: 0.7237\n",
      "---- Training ----\n",
      "Training loss: 120.0811\n",
      "Training acc over epoch: 0.7234\n",
      "---- Validation ----\n",
      "Validation loss: 51.8884\n",
      "Validation acc: 0.6591\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 368.8836, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 372.6219, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 20: 362.5704, Accuracy: 0.7392\n",
      "Training loss (for one batch) at step 30: 353.4350, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 40: 355.7658, Accuracy: 0.7372\n",
      "Training loss (for one batch) at step 50: 363.1278, Accuracy: 0.7385\n",
      "Training loss (for one batch) at step 60: 352.6088, Accuracy: 0.7369\n",
      "Training loss (for one batch) at step 70: 347.8782, Accuracy: 0.7347\n",
      "Training loss (for one batch) at step 80: 372.2581, Accuracy: 0.7322\n",
      "Training loss (for one batch) at step 90: 377.1885, Accuracy: 0.7289\n",
      "Training loss (for one batch) at step 100: 358.9262, Accuracy: 0.7247\n",
      "Training loss (for one batch) at step 110: 371.1043, Accuracy: 0.7235\n",
      "---- Training ----\n",
      "Training loss: 114.5192\n",
      "Training acc over epoch: 0.7231\n",
      "---- Validation ----\n",
      "Validation loss: 32.7659\n",
      "Validation acc: 0.6478\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 358.5785, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 364.6478, Accuracy: 0.7472\n",
      "Training loss (for one batch) at step 20: 358.3065, Accuracy: 0.7407\n",
      "Training loss (for one batch) at step 30: 349.8383, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 40: 344.4234, Accuracy: 0.7359\n",
      "Training loss (for one batch) at step 50: 358.7228, Accuracy: 0.7384\n",
      "Training loss (for one batch) at step 60: 361.6343, Accuracy: 0.7341\n",
      "Training loss (for one batch) at step 70: 358.0902, Accuracy: 0.7323\n",
      "Training loss (for one batch) at step 80: 372.1772, Accuracy: 0.7303\n",
      "Training loss (for one batch) at step 90: 341.4018, Accuracy: 0.7281\n",
      "Training loss (for one batch) at step 100: 371.1288, Accuracy: 0.7249\n",
      "Training loss (for one batch) at step 110: 361.7961, Accuracy: 0.7242\n",
      "---- Training ----\n",
      "Training loss: 111.8431\n",
      "Training acc over epoch: 0.7237\n",
      "---- Validation ----\n",
      "Validation loss: 36.7438\n",
      "Validation acc: 0.6569\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 366.4686, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 370.4954, Accuracy: 0.7599\n",
      "Training loss (for one batch) at step 20: 359.8572, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 30: 352.5737, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 40: 340.2609, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 50: 353.6297, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 60: 356.3584, Accuracy: 0.7398\n",
      "Training loss (for one batch) at step 70: 355.7669, Accuracy: 0.7354\n",
      "Training loss (for one batch) at step 80: 354.4680, Accuracy: 0.7338\n",
      "Training loss (for one batch) at step 90: 345.8940, Accuracy: 0.7306\n",
      "Training loss (for one batch) at step 100: 360.4284, Accuracy: 0.7269\n",
      "Training loss (for one batch) at step 110: 346.7587, Accuracy: 0.7262\n",
      "---- Training ----\n",
      "Training loss: 109.6411\n",
      "Training acc over epoch: 0.7255\n",
      "---- Validation ----\n",
      "Validation loss: 33.1284\n",
      "Validation acc: 0.6464\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 379.3452, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 364.2751, Accuracy: 0.7472\n",
      "Training loss (for one batch) at step 20: 346.0236, Accuracy: 0.7467\n",
      "Training loss (for one batch) at step 30: 349.4248, Accuracy: 0.7394\n",
      "Training loss (for one batch) at step 40: 346.6153, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 50: 342.3256, Accuracy: 0.7385\n",
      "Training loss (for one batch) at step 60: 349.8871, Accuracy: 0.7371\n",
      "Training loss (for one batch) at step 70: 344.2943, Accuracy: 0.7365\n",
      "Training loss (for one batch) at step 80: 360.8904, Accuracy: 0.7346\n",
      "Training loss (for one batch) at step 90: 341.3520, Accuracy: 0.7326\n",
      "Training loss (for one batch) at step 100: 347.5358, Accuracy: 0.7312\n",
      "Training loss (for one batch) at step 110: 352.6532, Accuracy: 0.7294\n",
      "---- Training ----\n",
      "Training loss: 102.6204\n",
      "Training acc over epoch: 0.7296\n",
      "---- Validation ----\n",
      "Validation loss: 40.7888\n",
      "Validation acc: 0.6518\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 376.1680, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 378.0226, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 20: 367.6829, Accuracy: 0.7452\n",
      "Training loss (for one batch) at step 30: 349.2179, Accuracy: 0.7465\n",
      "Training loss (for one batch) at step 40: 357.6277, Accuracy: 0.7431\n",
      "Training loss (for one batch) at step 50: 348.0041, Accuracy: 0.7431\n",
      "Training loss (for one batch) at step 60: 349.7180, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 70: 354.8190, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 80: 344.9229, Accuracy: 0.7385\n",
      "Training loss (for one batch) at step 90: 344.2676, Accuracy: 0.7345\n",
      "Training loss (for one batch) at step 100: 356.2303, Accuracy: 0.7305\n",
      "Training loss (for one batch) at step 110: 341.9605, Accuracy: 0.7300\n",
      "---- Training ----\n",
      "Training loss: 110.5978\n",
      "Training acc over epoch: 0.7283\n",
      "---- Validation ----\n",
      "Validation loss: 38.8685\n",
      "Validation acc: 0.6480\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 350.1676, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 354.6721, Accuracy: 0.7479\n",
      "Training loss (for one batch) at step 20: 353.8675, Accuracy: 0.7455\n",
      "Training loss (for one batch) at step 30: 338.2070, Accuracy: 0.7427\n",
      "Training loss (for one batch) at step 40: 340.0801, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 50: 335.9968, Accuracy: 0.7399\n",
      "Training loss (for one batch) at step 60: 346.8649, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 70: 363.4335, Accuracy: 0.7383\n",
      "Training loss (for one batch) at step 80: 346.0750, Accuracy: 0.7359\n",
      "Training loss (for one batch) at step 90: 335.4233, Accuracy: 0.7325\n",
      "Training loss (for one batch) at step 100: 363.1598, Accuracy: 0.7294\n",
      "Training loss (for one batch) at step 110: 347.0349, Accuracy: 0.7271\n",
      "---- Training ----\n",
      "Training loss: 111.7240\n",
      "Training acc over epoch: 0.7268\n",
      "---- Validation ----\n",
      "Validation loss: 38.5270\n",
      "Validation acc: 0.6754\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 355.4861, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 362.5121, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 20: 345.1036, Accuracy: 0.7504\n",
      "Training loss (for one batch) at step 30: 330.9962, Accuracy: 0.7440\n",
      "Training loss (for one batch) at step 40: 356.0714, Accuracy: 0.7401\n",
      "Training loss (for one batch) at step 50: 344.9872, Accuracy: 0.7434\n",
      "Training loss (for one batch) at step 60: 338.8781, Accuracy: 0.7407\n",
      "Training loss (for one batch) at step 70: 342.7639, Accuracy: 0.7375\n",
      "Training loss (for one batch) at step 80: 360.3826, Accuracy: 0.7357\n",
      "Training loss (for one batch) at step 90: 337.7848, Accuracy: 0.7345\n",
      "Training loss (for one batch) at step 100: 344.7010, Accuracy: 0.7294\n",
      "Training loss (for one batch) at step 110: 347.0519, Accuracy: 0.7280\n",
      "---- Training ----\n",
      "Training loss: 107.2539\n",
      "Training acc over epoch: 0.7284\n",
      "---- Validation ----\n",
      "Validation loss: 39.1350\n",
      "Validation acc: 0.6435\n",
      "Time taken: 10.51s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 356.8785, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 342.6869, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 20: 337.4719, Accuracy: 0.7522\n",
      "Training loss (for one batch) at step 30: 326.5320, Accuracy: 0.7465\n",
      "Training loss (for one batch) at step 40: 318.3307, Accuracy: 0.7428\n",
      "Training loss (for one batch) at step 50: 339.9059, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 60: 330.2487, Accuracy: 0.7431\n",
      "Training loss (for one batch) at step 70: 374.5668, Accuracy: 0.7389\n",
      "Training loss (for one batch) at step 80: 353.6440, Accuracy: 0.7385\n",
      "Training loss (for one batch) at step 90: 347.3058, Accuracy: 0.7342\n",
      "Training loss (for one batch) at step 100: 345.7463, Accuracy: 0.7310\n",
      "Training loss (for one batch) at step 110: 340.3526, Accuracy: 0.7294\n",
      "---- Training ----\n",
      "Training loss: 106.7165\n",
      "Training acc over epoch: 0.7290\n",
      "---- Validation ----\n",
      "Validation loss: 34.4118\n",
      "Validation acc: 0.6475\n",
      "Time taken: 10.16s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 349.5491, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 359.3942, Accuracy: 0.7443\n",
      "Training loss (for one batch) at step 20: 345.4274, Accuracy: 0.7440\n",
      "Training loss (for one batch) at step 30: 336.6346, Accuracy: 0.7455\n",
      "Training loss (for one batch) at step 40: 338.2079, Accuracy: 0.7460\n",
      "Training loss (for one batch) at step 50: 353.9251, Accuracy: 0.7449\n",
      "Training loss (for one batch) at step 60: 341.5120, Accuracy: 0.7436\n",
      "Training loss (for one batch) at step 70: 344.8049, Accuracy: 0.7406\n",
      "Training loss (for one batch) at step 80: 353.7378, Accuracy: 0.7368\n",
      "Training loss (for one batch) at step 90: 327.6494, Accuracy: 0.7368\n",
      "Training loss (for one batch) at step 100: 349.8967, Accuracy: 0.7321\n",
      "Training loss (for one batch) at step 110: 324.7924, Accuracy: 0.7306\n",
      "---- Training ----\n",
      "Training loss: 110.4861\n",
      "Training acc over epoch: 0.7296\n",
      "---- Validation ----\n",
      "Validation loss: 49.3567\n",
      "Validation acc: 0.6639\n",
      "Time taken: 10.16s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 366.2826, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 361.8561, Accuracy: 0.7294\n",
      "Training loss (for one batch) at step 20: 344.5947, Accuracy: 0.7392\n",
      "Training loss (for one batch) at step 30: 339.1531, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 40: 332.4467, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 50: 326.2607, Accuracy: 0.7411\n",
      "Training loss (for one batch) at step 60: 336.2497, Accuracy: 0.7396\n",
      "Training loss (for one batch) at step 70: 354.5162, Accuracy: 0.7368\n",
      "Training loss (for one batch) at step 80: 332.4607, Accuracy: 0.7334\n",
      "Training loss (for one batch) at step 90: 333.4453, Accuracy: 0.7328\n",
      "Training loss (for one batch) at step 100: 338.2968, Accuracy: 0.7298\n",
      "Training loss (for one batch) at step 110: 357.7806, Accuracy: 0.7284\n",
      "---- Training ----\n",
      "Training loss: 113.3818\n",
      "Training acc over epoch: 0.7278\n",
      "---- Validation ----\n",
      "Validation loss: 45.4458\n",
      "Validation acc: 0.6462\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 353.0343, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 341.9425, Accuracy: 0.7372\n",
      "Training loss (for one batch) at step 20: 340.4028, Accuracy: 0.7355\n",
      "Training loss (for one batch) at step 30: 330.1248, Accuracy: 0.7369\n",
      "Training loss (for one batch) at step 40: 330.9638, Accuracy: 0.7372\n",
      "Training loss (for one batch) at step 50: 332.9876, Accuracy: 0.7394\n",
      "Training loss (for one batch) at step 60: 322.6801, Accuracy: 0.7395\n",
      "Training loss (for one batch) at step 70: 359.1849, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 80: 334.0478, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 90: 330.0831, Accuracy: 0.7330\n",
      "Training loss (for one batch) at step 100: 345.2338, Accuracy: 0.7281\n",
      "Training loss (for one batch) at step 110: 335.7765, Accuracy: 0.7268\n",
      "---- Training ----\n",
      "Training loss: 113.1786\n",
      "Training acc over epoch: 0.7260\n",
      "---- Validation ----\n",
      "Validation loss: 36.1636\n",
      "Validation acc: 0.6599\n",
      "Time taken: 10.14s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 331.8519, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 350.1415, Accuracy: 0.7472\n",
      "Training loss (for one batch) at step 20: 347.9047, Accuracy: 0.7440\n",
      "Training loss (for one batch) at step 30: 348.1494, Accuracy: 0.7445\n",
      "Training loss (for one batch) at step 40: 330.2730, Accuracy: 0.7420\n",
      "Training loss (for one batch) at step 50: 325.1769, Accuracy: 0.7408\n",
      "Training loss (for one batch) at step 60: 328.8267, Accuracy: 0.7431\n",
      "Training loss (for one batch) at step 70: 360.8510, Accuracy: 0.7410\n",
      "Training loss (for one batch) at step 80: 340.6823, Accuracy: 0.7380\n",
      "Training loss (for one batch) at step 90: 331.9740, Accuracy: 0.7322\n",
      "Training loss (for one batch) at step 100: 357.7437, Accuracy: 0.7293\n",
      "Training loss (for one batch) at step 110: 340.5941, Accuracy: 0.7284\n",
      "---- Training ----\n",
      "Training loss: 100.3649\n",
      "Training acc over epoch: 0.7281\n",
      "---- Validation ----\n",
      "Validation loss: 35.8295\n",
      "Validation acc: 0.6580\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 353.7545, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 333.5745, Accuracy: 0.7358\n",
      "Training loss (for one batch) at step 20: 341.6173, Accuracy: 0.7381\n",
      "Training loss (for one batch) at step 30: 337.3474, Accuracy: 0.7394\n",
      "Training loss (for one batch) at step 40: 327.2118, Accuracy: 0.7407\n",
      "Training loss (for one batch) at step 50: 335.0205, Accuracy: 0.7434\n",
      "Training loss (for one batch) at step 60: 329.6823, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 70: 353.3052, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 80: 330.9015, Accuracy: 0.7389\n",
      "Training loss (for one batch) at step 90: 321.0484, Accuracy: 0.7336\n",
      "Training loss (for one batch) at step 100: 338.1437, Accuracy: 0.7319\n",
      "Training loss (for one batch) at step 110: 333.7941, Accuracy: 0.7288\n",
      "---- Training ----\n",
      "Training loss: 107.5447\n",
      "Training acc over epoch: 0.7285\n",
      "---- Validation ----\n",
      "Validation loss: 38.0866\n",
      "Validation acc: 0.6435\n",
      "Time taken: 10.70s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 352.1011, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 332.7094, Accuracy: 0.7408\n",
      "Training loss (for one batch) at step 20: 336.9119, Accuracy: 0.7507\n",
      "Training loss (for one batch) at step 30: 325.1218, Accuracy: 0.7475\n",
      "Training loss (for one batch) at step 40: 336.1651, Accuracy: 0.7418\n",
      "Training loss (for one batch) at step 50: 331.0670, Accuracy: 0.7428\n",
      "Training loss (for one batch) at step 60: 329.2673, Accuracy: 0.7413\n",
      "Training loss (for one batch) at step 70: 348.3477, Accuracy: 0.7393\n",
      "Training loss (for one batch) at step 80: 334.4002, Accuracy: 0.7378\n",
      "Training loss (for one batch) at step 90: 330.4877, Accuracy: 0.7347\n",
      "Training loss (for one batch) at step 100: 352.8261, Accuracy: 0.7327\n",
      "Training loss (for one batch) at step 110: 334.1887, Accuracy: 0.7300\n",
      "---- Training ----\n",
      "Training loss: 126.6618\n",
      "Training acc over epoch: 0.7301\n",
      "---- Validation ----\n",
      "Validation loss: 43.8208\n",
      "Validation acc: 0.6604\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 330.3134, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 344.2185, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 20: 340.8894, Accuracy: 0.7567\n",
      "Training loss (for one batch) at step 30: 342.7748, Accuracy: 0.7487\n",
      "Training loss (for one batch) at step 40: 310.1727, Accuracy: 0.7479\n",
      "Training loss (for one batch) at step 50: 333.9355, Accuracy: 0.7477\n",
      "Training loss (for one batch) at step 60: 333.5216, Accuracy: 0.7445\n",
      "Training loss (for one batch) at step 70: 349.0144, Accuracy: 0.7400\n",
      "Training loss (for one batch) at step 80: 327.5229, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 90: 327.5225, Accuracy: 0.7372\n",
      "Training loss (for one batch) at step 100: 329.5795, Accuracy: 0.7333\n",
      "Training loss (for one batch) at step 110: 337.0856, Accuracy: 0.7299\n",
      "---- Training ----\n",
      "Training loss: 110.0215\n",
      "Training acc over epoch: 0.7305\n",
      "---- Validation ----\n",
      "Validation loss: 43.0430\n",
      "Validation acc: 0.6642\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 348.2331, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 338.4661, Accuracy: 0.7486\n",
      "Training loss (for one batch) at step 20: 319.4119, Accuracy: 0.7478\n",
      "Training loss (for one batch) at step 30: 333.0653, Accuracy: 0.7419\n",
      "Training loss (for one batch) at step 40: 321.5389, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 50: 324.9446, Accuracy: 0.7410\n",
      "Training loss (for one batch) at step 60: 333.4098, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 70: 328.5677, Accuracy: 0.7381\n",
      "Training loss (for one batch) at step 80: 360.8511, Accuracy: 0.7379\n",
      "Training loss (for one batch) at step 90: 336.3720, Accuracy: 0.7353\n",
      "Training loss (for one batch) at step 100: 316.8627, Accuracy: 0.7329\n",
      "Training loss (for one batch) at step 110: 335.5809, Accuracy: 0.7308\n",
      "---- Training ----\n",
      "Training loss: 100.3824\n",
      "Training acc over epoch: 0.7300\n",
      "---- Validation ----\n",
      "Validation loss: 43.1915\n",
      "Validation acc: 0.6456\n",
      "Time taken: 10.51s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 336.1220, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 326.1432, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 20: 330.5030, Accuracy: 0.7467\n",
      "Training loss (for one batch) at step 30: 320.7181, Accuracy: 0.7447\n",
      "Training loss (for one batch) at step 40: 322.4478, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 50: 327.8600, Accuracy: 0.7439\n",
      "Training loss (for one batch) at step 60: 338.3292, Accuracy: 0.7419\n",
      "Training loss (for one batch) at step 70: 329.1430, Accuracy: 0.7383\n",
      "Training loss (for one batch) at step 80: 318.6037, Accuracy: 0.7384\n",
      "Training loss (for one batch) at step 90: 323.1188, Accuracy: 0.7369\n",
      "Training loss (for one batch) at step 100: 371.0714, Accuracy: 0.7333\n",
      "Training loss (for one batch) at step 110: 327.1611, Accuracy: 0.7313\n",
      "---- Training ----\n",
      "Training loss: 106.9176\n",
      "Training acc over epoch: 0.7311\n",
      "---- Validation ----\n",
      "Validation loss: 51.1347\n",
      "Validation acc: 0.6456\n",
      "Time taken: 10.05s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 359.5788, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 331.5977, Accuracy: 0.7514\n",
      "Training loss (for one batch) at step 20: 327.7486, Accuracy: 0.7574\n",
      "Training loss (for one batch) at step 30: 329.4779, Accuracy: 0.7492\n",
      "Training loss (for one batch) at step 40: 327.4467, Accuracy: 0.7447\n",
      "Training loss (for one batch) at step 50: 330.8265, Accuracy: 0.7440\n",
      "Training loss (for one batch) at step 60: 336.9587, Accuracy: 0.7415\n",
      "Training loss (for one batch) at step 70: 338.7776, Accuracy: 0.7395\n",
      "Training loss (for one batch) at step 80: 342.4199, Accuracy: 0.7374\n",
      "Training loss (for one batch) at step 90: 330.0613, Accuracy: 0.7350\n",
      "Training loss (for one batch) at step 100: 331.8432, Accuracy: 0.7300\n",
      "Training loss (for one batch) at step 110: 339.1277, Accuracy: 0.7297\n",
      "---- Training ----\n",
      "Training loss: 118.1789\n",
      "Training acc over epoch: 0.7286\n",
      "---- Validation ----\n",
      "Validation loss: 38.9211\n",
      "Validation acc: 0.6451\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 330.6604, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 340.9598, Accuracy: 0.7358\n",
      "Training loss (for one batch) at step 20: 333.5790, Accuracy: 0.7470\n",
      "Training loss (for one batch) at step 30: 331.6087, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 40: 328.3779, Accuracy: 0.7441\n",
      "Training loss (for one batch) at step 50: 305.2349, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 60: 341.0591, Accuracy: 0.7421\n",
      "Training loss (for one batch) at step 70: 329.6711, Accuracy: 0.7404\n",
      "Training loss (for one batch) at step 80: 347.8516, Accuracy: 0.7394\n",
      "Training loss (for one batch) at step 90: 328.9437, Accuracy: 0.7372\n",
      "Training loss (for one batch) at step 100: 337.1918, Accuracy: 0.7347\n",
      "Training loss (for one batch) at step 110: 312.4059, Accuracy: 0.7322\n",
      "---- Training ----\n",
      "Training loss: 133.2007\n",
      "Training acc over epoch: 0.7316\n",
      "---- Validation ----\n",
      "Validation loss: 30.9165\n",
      "Validation acc: 0.6510\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 334.6811, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 333.4186, Accuracy: 0.7315\n",
      "Training loss (for one batch) at step 20: 329.0266, Accuracy: 0.7493\n",
      "Training loss (for one batch) at step 30: 325.6493, Accuracy: 0.7480\n",
      "Training loss (for one batch) at step 40: 328.5856, Accuracy: 0.7471\n",
      "Training loss (for one batch) at step 50: 321.4454, Accuracy: 0.7492\n",
      "Training loss (for one batch) at step 60: 334.3214, Accuracy: 0.7469\n",
      "Training loss (for one batch) at step 70: 343.0536, Accuracy: 0.7424\n",
      "Training loss (for one batch) at step 80: 325.0238, Accuracy: 0.7406\n",
      "Training loss (for one batch) at step 90: 309.6500, Accuracy: 0.7387\n",
      "Training loss (for one batch) at step 100: 306.4916, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 110: 338.7358, Accuracy: 0.7332\n",
      "---- Training ----\n",
      "Training loss: 117.6868\n",
      "Training acc over epoch: 0.7321\n",
      "---- Validation ----\n",
      "Validation loss: 36.4911\n",
      "Validation acc: 0.6470\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 341.1666, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 359.4266, Accuracy: 0.7408\n",
      "Training loss (for one batch) at step 20: 332.8674, Accuracy: 0.7496\n",
      "Training loss (for one batch) at step 30: 317.3413, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 40: 333.2219, Accuracy: 0.7470\n",
      "Training loss (for one batch) at step 50: 313.4715, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 60: 314.4393, Accuracy: 0.7446\n",
      "Training loss (for one batch) at step 70: 330.8732, Accuracy: 0.7424\n",
      "Training loss (for one batch) at step 80: 330.6062, Accuracy: 0.7407\n",
      "Training loss (for one batch) at step 90: 324.1993, Accuracy: 0.7388\n",
      "Training loss (for one batch) at step 100: 324.7994, Accuracy: 0.7331\n",
      "Training loss (for one batch) at step 110: 317.7017, Accuracy: 0.7300\n",
      "---- Training ----\n",
      "Training loss: 110.7948\n",
      "Training acc over epoch: 0.7303\n",
      "---- Validation ----\n",
      "Validation loss: 76.0939\n",
      "Validation acc: 0.6631\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 334.9378, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 336.7313, Accuracy: 0.7536\n",
      "Training loss (for one batch) at step 20: 323.6620, Accuracy: 0.7467\n",
      "Training loss (for one batch) at step 30: 320.4595, Accuracy: 0.7455\n",
      "Training loss (for one batch) at step 40: 341.0597, Accuracy: 0.7439\n",
      "Training loss (for one batch) at step 50: 320.6665, Accuracy: 0.7459\n",
      "Training loss (for one batch) at step 60: 320.0687, Accuracy: 0.7455\n",
      "Training loss (for one batch) at step 70: 340.3135, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 80: 338.0559, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 90: 336.4565, Accuracy: 0.7371\n",
      "Training loss (for one batch) at step 100: 334.2547, Accuracy: 0.7337\n",
      "Training loss (for one batch) at step 110: 312.2650, Accuracy: 0.7316\n",
      "---- Training ----\n",
      "Training loss: 95.1934\n",
      "Training acc over epoch: 0.7303\n",
      "---- Validation ----\n",
      "Validation loss: 35.2979\n",
      "Validation acc: 0.6660\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 341.9959, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 339.6121, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 20: 318.8490, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 30: 321.5988, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 40: 303.4241, Accuracy: 0.7443\n",
      "Training loss (for one batch) at step 50: 320.1697, Accuracy: 0.7479\n",
      "Training loss (for one batch) at step 60: 331.7348, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 70: 327.8860, Accuracy: 0.7419\n",
      "Training loss (for one batch) at step 80: 322.3654, Accuracy: 0.7396\n",
      "Training loss (for one batch) at step 90: 318.7938, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 100: 338.3472, Accuracy: 0.7348\n",
      "Training loss (for one batch) at step 110: 340.2162, Accuracy: 0.7325\n",
      "---- Training ----\n",
      "Training loss: 103.5524\n",
      "Training acc over epoch: 0.7324\n",
      "---- Validation ----\n",
      "Validation loss: 37.0350\n",
      "Validation acc: 0.6451\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 344.3442, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 341.1514, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 20: 330.1561, Accuracy: 0.7560\n",
      "Training loss (for one batch) at step 30: 320.6543, Accuracy: 0.7523\n",
      "Training loss (for one batch) at step 40: 315.8181, Accuracy: 0.7532\n",
      "Training loss (for one batch) at step 50: 313.2998, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 60: 312.3170, Accuracy: 0.7505\n",
      "Training loss (for one batch) at step 70: 327.5310, Accuracy: 0.7480\n",
      "Training loss (for one batch) at step 80: 340.5153, Accuracy: 0.7431\n",
      "Training loss (for one batch) at step 90: 331.2090, Accuracy: 0.7385\n",
      "Training loss (for one batch) at step 100: 315.1302, Accuracy: 0.7343\n",
      "Training loss (for one batch) at step 110: 319.2424, Accuracy: 0.7340\n",
      "---- Training ----\n",
      "Training loss: 112.8328\n",
      "Training acc over epoch: 0.7323\n",
      "---- Validation ----\n",
      "Validation loss: 31.8099\n",
      "Validation acc: 0.6298\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 325.9113, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 320.1251, Accuracy: 0.7493\n",
      "Training loss (for one batch) at step 20: 326.8857, Accuracy: 0.7507\n",
      "Training loss (for one batch) at step 30: 321.0123, Accuracy: 0.7505\n",
      "Training loss (for one batch) at step 40: 321.7427, Accuracy: 0.7485\n",
      "Training loss (for one batch) at step 50: 324.5714, Accuracy: 0.7506\n",
      "Training loss (for one batch) at step 60: 308.8010, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 70: 326.1840, Accuracy: 0.7431\n",
      "Training loss (for one batch) at step 80: 320.1685, Accuracy: 0.7416\n",
      "Training loss (for one batch) at step 90: 342.5633, Accuracy: 0.7374\n",
      "Training loss (for one batch) at step 100: 323.9404, Accuracy: 0.7321\n",
      "Training loss (for one batch) at step 110: 321.9573, Accuracy: 0.7313\n",
      "---- Training ----\n",
      "Training loss: 108.8643\n",
      "Training acc over epoch: 0.7315\n",
      "---- Validation ----\n",
      "Validation loss: 45.4992\n",
      "Validation acc: 0.6550\n",
      "Time taken: 10.66s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 337.5956, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 363.8168, Accuracy: 0.7358\n",
      "Training loss (for one batch) at step 20: 317.7585, Accuracy: 0.7448\n",
      "Training loss (for one batch) at step 30: 308.9942, Accuracy: 0.7452\n",
      "Training loss (for one batch) at step 40: 325.6242, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 50: 317.1421, Accuracy: 0.7466\n",
      "Training loss (for one batch) at step 60: 315.8010, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 70: 327.3351, Accuracy: 0.7442\n",
      "Training loss (for one batch) at step 80: 334.0216, Accuracy: 0.7428\n",
      "Training loss (for one batch) at step 90: 323.7894, Accuracy: 0.7384\n",
      "Training loss (for one batch) at step 100: 333.9418, Accuracy: 0.7349\n",
      "Training loss (for one batch) at step 110: 328.6093, Accuracy: 0.7341\n",
      "---- Training ----\n",
      "Training loss: 108.7467\n",
      "Training acc over epoch: 0.7325\n",
      "---- Validation ----\n",
      "Validation loss: 44.2764\n",
      "Validation acc: 0.6561\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 327.5400, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 329.5218, Accuracy: 0.7585\n",
      "Training loss (for one batch) at step 20: 330.1417, Accuracy: 0.7530\n",
      "Training loss (for one batch) at step 30: 315.5808, Accuracy: 0.7497\n",
      "Training loss (for one batch) at step 40: 321.9239, Accuracy: 0.7487\n",
      "Training loss (for one batch) at step 50: 309.2697, Accuracy: 0.7488\n",
      "Training loss (for one batch) at step 60: 321.9937, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 70: 325.7249, Accuracy: 0.7460\n",
      "Training loss (for one batch) at step 80: 315.8113, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 90: 324.0510, Accuracy: 0.7406\n",
      "Training loss (for one batch) at step 100: 323.6772, Accuracy: 0.7360\n",
      "Training loss (for one batch) at step 110: 316.5177, Accuracy: 0.7342\n",
      "---- Training ----\n",
      "Training loss: 104.0106\n",
      "Training acc over epoch: 0.7336\n",
      "---- Validation ----\n",
      "Validation loss: 38.0238\n",
      "Validation acc: 0.6601\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 325.2446, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 322.3417, Accuracy: 0.7401\n",
      "Training loss (for one batch) at step 20: 313.0751, Accuracy: 0.7448\n",
      "Training loss (for one batch) at step 30: 324.2422, Accuracy: 0.7480\n",
      "Training loss (for one batch) at step 40: 321.3952, Accuracy: 0.7479\n",
      "Training loss (for one batch) at step 50: 308.8401, Accuracy: 0.7483\n",
      "Training loss (for one batch) at step 60: 325.7791, Accuracy: 0.7476\n",
      "Training loss (for one batch) at step 70: 333.2166, Accuracy: 0.7443\n",
      "Training loss (for one batch) at step 80: 316.9550, Accuracy: 0.7449\n",
      "Training loss (for one batch) at step 90: 312.6972, Accuracy: 0.7398\n",
      "Training loss (for one batch) at step 100: 328.2081, Accuracy: 0.7354\n",
      "Training loss (for one batch) at step 110: 301.4957, Accuracy: 0.7342\n",
      "---- Training ----\n",
      "Training loss: 109.3863\n",
      "Training acc over epoch: 0.7333\n",
      "---- Validation ----\n",
      "Validation loss: 42.5984\n",
      "Validation acc: 0.6539\n",
      "Time taken: 10.67s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 313.3060, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 332.8403, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 20: 314.7446, Accuracy: 0.7615\n",
      "Training loss (for one batch) at step 30: 327.9430, Accuracy: 0.7545\n",
      "Training loss (for one batch) at step 40: 300.0014, Accuracy: 0.7496\n",
      "Training loss (for one batch) at step 50: 318.0514, Accuracy: 0.7511\n",
      "Training loss (for one batch) at step 60: 317.1650, Accuracy: 0.7512\n",
      "Training loss (for one batch) at step 70: 342.1233, Accuracy: 0.7467\n",
      "Training loss (for one batch) at step 80: 319.1815, Accuracy: 0.7451\n",
      "Training loss (for one batch) at step 90: 328.5056, Accuracy: 0.7415\n",
      "Training loss (for one batch) at step 100: 318.6754, Accuracy: 0.7364\n",
      "Training loss (for one batch) at step 110: 309.2238, Accuracy: 0.7364\n",
      "---- Training ----\n",
      "Training loss: 104.9951\n",
      "Training acc over epoch: 0.7350\n",
      "---- Validation ----\n",
      "Validation loss: 44.8345\n",
      "Validation acc: 0.6625\n",
      "Time taken: 10.15s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 328.5121, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 330.8559, Accuracy: 0.7479\n",
      "Training loss (for one batch) at step 20: 311.9296, Accuracy: 0.7522\n",
      "Training loss (for one batch) at step 30: 298.3269, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 40: 308.1858, Accuracy: 0.7441\n",
      "Training loss (for one batch) at step 50: 313.7743, Accuracy: 0.7439\n",
      "Training loss (for one batch) at step 60: 302.6220, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 70: 320.3856, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 80: 301.3086, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 90: 309.7834, Accuracy: 0.7388\n",
      "Training loss (for one batch) at step 100: 332.4490, Accuracy: 0.7354\n",
      "Training loss (for one batch) at step 110: 320.5758, Accuracy: 0.7334\n",
      "---- Training ----\n",
      "Training loss: 113.8377\n",
      "Training acc over epoch: 0.7326\n",
      "---- Validation ----\n",
      "Validation loss: 54.2158\n",
      "Validation acc: 0.6545\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 318.7587, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 313.5777, Accuracy: 0.7685\n",
      "Training loss (for one batch) at step 20: 314.8234, Accuracy: 0.7563\n",
      "Training loss (for one batch) at step 30: 297.6683, Accuracy: 0.7495\n",
      "Training loss (for one batch) at step 40: 325.9279, Accuracy: 0.7502\n",
      "Training loss (for one batch) at step 50: 295.4861, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 60: 314.6942, Accuracy: 0.7495\n",
      "Training loss (for one batch) at step 70: 321.8872, Accuracy: 0.7466\n",
      "Training loss (for one batch) at step 80: 342.0582, Accuracy: 0.7440\n",
      "Training loss (for one batch) at step 90: 316.1385, Accuracy: 0.7406\n",
      "Training loss (for one batch) at step 100: 305.0889, Accuracy: 0.7378\n",
      "Training loss (for one batch) at step 110: 324.5859, Accuracy: 0.7363\n",
      "---- Training ----\n",
      "Training loss: 121.4929\n",
      "Training acc over epoch: 0.7354\n",
      "---- Validation ----\n",
      "Validation loss: 38.5794\n",
      "Validation acc: 0.6491\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 334.6644, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 315.9644, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 20: 312.2017, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 30: 309.1670, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 40: 309.8717, Accuracy: 0.7492\n",
      "Training loss (for one batch) at step 50: 310.7606, Accuracy: 0.7512\n",
      "Training loss (for one batch) at step 60: 309.3065, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 70: 319.2783, Accuracy: 0.7452\n",
      "Training loss (for one batch) at step 80: 333.2929, Accuracy: 0.7436\n",
      "Training loss (for one batch) at step 90: 316.8518, Accuracy: 0.7394\n",
      "Training loss (for one batch) at step 100: 315.6642, Accuracy: 0.7357\n",
      "Training loss (for one batch) at step 110: 309.3224, Accuracy: 0.7350\n",
      "---- Training ----\n",
      "Training loss: 111.9756\n",
      "Training acc over epoch: 0.7343\n",
      "---- Validation ----\n",
      "Validation loss: 30.9255\n",
      "Validation acc: 0.6663\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 328.7290, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 305.6365, Accuracy: 0.7507\n",
      "Training loss (for one batch) at step 20: 312.7060, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 30: 306.7498, Accuracy: 0.7505\n",
      "Training loss (for one batch) at step 40: 294.1295, Accuracy: 0.7479\n",
      "Training loss (for one batch) at step 50: 306.1494, Accuracy: 0.7475\n",
      "Training loss (for one batch) at step 60: 316.4888, Accuracy: 0.7480\n",
      "Training loss (for one batch) at step 70: 316.2892, Accuracy: 0.7471\n",
      "Training loss (for one batch) at step 80: 318.5838, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 90: 302.4429, Accuracy: 0.7417\n",
      "Training loss (for one batch) at step 100: 318.9133, Accuracy: 0.7354\n",
      "Training loss (for one batch) at step 110: 302.8636, Accuracy: 0.7342\n",
      "---- Training ----\n",
      "Training loss: 97.4642\n",
      "Training acc over epoch: 0.7349\n",
      "---- Validation ----\n",
      "Validation loss: 30.3034\n",
      "Validation acc: 0.6556\n",
      "Time taken: 10.09s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 306.0672, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 316.6781, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 20: 315.4607, Accuracy: 0.7504\n",
      "Training loss (for one batch) at step 30: 319.3871, Accuracy: 0.7513\n",
      "Training loss (for one batch) at step 40: 319.2598, Accuracy: 0.7490\n",
      "Training loss (for one batch) at step 50: 314.2802, Accuracy: 0.7514\n",
      "Training loss (for one batch) at step 60: 304.9786, Accuracy: 0.7509\n",
      "Training loss (for one batch) at step 70: 319.2947, Accuracy: 0.7463\n",
      "Training loss (for one batch) at step 80: 331.6684, Accuracy: 0.7453\n",
      "Training loss (for one batch) at step 90: 319.3577, Accuracy: 0.7423\n",
      "Training loss (for one batch) at step 100: 304.0826, Accuracy: 0.7375\n",
      "Training loss (for one batch) at step 110: 307.8963, Accuracy: 0.7357\n",
      "---- Training ----\n",
      "Training loss: 100.0686\n",
      "Training acc over epoch: 0.7361\n",
      "---- Validation ----\n",
      "Validation loss: 61.6646\n",
      "Validation acc: 0.6464\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 320.9069, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 330.5497, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 20: 312.6504, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 30: 318.1711, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 40: 306.6810, Accuracy: 0.7466\n",
      "Training loss (for one batch) at step 50: 306.5625, Accuracy: 0.7462\n",
      "Training loss (for one batch) at step 60: 317.5856, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 70: 339.9106, Accuracy: 0.7430\n",
      "Training loss (for one batch) at step 80: 316.0274, Accuracy: 0.7398\n",
      "Training loss (for one batch) at step 90: 310.0952, Accuracy: 0.7374\n",
      "Training loss (for one batch) at step 100: 324.3535, Accuracy: 0.7338\n",
      "Training loss (for one batch) at step 110: 307.3711, Accuracy: 0.7301\n",
      "---- Training ----\n",
      "Training loss: 94.6989\n",
      "Training acc over epoch: 0.7304\n",
      "---- Validation ----\n",
      "Validation loss: 49.7770\n",
      "Validation acc: 0.6604\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 321.7362, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 335.8535, Accuracy: 0.7401\n",
      "Training loss (for one batch) at step 20: 318.9998, Accuracy: 0.7459\n",
      "Training loss (for one batch) at step 30: 332.6186, Accuracy: 0.7477\n",
      "Training loss (for one batch) at step 40: 300.2061, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 50: 292.6969, Accuracy: 0.7488\n",
      "Training loss (for one batch) at step 60: 322.0718, Accuracy: 0.7496\n",
      "Training loss (for one batch) at step 70: 315.5464, Accuracy: 0.7480\n",
      "Training loss (for one batch) at step 80: 326.7661, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 90: 301.9066, Accuracy: 0.7425\n",
      "Training loss (for one batch) at step 100: 320.8412, Accuracy: 0.7381\n",
      "Training loss (for one batch) at step 110: 328.6640, Accuracy: 0.7349\n",
      "---- Training ----\n",
      "Training loss: 102.6391\n",
      "Training acc over epoch: 0.7349\n",
      "---- Validation ----\n",
      "Validation loss: 40.8708\n",
      "Validation acc: 0.6628\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 332.9789, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 324.8361, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 20: 295.4292, Accuracy: 0.7571\n",
      "Training loss (for one batch) at step 30: 304.0753, Accuracy: 0.7538\n",
      "Training loss (for one batch) at step 40: 290.1427, Accuracy: 0.7494\n",
      "Training loss (for one batch) at step 50: 300.1527, Accuracy: 0.7497\n",
      "Training loss (for one batch) at step 60: 313.5450, Accuracy: 0.7471\n",
      "Training loss (for one batch) at step 70: 316.1559, Accuracy: 0.7447\n",
      "Training loss (for one batch) at step 80: 307.8445, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 90: 328.1423, Accuracy: 0.7384\n",
      "Training loss (for one batch) at step 100: 317.3657, Accuracy: 0.7338\n",
      "Training loss (for one batch) at step 110: 302.0631, Accuracy: 0.7323\n",
      "---- Training ----\n",
      "Training loss: 96.4461\n",
      "Training acc over epoch: 0.7329\n",
      "---- Validation ----\n",
      "Validation loss: 42.8229\n",
      "Validation acc: 0.6510\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 340.4529, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 366.7446, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 20: 294.3311, Accuracy: 0.7507\n",
      "Training loss (for one batch) at step 30: 316.4678, Accuracy: 0.7472\n",
      "Training loss (for one batch) at step 40: 293.3381, Accuracy: 0.7449\n",
      "Training loss (for one batch) at step 50: 288.7850, Accuracy: 0.7505\n",
      "Training loss (for one batch) at step 60: 304.6249, Accuracy: 0.7492\n",
      "Training loss (for one batch) at step 70: 322.1951, Accuracy: 0.7455\n",
      "Training loss (for one batch) at step 80: 311.5031, Accuracy: 0.7441\n",
      "Training loss (for one batch) at step 90: 303.1464, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 100: 302.6085, Accuracy: 0.7390\n",
      "Training loss (for one batch) at step 110: 314.8419, Accuracy: 0.7370\n",
      "---- Training ----\n",
      "Training loss: 97.9398\n",
      "Training acc over epoch: 0.7367\n",
      "---- Validation ----\n",
      "Validation loss: 53.3989\n",
      "Validation acc: 0.6548\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 306.1018, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 314.2846, Accuracy: 0.7393\n",
      "Training loss (for one batch) at step 20: 298.8338, Accuracy: 0.7463\n",
      "Training loss (for one batch) at step 30: 308.4243, Accuracy: 0.7452\n",
      "Training loss (for one batch) at step 40: 304.5953, Accuracy: 0.7466\n",
      "Training loss (for one batch) at step 50: 306.1051, Accuracy: 0.7492\n",
      "Training loss (for one batch) at step 60: 301.8799, Accuracy: 0.7485\n",
      "Training loss (for one batch) at step 70: 307.4226, Accuracy: 0.7447\n",
      "Training loss (for one batch) at step 80: 311.0159, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 90: 315.0011, Accuracy: 0.7398\n",
      "Training loss (for one batch) at step 100: 301.4414, Accuracy: 0.7361\n",
      "Training loss (for one batch) at step 110: 325.6380, Accuracy: 0.7349\n",
      "---- Training ----\n",
      "Training loss: 94.9309\n",
      "Training acc over epoch: 0.7344\n",
      "---- Validation ----\n",
      "Validation loss: 46.4789\n",
      "Validation acc: 0.6717\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 314.2450, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 308.4143, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 20: 320.1051, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 30: 304.8794, Accuracy: 0.7513\n",
      "Training loss (for one batch) at step 40: 301.0738, Accuracy: 0.7513\n",
      "Training loss (for one batch) at step 50: 308.7606, Accuracy: 0.7479\n",
      "Training loss (for one batch) at step 60: 303.8584, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 70: 298.8109, Accuracy: 0.7464\n",
      "Training loss (for one batch) at step 80: 302.4322, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 90: 324.5869, Accuracy: 0.7424\n",
      "Training loss (for one batch) at step 100: 292.7782, Accuracy: 0.7369\n",
      "Training loss (for one batch) at step 110: 306.3073, Accuracy: 0.7344\n",
      "---- Training ----\n",
      "Training loss: 117.3651\n",
      "Training acc over epoch: 0.7347\n",
      "---- Validation ----\n",
      "Validation loss: 59.0303\n",
      "Validation acc: 0.6462\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 322.8332, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 316.4001, Accuracy: 0.7464\n",
      "Training loss (for one batch) at step 20: 296.4616, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 30: 293.1289, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 40: 302.5777, Accuracy: 0.7487\n",
      "Training loss (for one batch) at step 50: 302.9803, Accuracy: 0.7479\n",
      "Training loss (for one batch) at step 60: 300.2449, Accuracy: 0.7478\n",
      "Training loss (for one batch) at step 70: 316.8359, Accuracy: 0.7447\n",
      "Training loss (for one batch) at step 80: 331.8089, Accuracy: 0.7453\n",
      "Training loss (for one batch) at step 90: 304.4276, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 100: 312.4798, Accuracy: 0.7391\n",
      "Training loss (for one batch) at step 110: 310.3165, Accuracy: 0.7369\n",
      "---- Training ----\n",
      "Training loss: 107.1828\n",
      "Training acc over epoch: 0.7368\n",
      "---- Validation ----\n",
      "Validation loss: 67.0500\n",
      "Validation acc: 0.6556\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 328.8169, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 320.1938, Accuracy: 0.7706\n",
      "Training loss (for one batch) at step 20: 313.3130, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 30: 301.7090, Accuracy: 0.7487\n",
      "Training loss (for one batch) at step 40: 297.7473, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 50: 305.5107, Accuracy: 0.7482\n",
      "Training loss (for one batch) at step 60: 320.7238, Accuracy: 0.7477\n",
      "Training loss (for one batch) at step 70: 319.5737, Accuracy: 0.7465\n",
      "Training loss (for one batch) at step 80: 318.3312, Accuracy: 0.7439\n",
      "Training loss (for one batch) at step 90: 303.4048, Accuracy: 0.7418\n",
      "Training loss (for one batch) at step 100: 299.6374, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 110: 302.2097, Accuracy: 0.7375\n",
      "---- Training ----\n",
      "Training loss: 108.1749\n",
      "Training acc over epoch: 0.7372\n",
      "---- Validation ----\n",
      "Validation loss: 51.9433\n",
      "Validation acc: 0.6553\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 306.2822, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 346.4067, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 20: 308.1852, Accuracy: 0.7522\n",
      "Training loss (for one batch) at step 30: 282.2288, Accuracy: 0.7545\n",
      "Training loss (for one batch) at step 40: 298.0463, Accuracy: 0.7490\n",
      "Training loss (for one batch) at step 50: 284.5542, Accuracy: 0.7531\n",
      "Training loss (for one batch) at step 60: 306.6890, Accuracy: 0.7508\n",
      "Training loss (for one batch) at step 70: 316.3713, Accuracy: 0.7453\n",
      "Training loss (for one batch) at step 80: 323.3758, Accuracy: 0.7432\n",
      "Training loss (for one batch) at step 90: 292.4619, Accuracy: 0.7411\n",
      "Training loss (for one batch) at step 100: 303.4798, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 110: 300.5900, Accuracy: 0.7356\n",
      "---- Training ----\n",
      "Training loss: 124.8906\n",
      "Training acc over epoch: 0.7349\n",
      "---- Validation ----\n",
      "Validation loss: 44.1128\n",
      "Validation acc: 0.6526\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 310.0158, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 323.3609, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 20: 311.0649, Accuracy: 0.7496\n",
      "Training loss (for one batch) at step 30: 290.4360, Accuracy: 0.7485\n",
      "Training loss (for one batch) at step 40: 305.0889, Accuracy: 0.7496\n",
      "Training loss (for one batch) at step 50: 303.1355, Accuracy: 0.7502\n",
      "Training loss (for one batch) at step 60: 296.4487, Accuracy: 0.7476\n",
      "Training loss (for one batch) at step 70: 317.2186, Accuracy: 0.7434\n",
      "Training loss (for one batch) at step 80: 328.0895, Accuracy: 0.7434\n",
      "Training loss (for one batch) at step 90: 320.5476, Accuracy: 0.7393\n",
      "Training loss (for one batch) at step 100: 314.5184, Accuracy: 0.7355\n",
      "Training loss (for one batch) at step 110: 316.3276, Accuracy: 0.7340\n",
      "---- Training ----\n",
      "Training loss: 94.4635\n",
      "Training acc over epoch: 0.7341\n",
      "---- Validation ----\n",
      "Validation loss: 37.5434\n",
      "Validation acc: 0.6623\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 324.0233, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 314.8331, Accuracy: 0.7486\n",
      "Training loss (for one batch) at step 20: 316.2796, Accuracy: 0.7537\n",
      "Training loss (for one batch) at step 30: 293.9568, Accuracy: 0.7558\n",
      "Training loss (for one batch) at step 40: 289.4893, Accuracy: 0.7525\n",
      "Training loss (for one batch) at step 50: 295.8283, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 60: 305.2501, Accuracy: 0.7527\n",
      "Training loss (for one batch) at step 70: 317.0466, Accuracy: 0.7493\n",
      "Training loss (for one batch) at step 80: 308.3296, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 90: 307.6710, Accuracy: 0.7428\n",
      "Training loss (for one batch) at step 100: 314.9558, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 110: 307.2187, Accuracy: 0.7363\n",
      "---- Training ----\n",
      "Training loss: 97.4628\n",
      "Training acc over epoch: 0.7362\n",
      "---- Validation ----\n",
      "Validation loss: 36.5695\n",
      "Validation acc: 0.6443\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 329.9778, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 293.3674, Accuracy: 0.7614\n",
      "Training loss (for one batch) at step 20: 295.8420, Accuracy: 0.7600\n",
      "Training loss (for one batch) at step 30: 299.4708, Accuracy: 0.7573\n",
      "Training loss (for one batch) at step 40: 309.0079, Accuracy: 0.7546\n",
      "Training loss (for one batch) at step 50: 297.3465, Accuracy: 0.7506\n",
      "Training loss (for one batch) at step 60: 295.5658, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 70: 295.2218, Accuracy: 0.7497\n",
      "Training loss (for one batch) at step 80: 313.8509, Accuracy: 0.7465\n",
      "Training loss (for one batch) at step 90: 304.6206, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 100: 289.0399, Accuracy: 0.7369\n",
      "Training loss (for one batch) at step 110: 292.1129, Accuracy: 0.7361\n",
      "---- Training ----\n",
      "Training loss: 113.4845\n",
      "Training acc over epoch: 0.7362\n",
      "---- Validation ----\n",
      "Validation loss: 47.4764\n",
      "Validation acc: 0.6378\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 316.5634, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 300.9532, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 20: 297.4805, Accuracy: 0.7545\n",
      "Training loss (for one batch) at step 30: 295.4592, Accuracy: 0.7523\n",
      "Training loss (for one batch) at step 40: 291.6038, Accuracy: 0.7536\n",
      "Training loss (for one batch) at step 50: 311.7748, Accuracy: 0.7526\n",
      "Training loss (for one batch) at step 60: 305.4116, Accuracy: 0.7512\n",
      "Training loss (for one batch) at step 70: 301.9444, Accuracy: 0.7465\n",
      "Training loss (for one batch) at step 80: 300.4547, Accuracy: 0.7472\n",
      "Training loss (for one batch) at step 90: 302.5362, Accuracy: 0.7425\n",
      "Training loss (for one batch) at step 100: 292.6882, Accuracy: 0.7392\n",
      "Training loss (for one batch) at step 110: 311.8564, Accuracy: 0.7376\n",
      "---- Training ----\n",
      "Training loss: 94.2571\n",
      "Training acc over epoch: 0.7363\n",
      "---- Validation ----\n",
      "Validation loss: 40.7986\n",
      "Validation acc: 0.6679\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 309.7614, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 316.1323, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 20: 294.2423, Accuracy: 0.7463\n",
      "Training loss (for one batch) at step 30: 292.5576, Accuracy: 0.7510\n",
      "Training loss (for one batch) at step 40: 297.6409, Accuracy: 0.7517\n",
      "Training loss (for one batch) at step 50: 292.3718, Accuracy: 0.7538\n",
      "Training loss (for one batch) at step 60: 314.1715, Accuracy: 0.7504\n",
      "Training loss (for one batch) at step 70: 304.1946, Accuracy: 0.7465\n",
      "Training loss (for one batch) at step 80: 309.0007, Accuracy: 0.7462\n",
      "Training loss (for one batch) at step 90: 311.1303, Accuracy: 0.7430\n",
      "Training loss (for one batch) at step 100: 299.0076, Accuracy: 0.7410\n",
      "Training loss (for one batch) at step 110: 297.3916, Accuracy: 0.7366\n",
      "---- Training ----\n",
      "Training loss: 110.9631\n",
      "Training acc over epoch: 0.7368\n",
      "---- Validation ----\n",
      "Validation loss: 43.6400\n",
      "Validation acc: 0.6730\n",
      "Time taken: 10.08s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 332.5614, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 307.5682, Accuracy: 0.7436\n",
      "Training loss (for one batch) at step 20: 306.8945, Accuracy: 0.7463\n",
      "Training loss (for one batch) at step 30: 289.5587, Accuracy: 0.7475\n",
      "Training loss (for one batch) at step 40: 310.8621, Accuracy: 0.7485\n",
      "Training loss (for one batch) at step 50: 306.3843, Accuracy: 0.7497\n",
      "Training loss (for one batch) at step 60: 310.2726, Accuracy: 0.7505\n",
      "Training loss (for one batch) at step 70: 301.1478, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 80: 316.2158, Accuracy: 0.7445\n",
      "Training loss (for one batch) at step 90: 304.1279, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 100: 313.1263, Accuracy: 0.7378\n",
      "Training loss (for one batch) at step 110: 311.8281, Accuracy: 0.7356\n",
      "---- Training ----\n",
      "Training loss: 100.2984\n",
      "Training acc over epoch: 0.7350\n",
      "---- Validation ----\n",
      "Validation loss: 39.4427\n",
      "Validation acc: 0.6566\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 354.9806, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 313.7796, Accuracy: 0.7493\n",
      "Training loss (for one batch) at step 20: 295.3180, Accuracy: 0.7511\n",
      "Training loss (for one batch) at step 30: 283.0266, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 40: 310.6254, Accuracy: 0.7534\n",
      "Training loss (for one batch) at step 50: 292.5534, Accuracy: 0.7511\n",
      "Training loss (for one batch) at step 60: 297.4104, Accuracy: 0.7501\n",
      "Training loss (for one batch) at step 70: 303.0742, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 80: 313.6093, Accuracy: 0.7472\n",
      "Training loss (for one batch) at step 90: 300.1047, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 100: 312.7946, Accuracy: 0.7390\n",
      "Training loss (for one batch) at step 110: 305.9693, Accuracy: 0.7371\n",
      "---- Training ----\n",
      "Training loss: 102.4887\n",
      "Training acc over epoch: 0.7366\n",
      "---- Validation ----\n",
      "Validation loss: 50.1529\n",
      "Validation acc: 0.6677\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 307.9341, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 303.7347, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 20: 311.1720, Accuracy: 0.7548\n",
      "Training loss (for one batch) at step 30: 295.2405, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 40: 313.6588, Accuracy: 0.7496\n",
      "Training loss (for one batch) at step 50: 290.2092, Accuracy: 0.7506\n",
      "Training loss (for one batch) at step 60: 305.9479, Accuracy: 0.7491\n",
      "Training loss (for one batch) at step 70: 314.3638, Accuracy: 0.7456\n",
      "Training loss (for one batch) at step 80: 306.9290, Accuracy: 0.7443\n",
      "Training loss (for one batch) at step 90: 306.7535, Accuracy: 0.7396\n",
      "Training loss (for one batch) at step 100: 296.6468, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 110: 279.0363, Accuracy: 0.7335\n",
      "---- Training ----\n",
      "Training loss: 87.3198\n",
      "Training acc over epoch: 0.7337\n",
      "---- Validation ----\n",
      "Validation loss: 61.5103\n",
      "Validation acc: 0.6454\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 324.9496, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 10: 303.6095, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 20: 290.8942, Accuracy: 0.7582\n",
      "Training loss (for one batch) at step 30: 306.1338, Accuracy: 0.7455\n",
      "Training loss (for one batch) at step 40: 308.8244, Accuracy: 0.7443\n",
      "Training loss (for one batch) at step 50: 286.3434, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 60: 303.6339, Accuracy: 0.7505\n",
      "Training loss (for one batch) at step 70: 310.7962, Accuracy: 0.7465\n",
      "Training loss (for one batch) at step 80: 313.3759, Accuracy: 0.7459\n",
      "Training loss (for one batch) at step 90: 284.9032, Accuracy: 0.7430\n",
      "Training loss (for one batch) at step 100: 297.7647, Accuracy: 0.7372\n",
      "Training loss (for one batch) at step 110: 312.7534, Accuracy: 0.7357\n",
      "---- Training ----\n",
      "Training loss: 105.7178\n",
      "Training acc over epoch: 0.7368\n",
      "---- Validation ----\n",
      "Validation loss: 53.2767\n",
      "Validation acc: 0.6625\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 328.8856, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 293.9980, Accuracy: 0.7536\n",
      "Training loss (for one batch) at step 20: 297.4660, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 30: 287.5588, Accuracy: 0.7538\n",
      "Training loss (for one batch) at step 40: 290.8621, Accuracy: 0.7563\n",
      "Training loss (for one batch) at step 50: 310.7178, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 60: 284.4916, Accuracy: 0.7533\n",
      "Training loss (for one batch) at step 70: 307.6911, Accuracy: 0.7496\n",
      "Training loss (for one batch) at step 80: 313.0888, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 90: 315.5271, Accuracy: 0.7435\n",
      "Training loss (for one batch) at step 100: 300.9608, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 110: 292.6069, Accuracy: 0.7387\n",
      "---- Training ----\n",
      "Training loss: 88.8587\n",
      "Training acc over epoch: 0.7383\n",
      "---- Validation ----\n",
      "Validation loss: 39.5894\n",
      "Validation acc: 0.6601\n",
      "Time taken: 10.63s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 312.0280, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 297.6858, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 20: 301.3719, Accuracy: 0.7533\n",
      "Training loss (for one batch) at step 30: 297.0549, Accuracy: 0.7535\n",
      "Training loss (for one batch) at step 40: 302.0492, Accuracy: 0.7542\n",
      "Training loss (for one batch) at step 50: 283.9433, Accuracy: 0.7560\n",
      "Training loss (for one batch) at step 60: 294.8922, Accuracy: 0.7527\n",
      "Training loss (for one batch) at step 70: 285.5782, Accuracy: 0.7493\n",
      "Training loss (for one batch) at step 80: 335.3656, Accuracy: 0.7449\n",
      "Training loss (for one batch) at step 90: 298.6944, Accuracy: 0.7435\n",
      "Training loss (for one batch) at step 100: 318.1392, Accuracy: 0.7401\n",
      "Training loss (for one batch) at step 110: 289.3295, Accuracy: 0.7373\n",
      "---- Training ----\n",
      "Training loss: 124.1182\n",
      "Training acc over epoch: 0.7375\n",
      "---- Validation ----\n",
      "Validation loss: 55.0749\n",
      "Validation acc: 0.6545\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 303.4701, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 301.0370, Accuracy: 0.7493\n",
      "Training loss (for one batch) at step 20: 283.7599, Accuracy: 0.7519\n",
      "Training loss (for one batch) at step 30: 300.5761, Accuracy: 0.7495\n",
      "Training loss (for one batch) at step 40: 308.8598, Accuracy: 0.7504\n",
      "Training loss (for one batch) at step 50: 285.3427, Accuracy: 0.7541\n",
      "Training loss (for one batch) at step 60: 290.7108, Accuracy: 0.7503\n",
      "Training loss (for one batch) at step 70: 332.8026, Accuracy: 0.7485\n",
      "Training loss (for one batch) at step 80: 298.9595, Accuracy: 0.7470\n",
      "Training loss (for one batch) at step 90: 302.1780, Accuracy: 0.7426\n",
      "Training loss (for one batch) at step 100: 305.9301, Accuracy: 0.7389\n",
      "Training loss (for one batch) at step 110: 310.9125, Accuracy: 0.7387\n",
      "---- Training ----\n",
      "Training loss: 108.5035\n",
      "Training acc over epoch: 0.7374\n",
      "---- Validation ----\n",
      "Validation loss: 47.1763\n",
      "Validation acc: 0.6566\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 306.0725, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 309.3180, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 20: 296.1129, Accuracy: 0.7452\n",
      "Training loss (for one batch) at step 30: 286.6483, Accuracy: 0.7503\n",
      "Training loss (for one batch) at step 40: 288.5286, Accuracy: 0.7492\n",
      "Training loss (for one batch) at step 50: 313.2578, Accuracy: 0.7531\n",
      "Training loss (for one batch) at step 60: 304.4609, Accuracy: 0.7496\n",
      "Training loss (for one batch) at step 70: 317.0219, Accuracy: 0.7453\n",
      "Training loss (for one batch) at step 80: 312.3534, Accuracy: 0.7446\n",
      "Training loss (for one batch) at step 90: 316.3478, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 100: 312.2786, Accuracy: 0.7364\n",
      "Training loss (for one batch) at step 110: 301.8467, Accuracy: 0.7359\n",
      "---- Training ----\n",
      "Training loss: 100.8377\n",
      "Training acc over epoch: 0.7352\n",
      "---- Validation ----\n",
      "Validation loss: 47.7159\n",
      "Validation acc: 0.6736\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 303.2881, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 334.0012, Accuracy: 0.7514\n",
      "Training loss (for one batch) at step 20: 288.5269, Accuracy: 0.7507\n",
      "Training loss (for one batch) at step 30: 291.7695, Accuracy: 0.7520\n",
      "Training loss (for one batch) at step 40: 288.4255, Accuracy: 0.7525\n",
      "Training loss (for one batch) at step 50: 298.7497, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 60: 295.4860, Accuracy: 0.7523\n",
      "Training loss (for one batch) at step 70: 295.9134, Accuracy: 0.7490\n",
      "Training loss (for one batch) at step 80: 307.5943, Accuracy: 0.7463\n",
      "Training loss (for one batch) at step 90: 304.5935, Accuracy: 0.7438\n",
      "Training loss (for one batch) at step 100: 282.4391, Accuracy: 0.7389\n",
      "Training loss (for one batch) at step 110: 289.5727, Accuracy: 0.7373\n",
      "---- Training ----\n",
      "Training loss: 98.8832\n",
      "Training acc over epoch: 0.7374\n",
      "---- Validation ----\n",
      "Validation loss: 32.2533\n",
      "Validation acc: 0.6650\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 322.2044, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 295.3827, Accuracy: 0.7415\n",
      "Training loss (for one batch) at step 20: 295.3696, Accuracy: 0.7448\n",
      "Training loss (for one batch) at step 30: 276.5205, Accuracy: 0.7475\n",
      "Training loss (for one batch) at step 40: 288.7345, Accuracy: 0.7494\n",
      "Training loss (for one batch) at step 50: 272.2676, Accuracy: 0.7495\n",
      "Training loss (for one batch) at step 60: 315.3567, Accuracy: 0.7501\n",
      "Training loss (for one batch) at step 70: 350.1890, Accuracy: 0.7475\n",
      "Training loss (for one batch) at step 80: 318.9521, Accuracy: 0.7456\n",
      "Training loss (for one batch) at step 90: 297.6028, Accuracy: 0.7411\n",
      "Training loss (for one batch) at step 100: 283.3337, Accuracy: 0.7365\n",
      "Training loss (for one batch) at step 110: 321.8297, Accuracy: 0.7347\n",
      "---- Training ----\n",
      "Training loss: 94.8657\n",
      "Training acc over epoch: 0.7342\n",
      "---- Validation ----\n",
      "Validation loss: 50.3894\n",
      "Validation acc: 0.6617\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 302.5546, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 312.5703, Accuracy: 0.7592\n",
      "Training loss (for one batch) at step 20: 285.5103, Accuracy: 0.7533\n",
      "Training loss (for one batch) at step 30: 287.2287, Accuracy: 0.7520\n",
      "Training loss (for one batch) at step 40: 292.4604, Accuracy: 0.7490\n",
      "Training loss (for one batch) at step 50: 285.3104, Accuracy: 0.7512\n",
      "Training loss (for one batch) at step 60: 279.5697, Accuracy: 0.7501\n",
      "Training loss (for one batch) at step 70: 308.5494, Accuracy: 0.7444\n",
      "Training loss (for one batch) at step 80: 303.4756, Accuracy: 0.7448\n",
      "Training loss (for one batch) at step 90: 280.8224, Accuracy: 0.7414\n",
      "Training loss (for one batch) at step 100: 295.0567, Accuracy: 0.7401\n",
      "Training loss (for one batch) at step 110: 283.3124, Accuracy: 0.7373\n",
      "---- Training ----\n",
      "Training loss: 101.4526\n",
      "Training acc over epoch: 0.7379\n",
      "---- Validation ----\n",
      "Validation loss: 39.0102\n",
      "Validation acc: 0.6421\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 309.3577, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 294.7955, Accuracy: 0.7706\n",
      "Training loss (for one batch) at step 20: 319.1114, Accuracy: 0.7641\n",
      "Training loss (for one batch) at step 30: 279.8631, Accuracy: 0.7601\n",
      "Training loss (for one batch) at step 40: 290.9879, Accuracy: 0.7601\n",
      "Training loss (for one batch) at step 50: 272.8564, Accuracy: 0.7561\n",
      "Training loss (for one batch) at step 60: 281.5146, Accuracy: 0.7526\n",
      "Training loss (for one batch) at step 70: 304.6362, Accuracy: 0.7490\n",
      "Training loss (for one batch) at step 80: 295.0808, Accuracy: 0.7499\n",
      "Training loss (for one batch) at step 90: 286.6510, Accuracy: 0.7467\n",
      "Training loss (for one batch) at step 100: 290.8268, Accuracy: 0.7416\n",
      "Training loss (for one batch) at step 110: 298.5180, Accuracy: 0.7384\n",
      "---- Training ----\n",
      "Training loss: 88.1898\n",
      "Training acc over epoch: 0.7381\n",
      "---- Validation ----\n",
      "Validation loss: 39.2933\n",
      "Validation acc: 0.6674\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 311.0900, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 292.0405, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 20: 293.5392, Accuracy: 0.7533\n",
      "Training loss (for one batch) at step 30: 290.6475, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 40: 289.7757, Accuracy: 0.7470\n",
      "Training loss (for one batch) at step 50: 275.5072, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 60: 281.2143, Accuracy: 0.7497\n",
      "Training loss (for one batch) at step 70: 309.7167, Accuracy: 0.7469\n",
      "Training loss (for one batch) at step 80: 304.2426, Accuracy: 0.7461\n",
      "Training loss (for one batch) at step 90: 318.9772, Accuracy: 0.7421\n",
      "Training loss (for one batch) at step 100: 274.6422, Accuracy: 0.7391\n",
      "Training loss (for one batch) at step 110: 293.9503, Accuracy: 0.7368\n",
      "---- Training ----\n",
      "Training loss: 110.6826\n",
      "Training acc over epoch: 0.7365\n",
      "---- Validation ----\n",
      "Validation loss: 48.1530\n",
      "Validation acc: 0.6494\n",
      "Time taken: 10.21s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.6305\n",
      "Validation AUC: 0.6336\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 663.7021, Accuracy: 0.4688\n",
      "Training loss (for one batch) at step 10: 569.3007, Accuracy: 0.5064\n",
      "Training loss (for one batch) at step 20: 582.1025, Accuracy: 0.5138\n",
      "Training loss (for one batch) at step 30: 521.5743, Accuracy: 0.5096\n",
      "Training loss (for one batch) at step 40: 528.0381, Accuracy: 0.5061\n",
      "Training loss (for one batch) at step 50: 505.9241, Accuracy: 0.5110\n",
      "Training loss (for one batch) at step 60: 483.9406, Accuracy: 0.5108\n",
      "Training loss (for one batch) at step 70: 474.0908, Accuracy: 0.5142\n",
      "Training loss (for one batch) at step 80: 483.3331, Accuracy: 0.5163\n",
      "Training loss (for one batch) at step 90: 463.5138, Accuracy: 0.5155\n",
      "Training loss (for one batch) at step 100: 461.5815, Accuracy: 0.5147\n",
      "Training loss (for one batch) at step 110: 470.1223, Accuracy: 0.5143\n",
      "---- Training ----\n",
      "Training loss: 144.7418\n",
      "Training acc over epoch: 0.5152\n",
      "---- Validation ----\n",
      "Validation loss: 35.4003\n",
      "Validation acc: 0.5134\n",
      "Time taken: 11.97s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 450.6696, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 462.6809, Accuracy: 0.5071\n",
      "Training loss (for one batch) at step 20: 460.9600, Accuracy: 0.5019\n",
      "Training loss (for one batch) at step 30: 465.5482, Accuracy: 0.5134\n",
      "Training loss (for one batch) at step 40: 450.0684, Accuracy: 0.5126\n",
      "Training loss (for one batch) at step 50: 450.5599, Accuracy: 0.5178\n",
      "Training loss (for one batch) at step 60: 456.2120, Accuracy: 0.5206\n",
      "Training loss (for one batch) at step 70: 456.0898, Accuracy: 0.5201\n",
      "Training loss (for one batch) at step 80: 448.1545, Accuracy: 0.5225\n",
      "Training loss (for one batch) at step 90: 450.5359, Accuracy: 0.5221\n",
      "Training loss (for one batch) at step 100: 446.4069, Accuracy: 0.5235\n",
      "Training loss (for one batch) at step 110: 449.9756, Accuracy: 0.5228\n",
      "---- Training ----\n",
      "Training loss: 139.6617\n",
      "Training acc over epoch: 0.5242\n",
      "---- Validation ----\n",
      "Validation loss: 34.7035\n",
      "Validation acc: 0.5132\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 450.5297, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 448.0485, Accuracy: 0.5362\n",
      "Training loss (for one batch) at step 20: 450.8372, Accuracy: 0.5301\n",
      "Training loss (for one batch) at step 30: 447.3165, Accuracy: 0.5262\n",
      "Training loss (for one batch) at step 40: 447.1691, Accuracy: 0.5225\n",
      "Training loss (for one batch) at step 50: 445.4091, Accuracy: 0.5208\n",
      "Training loss (for one batch) at step 60: 447.3293, Accuracy: 0.5181\n",
      "Training loss (for one batch) at step 70: 447.1323, Accuracy: 0.5233\n",
      "Training loss (for one batch) at step 80: 448.9774, Accuracy: 0.5247\n",
      "Training loss (for one batch) at step 90: 446.7977, Accuracy: 0.5266\n",
      "Training loss (for one batch) at step 100: 443.9348, Accuracy: 0.5275\n",
      "Training loss (for one batch) at step 110: 444.6086, Accuracy: 0.5279\n",
      "---- Training ----\n",
      "Training loss: 139.4868\n",
      "Training acc over epoch: 0.5269\n",
      "---- Validation ----\n",
      "Validation loss: 34.9119\n",
      "Validation acc: 0.5333\n",
      "Time taken: 10.03s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 448.5752, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 447.4073, Accuracy: 0.5277\n",
      "Training loss (for one batch) at step 20: 443.9382, Accuracy: 0.5279\n",
      "Training loss (for one batch) at step 30: 444.8109, Accuracy: 0.5257\n",
      "Training loss (for one batch) at step 40: 445.6807, Accuracy: 0.5250\n",
      "Training loss (for one batch) at step 50: 446.1371, Accuracy: 0.5314\n",
      "Training loss (for one batch) at step 60: 444.2094, Accuracy: 0.5339\n",
      "Training loss (for one batch) at step 70: 444.6866, Accuracy: 0.5386\n",
      "Training loss (for one batch) at step 80: 443.4943, Accuracy: 0.5377\n",
      "Training loss (for one batch) at step 90: 445.5459, Accuracy: 0.5412\n",
      "Training loss (for one batch) at step 100: 445.9706, Accuracy: 0.5401\n",
      "Training loss (for one batch) at step 110: 442.8855, Accuracy: 0.5412\n",
      "---- Training ----\n",
      "Training loss: 139.3136\n",
      "Training acc over epoch: 0.5410\n",
      "---- Validation ----\n",
      "Validation loss: 34.8422\n",
      "Validation acc: 0.5610\n",
      "Time taken: 10.14s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 442.8604, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 446.4351, Accuracy: 0.5618\n",
      "Training loss (for one batch) at step 20: 444.1230, Accuracy: 0.5495\n",
      "Training loss (for one batch) at step 30: 442.7232, Accuracy: 0.5476\n",
      "Training loss (for one batch) at step 40: 445.1010, Accuracy: 0.5551\n",
      "Training loss (for one batch) at step 50: 439.9209, Accuracy: 0.5654\n",
      "Training loss (for one batch) at step 60: 445.1390, Accuracy: 0.5649\n",
      "Training loss (for one batch) at step 70: 442.4696, Accuracy: 0.5664\n",
      "Training loss (for one batch) at step 80: 444.6961, Accuracy: 0.5680\n",
      "Training loss (for one batch) at step 90: 443.9730, Accuracy: 0.5701\n",
      "Training loss (for one batch) at step 100: 443.9464, Accuracy: 0.5675\n",
      "Training loss (for one batch) at step 110: 443.6108, Accuracy: 0.5682\n",
      "---- Training ----\n",
      "Training loss: 138.6902\n",
      "Training acc over epoch: 0.5696\n",
      "---- Validation ----\n",
      "Validation loss: 34.8949\n",
      "Validation acc: 0.5973\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 442.6758, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 441.0237, Accuracy: 0.5767\n",
      "Training loss (for one batch) at step 20: 442.1043, Accuracy: 0.5744\n",
      "Training loss (for one batch) at step 30: 442.9578, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 40: 443.8965, Accuracy: 0.5791\n",
      "Training loss (for one batch) at step 50: 442.9023, Accuracy: 0.5789\n",
      "Training loss (for one batch) at step 60: 443.7232, Accuracy: 0.5771\n",
      "Training loss (for one batch) at step 70: 441.8370, Accuracy: 0.5800\n",
      "Training loss (for one batch) at step 80: 444.6254, Accuracy: 0.5833\n",
      "Training loss (for one batch) at step 90: 441.2252, Accuracy: 0.5822\n",
      "Training loss (for one batch) at step 100: 440.4703, Accuracy: 0.5792\n",
      "Training loss (for one batch) at step 110: 441.9171, Accuracy: 0.5802\n",
      "---- Training ----\n",
      "Training loss: 139.0486\n",
      "Training acc over epoch: 0.5798\n",
      "---- Validation ----\n",
      "Validation loss: 34.7310\n",
      "Validation acc: 0.6067\n",
      "Time taken: 10.05s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 444.2601, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 442.1172, Accuracy: 0.5916\n",
      "Training loss (for one batch) at step 20: 445.3633, Accuracy: 0.5833\n",
      "Training loss (for one batch) at step 30: 438.4156, Accuracy: 0.5862\n",
      "Training loss (for one batch) at step 40: 445.1159, Accuracy: 0.5850\n",
      "Training loss (for one batch) at step 50: 438.2095, Accuracy: 0.5866\n",
      "Training loss (for one batch) at step 60: 447.1423, Accuracy: 0.5898\n",
      "Training loss (for one batch) at step 70: 439.7545, Accuracy: 0.5935\n",
      "Training loss (for one batch) at step 80: 440.7513, Accuracy: 0.5959\n",
      "Training loss (for one batch) at step 90: 440.8391, Accuracy: 0.5940\n",
      "Training loss (for one batch) at step 100: 440.0795, Accuracy: 0.5943\n",
      "Training loss (for one batch) at step 110: 441.6595, Accuracy: 0.5938\n",
      "---- Training ----\n",
      "Training loss: 136.8214\n",
      "Training acc over epoch: 0.5929\n",
      "---- Validation ----\n",
      "Validation loss: 34.8952\n",
      "Validation acc: 0.5973\n",
      "Time taken: 10.08s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 441.9369, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 444.3557, Accuracy: 0.5732\n",
      "Training loss (for one batch) at step 20: 441.2038, Accuracy: 0.5900\n",
      "Training loss (for one batch) at step 30: 438.1725, Accuracy: 0.5940\n",
      "Training loss (for one batch) at step 40: 440.7638, Accuracy: 0.5991\n",
      "Training loss (for one batch) at step 50: 440.9020, Accuracy: 0.6054\n",
      "Training loss (for one batch) at step 60: 447.6875, Accuracy: 0.6095\n",
      "Training loss (for one batch) at step 70: 444.6173, Accuracy: 0.6151\n",
      "Training loss (for one batch) at step 80: 441.6664, Accuracy: 0.6127\n",
      "Training loss (for one batch) at step 90: 440.8179, Accuracy: 0.6070\n",
      "Training loss (for one batch) at step 100: 439.7823, Accuracy: 0.6078\n",
      "Training loss (for one batch) at step 110: 442.5503, Accuracy: 0.6077\n",
      "---- Training ----\n",
      "Training loss: 139.2324\n",
      "Training acc over epoch: 0.6064\n",
      "---- Validation ----\n",
      "Validation loss: 34.7627\n",
      "Validation acc: 0.6150\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 447.9313, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 445.6032, Accuracy: 0.5881\n",
      "Training loss (for one batch) at step 20: 444.4210, Accuracy: 0.6060\n",
      "Training loss (for one batch) at step 30: 439.1086, Accuracy: 0.6041\n",
      "Training loss (for one batch) at step 40: 436.6384, Accuracy: 0.6004\n",
      "Training loss (for one batch) at step 50: 435.4187, Accuracy: 0.6048\n",
      "Training loss (for one batch) at step 60: 436.3443, Accuracy: 0.6078\n",
      "Training loss (for one batch) at step 70: 442.0896, Accuracy: 0.6104\n",
      "Training loss (for one batch) at step 80: 445.9099, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 90: 441.9162, Accuracy: 0.6049\n",
      "Training loss (for one batch) at step 100: 440.7809, Accuracy: 0.6027\n",
      "Training loss (for one batch) at step 110: 442.5973, Accuracy: 0.6039\n",
      "---- Training ----\n",
      "Training loss: 138.5336\n",
      "Training acc over epoch: 0.6044\n",
      "---- Validation ----\n",
      "Validation loss: 33.6619\n",
      "Validation acc: 0.6212\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 441.4292, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 436.8430, Accuracy: 0.5952\n",
      "Training loss (for one batch) at step 20: 439.7827, Accuracy: 0.5897\n",
      "Training loss (for one batch) at step 30: 436.2083, Accuracy: 0.5940\n",
      "Training loss (for one batch) at step 40: 433.2129, Accuracy: 0.5991\n",
      "Training loss (for one batch) at step 50: 433.1393, Accuracy: 0.6039\n",
      "Training loss (for one batch) at step 60: 431.2503, Accuracy: 0.6121\n",
      "Training loss (for one batch) at step 70: 433.3245, Accuracy: 0.6169\n",
      "Training loss (for one batch) at step 80: 437.5908, Accuracy: 0.6165\n",
      "Training loss (for one batch) at step 90: 437.6587, Accuracy: 0.6116\n",
      "Training loss (for one batch) at step 100: 435.5411, Accuracy: 0.6109\n",
      "Training loss (for one batch) at step 110: 437.0626, Accuracy: 0.6121\n",
      "---- Training ----\n",
      "Training loss: 137.3340\n",
      "Training acc over epoch: 0.6122\n",
      "---- Validation ----\n",
      "Validation loss: 37.0800\n",
      "Validation acc: 0.6059\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 444.3878, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 438.5785, Accuracy: 0.6058\n",
      "Training loss (for one batch) at step 20: 434.2233, Accuracy: 0.5997\n",
      "Training loss (for one batch) at step 30: 436.5424, Accuracy: 0.6162\n",
      "Training loss (for one batch) at step 40: 432.7118, Accuracy: 0.6157\n",
      "Training loss (for one batch) at step 50: 424.5598, Accuracy: 0.6278\n",
      "Training loss (for one batch) at step 60: 441.2408, Accuracy: 0.6335\n",
      "Training loss (for one batch) at step 70: 440.1011, Accuracy: 0.6330\n",
      "Training loss (for one batch) at step 80: 443.7519, Accuracy: 0.6305\n",
      "Training loss (for one batch) at step 90: 439.6105, Accuracy: 0.6242\n",
      "Training loss (for one batch) at step 100: 434.3622, Accuracy: 0.6242\n",
      "Training loss (for one batch) at step 110: 442.7599, Accuracy: 0.6256\n",
      "---- Training ----\n",
      "Training loss: 138.9081\n",
      "Training acc over epoch: 0.6272\n",
      "---- Validation ----\n",
      "Validation loss: 33.8845\n",
      "Validation acc: 0.6061\n",
      "Time taken: 10.15s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 436.1308, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 433.5328, Accuracy: 0.6243\n",
      "Training loss (for one batch) at step 20: 430.7234, Accuracy: 0.6224\n",
      "Training loss (for one batch) at step 30: 426.1125, Accuracy: 0.6310\n",
      "Training loss (for one batch) at step 40: 442.1827, Accuracy: 0.6317\n",
      "Training loss (for one batch) at step 50: 436.9904, Accuracy: 0.6397\n",
      "Training loss (for one batch) at step 60: 432.2014, Accuracy: 0.6441\n",
      "Training loss (for one batch) at step 70: 436.4079, Accuracy: 0.6469\n",
      "Training loss (for one batch) at step 80: 437.3880, Accuracy: 0.6454\n",
      "Training loss (for one batch) at step 90: 434.1469, Accuracy: 0.6405\n",
      "Training loss (for one batch) at step 100: 430.6219, Accuracy: 0.6402\n",
      "Training loss (for one batch) at step 110: 434.1277, Accuracy: 0.6401\n",
      "---- Training ----\n",
      "Training loss: 139.7758\n",
      "Training acc over epoch: 0.6400\n",
      "---- Validation ----\n",
      "Validation loss: 37.7934\n",
      "Validation acc: 0.6467\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 434.7005, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 435.3528, Accuracy: 0.6548\n",
      "Training loss (for one batch) at step 20: 432.5231, Accuracy: 0.6477\n",
      "Training loss (for one batch) at step 30: 432.1494, Accuracy: 0.6497\n",
      "Training loss (for one batch) at step 40: 428.7595, Accuracy: 0.6490\n",
      "Training loss (for one batch) at step 50: 416.2836, Accuracy: 0.6550\n",
      "Training loss (for one batch) at step 60: 429.6351, Accuracy: 0.6580\n",
      "Training loss (for one batch) at step 70: 442.2401, Accuracy: 0.6585\n",
      "Training loss (for one batch) at step 80: 434.9341, Accuracy: 0.6538\n",
      "Training loss (for one batch) at step 90: 429.5804, Accuracy: 0.6492\n",
      "Training loss (for one batch) at step 100: 433.8413, Accuracy: 0.6475\n",
      "Training loss (for one batch) at step 110: 428.6306, Accuracy: 0.6484\n",
      "---- Training ----\n",
      "Training loss: 134.3051\n",
      "Training acc over epoch: 0.6468\n",
      "---- Validation ----\n",
      "Validation loss: 36.8486\n",
      "Validation acc: 0.6373\n",
      "Time taken: 10.58s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 436.2007, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 433.8596, Accuracy: 0.6619\n",
      "Training loss (for one batch) at step 20: 440.5554, Accuracy: 0.6399\n",
      "Training loss (for one batch) at step 30: 437.1726, Accuracy: 0.6396\n",
      "Training loss (for one batch) at step 40: 425.4110, Accuracy: 0.6410\n",
      "Training loss (for one batch) at step 50: 415.4851, Accuracy: 0.6513\n",
      "Training loss (for one batch) at step 60: 436.1294, Accuracy: 0.6574\n",
      "Training loss (for one batch) at step 70: 426.8716, Accuracy: 0.6596\n",
      "Training loss (for one batch) at step 80: 429.9099, Accuracy: 0.6581\n",
      "Training loss (for one batch) at step 90: 438.8788, Accuracy: 0.6550\n",
      "Training loss (for one batch) at step 100: 429.8684, Accuracy: 0.6552\n",
      "Training loss (for one batch) at step 110: 434.3876, Accuracy: 0.6555\n",
      "---- Training ----\n",
      "Training loss: 132.0156\n",
      "Training acc over epoch: 0.6559\n",
      "---- Validation ----\n",
      "Validation loss: 37.0776\n",
      "Validation acc: 0.6483\n",
      "Time taken: 10.15s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 438.4807, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 435.5724, Accuracy: 0.6918\n",
      "Training loss (for one batch) at step 20: 425.5841, Accuracy: 0.6682\n",
      "Training loss (for one batch) at step 30: 430.3568, Accuracy: 0.6699\n",
      "Training loss (for one batch) at step 40: 421.2629, Accuracy: 0.6736\n",
      "Training loss (for one batch) at step 50: 415.5764, Accuracy: 0.6763\n",
      "Training loss (for one batch) at step 60: 422.5192, Accuracy: 0.6815\n",
      "Training loss (for one batch) at step 70: 439.7300, Accuracy: 0.6839\n",
      "Training loss (for one batch) at step 80: 429.1849, Accuracy: 0.6788\n",
      "Training loss (for one batch) at step 90: 428.5197, Accuracy: 0.6741\n",
      "Training loss (for one batch) at step 100: 423.5536, Accuracy: 0.6734\n",
      "Training loss (for one batch) at step 110: 426.0730, Accuracy: 0.6736\n",
      "---- Training ----\n",
      "Training loss: 137.4174\n",
      "Training acc over epoch: 0.6721\n",
      "---- Validation ----\n",
      "Validation loss: 33.9854\n",
      "Validation acc: 0.6701\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 428.8010, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 436.1314, Accuracy: 0.6839\n",
      "Training loss (for one batch) at step 20: 423.6551, Accuracy: 0.6920\n",
      "Training loss (for one batch) at step 30: 422.5031, Accuracy: 0.6913\n",
      "Training loss (for one batch) at step 40: 407.6522, Accuracy: 0.6921\n",
      "Training loss (for one batch) at step 50: 407.3810, Accuracy: 0.6932\n",
      "Training loss (for one batch) at step 60: 423.8334, Accuracy: 0.6972\n",
      "Training loss (for one batch) at step 70: 424.5491, Accuracy: 0.7003\n",
      "Training loss (for one batch) at step 80: 426.3299, Accuracy: 0.6951\n",
      "Training loss (for one batch) at step 90: 421.1412, Accuracy: 0.6896\n",
      "Training loss (for one batch) at step 100: 416.0138, Accuracy: 0.6894\n",
      "Training loss (for one batch) at step 110: 425.2332, Accuracy: 0.6898\n",
      "---- Training ----\n",
      "Training loss: 137.4754\n",
      "Training acc over epoch: 0.6898\n",
      "---- Validation ----\n",
      "Validation loss: 33.5940\n",
      "Validation acc: 0.6615\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 431.1452, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 429.1709, Accuracy: 0.6903\n",
      "Training loss (for one batch) at step 20: 428.3867, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 30: 419.0109, Accuracy: 0.6913\n",
      "Training loss (for one batch) at step 40: 415.3044, Accuracy: 0.6957\n",
      "Training loss (for one batch) at step 50: 404.3796, Accuracy: 0.6987\n",
      "Training loss (for one batch) at step 60: 411.5480, Accuracy: 0.7020\n",
      "Training loss (for one batch) at step 70: 435.3513, Accuracy: 0.7055\n",
      "Training loss (for one batch) at step 80: 429.0415, Accuracy: 0.6990\n",
      "Training loss (for one batch) at step 90: 430.5704, Accuracy: 0.6955\n",
      "Training loss (for one batch) at step 100: 432.6361, Accuracy: 0.6930\n",
      "Training loss (for one batch) at step 110: 427.2781, Accuracy: 0.6948\n",
      "---- Training ----\n",
      "Training loss: 136.9894\n",
      "Training acc over epoch: 0.6949\n",
      "---- Validation ----\n",
      "Validation loss: 34.4486\n",
      "Validation acc: 0.6537\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 442.7137, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 428.2706, Accuracy: 0.6854\n",
      "Training loss (for one batch) at step 20: 420.5049, Accuracy: 0.6752\n",
      "Training loss (for one batch) at step 30: 414.9530, Accuracy: 0.6943\n",
      "Training loss (for one batch) at step 40: 403.0992, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 50: 406.7167, Accuracy: 0.7083\n",
      "Training loss (for one batch) at step 60: 420.2374, Accuracy: 0.7121\n",
      "Training loss (for one batch) at step 70: 427.4975, Accuracy: 0.7088\n",
      "Training loss (for one batch) at step 80: 429.0413, Accuracy: 0.7034\n",
      "Training loss (for one batch) at step 90: 420.8238, Accuracy: 0.6994\n",
      "Training loss (for one batch) at step 100: 423.3096, Accuracy: 0.6970\n",
      "Training loss (for one batch) at step 110: 427.3819, Accuracy: 0.6975\n",
      "---- Training ----\n",
      "Training loss: 130.4847\n",
      "Training acc over epoch: 0.6978\n",
      "---- Validation ----\n",
      "Validation loss: 36.4988\n",
      "Validation acc: 0.6851\n",
      "Time taken: 10.77s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 426.7738, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 417.2828, Accuracy: 0.6690\n",
      "Training loss (for one batch) at step 20: 411.7125, Accuracy: 0.6923\n",
      "Training loss (for one batch) at step 30: 406.0276, Accuracy: 0.7011\n",
      "Training loss (for one batch) at step 40: 400.6768, Accuracy: 0.7046\n",
      "Training loss (for one batch) at step 50: 400.1253, Accuracy: 0.7120\n",
      "Training loss (for one batch) at step 60: 403.8203, Accuracy: 0.7153\n",
      "Training loss (for one batch) at step 70: 428.2497, Accuracy: 0.7151\n",
      "Training loss (for one batch) at step 80: 417.6355, Accuracy: 0.7114\n",
      "Training loss (for one batch) at step 90: 415.8205, Accuracy: 0.7062\n",
      "Training loss (for one batch) at step 100: 425.6503, Accuracy: 0.7041\n",
      "Training loss (for one batch) at step 110: 411.7431, Accuracy: 0.7060\n",
      "---- Training ----\n",
      "Training loss: 130.5102\n",
      "Training acc over epoch: 0.7043\n",
      "---- Validation ----\n",
      "Validation loss: 35.3056\n",
      "Validation acc: 0.6161\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 436.5576, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 418.2631, Accuracy: 0.6903\n",
      "Training loss (for one batch) at step 20: 421.0037, Accuracy: 0.6908\n",
      "Training loss (for one batch) at step 30: 405.6814, Accuracy: 0.7014\n",
      "Training loss (for one batch) at step 40: 397.7584, Accuracy: 0.7094\n",
      "Training loss (for one batch) at step 50: 400.5499, Accuracy: 0.7172\n",
      "Training loss (for one batch) at step 60: 418.4700, Accuracy: 0.7237\n",
      "Training loss (for one batch) at step 70: 427.8373, Accuracy: 0.7250\n",
      "Training loss (for one batch) at step 80: 415.0172, Accuracy: 0.7247\n",
      "Training loss (for one batch) at step 90: 414.6163, Accuracy: 0.7192\n",
      "Training loss (for one batch) at step 100: 407.5984, Accuracy: 0.7171\n",
      "Training loss (for one batch) at step 110: 423.3619, Accuracy: 0.7172\n",
      "---- Training ----\n",
      "Training loss: 125.3512\n",
      "Training acc over epoch: 0.7176\n",
      "---- Validation ----\n",
      "Validation loss: 35.5980\n",
      "Validation acc: 0.6846\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 433.6697, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 418.1132, Accuracy: 0.7067\n",
      "Training loss (for one batch) at step 20: 412.4247, Accuracy: 0.7013\n",
      "Training loss (for one batch) at step 30: 410.1078, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 40: 391.9743, Accuracy: 0.7201\n",
      "Training loss (for one batch) at step 50: 387.9418, Accuracy: 0.7287\n",
      "Training loss (for one batch) at step 60: 412.5370, Accuracy: 0.7358\n",
      "Training loss (for one batch) at step 70: 433.7059, Accuracy: 0.7340\n",
      "Training loss (for one batch) at step 80: 417.1872, Accuracy: 0.7273\n",
      "Training loss (for one batch) at step 90: 420.6959, Accuracy: 0.7247\n",
      "Training loss (for one batch) at step 100: 401.3618, Accuracy: 0.7219\n",
      "Training loss (for one batch) at step 110: 407.3877, Accuracy: 0.7230\n",
      "---- Training ----\n",
      "Training loss: 135.5366\n",
      "Training acc over epoch: 0.7232\n",
      "---- Validation ----\n",
      "Validation loss: 39.8959\n",
      "Validation acc: 0.7123\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 430.1545, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 413.9912, Accuracy: 0.7138\n",
      "Training loss (for one batch) at step 20: 406.3427, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 30: 401.0397, Accuracy: 0.7253\n",
      "Training loss (for one batch) at step 40: 388.5862, Accuracy: 0.7346\n",
      "Training loss (for one batch) at step 50: 387.2912, Accuracy: 0.7425\n",
      "Training loss (for one batch) at step 60: 391.1426, Accuracy: 0.7464\n",
      "Training loss (for one batch) at step 70: 414.7494, Accuracy: 0.7453\n",
      "Training loss (for one batch) at step 80: 414.1846, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 90: 411.7262, Accuracy: 0.7329\n",
      "Training loss (for one batch) at step 100: 398.5624, Accuracy: 0.7287\n",
      "Training loss (for one batch) at step 110: 411.7468, Accuracy: 0.7298\n",
      "---- Training ----\n",
      "Training loss: 134.1483\n",
      "Training acc over epoch: 0.7290\n",
      "---- Validation ----\n",
      "Validation loss: 35.1632\n",
      "Validation acc: 0.6991\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 424.9713, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 418.4156, Accuracy: 0.7067\n",
      "Training loss (for one batch) at step 20: 407.1526, Accuracy: 0.7132\n",
      "Training loss (for one batch) at step 30: 405.9111, Accuracy: 0.7220\n",
      "Training loss (for one batch) at step 40: 398.5603, Accuracy: 0.7288\n",
      "Training loss (for one batch) at step 50: 375.9099, Accuracy: 0.7399\n",
      "Training loss (for one batch) at step 60: 386.8817, Accuracy: 0.7469\n",
      "Training loss (for one batch) at step 70: 415.0237, Accuracy: 0.7472\n",
      "Training loss (for one batch) at step 80: 409.2777, Accuracy: 0.7415\n",
      "Training loss (for one batch) at step 90: 402.7294, Accuracy: 0.7358\n",
      "Training loss (for one batch) at step 100: 404.8314, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 110: 402.9182, Accuracy: 0.7357\n",
      "---- Training ----\n",
      "Training loss: 124.3811\n",
      "Training acc over epoch: 0.7356\n",
      "---- Validation ----\n",
      "Validation loss: 33.6014\n",
      "Validation acc: 0.7096\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 416.4787, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 417.5087, Accuracy: 0.7060\n",
      "Training loss (for one batch) at step 20: 412.1317, Accuracy: 0.7098\n",
      "Training loss (for one batch) at step 30: 390.2678, Accuracy: 0.7235\n",
      "Training loss (for one batch) at step 40: 385.5392, Accuracy: 0.7319\n",
      "Training loss (for one batch) at step 50: 375.2608, Accuracy: 0.7391\n",
      "Training loss (for one batch) at step 60: 410.2508, Accuracy: 0.7446\n",
      "Training loss (for one batch) at step 70: 419.9643, Accuracy: 0.7449\n",
      "Training loss (for one batch) at step 80: 425.1534, Accuracy: 0.7401\n",
      "Training loss (for one batch) at step 90: 406.1425, Accuracy: 0.7358\n",
      "Training loss (for one batch) at step 100: 390.4990, Accuracy: 0.7324\n",
      "Training loss (for one batch) at step 110: 392.0176, Accuracy: 0.7325\n",
      "---- Training ----\n",
      "Training loss: 133.0195\n",
      "Training acc over epoch: 0.7326\n",
      "---- Validation ----\n",
      "Validation loss: 40.6700\n",
      "Validation acc: 0.7144\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 410.6239, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 424.1878, Accuracy: 0.7195\n",
      "Training loss (for one batch) at step 20: 382.6971, Accuracy: 0.7236\n",
      "Training loss (for one batch) at step 30: 392.3646, Accuracy: 0.7349\n",
      "Training loss (for one batch) at step 40: 371.0815, Accuracy: 0.7458\n",
      "Training loss (for one batch) at step 50: 378.0665, Accuracy: 0.7514\n",
      "Training loss (for one batch) at step 60: 374.4962, Accuracy: 0.7559\n",
      "Training loss (for one batch) at step 70: 416.7373, Accuracy: 0.7559\n",
      "Training loss (for one batch) at step 80: 396.7395, Accuracy: 0.7498\n",
      "Training loss (for one batch) at step 90: 403.5280, Accuracy: 0.7446\n",
      "Training loss (for one batch) at step 100: 386.6856, Accuracy: 0.7426\n",
      "Training loss (for one batch) at step 110: 391.3486, Accuracy: 0.7416\n",
      "---- Training ----\n",
      "Training loss: 130.1772\n",
      "Training acc over epoch: 0.7412\n",
      "---- Validation ----\n",
      "Validation loss: 32.0735\n",
      "Validation acc: 0.6983\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 409.2501, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 408.0316, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 20: 400.8018, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 30: 384.8746, Accuracy: 0.7369\n",
      "Training loss (for one batch) at step 40: 370.5472, Accuracy: 0.7456\n",
      "Training loss (for one batch) at step 50: 381.6634, Accuracy: 0.7558\n",
      "Training loss (for one batch) at step 60: 402.2690, Accuracy: 0.7601\n",
      "Training loss (for one batch) at step 70: 416.9225, Accuracy: 0.7574\n",
      "Training loss (for one batch) at step 80: 412.2931, Accuracy: 0.7514\n",
      "Training loss (for one batch) at step 90: 399.5894, Accuracy: 0.7447\n",
      "Training loss (for one batch) at step 100: 382.1000, Accuracy: 0.7431\n",
      "Training loss (for one batch) at step 110: 395.4725, Accuracy: 0.7434\n",
      "---- Training ----\n",
      "Training loss: 118.5548\n",
      "Training acc over epoch: 0.7427\n",
      "---- Validation ----\n",
      "Validation loss: 37.0102\n",
      "Validation acc: 0.6838\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 415.1603, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 399.3870, Accuracy: 0.7216\n",
      "Training loss (for one batch) at step 20: 395.1410, Accuracy: 0.7247\n",
      "Training loss (for one batch) at step 30: 389.6021, Accuracy: 0.7374\n",
      "Training loss (for one batch) at step 40: 369.0714, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 50: 366.8654, Accuracy: 0.7509\n",
      "Training loss (for one batch) at step 60: 372.6302, Accuracy: 0.7567\n",
      "Training loss (for one batch) at step 70: 413.6241, Accuracy: 0.7542\n",
      "Training loss (for one batch) at step 80: 414.9322, Accuracy: 0.7493\n",
      "Training loss (for one batch) at step 90: 396.6947, Accuracy: 0.7459\n",
      "Training loss (for one batch) at step 100: 382.5865, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 110: 384.8836, Accuracy: 0.7448\n",
      "---- Training ----\n",
      "Training loss: 118.0460\n",
      "Training acc over epoch: 0.7444\n",
      "---- Validation ----\n",
      "Validation loss: 42.0728\n",
      "Validation acc: 0.6945\n",
      "Time taken: 10.64s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 413.5034, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 401.1793, Accuracy: 0.7145\n",
      "Training loss (for one batch) at step 20: 374.0310, Accuracy: 0.7392\n",
      "Training loss (for one batch) at step 30: 385.2984, Accuracy: 0.7467\n",
      "Training loss (for one batch) at step 40: 359.0397, Accuracy: 0.7506\n",
      "Training loss (for one batch) at step 50: 365.9477, Accuracy: 0.7609\n",
      "Training loss (for one batch) at step 60: 373.1184, Accuracy: 0.7658\n",
      "Training loss (for one batch) at step 70: 396.6230, Accuracy: 0.7629\n",
      "Training loss (for one batch) at step 80: 397.5962, Accuracy: 0.7568\n",
      "Training loss (for one batch) at step 90: 378.2632, Accuracy: 0.7526\n",
      "Training loss (for one batch) at step 100: 379.6978, Accuracy: 0.7486\n",
      "Training loss (for one batch) at step 110: 392.0343, Accuracy: 0.7461\n",
      "---- Training ----\n",
      "Training loss: 130.3994\n",
      "Training acc over epoch: 0.7454\n",
      "---- Validation ----\n",
      "Validation loss: 38.5594\n",
      "Validation acc: 0.6749\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 408.7630, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 398.5688, Accuracy: 0.7173\n",
      "Training loss (for one batch) at step 20: 376.0312, Accuracy: 0.7150\n",
      "Training loss (for one batch) at step 30: 387.1987, Accuracy: 0.7336\n",
      "Training loss (for one batch) at step 40: 357.8482, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 50: 359.2213, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 60: 383.7346, Accuracy: 0.7585\n",
      "Training loss (for one batch) at step 70: 392.6179, Accuracy: 0.7584\n",
      "Training loss (for one batch) at step 80: 397.8020, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 90: 377.0122, Accuracy: 0.7459\n",
      "Training loss (for one batch) at step 100: 366.4348, Accuracy: 0.7438\n",
      "Training loss (for one batch) at step 110: 392.2663, Accuracy: 0.7430\n",
      "---- Training ----\n",
      "Training loss: 124.0782\n",
      "Training acc over epoch: 0.7432\n",
      "---- Validation ----\n",
      "Validation loss: 43.0854\n",
      "Validation acc: 0.6900\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 411.1837, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 391.7361, Accuracy: 0.7244\n",
      "Training loss (for one batch) at step 20: 371.1907, Accuracy: 0.7284\n",
      "Training loss (for one batch) at step 30: 356.0691, Accuracy: 0.7354\n",
      "Training loss (for one batch) at step 40: 358.0349, Accuracy: 0.7428\n",
      "Training loss (for one batch) at step 50: 342.7026, Accuracy: 0.7518\n",
      "Training loss (for one batch) at step 60: 350.6844, Accuracy: 0.7587\n",
      "Training loss (for one batch) at step 70: 397.9031, Accuracy: 0.7535\n",
      "Training loss (for one batch) at step 80: 404.7753, Accuracy: 0.7489\n",
      "Training loss (for one batch) at step 90: 379.6414, Accuracy: 0.7464\n",
      "Training loss (for one batch) at step 100: 365.7115, Accuracy: 0.7446\n",
      "Training loss (for one batch) at step 110: 385.7398, Accuracy: 0.7455\n",
      "---- Training ----\n",
      "Training loss: 127.6873\n",
      "Training acc over epoch: 0.7448\n",
      "---- Validation ----\n",
      "Validation loss: 34.9027\n",
      "Validation acc: 0.6918\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 394.8307, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 394.3643, Accuracy: 0.6960\n",
      "Training loss (for one batch) at step 20: 371.4312, Accuracy: 0.7281\n",
      "Training loss (for one batch) at step 30: 359.1809, Accuracy: 0.7424\n",
      "Training loss (for one batch) at step 40: 379.8834, Accuracy: 0.7496\n",
      "Training loss (for one batch) at step 50: 351.2717, Accuracy: 0.7598\n",
      "Training loss (for one batch) at step 60: 369.5799, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 70: 378.0134, Accuracy: 0.7602\n",
      "Training loss (for one batch) at step 80: 390.8647, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 90: 378.6500, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 100: 359.8591, Accuracy: 0.7496\n",
      "Training loss (for one batch) at step 110: 378.0765, Accuracy: 0.7488\n",
      "---- Training ----\n",
      "Training loss: 109.9221\n",
      "Training acc over epoch: 0.7475\n",
      "---- Validation ----\n",
      "Validation loss: 40.6270\n",
      "Validation acc: 0.6773\n",
      "Time taken: 10.16s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 419.3578, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 401.8014, Accuracy: 0.7138\n",
      "Training loss (for one batch) at step 20: 374.9875, Accuracy: 0.7307\n",
      "Training loss (for one batch) at step 30: 372.0984, Accuracy: 0.7460\n",
      "Training loss (for one batch) at step 40: 367.8889, Accuracy: 0.7530\n",
      "Training loss (for one batch) at step 50: 351.1367, Accuracy: 0.7586\n",
      "Training loss (for one batch) at step 60: 351.5895, Accuracy: 0.7631\n",
      "Training loss (for one batch) at step 70: 374.6656, Accuracy: 0.7595\n",
      "Training loss (for one batch) at step 80: 396.0317, Accuracy: 0.7518\n",
      "Training loss (for one batch) at step 90: 377.7749, Accuracy: 0.7486\n",
      "Training loss (for one batch) at step 100: 373.2022, Accuracy: 0.7488\n",
      "Training loss (for one batch) at step 110: 372.3305, Accuracy: 0.7462\n",
      "---- Training ----\n",
      "Training loss: 113.4509\n",
      "Training acc over epoch: 0.7456\n",
      "---- Validation ----\n",
      "Validation loss: 35.2879\n",
      "Validation acc: 0.6765\n",
      "Time taken: 10.16s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 399.3830, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 392.2194, Accuracy: 0.7067\n",
      "Training loss (for one batch) at step 20: 363.3861, Accuracy: 0.7329\n",
      "Training loss (for one batch) at step 30: 385.7243, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 40: 357.2561, Accuracy: 0.7475\n",
      "Training loss (for one batch) at step 50: 349.5592, Accuracy: 0.7558\n",
      "Training loss (for one batch) at step 60: 357.6616, Accuracy: 0.7631\n",
      "Training loss (for one batch) at step 70: 371.6621, Accuracy: 0.7597\n",
      "Training loss (for one batch) at step 80: 384.2820, Accuracy: 0.7523\n",
      "Training loss (for one batch) at step 90: 369.7240, Accuracy: 0.7507\n",
      "Training loss (for one batch) at step 100: 360.7113, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 110: 365.5609, Accuracy: 0.7504\n",
      "---- Training ----\n",
      "Training loss: 125.8267\n",
      "Training acc over epoch: 0.7508\n",
      "---- Validation ----\n",
      "Validation loss: 38.5844\n",
      "Validation acc: 0.6677\n",
      "Time taken: 10.65s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 400.0440, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 402.2770, Accuracy: 0.7038\n",
      "Training loss (for one batch) at step 20: 351.1999, Accuracy: 0.7295\n",
      "Training loss (for one batch) at step 30: 366.9500, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 40: 348.2566, Accuracy: 0.7487\n",
      "Training loss (for one batch) at step 50: 340.4953, Accuracy: 0.7569\n",
      "Training loss (for one batch) at step 60: 351.8766, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 70: 371.2355, Accuracy: 0.7580\n",
      "Training loss (for one batch) at step 80: 398.7816, Accuracy: 0.7547\n",
      "Training loss (for one batch) at step 90: 361.3611, Accuracy: 0.7502\n",
      "Training loss (for one batch) at step 100: 347.5350, Accuracy: 0.7495\n",
      "Training loss (for one batch) at step 110: 360.8779, Accuracy: 0.7499\n",
      "---- Training ----\n",
      "Training loss: 116.8437\n",
      "Training acc over epoch: 0.7496\n",
      "---- Validation ----\n",
      "Validation loss: 41.8461\n",
      "Validation acc: 0.6889\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 391.2645, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 375.0045, Accuracy: 0.6982\n",
      "Training loss (for one batch) at step 20: 366.6962, Accuracy: 0.7243\n",
      "Training loss (for one batch) at step 30: 343.7439, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 40: 344.0571, Accuracy: 0.7534\n",
      "Training loss (for one batch) at step 50: 335.6839, Accuracy: 0.7619\n",
      "Training loss (for one batch) at step 60: 349.1256, Accuracy: 0.7659\n",
      "Training loss (for one batch) at step 70: 361.6146, Accuracy: 0.7630\n",
      "Training loss (for one batch) at step 80: 376.7962, Accuracy: 0.7569\n",
      "Training loss (for one batch) at step 90: 360.7408, Accuracy: 0.7530\n",
      "Training loss (for one batch) at step 100: 365.1323, Accuracy: 0.7538\n",
      "Training loss (for one batch) at step 110: 384.1136, Accuracy: 0.7534\n",
      "---- Training ----\n",
      "Training loss: 113.3534\n",
      "Training acc over epoch: 0.7521\n",
      "---- Validation ----\n",
      "Validation loss: 41.1203\n",
      "Validation acc: 0.6784\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 390.2058, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 361.1343, Accuracy: 0.7088\n",
      "Training loss (for one batch) at step 20: 356.8620, Accuracy: 0.7374\n",
      "Training loss (for one batch) at step 30: 351.2252, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 40: 348.7918, Accuracy: 0.7553\n",
      "Training loss (for one batch) at step 50: 332.0066, Accuracy: 0.7639\n",
      "Training loss (for one batch) at step 60: 352.4732, Accuracy: 0.7678\n",
      "Training loss (for one batch) at step 70: 353.9780, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 80: 382.6418, Accuracy: 0.7545\n",
      "Training loss (for one batch) at step 90: 355.1228, Accuracy: 0.7533\n",
      "Training loss (for one batch) at step 100: 348.3502, Accuracy: 0.7522\n",
      "Training loss (for one batch) at step 110: 355.3754, Accuracy: 0.7521\n",
      "---- Training ----\n",
      "Training loss: 111.6803\n",
      "Training acc over epoch: 0.7521\n",
      "---- Validation ----\n",
      "Validation loss: 35.1864\n",
      "Validation acc: 0.6795\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 395.3027, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 374.6564, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 20: 353.9222, Accuracy: 0.7202\n",
      "Training loss (for one batch) at step 30: 339.1243, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 40: 334.3440, Accuracy: 0.7555\n",
      "Training loss (for one batch) at step 50: 326.4557, Accuracy: 0.7667\n",
      "Training loss (for one batch) at step 60: 360.6689, Accuracy: 0.7705\n",
      "Training loss (for one batch) at step 70: 382.7300, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 80: 367.4142, Accuracy: 0.7580\n",
      "Training loss (for one batch) at step 90: 356.4235, Accuracy: 0.7554\n",
      "Training loss (for one batch) at step 100: 344.9121, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 110: 349.2836, Accuracy: 0.7535\n",
      "---- Training ----\n",
      "Training loss: 121.9419\n",
      "Training acc over epoch: 0.7524\n",
      "---- Validation ----\n",
      "Validation loss: 44.7677\n",
      "Validation acc: 0.6934\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 379.9175, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 371.0677, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 20: 341.1926, Accuracy: 0.7262\n",
      "Training loss (for one batch) at step 30: 344.0322, Accuracy: 0.7387\n",
      "Training loss (for one batch) at step 40: 339.6386, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 50: 335.0725, Accuracy: 0.7639\n",
      "Training loss (for one batch) at step 60: 352.4259, Accuracy: 0.7660\n",
      "Training loss (for one batch) at step 70: 365.2875, Accuracy: 0.7622\n",
      "Training loss (for one batch) at step 80: 357.1786, Accuracy: 0.7560\n",
      "Training loss (for one batch) at step 90: 347.4612, Accuracy: 0.7532\n",
      "Training loss (for one batch) at step 100: 351.7831, Accuracy: 0.7535\n",
      "Training loss (for one batch) at step 110: 347.6820, Accuracy: 0.7544\n",
      "---- Training ----\n",
      "Training loss: 122.9373\n",
      "Training acc over epoch: 0.7542\n",
      "---- Validation ----\n",
      "Validation loss: 32.1125\n",
      "Validation acc: 0.6800\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 376.9046, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 371.0364, Accuracy: 0.7287\n",
      "Training loss (for one batch) at step 20: 352.2928, Accuracy: 0.7310\n",
      "Training loss (for one batch) at step 30: 349.3306, Accuracy: 0.7445\n",
      "Training loss (for one batch) at step 40: 340.0315, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 50: 337.7897, Accuracy: 0.7695\n",
      "Training loss (for one batch) at step 60: 350.9785, Accuracy: 0.7722\n",
      "Training loss (for one batch) at step 70: 363.3516, Accuracy: 0.7655\n",
      "Training loss (for one batch) at step 80: 365.8820, Accuracy: 0.7552\n",
      "Training loss (for one batch) at step 90: 344.2045, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 100: 324.4159, Accuracy: 0.7556\n",
      "Training loss (for one batch) at step 110: 353.3992, Accuracy: 0.7535\n",
      "---- Training ----\n",
      "Training loss: 109.2628\n",
      "Training acc over epoch: 0.7540\n",
      "---- Validation ----\n",
      "Validation loss: 41.7525\n",
      "Validation acc: 0.7053\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 385.2599, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 370.7431, Accuracy: 0.7010\n",
      "Training loss (for one batch) at step 20: 351.0382, Accuracy: 0.7247\n",
      "Training loss (for one batch) at step 30: 347.7000, Accuracy: 0.7452\n",
      "Training loss (for one batch) at step 40: 331.8073, Accuracy: 0.7559\n",
      "Training loss (for one batch) at step 50: 347.3528, Accuracy: 0.7653\n",
      "Training loss (for one batch) at step 60: 355.7717, Accuracy: 0.7710\n",
      "Training loss (for one batch) at step 70: 362.9795, Accuracy: 0.7639\n",
      "Training loss (for one batch) at step 80: 364.0773, Accuracy: 0.7534\n",
      "Training loss (for one batch) at step 90: 350.2513, Accuracy: 0.7535\n",
      "Training loss (for one batch) at step 100: 319.9396, Accuracy: 0.7551\n",
      "Training loss (for one batch) at step 110: 348.4787, Accuracy: 0.7539\n",
      "---- Training ----\n",
      "Training loss: 119.1285\n",
      "Training acc over epoch: 0.7532\n",
      "---- Validation ----\n",
      "Validation loss: 43.3369\n",
      "Validation acc: 0.6891\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 369.5861, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 353.6526, Accuracy: 0.7067\n",
      "Training loss (for one batch) at step 20: 335.1964, Accuracy: 0.7273\n",
      "Training loss (for one batch) at step 30: 340.7580, Accuracy: 0.7467\n",
      "Training loss (for one batch) at step 40: 331.8295, Accuracy: 0.7571\n",
      "Training loss (for one batch) at step 50: 321.4400, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 60: 353.5312, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 70: 355.6965, Accuracy: 0.7572\n",
      "Training loss (for one batch) at step 80: 373.1192, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 90: 322.9806, Accuracy: 0.7506\n",
      "Training loss (for one batch) at step 100: 342.9286, Accuracy: 0.7522\n",
      "Training loss (for one batch) at step 110: 351.6188, Accuracy: 0.7521\n",
      "---- Training ----\n",
      "Training loss: 117.7532\n",
      "Training acc over epoch: 0.7519\n",
      "---- Validation ----\n",
      "Validation loss: 31.2508\n",
      "Validation acc: 0.6862\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 374.6069, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 381.2854, Accuracy: 0.6839\n",
      "Training loss (for one batch) at step 20: 332.0117, Accuracy: 0.7206\n",
      "Training loss (for one batch) at step 30: 360.3448, Accuracy: 0.7414\n",
      "Training loss (for one batch) at step 40: 329.2678, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 50: 335.3804, Accuracy: 0.7629\n",
      "Training loss (for one batch) at step 60: 337.6382, Accuracy: 0.7658\n",
      "Training loss (for one batch) at step 70: 370.2955, Accuracy: 0.7613\n",
      "Training loss (for one batch) at step 80: 383.2146, Accuracy: 0.7506\n",
      "Training loss (for one batch) at step 90: 339.3225, Accuracy: 0.7495\n",
      "Training loss (for one batch) at step 100: 335.5649, Accuracy: 0.7498\n",
      "Training loss (for one batch) at step 110: 361.7101, Accuracy: 0.7507\n",
      "---- Training ----\n",
      "Training loss: 114.9278\n",
      "Training acc over epoch: 0.7491\n",
      "---- Validation ----\n",
      "Validation loss: 41.7876\n",
      "Validation acc: 0.6830\n",
      "Time taken: 10.56s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 372.6724, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 375.4147, Accuracy: 0.6918\n",
      "Training loss (for one batch) at step 20: 345.2327, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 30: 323.9414, Accuracy: 0.7364\n",
      "Training loss (for one batch) at step 40: 347.5603, Accuracy: 0.7489\n",
      "Training loss (for one batch) at step 50: 303.4675, Accuracy: 0.7623\n",
      "Training loss (for one batch) at step 60: 333.4393, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 70: 334.4971, Accuracy: 0.7619\n",
      "Training loss (for one batch) at step 80: 354.4160, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 90: 337.2054, Accuracy: 0.7487\n",
      "Training loss (for one batch) at step 100: 345.2218, Accuracy: 0.7489\n",
      "Training loss (for one batch) at step 110: 341.1862, Accuracy: 0.7504\n",
      "---- Training ----\n",
      "Training loss: 117.4801\n",
      "Training acc over epoch: 0.7489\n",
      "---- Validation ----\n",
      "Validation loss: 36.9441\n",
      "Validation acc: 0.6943\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 351.5096, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 353.1699, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 20: 338.7220, Accuracy: 0.7262\n",
      "Training loss (for one batch) at step 30: 344.7357, Accuracy: 0.7434\n",
      "Training loss (for one batch) at step 40: 326.2751, Accuracy: 0.7576\n",
      "Training loss (for one batch) at step 50: 303.1452, Accuracy: 0.7665\n",
      "Training loss (for one batch) at step 60: 323.7726, Accuracy: 0.7691\n",
      "Training loss (for one batch) at step 70: 373.3307, Accuracy: 0.7624\n",
      "Training loss (for one batch) at step 80: 353.9033, Accuracy: 0.7547\n",
      "Training loss (for one batch) at step 90: 337.4256, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 100: 327.9767, Accuracy: 0.7535\n",
      "Training loss (for one batch) at step 110: 349.5918, Accuracy: 0.7533\n",
      "---- Training ----\n",
      "Training loss: 111.9517\n",
      "Training acc over epoch: 0.7521\n",
      "---- Validation ----\n",
      "Validation loss: 36.8829\n",
      "Validation acc: 0.6940\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 363.8643, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 352.9962, Accuracy: 0.6989\n",
      "Training loss (for one batch) at step 20: 335.1543, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 30: 317.0126, Accuracy: 0.7510\n",
      "Training loss (for one batch) at step 40: 317.7714, Accuracy: 0.7630\n",
      "Training loss (for one batch) at step 50: 308.6040, Accuracy: 0.7747\n",
      "Training loss (for one batch) at step 60: 333.0409, Accuracy: 0.7779\n",
      "Training loss (for one batch) at step 70: 350.6427, Accuracy: 0.7682\n",
      "Training loss (for one batch) at step 80: 385.0320, Accuracy: 0.7586\n",
      "Training loss (for one batch) at step 90: 340.3857, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 100: 320.4262, Accuracy: 0.7571\n",
      "Training loss (for one batch) at step 110: 337.4615, Accuracy: 0.7572\n",
      "---- Training ----\n",
      "Training loss: 105.7130\n",
      "Training acc over epoch: 0.7559\n",
      "---- Validation ----\n",
      "Validation loss: 38.1543\n",
      "Validation acc: 0.7002\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 357.0942, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 349.4703, Accuracy: 0.7088\n",
      "Training loss (for one batch) at step 20: 323.8980, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 30: 326.8727, Accuracy: 0.7508\n",
      "Training loss (for one batch) at step 40: 326.5410, Accuracy: 0.7601\n",
      "Training loss (for one batch) at step 50: 307.1497, Accuracy: 0.7688\n",
      "Training loss (for one batch) at step 60: 332.5068, Accuracy: 0.7699\n",
      "Training loss (for one batch) at step 70: 354.2506, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 80: 357.3259, Accuracy: 0.7553\n",
      "Training loss (for one batch) at step 90: 316.8227, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 100: 341.6884, Accuracy: 0.7554\n",
      "Training loss (for one batch) at step 110: 332.7411, Accuracy: 0.7549\n",
      "---- Training ----\n",
      "Training loss: 108.0399\n",
      "Training acc over epoch: 0.7544\n",
      "---- Validation ----\n",
      "Validation loss: 37.9581\n",
      "Validation acc: 0.6980\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 372.9501, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 350.5121, Accuracy: 0.6918\n",
      "Training loss (for one batch) at step 20: 336.4653, Accuracy: 0.7180\n",
      "Training loss (for one batch) at step 30: 316.2896, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 40: 318.7932, Accuracy: 0.7538\n",
      "Training loss (for one batch) at step 50: 307.4390, Accuracy: 0.7655\n",
      "Training loss (for one batch) at step 60: 336.1594, Accuracy: 0.7715\n",
      "Training loss (for one batch) at step 70: 340.7570, Accuracy: 0.7665\n",
      "Training loss (for one batch) at step 80: 355.1631, Accuracy: 0.7570\n",
      "Training loss (for one batch) at step 90: 323.4872, Accuracy: 0.7537\n",
      "Training loss (for one batch) at step 100: 329.6966, Accuracy: 0.7545\n",
      "Training loss (for one batch) at step 110: 329.2394, Accuracy: 0.7549\n",
      "---- Training ----\n",
      "Training loss: 117.9721\n",
      "Training acc over epoch: 0.7536\n",
      "---- Validation ----\n",
      "Validation loss: 39.1125\n",
      "Validation acc: 0.6937\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 355.8158, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 341.5925, Accuracy: 0.6903\n",
      "Training loss (for one batch) at step 20: 324.8574, Accuracy: 0.7214\n",
      "Training loss (for one batch) at step 30: 307.5347, Accuracy: 0.7452\n",
      "Training loss (for one batch) at step 40: 326.0462, Accuracy: 0.7586\n",
      "Training loss (for one batch) at step 50: 310.4897, Accuracy: 0.7676\n",
      "Training loss (for one batch) at step 60: 323.4273, Accuracy: 0.7731\n",
      "Training loss (for one batch) at step 70: 354.4072, Accuracy: 0.7660\n",
      "Training loss (for one batch) at step 80: 360.5613, Accuracy: 0.7562\n",
      "Training loss (for one batch) at step 90: 322.1021, Accuracy: 0.7526\n",
      "Training loss (for one batch) at step 100: 313.3849, Accuracy: 0.7536\n",
      "Training loss (for one batch) at step 110: 332.9023, Accuracy: 0.7533\n",
      "---- Training ----\n",
      "Training loss: 104.6719\n",
      "Training acc over epoch: 0.7532\n",
      "---- Validation ----\n",
      "Validation loss: 44.1213\n",
      "Validation acc: 0.6733\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 366.7753, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 342.7367, Accuracy: 0.6889\n",
      "Training loss (for one batch) at step 20: 337.1324, Accuracy: 0.7091\n",
      "Training loss (for one batch) at step 30: 319.9619, Accuracy: 0.7366\n",
      "Training loss (for one batch) at step 40: 321.6771, Accuracy: 0.7538\n",
      "Training loss (for one batch) at step 50: 316.0632, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 60: 304.5214, Accuracy: 0.7688\n",
      "Training loss (for one batch) at step 70: 344.8359, Accuracy: 0.7619\n",
      "Training loss (for one batch) at step 80: 351.7895, Accuracy: 0.7524\n",
      "Training loss (for one batch) at step 90: 324.9223, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 100: 320.1223, Accuracy: 0.7526\n",
      "Training loss (for one batch) at step 110: 341.4465, Accuracy: 0.7523\n",
      "---- Training ----\n",
      "Training loss: 119.0877\n",
      "Training acc over epoch: 0.7515\n",
      "---- Validation ----\n",
      "Validation loss: 31.0498\n",
      "Validation acc: 0.6690\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 352.0795, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 383.1599, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 20: 325.9447, Accuracy: 0.7228\n",
      "Training loss (for one batch) at step 30: 319.4742, Accuracy: 0.7434\n",
      "Training loss (for one batch) at step 40: 304.2136, Accuracy: 0.7559\n",
      "Training loss (for one batch) at step 50: 324.7296, Accuracy: 0.7707\n",
      "Training loss (for one batch) at step 60: 334.0537, Accuracy: 0.7720\n",
      "Training loss (for one batch) at step 70: 330.8945, Accuracy: 0.7639\n",
      "Training loss (for one batch) at step 80: 345.8087, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 90: 331.7315, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 100: 317.4422, Accuracy: 0.7540\n",
      "Training loss (for one batch) at step 110: 329.5350, Accuracy: 0.7553\n",
      "---- Training ----\n",
      "Training loss: 124.9001\n",
      "Training acc over epoch: 0.7536\n",
      "---- Validation ----\n",
      "Validation loss: 41.4512\n",
      "Validation acc: 0.7026\n",
      "Time taken: 10.16s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 373.8565, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 341.7963, Accuracy: 0.6939\n",
      "Training loss (for one batch) at step 20: 321.7956, Accuracy: 0.7284\n",
      "Training loss (for one batch) at step 30: 300.0091, Accuracy: 0.7465\n",
      "Training loss (for one batch) at step 40: 327.7523, Accuracy: 0.7563\n",
      "Training loss (for one batch) at step 50: 308.5709, Accuracy: 0.7670\n",
      "Training loss (for one batch) at step 60: 325.0793, Accuracy: 0.7714\n",
      "Training loss (for one batch) at step 70: 343.0569, Accuracy: 0.7620\n",
      "Training loss (for one batch) at step 80: 355.6523, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 90: 329.1980, Accuracy: 0.7492\n",
      "Training loss (for one batch) at step 100: 306.9305, Accuracy: 0.7510\n",
      "Training loss (for one batch) at step 110: 346.4615, Accuracy: 0.7513\n",
      "---- Training ----\n",
      "Training loss: 113.4292\n",
      "Training acc over epoch: 0.7503\n",
      "---- Validation ----\n",
      "Validation loss: 32.7925\n",
      "Validation acc: 0.6859\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 351.2208, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 341.5417, Accuracy: 0.7003\n",
      "Training loss (for one batch) at step 20: 314.3345, Accuracy: 0.7258\n",
      "Training loss (for one batch) at step 30: 303.5138, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 40: 305.0654, Accuracy: 0.7540\n",
      "Training loss (for one batch) at step 50: 298.7406, Accuracy: 0.7675\n",
      "Training loss (for one batch) at step 60: 327.0258, Accuracy: 0.7718\n",
      "Training loss (for one batch) at step 70: 353.5963, Accuracy: 0.7629\n",
      "Training loss (for one batch) at step 80: 368.3015, Accuracy: 0.7538\n",
      "Training loss (for one batch) at step 90: 339.2839, Accuracy: 0.7527\n",
      "Training loss (for one batch) at step 100: 337.1728, Accuracy: 0.7512\n",
      "Training loss (for one batch) at step 110: 321.3892, Accuracy: 0.7518\n",
      "---- Training ----\n",
      "Training loss: 106.5690\n",
      "Training acc over epoch: 0.7509\n",
      "---- Validation ----\n",
      "Validation loss: 45.8574\n",
      "Validation acc: 0.6918\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 353.9571, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 339.1958, Accuracy: 0.6861\n",
      "Training loss (for one batch) at step 20: 337.5244, Accuracy: 0.7199\n",
      "Training loss (for one batch) at step 30: 302.4273, Accuracy: 0.7445\n",
      "Training loss (for one batch) at step 40: 292.6620, Accuracy: 0.7574\n",
      "Training loss (for one batch) at step 50: 316.8597, Accuracy: 0.7684\n",
      "Training loss (for one batch) at step 60: 320.8220, Accuracy: 0.7704\n",
      "Training loss (for one batch) at step 70: 348.1825, Accuracy: 0.7629\n",
      "Training loss (for one batch) at step 80: 358.7824, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 90: 316.5591, Accuracy: 0.7514\n",
      "Training loss (for one batch) at step 100: 318.4284, Accuracy: 0.7525\n",
      "Training loss (for one batch) at step 110: 333.8837, Accuracy: 0.7523\n",
      "---- Training ----\n",
      "Training loss: 103.8242\n",
      "Training acc over epoch: 0.7505\n",
      "---- Validation ----\n",
      "Validation loss: 42.8092\n",
      "Validation acc: 0.6902\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 356.3759, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 339.5460, Accuracy: 0.6974\n",
      "Training loss (for one batch) at step 20: 334.2116, Accuracy: 0.7281\n",
      "Training loss (for one batch) at step 30: 306.5176, Accuracy: 0.7470\n",
      "Training loss (for one batch) at step 40: 294.5305, Accuracy: 0.7593\n",
      "Training loss (for one batch) at step 50: 294.4308, Accuracy: 0.7721\n",
      "Training loss (for one batch) at step 60: 318.7600, Accuracy: 0.7751\n",
      "Training loss (for one batch) at step 70: 335.7690, Accuracy: 0.7657\n",
      "Training loss (for one batch) at step 80: 349.4217, Accuracy: 0.7571\n",
      "Training loss (for one batch) at step 90: 341.5479, Accuracy: 0.7539\n",
      "Training loss (for one batch) at step 100: 304.6089, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 110: 319.1872, Accuracy: 0.7541\n",
      "---- Training ----\n",
      "Training loss: 100.6575\n",
      "Training acc over epoch: 0.7538\n",
      "---- Validation ----\n",
      "Validation loss: 51.2585\n",
      "Validation acc: 0.6811\n",
      "Time taken: 10.57s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 367.7793, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 376.2979, Accuracy: 0.6832\n",
      "Training loss (for one batch) at step 20: 338.7646, Accuracy: 0.7135\n",
      "Training loss (for one batch) at step 30: 311.4800, Accuracy: 0.7407\n",
      "Training loss (for one batch) at step 40: 281.0970, Accuracy: 0.7571\n",
      "Training loss (for one batch) at step 50: 319.0191, Accuracy: 0.7675\n",
      "Training loss (for one batch) at step 60: 317.4905, Accuracy: 0.7751\n",
      "Training loss (for one batch) at step 70: 331.8541, Accuracy: 0.7657\n",
      "Training loss (for one batch) at step 80: 340.3990, Accuracy: 0.7565\n",
      "Training loss (for one batch) at step 90: 314.4589, Accuracy: 0.7540\n",
      "Training loss (for one batch) at step 100: 310.3534, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 110: 307.6516, Accuracy: 0.7545\n",
      "---- Training ----\n",
      "Training loss: 116.5363\n",
      "Training acc over epoch: 0.7535\n",
      "---- Validation ----\n",
      "Validation loss: 33.9271\n",
      "Validation acc: 0.6985\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 354.3134, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 361.0988, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 20: 314.0591, Accuracy: 0.7325\n",
      "Training loss (for one batch) at step 30: 302.0490, Accuracy: 0.7535\n",
      "Training loss (for one batch) at step 40: 302.7486, Accuracy: 0.7639\n",
      "Training loss (for one batch) at step 50: 295.0403, Accuracy: 0.7731\n",
      "Training loss (for one batch) at step 60: 315.0533, Accuracy: 0.7755\n",
      "Training loss (for one batch) at step 70: 335.4745, Accuracy: 0.7671\n",
      "Training loss (for one batch) at step 80: 336.8741, Accuracy: 0.7593\n",
      "Training loss (for one batch) at step 90: 326.0643, Accuracy: 0.7570\n",
      "Training loss (for one batch) at step 100: 315.6900, Accuracy: 0.7576\n",
      "Training loss (for one batch) at step 110: 317.4028, Accuracy: 0.7569\n",
      "---- Training ----\n",
      "Training loss: 95.3749\n",
      "Training acc over epoch: 0.7556\n",
      "---- Validation ----\n",
      "Validation loss: 50.1396\n",
      "Validation acc: 0.6996\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 341.0829, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 350.5965, Accuracy: 0.6882\n",
      "Training loss (for one batch) at step 20: 313.9971, Accuracy: 0.7173\n",
      "Training loss (for one batch) at step 30: 316.8628, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 40: 306.3705, Accuracy: 0.7536\n",
      "Training loss (for one batch) at step 50: 289.1671, Accuracy: 0.7667\n",
      "Training loss (for one batch) at step 60: 314.3304, Accuracy: 0.7716\n",
      "Training loss (for one batch) at step 70: 319.9719, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 80: 324.4835, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 90: 319.7314, Accuracy: 0.7525\n",
      "Training loss (for one batch) at step 100: 294.1039, Accuracy: 0.7532\n",
      "Training loss (for one batch) at step 110: 325.7800, Accuracy: 0.7534\n",
      "---- Training ----\n",
      "Training loss: 114.9279\n",
      "Training acc over epoch: 0.7521\n",
      "---- Validation ----\n",
      "Validation loss: 44.2129\n",
      "Validation acc: 0.6819\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 352.2277, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 350.4806, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 20: 313.1894, Accuracy: 0.7169\n",
      "Training loss (for one batch) at step 30: 289.6422, Accuracy: 0.7427\n",
      "Training loss (for one batch) at step 40: 300.6945, Accuracy: 0.7586\n",
      "Training loss (for one batch) at step 50: 295.1687, Accuracy: 0.7667\n",
      "Training loss (for one batch) at step 60: 303.0460, Accuracy: 0.7716\n",
      "Training loss (for one batch) at step 70: 334.7166, Accuracy: 0.7632\n",
      "Training loss (for one batch) at step 80: 328.6739, Accuracy: 0.7545\n",
      "Training loss (for one batch) at step 90: 314.8644, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 100: 296.4387, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 110: 333.2069, Accuracy: 0.7527\n",
      "---- Training ----\n",
      "Training loss: 103.1613\n",
      "Training acc over epoch: 0.7519\n",
      "---- Validation ----\n",
      "Validation loss: 48.0522\n",
      "Validation acc: 0.6848\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 358.4669, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 363.9837, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 20: 304.1876, Accuracy: 0.7225\n",
      "Training loss (for one batch) at step 30: 301.7701, Accuracy: 0.7417\n",
      "Training loss (for one batch) at step 40: 291.9177, Accuracy: 0.7584\n",
      "Training loss (for one batch) at step 50: 293.5009, Accuracy: 0.7659\n",
      "Training loss (for one batch) at step 60: 308.0012, Accuracy: 0.7716\n",
      "Training loss (for one batch) at step 70: 321.3762, Accuracy: 0.7655\n",
      "Training loss (for one batch) at step 80: 357.6469, Accuracy: 0.7569\n",
      "Training loss (for one batch) at step 90: 305.7450, Accuracy: 0.7540\n",
      "Training loss (for one batch) at step 100: 294.1299, Accuracy: 0.7545\n",
      "Training loss (for one batch) at step 110: 312.3615, Accuracy: 0.7565\n",
      "---- Training ----\n",
      "Training loss: 103.2281\n",
      "Training acc over epoch: 0.7538\n",
      "---- Validation ----\n",
      "Validation loss: 35.8602\n",
      "Validation acc: 0.7039\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 355.7466, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 338.0838, Accuracy: 0.7145\n",
      "Training loss (for one batch) at step 20: 299.8987, Accuracy: 0.7277\n",
      "Training loss (for one batch) at step 30: 301.5497, Accuracy: 0.7462\n",
      "Training loss (for one batch) at step 40: 293.6950, Accuracy: 0.7580\n",
      "Training loss (for one batch) at step 50: 320.5386, Accuracy: 0.7687\n",
      "Training loss (for one batch) at step 60: 315.8567, Accuracy: 0.7727\n",
      "Training loss (for one batch) at step 70: 321.4410, Accuracy: 0.7652\n",
      "Training loss (for one batch) at step 80: 325.9498, Accuracy: 0.7551\n",
      "Training loss (for one batch) at step 90: 308.9798, Accuracy: 0.7518\n",
      "Training loss (for one batch) at step 100: 299.2043, Accuracy: 0.7527\n",
      "Training loss (for one batch) at step 110: 336.7219, Accuracy: 0.7537\n",
      "---- Training ----\n",
      "Training loss: 104.4141\n",
      "Training acc over epoch: 0.7517\n",
      "---- Validation ----\n",
      "Validation loss: 52.0789\n",
      "Validation acc: 0.6908\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 357.2690, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 340.7491, Accuracy: 0.6918\n",
      "Training loss (for one batch) at step 20: 313.9897, Accuracy: 0.7176\n",
      "Training loss (for one batch) at step 30: 304.5096, Accuracy: 0.7417\n",
      "Training loss (for one batch) at step 40: 292.4609, Accuracy: 0.7496\n",
      "Training loss (for one batch) at step 50: 298.4927, Accuracy: 0.7623\n",
      "Training loss (for one batch) at step 60: 315.1769, Accuracy: 0.7669\n",
      "Training loss (for one batch) at step 70: 324.0050, Accuracy: 0.7595\n",
      "Training loss (for one batch) at step 80: 344.3671, Accuracy: 0.7503\n",
      "Training loss (for one batch) at step 90: 305.2924, Accuracy: 0.7482\n",
      "Training loss (for one batch) at step 100: 296.1748, Accuracy: 0.7506\n",
      "Training loss (for one batch) at step 110: 303.7892, Accuracy: 0.7494\n",
      "---- Training ----\n",
      "Training loss: 102.3369\n",
      "Training acc over epoch: 0.7486\n",
      "---- Validation ----\n",
      "Validation loss: 30.1923\n",
      "Validation acc: 0.7010\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 344.1755, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 337.5914, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 20: 296.1193, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 30: 313.0820, Accuracy: 0.7402\n",
      "Training loss (for one batch) at step 40: 290.2390, Accuracy: 0.7527\n",
      "Training loss (for one batch) at step 50: 283.8004, Accuracy: 0.7667\n",
      "Training loss (for one batch) at step 60: 301.8099, Accuracy: 0.7710\n",
      "Training loss (for one batch) at step 70: 319.8844, Accuracy: 0.7651\n",
      "Training loss (for one batch) at step 80: 323.6378, Accuracy: 0.7547\n",
      "Training loss (for one batch) at step 90: 304.2702, Accuracy: 0.7514\n",
      "Training loss (for one batch) at step 100: 295.6258, Accuracy: 0.7523\n",
      "Training loss (for one batch) at step 110: 349.2973, Accuracy: 0.7524\n",
      "---- Training ----\n",
      "Training loss: 101.9583\n",
      "Training acc over epoch: 0.7519\n",
      "---- Validation ----\n",
      "Validation loss: 46.4087\n",
      "Validation acc: 0.6980\n",
      "Time taken: 10.14s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 348.2493, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 317.6130, Accuracy: 0.7010\n",
      "Training loss (for one batch) at step 20: 304.4251, Accuracy: 0.7232\n",
      "Training loss (for one batch) at step 30: 313.6871, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 40: 296.0165, Accuracy: 0.7565\n",
      "Training loss (for one batch) at step 50: 279.0783, Accuracy: 0.7685\n",
      "Training loss (for one batch) at step 60: 313.9951, Accuracy: 0.7713\n",
      "Training loss (for one batch) at step 70: 333.5574, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 80: 340.6492, Accuracy: 0.7556\n",
      "Training loss (for one batch) at step 90: 309.6396, Accuracy: 0.7522\n",
      "Training loss (for one batch) at step 100: 297.9110, Accuracy: 0.7539\n",
      "Training loss (for one batch) at step 110: 321.0871, Accuracy: 0.7544\n",
      "---- Training ----\n",
      "Training loss: 104.2493\n",
      "Training acc over epoch: 0.7528\n",
      "---- Validation ----\n",
      "Validation loss: 42.7814\n",
      "Validation acc: 0.6999\n",
      "Time taken: 10.66s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 331.0708, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 339.6577, Accuracy: 0.6939\n",
      "Training loss (for one batch) at step 20: 310.0713, Accuracy: 0.7180\n",
      "Training loss (for one batch) at step 30: 301.3274, Accuracy: 0.7455\n",
      "Training loss (for one batch) at step 40: 340.5249, Accuracy: 0.7597\n",
      "Training loss (for one batch) at step 50: 279.2740, Accuracy: 0.7733\n",
      "Training loss (for one batch) at step 60: 350.3393, Accuracy: 0.7742\n",
      "Training loss (for one batch) at step 70: 312.4897, Accuracy: 0.7667\n",
      "Training loss (for one batch) at step 80: 325.0819, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 90: 319.9326, Accuracy: 0.7522\n",
      "Training loss (for one batch) at step 100: 297.1914, Accuracy: 0.7538\n",
      "Training loss (for one batch) at step 110: 325.0528, Accuracy: 0.7545\n",
      "---- Training ----\n",
      "Training loss: 105.1175\n",
      "Training acc over epoch: 0.7532\n",
      "---- Validation ----\n",
      "Validation loss: 45.7651\n",
      "Validation acc: 0.6913\n",
      "Time taken: 10.13s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 342.0926, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 351.3564, Accuracy: 0.6861\n",
      "Training loss (for one batch) at step 20: 319.0421, Accuracy: 0.7165\n",
      "Training loss (for one batch) at step 30: 296.2394, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 40: 310.4292, Accuracy: 0.7555\n",
      "Training loss (for one batch) at step 50: 295.0598, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 60: 303.6240, Accuracy: 0.7724\n",
      "Training loss (for one batch) at step 70: 320.9549, Accuracy: 0.7602\n",
      "Training loss (for one batch) at step 80: 330.5990, Accuracy: 0.7537\n",
      "Training loss (for one batch) at step 90: 309.4531, Accuracy: 0.7514\n",
      "Training loss (for one batch) at step 100: 297.3956, Accuracy: 0.7527\n",
      "Training loss (for one batch) at step 110: 305.0123, Accuracy: 0.7525\n",
      "---- Training ----\n",
      "Training loss: 101.5012\n",
      "Training acc over epoch: 0.7520\n",
      "---- Validation ----\n",
      "Validation loss: 44.2623\n",
      "Validation acc: 0.6929\n",
      "Time taken: 10.07s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 341.1099, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 336.4284, Accuracy: 0.6783\n",
      "Training loss (for one batch) at step 20: 304.6883, Accuracy: 0.7094\n",
      "Training loss (for one batch) at step 30: 299.9514, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 40: 286.5500, Accuracy: 0.7473\n",
      "Training loss (for one batch) at step 50: 291.8822, Accuracy: 0.7632\n",
      "Training loss (for one batch) at step 60: 323.7610, Accuracy: 0.7663\n",
      "Training loss (for one batch) at step 70: 317.9201, Accuracy: 0.7622\n",
      "Training loss (for one batch) at step 80: 327.8139, Accuracy: 0.7535\n",
      "Training loss (for one batch) at step 90: 293.8787, Accuracy: 0.7494\n",
      "Training loss (for one batch) at step 100: 312.8730, Accuracy: 0.7514\n",
      "Training loss (for one batch) at step 110: 327.9738, Accuracy: 0.7524\n",
      "---- Training ----\n",
      "Training loss: 103.9020\n",
      "Training acc over epoch: 0.7520\n",
      "---- Validation ----\n",
      "Validation loss: 53.2206\n",
      "Validation acc: 0.6967\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 348.9604, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 344.8423, Accuracy: 0.6825\n",
      "Training loss (for one batch) at step 20: 309.4009, Accuracy: 0.7217\n",
      "Training loss (for one batch) at step 30: 305.8912, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 40: 284.4662, Accuracy: 0.7571\n",
      "Training loss (for one batch) at step 50: 272.2628, Accuracy: 0.7702\n",
      "Training loss (for one batch) at step 60: 315.4760, Accuracy: 0.7722\n",
      "Training loss (for one batch) at step 70: 318.0171, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 80: 330.5743, Accuracy: 0.7547\n",
      "Training loss (for one batch) at step 90: 294.9149, Accuracy: 0.7511\n",
      "Training loss (for one batch) at step 100: 294.9268, Accuracy: 0.7514\n",
      "Training loss (for one batch) at step 110: 330.7784, Accuracy: 0.7506\n",
      "---- Training ----\n",
      "Training loss: 97.3476\n",
      "Training acc over epoch: 0.7493\n",
      "---- Validation ----\n",
      "Validation loss: 40.0458\n",
      "Validation acc: 0.7002\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 332.1970, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 332.6474, Accuracy: 0.6939\n",
      "Training loss (for one batch) at step 20: 318.9781, Accuracy: 0.7180\n",
      "Training loss (for one batch) at step 30: 300.8803, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 40: 297.4854, Accuracy: 0.7553\n",
      "Training loss (for one batch) at step 50: 282.9436, Accuracy: 0.7659\n",
      "Training loss (for one batch) at step 60: 323.0936, Accuracy: 0.7700\n",
      "Training loss (for one batch) at step 70: 312.0949, Accuracy: 0.7660\n",
      "Training loss (for one batch) at step 80: 317.7813, Accuracy: 0.7547\n",
      "Training loss (for one batch) at step 90: 309.1336, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 100: 295.5703, Accuracy: 0.7532\n",
      "Training loss (for one batch) at step 110: 300.0564, Accuracy: 0.7518\n",
      "---- Training ----\n",
      "Training loss: 104.4287\n",
      "Training acc over epoch: 0.7508\n",
      "---- Validation ----\n",
      "Validation loss: 32.4774\n",
      "Validation acc: 0.7004\n",
      "Time taken: 10.14s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 332.7697, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 327.0623, Accuracy: 0.6996\n",
      "Training loss (for one batch) at step 20: 301.3920, Accuracy: 0.7281\n",
      "Training loss (for one batch) at step 30: 272.7277, Accuracy: 0.7472\n",
      "Training loss (for one batch) at step 40: 295.6035, Accuracy: 0.7590\n",
      "Training loss (for one batch) at step 50: 285.9782, Accuracy: 0.7688\n",
      "Training loss (for one batch) at step 60: 298.4553, Accuracy: 0.7731\n",
      "Training loss (for one batch) at step 70: 339.5793, Accuracy: 0.7664\n",
      "Training loss (for one batch) at step 80: 317.1331, Accuracy: 0.7554\n",
      "Training loss (for one batch) at step 90: 300.7565, Accuracy: 0.7507\n",
      "Training loss (for one batch) at step 100: 300.6960, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 110: 309.0883, Accuracy: 0.7550\n",
      "---- Training ----\n",
      "Training loss: 93.5881\n",
      "Training acc over epoch: 0.7534\n",
      "---- Validation ----\n",
      "Validation loss: 49.1989\n",
      "Validation acc: 0.6980\n",
      "Time taken: 10.62s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 337.4265, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 337.7661, Accuracy: 0.6982\n",
      "Training loss (for one batch) at step 20: 321.4695, Accuracy: 0.7240\n",
      "Training loss (for one batch) at step 30: 295.7387, Accuracy: 0.7477\n",
      "Training loss (for one batch) at step 40: 280.3942, Accuracy: 0.7620\n",
      "Training loss (for one batch) at step 50: 268.2946, Accuracy: 0.7730\n",
      "Training loss (for one batch) at step 60: 311.5232, Accuracy: 0.7752\n",
      "Training loss (for one batch) at step 70: 321.3271, Accuracy: 0.7691\n",
      "Training loss (for one batch) at step 80: 343.9474, Accuracy: 0.7584\n",
      "Training loss (for one batch) at step 90: 286.1086, Accuracy: 0.7536\n",
      "Training loss (for one batch) at step 100: 313.3955, Accuracy: 0.7555\n",
      "Training loss (for one batch) at step 110: 311.2775, Accuracy: 0.7544\n",
      "---- Training ----\n",
      "Training loss: 105.8895\n",
      "Training acc over epoch: 0.7532\n",
      "---- Validation ----\n",
      "Validation loss: 40.1517\n",
      "Validation acc: 0.6999\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 343.2919, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 323.7204, Accuracy: 0.6960\n",
      "Training loss (for one batch) at step 20: 317.0036, Accuracy: 0.7236\n",
      "Training loss (for one batch) at step 30: 293.4860, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 40: 286.8520, Accuracy: 0.7572\n",
      "Training loss (for one batch) at step 50: 283.1574, Accuracy: 0.7684\n",
      "Training loss (for one batch) at step 60: 304.8441, Accuracy: 0.7718\n",
      "Training loss (for one batch) at step 70: 327.1876, Accuracy: 0.7640\n",
      "Training loss (for one batch) at step 80: 315.7877, Accuracy: 0.7524\n",
      "Training loss (for one batch) at step 90: 305.8164, Accuracy: 0.7520\n",
      "Training loss (for one batch) at step 100: 295.1621, Accuracy: 0.7527\n",
      "Training loss (for one batch) at step 110: 315.7031, Accuracy: 0.7531\n",
      "---- Training ----\n",
      "Training loss: 102.6144\n",
      "Training acc over epoch: 0.7518\n",
      "---- Validation ----\n",
      "Validation loss: 35.4389\n",
      "Validation acc: 0.6937\n",
      "Time taken: 10.12s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 320.4113, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 323.8503, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 20: 305.7194, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 30: 291.8065, Accuracy: 0.7384\n",
      "Training loss (for one batch) at step 40: 291.8591, Accuracy: 0.7532\n",
      "Training loss (for one batch) at step 50: 285.4498, Accuracy: 0.7638\n",
      "Training loss (for one batch) at step 60: 309.4275, Accuracy: 0.7684\n",
      "Training loss (for one batch) at step 70: 316.6344, Accuracy: 0.7625\n",
      "Training loss (for one batch) at step 80: 311.8987, Accuracy: 0.7514\n",
      "Training loss (for one batch) at step 90: 314.4881, Accuracy: 0.7497\n",
      "Training loss (for one batch) at step 100: 299.7787, Accuracy: 0.7509\n",
      "Training loss (for one batch) at step 110: 305.2691, Accuracy: 0.7515\n",
      "---- Training ----\n",
      "Training loss: 104.2476\n",
      "Training acc over epoch: 0.7501\n",
      "---- Validation ----\n",
      "Validation loss: 33.8273\n",
      "Validation acc: 0.6822\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 315.3707, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 321.0461, Accuracy: 0.6903\n",
      "Training loss (for one batch) at step 20: 313.0264, Accuracy: 0.7169\n",
      "Training loss (for one batch) at step 30: 276.1405, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 40: 294.1291, Accuracy: 0.7563\n",
      "Training loss (for one batch) at step 50: 282.1005, Accuracy: 0.7669\n",
      "Training loss (for one batch) at step 60: 307.9624, Accuracy: 0.7719\n",
      "Training loss (for one batch) at step 70: 322.8593, Accuracy: 0.7650\n",
      "Training loss (for one batch) at step 80: 314.4372, Accuracy: 0.7564\n",
      "Training loss (for one batch) at step 90: 293.4505, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 100: 277.9779, Accuracy: 0.7559\n",
      "Training loss (for one batch) at step 110: 307.7856, Accuracy: 0.7559\n",
      "---- Training ----\n",
      "Training loss: 104.2059\n",
      "Training acc over epoch: 0.7535\n",
      "---- Validation ----\n",
      "Validation loss: 38.5089\n",
      "Validation acc: 0.6994\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 315.6552, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 334.5103, Accuracy: 0.6683\n",
      "Training loss (for one batch) at step 20: 292.4491, Accuracy: 0.7121\n",
      "Training loss (for one batch) at step 30: 300.3046, Accuracy: 0.7404\n",
      "Training loss (for one batch) at step 40: 290.9356, Accuracy: 0.7555\n",
      "Training loss (for one batch) at step 50: 279.0062, Accuracy: 0.7639\n",
      "Training loss (for one batch) at step 60: 297.6472, Accuracy: 0.7682\n",
      "Training loss (for one batch) at step 70: 326.6827, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 80: 299.5053, Accuracy: 0.7542\n",
      "Training loss (for one batch) at step 90: 284.6152, Accuracy: 0.7510\n",
      "Training loss (for one batch) at step 100: 318.7424, Accuracy: 0.7505\n",
      "Training loss (for one batch) at step 110: 292.5221, Accuracy: 0.7512\n",
      "---- Training ----\n",
      "Training loss: 91.9711\n",
      "Training acc over epoch: 0.7503\n",
      "---- Validation ----\n",
      "Validation loss: 31.8386\n",
      "Validation acc: 0.6808\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 357.4974, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 323.2238, Accuracy: 0.6754\n",
      "Training loss (for one batch) at step 20: 309.2852, Accuracy: 0.7139\n",
      "Training loss (for one batch) at step 30: 328.8658, Accuracy: 0.7407\n",
      "Training loss (for one batch) at step 40: 292.2465, Accuracy: 0.7538\n",
      "Training loss (for one batch) at step 50: 292.7652, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 60: 310.0783, Accuracy: 0.7697\n",
      "Training loss (for one batch) at step 70: 293.8071, Accuracy: 0.7612\n",
      "Training loss (for one batch) at step 80: 321.6529, Accuracy: 0.7511\n",
      "Training loss (for one batch) at step 90: 299.3210, Accuracy: 0.7470\n",
      "Training loss (for one batch) at step 100: 293.9133, Accuracy: 0.7492\n",
      "Training loss (for one batch) at step 110: 300.1160, Accuracy: 0.7496\n",
      "---- Training ----\n",
      "Training loss: 97.6513\n",
      "Training acc over epoch: 0.7502\n",
      "---- Validation ----\n",
      "Validation loss: 60.3163\n",
      "Validation acc: 0.6953\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 333.1592, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 319.0770, Accuracy: 0.6676\n",
      "Training loss (for one batch) at step 20: 288.1611, Accuracy: 0.7087\n",
      "Training loss (for one batch) at step 30: 294.7948, Accuracy: 0.7374\n",
      "Training loss (for one batch) at step 40: 284.7494, Accuracy: 0.7542\n",
      "Training loss (for one batch) at step 50: 285.2418, Accuracy: 0.7672\n",
      "Training loss (for one batch) at step 60: 295.4828, Accuracy: 0.7737\n",
      "Training loss (for one batch) at step 70: 323.7865, Accuracy: 0.7669\n",
      "Training loss (for one batch) at step 80: 348.7825, Accuracy: 0.7583\n",
      "Training loss (for one batch) at step 90: 310.4547, Accuracy: 0.7570\n",
      "Training loss (for one batch) at step 100: 284.4122, Accuracy: 0.7560\n",
      "Training loss (for one batch) at step 110: 298.9834, Accuracy: 0.7561\n",
      "---- Training ----\n",
      "Training loss: 112.7920\n",
      "Training acc over epoch: 0.7550\n",
      "---- Validation ----\n",
      "Validation loss: 50.3582\n",
      "Validation acc: 0.6964\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 337.9811, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 302.8444, Accuracy: 0.6946\n",
      "Training loss (for one batch) at step 20: 304.4630, Accuracy: 0.7214\n",
      "Training loss (for one batch) at step 30: 280.2640, Accuracy: 0.7492\n",
      "Training loss (for one batch) at step 40: 285.6111, Accuracy: 0.7614\n",
      "Training loss (for one batch) at step 50: 285.5288, Accuracy: 0.7718\n",
      "Training loss (for one batch) at step 60: 275.6738, Accuracy: 0.7756\n",
      "Training loss (for one batch) at step 70: 335.1957, Accuracy: 0.7680\n",
      "Training loss (for one batch) at step 80: 354.3445, Accuracy: 0.7582\n",
      "Training loss (for one batch) at step 90: 295.6436, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 100: 286.4712, Accuracy: 0.7558\n",
      "Training loss (for one batch) at step 110: 314.7121, Accuracy: 0.7546\n",
      "---- Training ----\n",
      "Training loss: 91.5694\n",
      "Training acc over epoch: 0.7534\n",
      "---- Validation ----\n",
      "Validation loss: 53.8415\n",
      "Validation acc: 0.6969\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 333.0592, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 306.7878, Accuracy: 0.6662\n",
      "Training loss (for one batch) at step 20: 289.8149, Accuracy: 0.7039\n",
      "Training loss (for one batch) at step 30: 282.8567, Accuracy: 0.7354\n",
      "Training loss (for one batch) at step 40: 285.3147, Accuracy: 0.7510\n",
      "Training loss (for one batch) at step 50: 286.2666, Accuracy: 0.7609\n",
      "Training loss (for one batch) at step 60: 287.8332, Accuracy: 0.7670\n",
      "Training loss (for one batch) at step 70: 322.2847, Accuracy: 0.7611\n",
      "Training loss (for one batch) at step 80: 301.7973, Accuracy: 0.7535\n",
      "Training loss (for one batch) at step 90: 305.7020, Accuracy: 0.7514\n",
      "Training loss (for one batch) at step 100: 280.2545, Accuracy: 0.7510\n",
      "Training loss (for one batch) at step 110: 302.4895, Accuracy: 0.7504\n",
      "---- Training ----\n",
      "Training loss: 98.3559\n",
      "Training acc over epoch: 0.7497\n",
      "---- Validation ----\n",
      "Validation loss: 60.0509\n",
      "Validation acc: 0.6956\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 321.4366, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 323.6010, Accuracy: 0.6818\n",
      "Training loss (for one batch) at step 20: 299.8796, Accuracy: 0.7173\n",
      "Training loss (for one batch) at step 30: 275.5125, Accuracy: 0.7485\n",
      "Training loss (for one batch) at step 40: 281.7271, Accuracy: 0.7611\n",
      "Training loss (for one batch) at step 50: 295.0941, Accuracy: 0.7719\n",
      "Training loss (for one batch) at step 60: 327.3012, Accuracy: 0.7731\n",
      "Training loss (for one batch) at step 70: 323.5993, Accuracy: 0.7655\n",
      "Training loss (for one batch) at step 80: 319.1671, Accuracy: 0.7566\n",
      "Training loss (for one batch) at step 90: 286.6779, Accuracy: 0.7522\n",
      "Training loss (for one batch) at step 100: 291.0898, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 110: 321.6329, Accuracy: 0.7559\n",
      "---- Training ----\n",
      "Training loss: 111.4799\n",
      "Training acc over epoch: 0.7535\n",
      "---- Validation ----\n",
      "Validation loss: 45.7902\n",
      "Validation acc: 0.6805\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 339.0048, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 319.1252, Accuracy: 0.6832\n",
      "Training loss (for one batch) at step 20: 296.4687, Accuracy: 0.7191\n",
      "Training loss (for one batch) at step 30: 275.8578, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 40: 289.5662, Accuracy: 0.7563\n",
      "Training loss (for one batch) at step 50: 266.3038, Accuracy: 0.7684\n",
      "Training loss (for one batch) at step 60: 277.7143, Accuracy: 0.7711\n",
      "Training loss (for one batch) at step 70: 292.7474, Accuracy: 0.7654\n",
      "Training loss (for one batch) at step 80: 311.2780, Accuracy: 0.7546\n",
      "Training loss (for one batch) at step 90: 275.0514, Accuracy: 0.7518\n",
      "Training loss (for one batch) at step 100: 272.1415, Accuracy: 0.7527\n",
      "Training loss (for one batch) at step 110: 303.7529, Accuracy: 0.7535\n",
      "---- Training ----\n",
      "Training loss: 104.6854\n",
      "Training acc over epoch: 0.7523\n",
      "---- Validation ----\n",
      "Validation loss: 39.3520\n",
      "Validation acc: 0.7039\n",
      "Time taken: 10.12s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 331.2811, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 310.6634, Accuracy: 0.6740\n",
      "Training loss (for one batch) at step 20: 292.9812, Accuracy: 0.7143\n",
      "Training loss (for one batch) at step 30: 273.6328, Accuracy: 0.7442\n",
      "Training loss (for one batch) at step 40: 292.9985, Accuracy: 0.7574\n",
      "Training loss (for one batch) at step 50: 268.2719, Accuracy: 0.7675\n",
      "Training loss (for one batch) at step 60: 289.8705, Accuracy: 0.7709\n",
      "Training loss (for one batch) at step 70: 346.7901, Accuracy: 0.7641\n",
      "Training loss (for one batch) at step 80: 337.7425, Accuracy: 0.7519\n",
      "Training loss (for one batch) at step 90: 304.9878, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 100: 280.6466, Accuracy: 0.7548\n",
      "Training loss (for one batch) at step 110: 297.9809, Accuracy: 0.7536\n",
      "---- Training ----\n",
      "Training loss: 89.0182\n",
      "Training acc over epoch: 0.7530\n",
      "---- Validation ----\n",
      "Validation loss: 45.7886\n",
      "Validation acc: 0.6991\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 331.9091, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 317.1326, Accuracy: 0.6932\n",
      "Training loss (for one batch) at step 20: 280.0373, Accuracy: 0.7254\n",
      "Training loss (for one batch) at step 30: 280.9043, Accuracy: 0.7508\n",
      "Training loss (for one batch) at step 40: 276.1133, Accuracy: 0.7611\n",
      "Training loss (for one batch) at step 50: 277.0465, Accuracy: 0.7714\n",
      "Training loss (for one batch) at step 60: 291.8670, Accuracy: 0.7766\n",
      "Training loss (for one batch) at step 70: 313.5255, Accuracy: 0.7678\n",
      "Training loss (for one batch) at step 80: 320.0087, Accuracy: 0.7584\n",
      "Training loss (for one batch) at step 90: 285.9667, Accuracy: 0.7548\n",
      "Training loss (for one batch) at step 100: 280.3135, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 110: 313.7916, Accuracy: 0.7542\n",
      "---- Training ----\n",
      "Training loss: 89.9354\n",
      "Training acc over epoch: 0.7543\n",
      "---- Validation ----\n",
      "Validation loss: 56.2386\n",
      "Validation acc: 0.7028\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 321.8732, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 316.5194, Accuracy: 0.6903\n",
      "Training loss (for one batch) at step 20: 305.4249, Accuracy: 0.7176\n",
      "Training loss (for one batch) at step 30: 280.1295, Accuracy: 0.7407\n",
      "Training loss (for one batch) at step 40: 274.1387, Accuracy: 0.7561\n",
      "Training loss (for one batch) at step 50: 290.3466, Accuracy: 0.7664\n",
      "Training loss (for one batch) at step 60: 284.3968, Accuracy: 0.7715\n",
      "Training loss (for one batch) at step 70: 326.2224, Accuracy: 0.7660\n",
      "Training loss (for one batch) at step 80: 309.5670, Accuracy: 0.7555\n",
      "Training loss (for one batch) at step 90: 291.4721, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 100: 286.0983, Accuracy: 0.7539\n",
      "Training loss (for one batch) at step 110: 319.9624, Accuracy: 0.7534\n",
      "---- Training ----\n",
      "Training loss: 101.1322\n",
      "Training acc over epoch: 0.7528\n",
      "---- Validation ----\n",
      "Validation loss: 52.1776\n",
      "Validation acc: 0.6819\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 350.7472, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 314.8991, Accuracy: 0.6847\n",
      "Training loss (for one batch) at step 20: 291.6180, Accuracy: 0.7221\n",
      "Training loss (for one batch) at step 30: 279.9211, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 40: 268.2529, Accuracy: 0.7565\n",
      "Training loss (for one batch) at step 50: 283.0675, Accuracy: 0.7708\n",
      "Training loss (for one batch) at step 60: 300.4533, Accuracy: 0.7743\n",
      "Training loss (for one batch) at step 70: 324.2295, Accuracy: 0.7654\n",
      "Training loss (for one batch) at step 80: 301.0505, Accuracy: 0.7569\n",
      "Training loss (for one batch) at step 90: 290.4907, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 100: 311.1968, Accuracy: 0.7552\n",
      "Training loss (for one batch) at step 110: 290.2755, Accuracy: 0.7548\n",
      "---- Training ----\n",
      "Training loss: 87.1165\n",
      "Training acc over epoch: 0.7540\n",
      "---- Validation ----\n",
      "Validation loss: 46.6448\n",
      "Validation acc: 0.6857\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 332.2159, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 321.0116, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 20: 302.4662, Accuracy: 0.7143\n",
      "Training loss (for one batch) at step 30: 290.2343, Accuracy: 0.7414\n",
      "Training loss (for one batch) at step 40: 267.2999, Accuracy: 0.7565\n",
      "Training loss (for one batch) at step 50: 281.7016, Accuracy: 0.7678\n",
      "Training loss (for one batch) at step 60: 293.3327, Accuracy: 0.7724\n",
      "Training loss (for one batch) at step 70: 309.1778, Accuracy: 0.7631\n",
      "Training loss (for one batch) at step 80: 320.7517, Accuracy: 0.7545\n",
      "Training loss (for one batch) at step 90: 302.8687, Accuracy: 0.7533\n",
      "Training loss (for one batch) at step 100: 282.9710, Accuracy: 0.7532\n",
      "Training loss (for one batch) at step 110: 280.5883, Accuracy: 0.7541\n",
      "---- Training ----\n",
      "Training loss: 104.9476\n",
      "Training acc over epoch: 0.7522\n",
      "---- Validation ----\n",
      "Validation loss: 50.3343\n",
      "Validation acc: 0.6967\n",
      "Time taken: 10.57s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 313.9303, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 325.2101, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 20: 292.0216, Accuracy: 0.7158\n",
      "Training loss (for one batch) at step 30: 277.7830, Accuracy: 0.7417\n",
      "Training loss (for one batch) at step 40: 286.5764, Accuracy: 0.7567\n",
      "Training loss (for one batch) at step 50: 288.8101, Accuracy: 0.7705\n",
      "Training loss (for one batch) at step 60: 305.3517, Accuracy: 0.7743\n",
      "Training loss (for one batch) at step 70: 294.5198, Accuracy: 0.7675\n",
      "Training loss (for one batch) at step 80: 326.7899, Accuracy: 0.7584\n",
      "Training loss (for one batch) at step 90: 298.7819, Accuracy: 0.7558\n",
      "Training loss (for one batch) at step 100: 293.4536, Accuracy: 0.7572\n",
      "Training loss (for one batch) at step 110: 267.6556, Accuracy: 0.7558\n",
      "---- Training ----\n",
      "Training loss: 91.3776\n",
      "Training acc over epoch: 0.7544\n",
      "---- Validation ----\n",
      "Validation loss: 45.1443\n",
      "Validation acc: 0.6953\n",
      "Time taken: 10.15s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 320.7545, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 332.8677, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 20: 276.7472, Accuracy: 0.7147\n",
      "Training loss (for one batch) at step 30: 282.5933, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 40: 276.4252, Accuracy: 0.7561\n",
      "Training loss (for one batch) at step 50: 292.1243, Accuracy: 0.7682\n",
      "Training loss (for one batch) at step 60: 293.1542, Accuracy: 0.7723\n",
      "Training loss (for one batch) at step 70: 315.1227, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 80: 327.5983, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 90: 302.7790, Accuracy: 0.7497\n",
      "Training loss (for one batch) at step 100: 304.2419, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 110: 288.5210, Accuracy: 0.7531\n",
      "---- Training ----\n",
      "Training loss: 120.8568\n",
      "Training acc over epoch: 0.7512\n",
      "---- Validation ----\n",
      "Validation loss: 53.5802\n",
      "Validation acc: 0.6873\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 320.6169, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 310.2697, Accuracy: 0.6818\n",
      "Training loss (for one batch) at step 20: 280.7952, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 30: 275.8527, Accuracy: 0.7434\n",
      "Training loss (for one batch) at step 40: 270.6592, Accuracy: 0.7548\n",
      "Training loss (for one batch) at step 50: 277.0499, Accuracy: 0.7695\n",
      "Training loss (for one batch) at step 60: 283.1879, Accuracy: 0.7756\n",
      "Training loss (for one batch) at step 70: 304.9516, Accuracy: 0.7657\n",
      "Training loss (for one batch) at step 80: 305.6900, Accuracy: 0.7576\n",
      "Training loss (for one batch) at step 90: 284.8573, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 100: 278.7816, Accuracy: 0.7583\n",
      "Training loss (for one batch) at step 110: 312.0142, Accuracy: 0.7569\n",
      "---- Training ----\n",
      "Training loss: 110.5942\n",
      "Training acc over epoch: 0.7568\n",
      "---- Validation ----\n",
      "Validation loss: 59.1047\n",
      "Validation acc: 0.7007\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 327.9602, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 307.3510, Accuracy: 0.7003\n",
      "Training loss (for one batch) at step 20: 281.2397, Accuracy: 0.7206\n",
      "Training loss (for one batch) at step 30: 278.9076, Accuracy: 0.7497\n",
      "Training loss (for one batch) at step 40: 266.7166, Accuracy: 0.7607\n",
      "Training loss (for one batch) at step 50: 276.2363, Accuracy: 0.7710\n",
      "Training loss (for one batch) at step 60: 320.2390, Accuracy: 0.7727\n",
      "Training loss (for one batch) at step 70: 300.4586, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 80: 313.2730, Accuracy: 0.7548\n",
      "Training loss (for one batch) at step 90: 287.8855, Accuracy: 0.7541\n",
      "Training loss (for one batch) at step 100: 313.3139, Accuracy: 0.7546\n",
      "Training loss (for one batch) at step 110: 301.2597, Accuracy: 0.7550\n",
      "---- Training ----\n",
      "Training loss: 106.0264\n",
      "Training acc over epoch: 0.7536\n",
      "---- Validation ----\n",
      "Validation loss: 42.0107\n",
      "Validation acc: 0.7007\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 332.5393, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 320.5012, Accuracy: 0.6605\n",
      "Training loss (for one batch) at step 20: 278.1827, Accuracy: 0.7150\n",
      "Training loss (for one batch) at step 30: 264.4162, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 40: 279.1942, Accuracy: 0.7572\n",
      "Training loss (for one batch) at step 50: 262.9538, Accuracy: 0.7701\n",
      "Training loss (for one batch) at step 60: 309.7310, Accuracy: 0.7748\n",
      "Training loss (for one batch) at step 70: 300.7779, Accuracy: 0.7661\n",
      "Training loss (for one batch) at step 80: 286.5775, Accuracy: 0.7545\n",
      "Training loss (for one batch) at step 90: 271.9997, Accuracy: 0.7538\n",
      "Training loss (for one batch) at step 100: 298.4838, Accuracy: 0.7544\n",
      "Training loss (for one batch) at step 110: 290.3029, Accuracy: 0.7552\n",
      "---- Training ----\n",
      "Training loss: 89.6545\n",
      "Training acc over epoch: 0.7536\n",
      "---- Validation ----\n",
      "Validation loss: 46.0854\n",
      "Validation acc: 0.6905\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 329.7635, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 309.8621, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 20: 297.0332, Accuracy: 0.7083\n",
      "Training loss (for one batch) at step 30: 284.0291, Accuracy: 0.7404\n",
      "Training loss (for one batch) at step 40: 285.1686, Accuracy: 0.7559\n",
      "Training loss (for one batch) at step 50: 278.6640, Accuracy: 0.7667\n",
      "Training loss (for one batch) at step 60: 284.7452, Accuracy: 0.7731\n",
      "Training loss (for one batch) at step 70: 280.3686, Accuracy: 0.7655\n",
      "Training loss (for one batch) at step 80: 314.5806, Accuracy: 0.7546\n",
      "Training loss (for one batch) at step 90: 271.4642, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 100: 289.0156, Accuracy: 0.7533\n",
      "Training loss (for one batch) at step 110: 299.8801, Accuracy: 0.7542\n",
      "---- Training ----\n",
      "Training loss: 90.3138\n",
      "Training acc over epoch: 0.7538\n",
      "---- Validation ----\n",
      "Validation loss: 64.7468\n",
      "Validation acc: 0.6859\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 318.1456, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 313.3158, Accuracy: 0.6832\n",
      "Training loss (for one batch) at step 20: 286.9635, Accuracy: 0.7225\n",
      "Training loss (for one batch) at step 30: 303.7516, Accuracy: 0.7419\n",
      "Training loss (for one batch) at step 40: 288.1187, Accuracy: 0.7591\n",
      "Training loss (for one batch) at step 50: 274.8397, Accuracy: 0.7672\n",
      "Training loss (for one batch) at step 60: 297.6646, Accuracy: 0.7723\n",
      "Training loss (for one batch) at step 70: 339.9360, Accuracy: 0.7660\n",
      "Training loss (for one batch) at step 80: 332.0804, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 90: 273.4449, Accuracy: 0.7508\n",
      "Training loss (for one batch) at step 100: 300.4480, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 110: 284.6494, Accuracy: 0.7517\n",
      "---- Training ----\n",
      "Training loss: 85.5635\n",
      "Training acc over epoch: 0.7502\n",
      "---- Validation ----\n",
      "Validation loss: 57.6084\n",
      "Validation acc: 0.6913\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 325.1096, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 304.2534, Accuracy: 0.6776\n",
      "Training loss (for one batch) at step 20: 295.0486, Accuracy: 0.7143\n",
      "Training loss (for one batch) at step 30: 294.5534, Accuracy: 0.7387\n",
      "Training loss (for one batch) at step 40: 275.8651, Accuracy: 0.7536\n",
      "Training loss (for one batch) at step 50: 269.5096, Accuracy: 0.7667\n",
      "Training loss (for one batch) at step 60: 310.7000, Accuracy: 0.7713\n",
      "Training loss (for one batch) at step 70: 301.4374, Accuracy: 0.7634\n",
      "Training loss (for one batch) at step 80: 314.7967, Accuracy: 0.7526\n",
      "Training loss (for one batch) at step 90: 299.1419, Accuracy: 0.7486\n",
      "Training loss (for one batch) at step 100: 287.4589, Accuracy: 0.7498\n",
      "Training loss (for one batch) at step 110: 300.5756, Accuracy: 0.7506\n",
      "---- Training ----\n",
      "Training loss: 100.7149\n",
      "Training acc over epoch: 0.7499\n",
      "---- Validation ----\n",
      "Validation loss: 36.2567\n",
      "Validation acc: 0.7007\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 306.8355, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 326.4123, Accuracy: 0.6783\n",
      "Training loss (for one batch) at step 20: 277.4126, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 30: 272.1044, Accuracy: 0.7455\n",
      "Training loss (for one batch) at step 40: 303.6086, Accuracy: 0.7534\n",
      "Training loss (for one batch) at step 50: 276.0267, Accuracy: 0.7676\n",
      "Training loss (for one batch) at step 60: 297.6092, Accuracy: 0.7700\n",
      "Training loss (for one batch) at step 70: 316.2861, Accuracy: 0.7638\n",
      "Training loss (for one batch) at step 80: 312.7629, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 90: 295.9981, Accuracy: 0.7525\n",
      "Training loss (for one batch) at step 100: 281.0033, Accuracy: 0.7526\n",
      "Training loss (for one batch) at step 110: 299.6078, Accuracy: 0.7530\n",
      "---- Training ----\n",
      "Training loss: 88.4865\n",
      "Training acc over epoch: 0.7519\n",
      "---- Validation ----\n",
      "Validation loss: 43.0205\n",
      "Validation acc: 0.7085\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 329.5304, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 306.4839, Accuracy: 0.6754\n",
      "Training loss (for one batch) at step 20: 263.1743, Accuracy: 0.7184\n",
      "Training loss (for one batch) at step 30: 272.2392, Accuracy: 0.7470\n",
      "Training loss (for one batch) at step 40: 259.5800, Accuracy: 0.7572\n",
      "Training loss (for one batch) at step 50: 288.0467, Accuracy: 0.7704\n",
      "Training loss (for one batch) at step 60: 295.1221, Accuracy: 0.7723\n",
      "Training loss (for one batch) at step 70: 309.2271, Accuracy: 0.7641\n",
      "Training loss (for one batch) at step 80: 334.3877, Accuracy: 0.7536\n",
      "Training loss (for one batch) at step 90: 278.9368, Accuracy: 0.7516\n",
      "Training loss (for one batch) at step 100: 273.0367, Accuracy: 0.7520\n",
      "Training loss (for one batch) at step 110: 291.2130, Accuracy: 0.7530\n",
      "---- Training ----\n",
      "Training loss: 94.6970\n",
      "Training acc over epoch: 0.7521\n",
      "---- Validation ----\n",
      "Validation loss: 63.3586\n",
      "Validation acc: 0.6953\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 322.2822, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 315.4283, Accuracy: 0.6932\n",
      "Training loss (for one batch) at step 20: 307.4405, Accuracy: 0.7202\n",
      "Training loss (for one batch) at step 30: 263.4380, Accuracy: 0.7434\n",
      "Training loss (for one batch) at step 40: 262.3595, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 50: 268.7594, Accuracy: 0.7707\n",
      "Training loss (for one batch) at step 60: 305.1239, Accuracy: 0.7745\n",
      "Training loss (for one batch) at step 70: 310.2724, Accuracy: 0.7655\n",
      "Training loss (for one batch) at step 80: 320.9008, Accuracy: 0.7546\n",
      "Training loss (for one batch) at step 90: 271.0069, Accuracy: 0.7547\n",
      "Training loss (for one batch) at step 100: 266.3873, Accuracy: 0.7555\n",
      "Training loss (for one batch) at step 110: 270.5380, Accuracy: 0.7536\n",
      "---- Training ----\n",
      "Training loss: 108.3663\n",
      "Training acc over epoch: 0.7528\n",
      "---- Validation ----\n",
      "Validation loss: 36.9206\n",
      "Validation acc: 0.6795\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 331.5400, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 322.8142, Accuracy: 0.6911\n",
      "Training loss (for one batch) at step 20: 277.8157, Accuracy: 0.7247\n",
      "Training loss (for one batch) at step 30: 278.4095, Accuracy: 0.7462\n",
      "Training loss (for one batch) at step 40: 272.5168, Accuracy: 0.7586\n",
      "Training loss (for one batch) at step 50: 274.3685, Accuracy: 0.7710\n",
      "Training loss (for one batch) at step 60: 301.9108, Accuracy: 0.7772\n",
      "Training loss (for one batch) at step 70: 293.4562, Accuracy: 0.7661\n",
      "Training loss (for one batch) at step 80: 298.9227, Accuracy: 0.7573\n",
      "Training loss (for one batch) at step 90: 283.1006, Accuracy: 0.7546\n",
      "Training loss (for one batch) at step 100: 297.5275, Accuracy: 0.7549\n",
      "Training loss (for one batch) at step 110: 272.3106, Accuracy: 0.7542\n",
      "---- Training ----\n",
      "Training loss: 111.8883\n",
      "Training acc over epoch: 0.7532\n",
      "---- Validation ----\n",
      "Validation loss: 75.0190\n",
      "Validation acc: 0.6924\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 314.3708, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 316.1695, Accuracy: 0.6847\n",
      "Training loss (for one batch) at step 20: 287.5275, Accuracy: 0.7184\n",
      "Training loss (for one batch) at step 30: 278.8848, Accuracy: 0.7442\n",
      "Training loss (for one batch) at step 40: 275.8571, Accuracy: 0.7590\n",
      "Training loss (for one batch) at step 50: 284.8425, Accuracy: 0.7705\n",
      "Training loss (for one batch) at step 60: 301.8693, Accuracy: 0.7733\n",
      "Training loss (for one batch) at step 70: 290.2993, Accuracy: 0.7661\n",
      "Training loss (for one batch) at step 80: 310.9491, Accuracy: 0.7544\n",
      "Training loss (for one batch) at step 90: 266.0097, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 100: 272.1272, Accuracy: 0.7527\n",
      "Training loss (for one batch) at step 110: 279.1778, Accuracy: 0.7538\n",
      "---- Training ----\n",
      "Training loss: 88.0796\n",
      "Training acc over epoch: 0.7522\n",
      "---- Validation ----\n",
      "Validation loss: 51.9575\n",
      "Validation acc: 0.6964\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 320.5574, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 299.5566, Accuracy: 0.6839\n",
      "Training loss (for one batch) at step 20: 279.0065, Accuracy: 0.7135\n",
      "Training loss (for one batch) at step 30: 269.1511, Accuracy: 0.7402\n",
      "Training loss (for one batch) at step 40: 272.3829, Accuracy: 0.7614\n",
      "Training loss (for one batch) at step 50: 278.4805, Accuracy: 0.7698\n",
      "Training loss (for one batch) at step 60: 271.8136, Accuracy: 0.7743\n",
      "Training loss (for one batch) at step 70: 318.5933, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 80: 298.3469, Accuracy: 0.7541\n",
      "Training loss (for one batch) at step 90: 297.5193, Accuracy: 0.7509\n",
      "Training loss (for one batch) at step 100: 284.6262, Accuracy: 0.7520\n",
      "Training loss (for one batch) at step 110: 289.1216, Accuracy: 0.7520\n",
      "---- Training ----\n",
      "Training loss: 100.6511\n",
      "Training acc over epoch: 0.7513\n",
      "---- Validation ----\n",
      "Validation loss: 50.4830\n",
      "Validation acc: 0.6994\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 323.1297, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 296.5073, Accuracy: 0.6861\n",
      "Training loss (for one batch) at step 20: 277.2068, Accuracy: 0.7135\n",
      "Training loss (for one batch) at step 30: 256.5704, Accuracy: 0.7417\n",
      "Training loss (for one batch) at step 40: 256.8940, Accuracy: 0.7563\n",
      "Training loss (for one batch) at step 50: 292.4610, Accuracy: 0.7652\n",
      "Training loss (for one batch) at step 60: 278.0043, Accuracy: 0.7724\n",
      "Training loss (for one batch) at step 70: 299.7084, Accuracy: 0.7655\n",
      "Training loss (for one batch) at step 80: 299.7282, Accuracy: 0.7568\n",
      "Training loss (for one batch) at step 90: 276.7564, Accuracy: 0.7539\n",
      "Training loss (for one batch) at step 100: 270.9065, Accuracy: 0.7569\n",
      "Training loss (for one batch) at step 110: 294.2286, Accuracy: 0.7569\n",
      "---- Training ----\n",
      "Training loss: 94.5947\n",
      "Training acc over epoch: 0.7552\n",
      "---- Validation ----\n",
      "Validation loss: 60.5356\n",
      "Validation acc: 0.6929\n",
      "Time taken: 10.46s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.6993\n",
      "Validation AUC: 0.7005\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 635.8860, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 593.9790, Accuracy: 0.5241\n",
      "Training loss (for one batch) at step 20: 551.5685, Accuracy: 0.5208\n",
      "Training loss (for one batch) at step 30: 519.0139, Accuracy: 0.5197\n",
      "Training loss (for one batch) at step 40: 505.7303, Accuracy: 0.5183\n",
      "Training loss (for one batch) at step 50: 487.8184, Accuracy: 0.5159\n",
      "Training loss (for one batch) at step 60: 502.7401, Accuracy: 0.5160\n",
      "Training loss (for one batch) at step 70: 477.6386, Accuracy: 0.5141\n",
      "Training loss (for one batch) at step 80: 462.2960, Accuracy: 0.5136\n",
      "Training loss (for one batch) at step 90: 472.6207, Accuracy: 0.5154\n",
      "Training loss (for one batch) at step 100: 469.2187, Accuracy: 0.5148\n",
      "Training loss (for one batch) at step 110: 463.4043, Accuracy: 0.5155\n",
      "---- Training ----\n",
      "Training loss: 143.3799\n",
      "Training acc over epoch: 0.5146\n",
      "---- Validation ----\n",
      "Validation loss: 34.5606\n",
      "Validation acc: 0.5137\n",
      "Time taken: 11.87s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 455.2162, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 446.3503, Accuracy: 0.4808\n",
      "Training loss (for one batch) at step 20: 456.9132, Accuracy: 0.5041\n",
      "Training loss (for one batch) at step 30: 454.1962, Accuracy: 0.5144\n",
      "Training loss (for one batch) at step 40: 455.2361, Accuracy: 0.5147\n",
      "Training loss (for one batch) at step 50: 456.8120, Accuracy: 0.5133\n",
      "Training loss (for one batch) at step 60: 448.5419, Accuracy: 0.5149\n",
      "Training loss (for one batch) at step 70: 448.4806, Accuracy: 0.5187\n",
      "Training loss (for one batch) at step 80: 449.2812, Accuracy: 0.5181\n",
      "Training loss (for one batch) at step 90: 453.0105, Accuracy: 0.5193\n",
      "Training loss (for one batch) at step 100: 451.7106, Accuracy: 0.5201\n",
      "Training loss (for one batch) at step 110: 444.7042, Accuracy: 0.5211\n",
      "---- Training ----\n",
      "Training loss: 137.6744\n",
      "Training acc over epoch: 0.5208\n",
      "---- Validation ----\n",
      "Validation loss: 34.5546\n",
      "Validation acc: 0.5126\n",
      "Time taken: 10.65s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 449.6223, Accuracy: 0.4688\n",
      "Training loss (for one batch) at step 10: 446.5075, Accuracy: 0.5291\n",
      "Training loss (for one batch) at step 20: 448.9316, Accuracy: 0.5279\n",
      "Training loss (for one batch) at step 30: 447.5045, Accuracy: 0.5229\n",
      "Training loss (for one batch) at step 40: 444.9371, Accuracy: 0.5183\n",
      "Training loss (for one batch) at step 50: 445.9994, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 60: 443.8220, Accuracy: 0.5187\n",
      "Training loss (for one batch) at step 70: 441.9359, Accuracy: 0.5216\n",
      "Training loss (for one batch) at step 80: 444.1579, Accuracy: 0.5259\n",
      "Training loss (for one batch) at step 90: 446.0019, Accuracy: 0.5280\n",
      "Training loss (for one batch) at step 100: 443.8041, Accuracy: 0.5304\n",
      "Training loss (for one batch) at step 110: 441.0455, Accuracy: 0.5304\n",
      "---- Training ----\n",
      "Training loss: 138.6045\n",
      "Training acc over epoch: 0.5310\n",
      "---- Validation ----\n",
      "Validation loss: 34.6315\n",
      "Validation acc: 0.5148\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 445.4919, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 443.0538, Accuracy: 0.5376\n",
      "Training loss (for one batch) at step 20: 444.6105, Accuracy: 0.5368\n",
      "Training loss (for one batch) at step 30: 444.6868, Accuracy: 0.5403\n",
      "Training loss (for one batch) at step 40: 445.1296, Accuracy: 0.5427\n",
      "Training loss (for one batch) at step 50: 443.1981, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 60: 445.3282, Accuracy: 0.5516\n",
      "Training loss (for one batch) at step 70: 447.4221, Accuracy: 0.5542\n",
      "Training loss (for one batch) at step 80: 444.5751, Accuracy: 0.5567\n",
      "Training loss (for one batch) at step 90: 442.6318, Accuracy: 0.5588\n",
      "Training loss (for one batch) at step 100: 445.0613, Accuracy: 0.5596\n",
      "Training loss (for one batch) at step 110: 442.9763, Accuracy: 0.5626\n",
      "---- Training ----\n",
      "Training loss: 137.7512\n",
      "Training acc over epoch: 0.5640\n",
      "---- Validation ----\n",
      "Validation loss: 34.6491\n",
      "Validation acc: 0.5844\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 444.1838, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 444.3390, Accuracy: 0.5895\n",
      "Training loss (for one batch) at step 20: 440.0518, Accuracy: 0.5804\n",
      "Training loss (for one batch) at step 30: 441.1758, Accuracy: 0.5847\n",
      "Training loss (for one batch) at step 40: 441.1255, Accuracy: 0.5776\n",
      "Training loss (for one batch) at step 50: 440.8831, Accuracy: 0.5766\n",
      "Training loss (for one batch) at step 60: 440.0232, Accuracy: 0.5812\n",
      "Training loss (for one batch) at step 70: 440.4196, Accuracy: 0.5880\n",
      "Training loss (for one batch) at step 80: 444.4218, Accuracy: 0.5874\n",
      "Training loss (for one batch) at step 90: 441.6878, Accuracy: 0.5858\n",
      "Training loss (for one batch) at step 100: 444.3919, Accuracy: 0.5848\n",
      "Training loss (for one batch) at step 110: 444.3875, Accuracy: 0.5869\n",
      "---- Training ----\n",
      "Training loss: 138.4563\n",
      "Training acc over epoch: 0.5866\n",
      "---- Validation ----\n",
      "Validation loss: 34.2240\n",
      "Validation acc: 0.5967\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 444.1934, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 442.5284, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 20: 440.4337, Accuracy: 0.5919\n",
      "Training loss (for one batch) at step 30: 439.3073, Accuracy: 0.5993\n",
      "Training loss (for one batch) at step 40: 441.5304, Accuracy: 0.5960\n",
      "Training loss (for one batch) at step 50: 438.7372, Accuracy: 0.5988\n",
      "Training loss (for one batch) at step 60: 444.2458, Accuracy: 0.6012\n",
      "Training loss (for one batch) at step 70: 444.2109, Accuracy: 0.6024\n",
      "Training loss (for one batch) at step 80: 444.4183, Accuracy: 0.6009\n",
      "Training loss (for one batch) at step 90: 441.8555, Accuracy: 0.5996\n",
      "Training loss (for one batch) at step 100: 442.9055, Accuracy: 0.5975\n",
      "Training loss (for one batch) at step 110: 439.1367, Accuracy: 0.5964\n",
      "---- Training ----\n",
      "Training loss: 136.5430\n",
      "Training acc over epoch: 0.5973\n",
      "---- Validation ----\n",
      "Validation loss: 34.6845\n",
      "Validation acc: 0.6260\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 444.2823, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 443.9052, Accuracy: 0.6001\n",
      "Training loss (for one batch) at step 20: 440.1465, Accuracy: 0.6008\n",
      "Training loss (for one batch) at step 30: 437.8336, Accuracy: 0.6018\n",
      "Training loss (for one batch) at step 40: 440.9977, Accuracy: 0.6048\n",
      "Training loss (for one batch) at step 50: 437.6044, Accuracy: 0.6092\n",
      "Training loss (for one batch) at step 60: 442.2779, Accuracy: 0.6149\n",
      "Training loss (for one batch) at step 70: 443.1578, Accuracy: 0.6170\n",
      "Training loss (for one batch) at step 80: 442.1343, Accuracy: 0.6142\n",
      "Training loss (for one batch) at step 90: 440.6183, Accuracy: 0.6097\n",
      "Training loss (for one batch) at step 100: 437.8188, Accuracy: 0.6111\n",
      "Training loss (for one batch) at step 110: 443.4460, Accuracy: 0.6119\n",
      "---- Training ----\n",
      "Training loss: 138.0583\n",
      "Training acc over epoch: 0.6112\n",
      "---- Validation ----\n",
      "Validation loss: 34.4526\n",
      "Validation acc: 0.5881\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 442.8877, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 441.7484, Accuracy: 0.5668\n",
      "Training loss (for one batch) at step 20: 439.5031, Accuracy: 0.5774\n",
      "Training loss (for one batch) at step 30: 447.0612, Accuracy: 0.5862\n",
      "Training loss (for one batch) at step 40: 439.1691, Accuracy: 0.5934\n",
      "Training loss (for one batch) at step 50: 437.1757, Accuracy: 0.5950\n",
      "Training loss (for one batch) at step 60: 440.1751, Accuracy: 0.6013\n",
      "Training loss (for one batch) at step 70: 440.7672, Accuracy: 0.6052\n",
      "Training loss (for one batch) at step 80: 442.2491, Accuracy: 0.6054\n",
      "Training loss (for one batch) at step 90: 442.0500, Accuracy: 0.6004\n",
      "Training loss (for one batch) at step 100: 436.1324, Accuracy: 0.6010\n",
      "Training loss (for one batch) at step 110: 437.8147, Accuracy: 0.6023\n",
      "---- Training ----\n",
      "Training loss: 137.8110\n",
      "Training acc over epoch: 0.6021\n",
      "---- Validation ----\n",
      "Validation loss: 35.3411\n",
      "Validation acc: 0.5919\n",
      "Time taken: 10.63s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 441.5806, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 443.6630, Accuracy: 0.5945\n",
      "Training loss (for one batch) at step 20: 437.5424, Accuracy: 0.5978\n",
      "Training loss (for one batch) at step 30: 435.4816, Accuracy: 0.6028\n",
      "Training loss (for one batch) at step 40: 440.2523, Accuracy: 0.6033\n",
      "Training loss (for one batch) at step 50: 439.0785, Accuracy: 0.6068\n",
      "Training loss (for one batch) at step 60: 441.8907, Accuracy: 0.6086\n",
      "Training loss (for one batch) at step 70: 442.0359, Accuracy: 0.6131\n",
      "Training loss (for one batch) at step 80: 441.0928, Accuracy: 0.6111\n",
      "Training loss (for one batch) at step 90: 440.0909, Accuracy: 0.6051\n",
      "Training loss (for one batch) at step 100: 438.5986, Accuracy: 0.6047\n",
      "Training loss (for one batch) at step 110: 439.8754, Accuracy: 0.6081\n",
      "---- Training ----\n",
      "Training loss: 138.2148\n",
      "Training acc over epoch: 0.6097\n",
      "---- Validation ----\n",
      "Validation loss: 35.2571\n",
      "Validation acc: 0.6061\n",
      "Time taken: 10.08s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 437.1027, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 438.5912, Accuracy: 0.5973\n",
      "Training loss (for one batch) at step 20: 443.2820, Accuracy: 0.5796\n",
      "Training loss (for one batch) at step 30: 433.2614, Accuracy: 0.5897\n",
      "Training loss (for one batch) at step 40: 437.5000, Accuracy: 0.5981\n",
      "Training loss (for one batch) at step 50: 438.7955, Accuracy: 0.6127\n",
      "Training loss (for one batch) at step 60: 436.8040, Accuracy: 0.6176\n",
      "Training loss (for one batch) at step 70: 442.3742, Accuracy: 0.6204\n",
      "Training loss (for one batch) at step 80: 439.6669, Accuracy: 0.6146\n",
      "Training loss (for one batch) at step 90: 438.4027, Accuracy: 0.6090\n",
      "Training loss (for one batch) at step 100: 433.0325, Accuracy: 0.6117\n",
      "Training loss (for one batch) at step 110: 435.8620, Accuracy: 0.6134\n",
      "---- Training ----\n",
      "Training loss: 139.7144\n",
      "Training acc over epoch: 0.6139\n",
      "---- Validation ----\n",
      "Validation loss: 33.3995\n",
      "Validation acc: 0.5946\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 439.1894, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 441.6993, Accuracy: 0.5973\n",
      "Training loss (for one batch) at step 20: 437.4723, Accuracy: 0.5897\n",
      "Training loss (for one batch) at step 30: 438.6352, Accuracy: 0.5963\n",
      "Training loss (for one batch) at step 40: 435.9193, Accuracy: 0.6099\n",
      "Training loss (for one batch) at step 50: 437.9362, Accuracy: 0.6161\n",
      "Training loss (for one batch) at step 60: 437.9310, Accuracy: 0.6206\n",
      "Training loss (for one batch) at step 70: 434.9251, Accuracy: 0.6224\n",
      "Training loss (for one batch) at step 80: 438.1146, Accuracy: 0.6180\n",
      "Training loss (for one batch) at step 90: 439.6429, Accuracy: 0.6133\n",
      "Training loss (for one batch) at step 100: 430.0716, Accuracy: 0.6139\n",
      "Training loss (for one batch) at step 110: 438.6713, Accuracy: 0.6152\n",
      "---- Training ----\n",
      "Training loss: 137.9941\n",
      "Training acc over epoch: 0.6159\n",
      "---- Validation ----\n",
      "Validation loss: 33.4099\n",
      "Validation acc: 0.6016\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 439.7764, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 439.2767, Accuracy: 0.5916\n",
      "Training loss (for one batch) at step 20: 438.0128, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 30: 434.1070, Accuracy: 0.6041\n",
      "Training loss (for one batch) at step 40: 435.6723, Accuracy: 0.6098\n",
      "Training loss (for one batch) at step 50: 431.2454, Accuracy: 0.6203\n",
      "Training loss (for one batch) at step 60: 442.0507, Accuracy: 0.6242\n",
      "Training loss (for one batch) at step 70: 445.0338, Accuracy: 0.6307\n",
      "Training loss (for one batch) at step 80: 439.7010, Accuracy: 0.6223\n",
      "Training loss (for one batch) at step 90: 436.4283, Accuracy: 0.6150\n",
      "Training loss (for one batch) at step 100: 437.6472, Accuracy: 0.6158\n",
      "Training loss (for one batch) at step 110: 433.6844, Accuracy: 0.6157\n",
      "---- Training ----\n",
      "Training loss: 137.8443\n",
      "Training acc over epoch: 0.6156\n",
      "---- Validation ----\n",
      "Validation loss: 35.0508\n",
      "Validation acc: 0.5639\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 439.3003, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 433.9910, Accuracy: 0.5760\n",
      "Training loss (for one batch) at step 20: 436.1073, Accuracy: 0.5744\n",
      "Training loss (for one batch) at step 30: 430.7623, Accuracy: 0.5869\n",
      "Training loss (for one batch) at step 40: 431.5198, Accuracy: 0.5966\n",
      "Training loss (for one batch) at step 50: 435.9169, Accuracy: 0.6091\n",
      "Training loss (for one batch) at step 60: 423.3376, Accuracy: 0.6167\n",
      "Training loss (for one batch) at step 70: 435.4270, Accuracy: 0.6213\n",
      "Training loss (for one batch) at step 80: 430.5179, Accuracy: 0.6155\n",
      "Training loss (for one batch) at step 90: 434.6375, Accuracy: 0.6078\n",
      "Training loss (for one batch) at step 100: 431.0920, Accuracy: 0.6104\n",
      "Training loss (for one batch) at step 110: 433.3906, Accuracy: 0.6123\n",
      "---- Training ----\n",
      "Training loss: 135.6404\n",
      "Training acc over epoch: 0.6120\n",
      "---- Validation ----\n",
      "Validation loss: 33.4157\n",
      "Validation acc: 0.5903\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 446.8827, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 439.4635, Accuracy: 0.5895\n",
      "Training loss (for one batch) at step 20: 429.6591, Accuracy: 0.5804\n",
      "Training loss (for one batch) at step 30: 434.8824, Accuracy: 0.5965\n",
      "Training loss (for one batch) at step 40: 419.1256, Accuracy: 0.6092\n",
      "Training loss (for one batch) at step 50: 424.8965, Accuracy: 0.6187\n",
      "Training loss (for one batch) at step 60: 435.7876, Accuracy: 0.6258\n",
      "Training loss (for one batch) at step 70: 435.6107, Accuracy: 0.6305\n",
      "Training loss (for one batch) at step 80: 436.0131, Accuracy: 0.6252\n",
      "Training loss (for one batch) at step 90: 435.0356, Accuracy: 0.6189\n",
      "Training loss (for one batch) at step 100: 430.1163, Accuracy: 0.6190\n",
      "Training loss (for one batch) at step 110: 436.6096, Accuracy: 0.6217\n",
      "---- Training ----\n",
      "Training loss: 131.1912\n",
      "Training acc over epoch: 0.6213\n",
      "---- Validation ----\n",
      "Validation loss: 34.4666\n",
      "Validation acc: 0.5626\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 444.7235, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 434.9550, Accuracy: 0.5973\n",
      "Training loss (for one batch) at step 20: 437.3187, Accuracy: 0.5863\n",
      "Training loss (for one batch) at step 30: 420.9924, Accuracy: 0.5960\n",
      "Training loss (for one batch) at step 40: 421.8318, Accuracy: 0.6128\n",
      "Training loss (for one batch) at step 50: 418.6783, Accuracy: 0.6239\n",
      "Training loss (for one batch) at step 60: 411.4970, Accuracy: 0.6337\n",
      "Training loss (for one batch) at step 70: 429.1372, Accuracy: 0.6364\n",
      "Training loss (for one batch) at step 80: 436.4067, Accuracy: 0.6318\n",
      "Training loss (for one batch) at step 90: 432.9756, Accuracy: 0.6265\n",
      "Training loss (for one batch) at step 100: 421.2653, Accuracy: 0.6268\n",
      "Training loss (for one batch) at step 110: 418.7936, Accuracy: 0.6267\n",
      "---- Training ----\n",
      "Training loss: 132.2829\n",
      "Training acc over epoch: 0.6267\n",
      "---- Validation ----\n",
      "Validation loss: 32.6682\n",
      "Validation acc: 0.6139\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 437.5146, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 436.5804, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 20: 429.4088, Accuracy: 0.5967\n",
      "Training loss (for one batch) at step 30: 434.3039, Accuracy: 0.6021\n",
      "Training loss (for one batch) at step 40: 426.6031, Accuracy: 0.6075\n",
      "Training loss (for one batch) at step 50: 413.8665, Accuracy: 0.6183\n",
      "Training loss (for one batch) at step 60: 421.5919, Accuracy: 0.6294\n",
      "Training loss (for one batch) at step 70: 430.8994, Accuracy: 0.6352\n",
      "Training loss (for one batch) at step 80: 424.4890, Accuracy: 0.6275\n",
      "Training loss (for one batch) at step 90: 430.6422, Accuracy: 0.6229\n",
      "Training loss (for one batch) at step 100: 432.8689, Accuracy: 0.6211\n",
      "Training loss (for one batch) at step 110: 419.5037, Accuracy: 0.6237\n",
      "---- Training ----\n",
      "Training loss: 131.8502\n",
      "Training acc over epoch: 0.6236\n",
      "---- Validation ----\n",
      "Validation loss: 33.0618\n",
      "Validation acc: 0.6357\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 431.9005, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 432.6495, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 20: 437.7187, Accuracy: 0.6034\n",
      "Training loss (for one batch) at step 30: 420.0230, Accuracy: 0.6109\n",
      "Training loss (for one batch) at step 40: 409.0926, Accuracy: 0.6204\n",
      "Training loss (for one batch) at step 50: 418.3212, Accuracy: 0.6304\n",
      "Training loss (for one batch) at step 60: 423.4703, Accuracy: 0.6395\n",
      "Training loss (for one batch) at step 70: 430.7293, Accuracy: 0.6435\n",
      "Training loss (for one batch) at step 80: 422.3447, Accuracy: 0.6398\n",
      "Training loss (for one batch) at step 90: 423.3359, Accuracy: 0.6392\n",
      "Training loss (for one batch) at step 100: 428.3970, Accuracy: 0.6351\n",
      "Training loss (for one batch) at step 110: 434.2171, Accuracy: 0.6350\n",
      "---- Training ----\n",
      "Training loss: 134.0195\n",
      "Training acc over epoch: 0.6362\n",
      "---- Validation ----\n",
      "Validation loss: 36.0780\n",
      "Validation acc: 0.6295\n",
      "Time taken: 10.70s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 425.6826, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 426.5374, Accuracy: 0.6136\n",
      "Training loss (for one batch) at step 20: 429.2834, Accuracy: 0.6179\n",
      "Training loss (for one batch) at step 30: 424.3924, Accuracy: 0.6217\n",
      "Training loss (for one batch) at step 40: 420.6527, Accuracy: 0.6301\n",
      "Training loss (for one batch) at step 50: 409.1633, Accuracy: 0.6397\n",
      "Training loss (for one batch) at step 60: 426.0190, Accuracy: 0.6491\n",
      "Training loss (for one batch) at step 70: 427.4282, Accuracy: 0.6548\n",
      "Training loss (for one batch) at step 80: 421.8425, Accuracy: 0.6545\n",
      "Training loss (for one batch) at step 90: 428.9094, Accuracy: 0.6517\n",
      "Training loss (for one batch) at step 100: 420.6907, Accuracy: 0.6496\n",
      "Training loss (for one batch) at step 110: 431.6287, Accuracy: 0.6482\n",
      "---- Training ----\n",
      "Training loss: 128.8899\n",
      "Training acc over epoch: 0.6486\n",
      "---- Validation ----\n",
      "Validation loss: 33.8231\n",
      "Validation acc: 0.6421\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 438.9390, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 422.1863, Accuracy: 0.6108\n",
      "Training loss (for one batch) at step 20: 423.6039, Accuracy: 0.6254\n",
      "Training loss (for one batch) at step 30: 428.0593, Accuracy: 0.6379\n",
      "Training loss (for one batch) at step 40: 409.7037, Accuracy: 0.6481\n",
      "Training loss (for one batch) at step 50: 415.0841, Accuracy: 0.6581\n",
      "Training loss (for one batch) at step 60: 431.0296, Accuracy: 0.6691\n",
      "Training loss (for one batch) at step 70: 426.9864, Accuracy: 0.6675\n",
      "Training loss (for one batch) at step 80: 421.9719, Accuracy: 0.6633\n",
      "Training loss (for one batch) at step 90: 425.6937, Accuracy: 0.6576\n",
      "Training loss (for one batch) at step 100: 425.2505, Accuracy: 0.6554\n",
      "Training loss (for one batch) at step 110: 420.8682, Accuracy: 0.6563\n",
      "---- Training ----\n",
      "Training loss: 132.5952\n",
      "Training acc over epoch: 0.6552\n",
      "---- Validation ----\n",
      "Validation loss: 35.2757\n",
      "Validation acc: 0.6193\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 430.5535, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 433.7971, Accuracy: 0.6271\n",
      "Training loss (for one batch) at step 20: 425.9313, Accuracy: 0.6298\n",
      "Training loss (for one batch) at step 30: 409.9679, Accuracy: 0.6449\n",
      "Training loss (for one batch) at step 40: 406.3464, Accuracy: 0.6542\n",
      "Training loss (for one batch) at step 50: 405.3071, Accuracy: 0.6610\n",
      "Training loss (for one batch) at step 60: 424.9629, Accuracy: 0.6698\n",
      "Training loss (for one batch) at step 70: 426.8028, Accuracy: 0.6702\n",
      "Training loss (for one batch) at step 80: 427.7577, Accuracy: 0.6652\n",
      "Training loss (for one batch) at step 90: 413.4153, Accuracy: 0.6611\n",
      "Training loss (for one batch) at step 100: 413.2124, Accuracy: 0.6607\n",
      "Training loss (for one batch) at step 110: 403.7224, Accuracy: 0.6626\n",
      "---- Training ----\n",
      "Training loss: 131.0895\n",
      "Training acc over epoch: 0.6639\n",
      "---- Validation ----\n",
      "Validation loss: 36.2161\n",
      "Validation acc: 0.6491\n",
      "Time taken: 10.68s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 433.4192, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 428.0193, Accuracy: 0.6541\n",
      "Training loss (for one batch) at step 20: 436.5465, Accuracy: 0.6477\n",
      "Training loss (for one batch) at step 30: 404.8586, Accuracy: 0.6598\n",
      "Training loss (for one batch) at step 40: 410.2550, Accuracy: 0.6675\n",
      "Training loss (for one batch) at step 50: 394.2208, Accuracy: 0.6762\n",
      "Training loss (for one batch) at step 60: 412.4108, Accuracy: 0.6861\n",
      "Training loss (for one batch) at step 70: 430.1249, Accuracy: 0.6865\n",
      "Training loss (for one batch) at step 80: 415.9848, Accuracy: 0.6819\n",
      "Training loss (for one batch) at step 90: 417.2362, Accuracy: 0.6749\n",
      "Training loss (for one batch) at step 100: 402.8593, Accuracy: 0.6723\n",
      "Training loss (for one batch) at step 110: 414.8032, Accuracy: 0.6743\n",
      "---- Training ----\n",
      "Training loss: 126.7696\n",
      "Training acc over epoch: 0.6731\n",
      "---- Validation ----\n",
      "Validation loss: 37.5738\n",
      "Validation acc: 0.6440\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 432.4997, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 424.2851, Accuracy: 0.6442\n",
      "Training loss (for one batch) at step 20: 418.4958, Accuracy: 0.6559\n",
      "Training loss (for one batch) at step 30: 408.6905, Accuracy: 0.6610\n",
      "Training loss (for one batch) at step 40: 404.1073, Accuracy: 0.6698\n",
      "Training loss (for one batch) at step 50: 387.0641, Accuracy: 0.6840\n",
      "Training loss (for one batch) at step 60: 407.4423, Accuracy: 0.6912\n",
      "Training loss (for one batch) at step 70: 416.7698, Accuracy: 0.6882\n",
      "Training loss (for one batch) at step 80: 420.3424, Accuracy: 0.6817\n",
      "Training loss (for one batch) at step 90: 406.1671, Accuracy: 0.6769\n",
      "Training loss (for one batch) at step 100: 404.6819, Accuracy: 0.6763\n",
      "Training loss (for one batch) at step 110: 405.6150, Accuracy: 0.6759\n",
      "---- Training ----\n",
      "Training loss: 119.8338\n",
      "Training acc over epoch: 0.6756\n",
      "---- Validation ----\n",
      "Validation loss: 35.5326\n",
      "Validation acc: 0.6572\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 440.1965, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 422.4201, Accuracy: 0.6697\n",
      "Training loss (for one batch) at step 20: 419.2408, Accuracy: 0.6682\n",
      "Training loss (for one batch) at step 30: 408.0739, Accuracy: 0.6759\n",
      "Training loss (for one batch) at step 40: 396.6635, Accuracy: 0.6835\n",
      "Training loss (for one batch) at step 50: 372.7107, Accuracy: 0.6945\n",
      "Training loss (for one batch) at step 60: 386.6148, Accuracy: 0.7007\n",
      "Training loss (for one batch) at step 70: 417.9868, Accuracy: 0.7028\n",
      "Training loss (for one batch) at step 80: 401.0511, Accuracy: 0.6960\n",
      "Training loss (for one batch) at step 90: 407.1751, Accuracy: 0.6938\n",
      "Training loss (for one batch) at step 100: 398.7279, Accuracy: 0.6897\n",
      "Training loss (for one batch) at step 110: 407.1316, Accuracy: 0.6902\n",
      "---- Training ----\n",
      "Training loss: 129.3435\n",
      "Training acc over epoch: 0.6898\n",
      "---- Validation ----\n",
      "Validation loss: 42.3872\n",
      "Validation acc: 0.6279\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 413.6110, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 415.0693, Accuracy: 0.6527\n",
      "Training loss (for one batch) at step 20: 411.5908, Accuracy: 0.6529\n",
      "Training loss (for one batch) at step 30: 392.5612, Accuracy: 0.6683\n",
      "Training loss (for one batch) at step 40: 401.4043, Accuracy: 0.6778\n",
      "Training loss (for one batch) at step 50: 393.2407, Accuracy: 0.6852\n",
      "Training loss (for one batch) at step 60: 390.1960, Accuracy: 0.6936\n",
      "Training loss (for one batch) at step 70: 409.2687, Accuracy: 0.6965\n",
      "Training loss (for one batch) at step 80: 416.6042, Accuracy: 0.6885\n",
      "Training loss (for one batch) at step 90: 414.0685, Accuracy: 0.6835\n",
      "Training loss (for one batch) at step 100: 388.9522, Accuracy: 0.6822\n",
      "Training loss (for one batch) at step 110: 391.3025, Accuracy: 0.6837\n",
      "---- Training ----\n",
      "Training loss: 128.1790\n",
      "Training acc over epoch: 0.6844\n",
      "---- Validation ----\n",
      "Validation loss: 35.1384\n",
      "Validation acc: 0.6588\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 428.8602, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 413.6831, Accuracy: 0.6491\n",
      "Training loss (for one batch) at step 20: 404.5892, Accuracy: 0.6551\n",
      "Training loss (for one batch) at step 30: 391.6225, Accuracy: 0.6741\n",
      "Training loss (for one batch) at step 40: 374.5788, Accuracy: 0.6862\n",
      "Training loss (for one batch) at step 50: 373.8885, Accuracy: 0.6968\n",
      "Training loss (for one batch) at step 60: 391.3242, Accuracy: 0.7050\n",
      "Training loss (for one batch) at step 70: 415.4803, Accuracy: 0.7017\n",
      "Training loss (for one batch) at step 80: 430.5156, Accuracy: 0.6966\n",
      "Training loss (for one batch) at step 90: 401.8554, Accuracy: 0.6914\n",
      "Training loss (for one batch) at step 100: 373.3105, Accuracy: 0.6931\n",
      "Training loss (for one batch) at step 110: 400.1744, Accuracy: 0.6928\n",
      "---- Training ----\n",
      "Training loss: 126.5718\n",
      "Training acc over epoch: 0.6932\n",
      "---- Validation ----\n",
      "Validation loss: 39.4051\n",
      "Validation acc: 0.6376\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 432.4169, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 395.5702, Accuracy: 0.6747\n",
      "Training loss (for one batch) at step 20: 413.1171, Accuracy: 0.6734\n",
      "Training loss (for one batch) at step 30: 390.0893, Accuracy: 0.6908\n",
      "Training loss (for one batch) at step 40: 389.2948, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 50: 357.7336, Accuracy: 0.7036\n",
      "Training loss (for one batch) at step 60: 379.8743, Accuracy: 0.7097\n",
      "Training loss (for one batch) at step 70: 397.9568, Accuracy: 0.7048\n",
      "Training loss (for one batch) at step 80: 420.0706, Accuracy: 0.6990\n",
      "Training loss (for one batch) at step 90: 393.4248, Accuracy: 0.6944\n",
      "Training loss (for one batch) at step 100: 385.6151, Accuracy: 0.6918\n",
      "Training loss (for one batch) at step 110: 404.8523, Accuracy: 0.6912\n",
      "---- Training ----\n",
      "Training loss: 124.6055\n",
      "Training acc over epoch: 0.6914\n",
      "---- Validation ----\n",
      "Validation loss: 35.6777\n",
      "Validation acc: 0.6521\n",
      "Time taken: 10.14s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 405.7493, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 408.5085, Accuracy: 0.6364\n",
      "Training loss (for one batch) at step 20: 392.1901, Accuracy: 0.6540\n",
      "Training loss (for one batch) at step 30: 384.9254, Accuracy: 0.6759\n",
      "Training loss (for one batch) at step 40: 376.0117, Accuracy: 0.6852\n",
      "Training loss (for one batch) at step 50: 371.1584, Accuracy: 0.6999\n",
      "Training loss (for one batch) at step 60: 383.2533, Accuracy: 0.7093\n",
      "Training loss (for one batch) at step 70: 406.2626, Accuracy: 0.7079\n",
      "Training loss (for one batch) at step 80: 392.6288, Accuracy: 0.7012\n",
      "Training loss (for one batch) at step 90: 398.7177, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 100: 391.7715, Accuracy: 0.6959\n",
      "Training loss (for one batch) at step 110: 390.9333, Accuracy: 0.6955\n",
      "---- Training ----\n",
      "Training loss: 119.9050\n",
      "Training acc over epoch: 0.6953\n",
      "---- Validation ----\n",
      "Validation loss: 42.0206\n",
      "Validation acc: 0.6513\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 412.2117, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 411.2389, Accuracy: 0.6456\n",
      "Training loss (for one batch) at step 20: 392.0842, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 30: 386.4392, Accuracy: 0.6812\n",
      "Training loss (for one batch) at step 40: 387.5651, Accuracy: 0.6883\n",
      "Training loss (for one batch) at step 50: 377.0733, Accuracy: 0.6987\n",
      "Training loss (for one batch) at step 60: 385.2484, Accuracy: 0.7085\n",
      "Training loss (for one batch) at step 70: 384.9327, Accuracy: 0.7073\n",
      "Training loss (for one batch) at step 80: 404.7691, Accuracy: 0.7000\n",
      "Training loss (for one batch) at step 90: 379.8047, Accuracy: 0.6963\n",
      "Training loss (for one batch) at step 100: 369.8326, Accuracy: 0.6957\n",
      "Training loss (for one batch) at step 110: 379.6447, Accuracy: 0.6968\n",
      "---- Training ----\n",
      "Training loss: 120.9335\n",
      "Training acc over epoch: 0.6959\n",
      "---- Validation ----\n",
      "Validation loss: 38.0016\n",
      "Validation acc: 0.6402\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 410.6569, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 394.4737, Accuracy: 0.6456\n",
      "Training loss (for one batch) at step 20: 379.7472, Accuracy: 0.6689\n",
      "Training loss (for one batch) at step 30: 380.2203, Accuracy: 0.6837\n",
      "Training loss (for one batch) at step 40: 365.1686, Accuracy: 0.7006\n",
      "Training loss (for one batch) at step 50: 374.4036, Accuracy: 0.7126\n",
      "Training loss (for one batch) at step 60: 369.1939, Accuracy: 0.7191\n",
      "Training loss (for one batch) at step 70: 375.1916, Accuracy: 0.7140\n",
      "Training loss (for one batch) at step 80: 400.3820, Accuracy: 0.7048\n",
      "Training loss (for one batch) at step 90: 381.6000, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 100: 378.3156, Accuracy: 0.7013\n",
      "Training loss (for one batch) at step 110: 386.7337, Accuracy: 0.7012\n",
      "---- Training ----\n",
      "Training loss: 118.3763\n",
      "Training acc over epoch: 0.7004\n",
      "---- Validation ----\n",
      "Validation loss: 33.3485\n",
      "Validation acc: 0.6542\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 387.4029, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 397.4678, Accuracy: 0.6669\n",
      "Training loss (for one batch) at step 20: 381.9103, Accuracy: 0.6801\n",
      "Training loss (for one batch) at step 30: 366.4395, Accuracy: 0.6986\n",
      "Training loss (for one batch) at step 40: 377.6591, Accuracy: 0.7079\n",
      "Training loss (for one batch) at step 50: 354.6327, Accuracy: 0.7165\n",
      "Training loss (for one batch) at step 60: 357.5797, Accuracy: 0.7235\n",
      "Training loss (for one batch) at step 70: 403.6763, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 80: 382.9220, Accuracy: 0.7113\n",
      "Training loss (for one batch) at step 90: 375.9883, Accuracy: 0.7046\n",
      "Training loss (for one batch) at step 100: 365.1910, Accuracy: 0.7041\n",
      "Training loss (for one batch) at step 110: 376.7285, Accuracy: 0.7043\n",
      "---- Training ----\n",
      "Training loss: 131.1300\n",
      "Training acc over epoch: 0.7051\n",
      "---- Validation ----\n",
      "Validation loss: 43.4761\n",
      "Validation acc: 0.6566\n",
      "Time taken: 11.29s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 398.8678, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 389.5236, Accuracy: 0.6626\n",
      "Training loss (for one batch) at step 20: 376.0081, Accuracy: 0.6771\n",
      "Training loss (for one batch) at step 30: 356.5399, Accuracy: 0.7041\n",
      "Training loss (for one batch) at step 40: 357.1077, Accuracy: 0.7090\n",
      "Training loss (for one batch) at step 50: 354.7459, Accuracy: 0.7223\n",
      "Training loss (for one batch) at step 60: 368.5401, Accuracy: 0.7253\n",
      "Training loss (for one batch) at step 70: 388.6525, Accuracy: 0.7194\n",
      "Training loss (for one batch) at step 80: 381.7777, Accuracy: 0.7112\n",
      "Training loss (for one batch) at step 90: 365.2902, Accuracy: 0.7054\n",
      "Training loss (for one batch) at step 100: 371.5275, Accuracy: 0.7037\n",
      "Training loss (for one batch) at step 110: 378.9494, Accuracy: 0.7039\n",
      "---- Training ----\n",
      "Training loss: 120.7641\n",
      "Training acc over epoch: 0.7040\n",
      "---- Validation ----\n",
      "Validation loss: 45.7718\n",
      "Validation acc: 0.6464\n",
      "Time taken: 12.19s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 422.0038, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 392.0265, Accuracy: 0.6648\n",
      "Training loss (for one batch) at step 20: 367.7356, Accuracy: 0.6808\n",
      "Training loss (for one batch) at step 30: 369.3170, Accuracy: 0.6993\n",
      "Training loss (for one batch) at step 40: 364.2365, Accuracy: 0.7127\n",
      "Training loss (for one batch) at step 50: 347.5417, Accuracy: 0.7220\n",
      "Training loss (for one batch) at step 60: 372.1507, Accuracy: 0.7262\n",
      "Training loss (for one batch) at step 70: 373.7992, Accuracy: 0.7204\n",
      "Training loss (for one batch) at step 80: 389.7424, Accuracy: 0.7110\n",
      "Training loss (for one batch) at step 90: 375.4104, Accuracy: 0.7096\n",
      "Training loss (for one batch) at step 100: 364.6117, Accuracy: 0.7077\n",
      "Training loss (for one batch) at step 110: 371.6334, Accuracy: 0.7066\n",
      "---- Training ----\n",
      "Training loss: 126.1730\n",
      "Training acc over epoch: 0.7062\n",
      "---- Validation ----\n",
      "Validation loss: 32.2229\n",
      "Validation acc: 0.6214\n",
      "Time taken: 10.51s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 395.0305, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 361.5311, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 20: 378.6749, Accuracy: 0.6812\n",
      "Training loss (for one batch) at step 30: 349.5057, Accuracy: 0.6971\n",
      "Training loss (for one batch) at step 40: 355.7311, Accuracy: 0.7115\n",
      "Training loss (for one batch) at step 50: 330.5703, Accuracy: 0.7175\n",
      "Training loss (for one batch) at step 60: 360.2995, Accuracy: 0.7230\n",
      "Training loss (for one batch) at step 70: 363.9060, Accuracy: 0.7173\n",
      "Training loss (for one batch) at step 80: 375.2030, Accuracy: 0.7105\n",
      "Training loss (for one batch) at step 90: 364.9271, Accuracy: 0.7057\n",
      "Training loss (for one batch) at step 100: 364.8786, Accuracy: 0.7050\n",
      "Training loss (for one batch) at step 110: 374.8167, Accuracy: 0.7057\n",
      "---- Training ----\n",
      "Training loss: 120.0550\n",
      "Training acc over epoch: 0.7059\n",
      "---- Validation ----\n",
      "Validation loss: 36.9044\n",
      "Validation acc: 0.6445\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 399.8241, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 391.7249, Accuracy: 0.6768\n",
      "Training loss (for one batch) at step 20: 352.6729, Accuracy: 0.6897\n",
      "Training loss (for one batch) at step 30: 349.3596, Accuracy: 0.7021\n",
      "Training loss (for one batch) at step 40: 337.5013, Accuracy: 0.7092\n",
      "Training loss (for one batch) at step 50: 341.7201, Accuracy: 0.7207\n",
      "Training loss (for one batch) at step 60: 354.1206, Accuracy: 0.7275\n",
      "Training loss (for one batch) at step 70: 373.3303, Accuracy: 0.7203\n",
      "Training loss (for one batch) at step 80: 376.6382, Accuracy: 0.7131\n",
      "Training loss (for one batch) at step 90: 363.4235, Accuracy: 0.7072\n",
      "Training loss (for one batch) at step 100: 356.9855, Accuracy: 0.7061\n",
      "Training loss (for one batch) at step 110: 375.0024, Accuracy: 0.7092\n",
      "---- Training ----\n",
      "Training loss: 111.3231\n",
      "Training acc over epoch: 0.7089\n",
      "---- Validation ----\n",
      "Validation loss: 44.5618\n",
      "Validation acc: 0.6572\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 388.7613, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 390.3793, Accuracy: 0.6662\n",
      "Training loss (for one batch) at step 20: 360.5916, Accuracy: 0.6853\n",
      "Training loss (for one batch) at step 30: 365.4194, Accuracy: 0.7021\n",
      "Training loss (for one batch) at step 40: 345.3167, Accuracy: 0.7125\n",
      "Training loss (for one batch) at step 50: 315.0141, Accuracy: 0.7267\n",
      "Training loss (for one batch) at step 60: 341.3316, Accuracy: 0.7340\n",
      "Training loss (for one batch) at step 70: 385.3573, Accuracy: 0.7278\n",
      "Training loss (for one batch) at step 80: 385.5650, Accuracy: 0.7184\n",
      "Training loss (for one batch) at step 90: 366.1307, Accuracy: 0.7133\n",
      "Training loss (for one batch) at step 100: 351.0605, Accuracy: 0.7138\n",
      "Training loss (for one batch) at step 110: 359.9344, Accuracy: 0.7111\n",
      "---- Training ----\n",
      "Training loss: 108.4830\n",
      "Training acc over epoch: 0.7098\n",
      "---- Validation ----\n",
      "Validation loss: 37.6136\n",
      "Validation acc: 0.6523\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 395.8007, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 376.0215, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 20: 355.9445, Accuracy: 0.6682\n",
      "Training loss (for one batch) at step 30: 348.7269, Accuracy: 0.6900\n",
      "Training loss (for one batch) at step 40: 348.0658, Accuracy: 0.7010\n",
      "Training loss (for one batch) at step 50: 350.2504, Accuracy: 0.7155\n",
      "Training loss (for one batch) at step 60: 365.0325, Accuracy: 0.7228\n",
      "Training loss (for one batch) at step 70: 408.2074, Accuracy: 0.7178\n",
      "Training loss (for one batch) at step 80: 375.5709, Accuracy: 0.7081\n",
      "Training loss (for one batch) at step 90: 357.0243, Accuracy: 0.7052\n",
      "Training loss (for one batch) at step 100: 346.0207, Accuracy: 0.7061\n",
      "Training loss (for one batch) at step 110: 354.8882, Accuracy: 0.7060\n",
      "---- Training ----\n",
      "Training loss: 120.5391\n",
      "Training acc over epoch: 0.7057\n",
      "---- Validation ----\n",
      "Validation loss: 38.6120\n",
      "Validation acc: 0.6376\n",
      "Time taken: 10.40s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 406.2399, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 379.2462, Accuracy: 0.6420\n",
      "Training loss (for one batch) at step 20: 337.9301, Accuracy: 0.6696\n",
      "Training loss (for one batch) at step 30: 349.2654, Accuracy: 0.6885\n",
      "Training loss (for one batch) at step 40: 342.3884, Accuracy: 0.7069\n",
      "Training loss (for one batch) at step 50: 333.6564, Accuracy: 0.7218\n",
      "Training loss (for one batch) at step 60: 368.4825, Accuracy: 0.7267\n",
      "Training loss (for one batch) at step 70: 350.7719, Accuracy: 0.7189\n",
      "Training loss (for one batch) at step 80: 385.9657, Accuracy: 0.7111\n",
      "Training loss (for one batch) at step 90: 349.3687, Accuracy: 0.7066\n",
      "Training loss (for one batch) at step 100: 339.5627, Accuracy: 0.7065\n",
      "Training loss (for one batch) at step 110: 349.1967, Accuracy: 0.7069\n",
      "---- Training ----\n",
      "Training loss: 105.4745\n",
      "Training acc over epoch: 0.7065\n",
      "---- Validation ----\n",
      "Validation loss: 48.4763\n",
      "Validation acc: 0.6580\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 380.6907, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 351.7065, Accuracy: 0.6584\n",
      "Training loss (for one batch) at step 20: 351.4167, Accuracy: 0.6842\n",
      "Training loss (for one batch) at step 30: 339.0676, Accuracy: 0.6993\n",
      "Training loss (for one batch) at step 40: 327.2021, Accuracy: 0.7146\n",
      "Training loss (for one batch) at step 50: 326.1074, Accuracy: 0.7267\n",
      "Training loss (for one batch) at step 60: 354.7333, Accuracy: 0.7317\n",
      "Training loss (for one batch) at step 70: 376.8756, Accuracy: 0.7234\n",
      "Training loss (for one batch) at step 80: 374.6837, Accuracy: 0.7143\n",
      "Training loss (for one batch) at step 90: 341.6039, Accuracy: 0.7096\n",
      "Training loss (for one batch) at step 100: 351.7606, Accuracy: 0.7103\n",
      "Training loss (for one batch) at step 110: 343.5171, Accuracy: 0.7111\n",
      "---- Training ----\n",
      "Training loss: 105.8330\n",
      "Training acc over epoch: 0.7108\n",
      "---- Validation ----\n",
      "Validation loss: 34.8605\n",
      "Validation acc: 0.6470\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 366.3489, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 350.5786, Accuracy: 0.6435\n",
      "Training loss (for one batch) at step 20: 342.5337, Accuracy: 0.6749\n",
      "Training loss (for one batch) at step 30: 330.1980, Accuracy: 0.7001\n",
      "Training loss (for one batch) at step 40: 343.1546, Accuracy: 0.7149\n",
      "Training loss (for one batch) at step 50: 329.1336, Accuracy: 0.7230\n",
      "Training loss (for one batch) at step 60: 342.1407, Accuracy: 0.7287\n",
      "Training loss (for one batch) at step 70: 353.6490, Accuracy: 0.7204\n",
      "Training loss (for one batch) at step 80: 340.4746, Accuracy: 0.7123\n",
      "Training loss (for one batch) at step 90: 335.4141, Accuracy: 0.7101\n",
      "Training loss (for one batch) at step 100: 353.3842, Accuracy: 0.7112\n",
      "Training loss (for one batch) at step 110: 359.8545, Accuracy: 0.7104\n",
      "---- Training ----\n",
      "Training loss: 114.4458\n",
      "Training acc over epoch: 0.7100\n",
      "---- Validation ----\n",
      "Validation loss: 35.2386\n",
      "Validation acc: 0.6625\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 372.8420, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 363.0014, Accuracy: 0.6697\n",
      "Training loss (for one batch) at step 20: 346.6495, Accuracy: 0.6905\n",
      "Training loss (for one batch) at step 30: 349.0076, Accuracy: 0.7029\n",
      "Training loss (for one batch) at step 40: 316.9874, Accuracy: 0.7157\n",
      "Training loss (for one batch) at step 50: 340.0175, Accuracy: 0.7264\n",
      "Training loss (for one batch) at step 60: 342.3655, Accuracy: 0.7336\n",
      "Training loss (for one batch) at step 70: 354.7007, Accuracy: 0.7256\n",
      "Training loss (for one batch) at step 80: 372.3325, Accuracy: 0.7131\n",
      "Training loss (for one batch) at step 90: 367.7968, Accuracy: 0.7096\n",
      "Training loss (for one batch) at step 100: 327.1487, Accuracy: 0.7099\n",
      "Training loss (for one batch) at step 110: 357.9211, Accuracy: 0.7107\n",
      "---- Training ----\n",
      "Training loss: 108.5872\n",
      "Training acc over epoch: 0.7086\n",
      "---- Validation ----\n",
      "Validation loss: 45.3256\n",
      "Validation acc: 0.6515\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 361.7023, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 358.9377, Accuracy: 0.6491\n",
      "Training loss (for one batch) at step 20: 374.1506, Accuracy: 0.6696\n",
      "Training loss (for one batch) at step 30: 342.2412, Accuracy: 0.6966\n",
      "Training loss (for one batch) at step 40: 324.1835, Accuracy: 0.7100\n",
      "Training loss (for one batch) at step 50: 309.4109, Accuracy: 0.7206\n",
      "Training loss (for one batch) at step 60: 348.2733, Accuracy: 0.7262\n",
      "Training loss (for one batch) at step 70: 354.3849, Accuracy: 0.7185\n",
      "Training loss (for one batch) at step 80: 352.4062, Accuracy: 0.7115\n",
      "Training loss (for one batch) at step 90: 348.9014, Accuracy: 0.7069\n",
      "Training loss (for one batch) at step 100: 325.6664, Accuracy: 0.7082\n",
      "Training loss (for one batch) at step 110: 329.8313, Accuracy: 0.7082\n",
      "---- Training ----\n",
      "Training loss: 117.5464\n",
      "Training acc over epoch: 0.7081\n",
      "---- Validation ----\n",
      "Validation loss: 46.3713\n",
      "Validation acc: 0.6488\n",
      "Time taken: 10.67s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 366.5514, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 354.2806, Accuracy: 0.6499\n",
      "Training loss (for one batch) at step 20: 346.4790, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 30: 345.3723, Accuracy: 0.7006\n",
      "Training loss (for one batch) at step 40: 332.0597, Accuracy: 0.7170\n",
      "Training loss (for one batch) at step 50: 333.4581, Accuracy: 0.7267\n",
      "Training loss (for one batch) at step 60: 323.6428, Accuracy: 0.7334\n",
      "Training loss (for one batch) at step 70: 368.7573, Accuracy: 0.7230\n",
      "Training loss (for one batch) at step 80: 371.3370, Accuracy: 0.7145\n",
      "Training loss (for one batch) at step 90: 341.9140, Accuracy: 0.7105\n",
      "Training loss (for one batch) at step 100: 325.8442, Accuracy: 0.7119\n",
      "Training loss (for one batch) at step 110: 335.3538, Accuracy: 0.7109\n",
      "---- Training ----\n",
      "Training loss: 113.2872\n",
      "Training acc over epoch: 0.7108\n",
      "---- Validation ----\n",
      "Validation loss: 72.8730\n",
      "Validation acc: 0.6550\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 350.4925, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 385.0147, Accuracy: 0.6598\n",
      "Training loss (for one batch) at step 20: 342.6980, Accuracy: 0.6946\n",
      "Training loss (for one batch) at step 30: 332.8332, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 40: 320.5020, Accuracy: 0.7250\n",
      "Training loss (for one batch) at step 50: 321.9221, Accuracy: 0.7333\n",
      "Training loss (for one batch) at step 60: 326.5078, Accuracy: 0.7390\n",
      "Training loss (for one batch) at step 70: 360.6611, Accuracy: 0.7294\n",
      "Training loss (for one batch) at step 80: 353.3331, Accuracy: 0.7183\n",
      "Training loss (for one batch) at step 90: 330.9453, Accuracy: 0.7156\n",
      "Training loss (for one batch) at step 100: 345.6359, Accuracy: 0.7164\n",
      "Training loss (for one batch) at step 110: 331.3849, Accuracy: 0.7170\n",
      "---- Training ----\n",
      "Training loss: 117.3638\n",
      "Training acc over epoch: 0.7159\n",
      "---- Validation ----\n",
      "Validation loss: 47.0045\n",
      "Validation acc: 0.6373\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 375.2150, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 368.5721, Accuracy: 0.6463\n",
      "Training loss (for one batch) at step 20: 325.5141, Accuracy: 0.6763\n",
      "Training loss (for one batch) at step 30: 326.6799, Accuracy: 0.6983\n",
      "Training loss (for one batch) at step 40: 343.1075, Accuracy: 0.7054\n",
      "Training loss (for one batch) at step 50: 311.8933, Accuracy: 0.7232\n",
      "Training loss (for one batch) at step 60: 341.1591, Accuracy: 0.7287\n",
      "Training loss (for one batch) at step 70: 362.5487, Accuracy: 0.7210\n",
      "Training loss (for one batch) at step 80: 363.2787, Accuracy: 0.7115\n",
      "Training loss (for one batch) at step 90: 338.0331, Accuracy: 0.7097\n",
      "Training loss (for one batch) at step 100: 328.8445, Accuracy: 0.7111\n",
      "Training loss (for one batch) at step 110: 330.0342, Accuracy: 0.7114\n",
      "---- Training ----\n",
      "Training loss: 106.3711\n",
      "Training acc over epoch: 0.7113\n",
      "---- Validation ----\n",
      "Validation loss: 42.1922\n",
      "Validation acc: 0.6376\n",
      "Time taken: 10.40s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 365.3486, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 353.2776, Accuracy: 0.6413\n",
      "Training loss (for one batch) at step 20: 336.2479, Accuracy: 0.6722\n",
      "Training loss (for one batch) at step 30: 331.3547, Accuracy: 0.6958\n",
      "Training loss (for one batch) at step 40: 319.8148, Accuracy: 0.7102\n",
      "Training loss (for one batch) at step 50: 308.8466, Accuracy: 0.7264\n",
      "Training loss (for one batch) at step 60: 323.6310, Accuracy: 0.7328\n",
      "Training loss (for one batch) at step 70: 360.7274, Accuracy: 0.7236\n",
      "Training loss (for one batch) at step 80: 361.5551, Accuracy: 0.7137\n",
      "Training loss (for one batch) at step 90: 328.5430, Accuracy: 0.7089\n",
      "Training loss (for one batch) at step 100: 338.2743, Accuracy: 0.7108\n",
      "Training loss (for one batch) at step 110: 328.1826, Accuracy: 0.7111\n",
      "---- Training ----\n",
      "Training loss: 110.9466\n",
      "Training acc over epoch: 0.7102\n",
      "---- Validation ----\n",
      "Validation loss: 42.8430\n",
      "Validation acc: 0.6609\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 362.1894, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 353.8573, Accuracy: 0.6648\n",
      "Training loss (for one batch) at step 20: 347.5197, Accuracy: 0.6830\n",
      "Training loss (for one batch) at step 30: 322.5881, Accuracy: 0.7021\n",
      "Training loss (for one batch) at step 40: 300.5858, Accuracy: 0.7172\n",
      "Training loss (for one batch) at step 50: 318.7891, Accuracy: 0.7264\n",
      "Training loss (for one batch) at step 60: 340.5341, Accuracy: 0.7340\n",
      "Training loss (for one batch) at step 70: 342.9028, Accuracy: 0.7260\n",
      "Training loss (for one batch) at step 80: 367.1940, Accuracy: 0.7174\n",
      "Training loss (for one batch) at step 90: 317.5741, Accuracy: 0.7119\n",
      "Training loss (for one batch) at step 100: 311.7258, Accuracy: 0.7138\n",
      "Training loss (for one batch) at step 110: 320.2265, Accuracy: 0.7126\n",
      "---- Training ----\n",
      "Training loss: 98.5571\n",
      "Training acc over epoch: 0.7109\n",
      "---- Validation ----\n",
      "Validation loss: 34.6998\n",
      "Validation acc: 0.6464\n",
      "Time taken: 12.70s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 375.8122, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 376.9221, Accuracy: 0.6690\n",
      "Training loss (for one batch) at step 20: 332.1642, Accuracy: 0.6793\n",
      "Training loss (for one batch) at step 30: 315.4095, Accuracy: 0.7019\n",
      "Training loss (for one batch) at step 40: 319.4224, Accuracy: 0.7119\n",
      "Training loss (for one batch) at step 50: 307.9105, Accuracy: 0.7253\n",
      "Training loss (for one batch) at step 60: 330.1432, Accuracy: 0.7327\n",
      "Training loss (for one batch) at step 70: 365.4248, Accuracy: 0.7238\n",
      "Training loss (for one batch) at step 80: 348.4172, Accuracy: 0.7133\n",
      "Training loss (for one batch) at step 90: 338.7339, Accuracy: 0.7108\n",
      "Training loss (for one batch) at step 100: 318.7526, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 110: 316.6461, Accuracy: 0.7122\n",
      "---- Training ----\n",
      "Training loss: 103.0629\n",
      "Training acc over epoch: 0.7114\n",
      "---- Validation ----\n",
      "Validation loss: 49.6918\n",
      "Validation acc: 0.6526\n",
      "Time taken: 10.66s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 359.9941, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 361.5914, Accuracy: 0.6527\n",
      "Training loss (for one batch) at step 20: 335.9961, Accuracy: 0.6812\n",
      "Training loss (for one batch) at step 30: 307.0482, Accuracy: 0.7026\n",
      "Training loss (for one batch) at step 40: 314.8206, Accuracy: 0.7153\n",
      "Training loss (for one batch) at step 50: 298.5941, Accuracy: 0.7269\n",
      "Training loss (for one batch) at step 60: 315.3130, Accuracy: 0.7331\n",
      "Training loss (for one batch) at step 70: 342.6002, Accuracy: 0.7244\n",
      "Training loss (for one batch) at step 80: 341.3903, Accuracy: 0.7136\n",
      "Training loss (for one batch) at step 90: 328.2911, Accuracy: 0.7097\n",
      "Training loss (for one batch) at step 100: 322.4820, Accuracy: 0.7111\n",
      "Training loss (for one batch) at step 110: 328.8881, Accuracy: 0.7113\n",
      "---- Training ----\n",
      "Training loss: 105.7852\n",
      "Training acc over epoch: 0.7102\n",
      "---- Validation ----\n",
      "Validation loss: 49.3206\n",
      "Validation acc: 0.6499\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 363.4676, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 346.2299, Accuracy: 0.6534\n",
      "Training loss (for one batch) at step 20: 346.9663, Accuracy: 0.6827\n",
      "Training loss (for one batch) at step 30: 303.4323, Accuracy: 0.7041\n",
      "Training loss (for one batch) at step 40: 309.5841, Accuracy: 0.7214\n",
      "Training loss (for one batch) at step 50: 304.1176, Accuracy: 0.7335\n",
      "Training loss (for one batch) at step 60: 335.5953, Accuracy: 0.7358\n",
      "Training loss (for one batch) at step 70: 352.8982, Accuracy: 0.7229\n",
      "Training loss (for one batch) at step 80: 338.5437, Accuracy: 0.7146\n",
      "Training loss (for one batch) at step 90: 329.9382, Accuracy: 0.7108\n",
      "Training loss (for one batch) at step 100: 320.2148, Accuracy: 0.7095\n",
      "Training loss (for one batch) at step 110: 327.3660, Accuracy: 0.7110\n",
      "---- Training ----\n",
      "Training loss: 108.5962\n",
      "Training acc over epoch: 0.7107\n",
      "---- Validation ----\n",
      "Validation loss: 38.3716\n",
      "Validation acc: 0.6529\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 353.2349, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 349.9385, Accuracy: 0.6648\n",
      "Training loss (for one batch) at step 20: 309.4529, Accuracy: 0.6812\n",
      "Training loss (for one batch) at step 30: 304.5126, Accuracy: 0.6978\n",
      "Training loss (for one batch) at step 40: 305.9761, Accuracy: 0.7121\n",
      "Training loss (for one batch) at step 50: 303.4037, Accuracy: 0.7235\n",
      "Training loss (for one batch) at step 60: 313.6484, Accuracy: 0.7289\n",
      "Training loss (for one batch) at step 70: 340.2781, Accuracy: 0.7203\n",
      "Training loss (for one batch) at step 80: 334.9407, Accuracy: 0.7123\n",
      "Training loss (for one batch) at step 90: 336.2670, Accuracy: 0.7072\n",
      "Training loss (for one batch) at step 100: 321.4911, Accuracy: 0.7090\n",
      "Training loss (for one batch) at step 110: 321.4479, Accuracy: 0.7092\n",
      "---- Training ----\n",
      "Training loss: 97.3923\n",
      "Training acc over epoch: 0.7090\n",
      "---- Validation ----\n",
      "Validation loss: 35.9138\n",
      "Validation acc: 0.6510\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 343.5872, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 353.6132, Accuracy: 0.6463\n",
      "Training loss (for one batch) at step 20: 327.5952, Accuracy: 0.6771\n",
      "Training loss (for one batch) at step 30: 302.1464, Accuracy: 0.7009\n",
      "Training loss (for one batch) at step 40: 304.8513, Accuracy: 0.7140\n",
      "Training loss (for one batch) at step 50: 290.8011, Accuracy: 0.7299\n",
      "Training loss (for one batch) at step 60: 318.2678, Accuracy: 0.7340\n",
      "Training loss (for one batch) at step 70: 337.5380, Accuracy: 0.7244\n",
      "Training loss (for one batch) at step 80: 327.0921, Accuracy: 0.7125\n",
      "Training loss (for one batch) at step 90: 308.6171, Accuracy: 0.7083\n",
      "Training loss (for one batch) at step 100: 340.3694, Accuracy: 0.7085\n",
      "Training loss (for one batch) at step 110: 319.6446, Accuracy: 0.7092\n",
      "---- Training ----\n",
      "Training loss: 103.7855\n",
      "Training acc over epoch: 0.7103\n",
      "---- Validation ----\n",
      "Validation loss: 38.6809\n",
      "Validation acc: 0.6488\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 357.6287, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 355.1992, Accuracy: 0.6420\n",
      "Training loss (for one batch) at step 20: 308.5407, Accuracy: 0.6741\n",
      "Training loss (for one batch) at step 30: 302.8344, Accuracy: 0.6991\n",
      "Training loss (for one batch) at step 40: 320.4262, Accuracy: 0.7100\n",
      "Training loss (for one batch) at step 50: 301.4570, Accuracy: 0.7210\n",
      "Training loss (for one batch) at step 60: 308.4926, Accuracy: 0.7286\n",
      "Training loss (for one batch) at step 70: 336.5793, Accuracy: 0.7186\n",
      "Training loss (for one batch) at step 80: 328.9086, Accuracy: 0.7101\n",
      "Training loss (for one batch) at step 90: 316.9226, Accuracy: 0.7077\n",
      "Training loss (for one batch) at step 100: 307.5984, Accuracy: 0.7078\n",
      "Training loss (for one batch) at step 110: 317.6131, Accuracy: 0.7092\n",
      "---- Training ----\n",
      "Training loss: 106.2154\n",
      "Training acc over epoch: 0.7081\n",
      "---- Validation ----\n",
      "Validation loss: 32.3370\n",
      "Validation acc: 0.6548\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 354.5351, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 339.7183, Accuracy: 0.6506\n",
      "Training loss (for one batch) at step 20: 314.1729, Accuracy: 0.6815\n",
      "Training loss (for one batch) at step 30: 310.6850, Accuracy: 0.7051\n",
      "Training loss (for one batch) at step 40: 301.9405, Accuracy: 0.7189\n",
      "Training loss (for one batch) at step 50: 281.4833, Accuracy: 0.7327\n",
      "Training loss (for one batch) at step 60: 333.8583, Accuracy: 0.7360\n",
      "Training loss (for one batch) at step 70: 350.9283, Accuracy: 0.7245\n",
      "Training loss (for one batch) at step 80: 356.6786, Accuracy: 0.7151\n",
      "Training loss (for one batch) at step 90: 334.0965, Accuracy: 0.7127\n",
      "Training loss (for one batch) at step 100: 319.7593, Accuracy: 0.7130\n",
      "Training loss (for one batch) at step 110: 330.8271, Accuracy: 0.7119\n",
      "---- Training ----\n",
      "Training loss: 115.9503\n",
      "Training acc over epoch: 0.7112\n",
      "---- Validation ----\n",
      "Validation loss: 42.1967\n",
      "Validation acc: 0.6381\n",
      "Time taken: 10.69s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 355.6003, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 349.6792, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 20: 335.8799, Accuracy: 0.6864\n",
      "Training loss (for one batch) at step 30: 305.7561, Accuracy: 0.7067\n",
      "Training loss (for one batch) at step 40: 308.0554, Accuracy: 0.7201\n",
      "Training loss (for one batch) at step 50: 307.7824, Accuracy: 0.7312\n",
      "Training loss (for one batch) at step 60: 338.1073, Accuracy: 0.7364\n",
      "Training loss (for one batch) at step 70: 335.6459, Accuracy: 0.7296\n",
      "Training loss (for one batch) at step 80: 332.8931, Accuracy: 0.7199\n",
      "Training loss (for one batch) at step 90: 329.7810, Accuracy: 0.7157\n",
      "Training loss (for one batch) at step 100: 306.8683, Accuracy: 0.7172\n",
      "Training loss (for one batch) at step 110: 322.9661, Accuracy: 0.7178\n",
      "---- Training ----\n",
      "Training loss: 123.1836\n",
      "Training acc over epoch: 0.7161\n",
      "---- Validation ----\n",
      "Validation loss: 34.3086\n",
      "Validation acc: 0.6488\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 341.1373, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 327.0091, Accuracy: 0.6591\n",
      "Training loss (for one batch) at step 20: 330.4985, Accuracy: 0.6815\n",
      "Training loss (for one batch) at step 30: 306.8971, Accuracy: 0.7014\n",
      "Training loss (for one batch) at step 40: 305.7418, Accuracy: 0.7147\n",
      "Training loss (for one batch) at step 50: 315.0515, Accuracy: 0.7299\n",
      "Training loss (for one batch) at step 60: 318.8816, Accuracy: 0.7322\n",
      "Training loss (for one batch) at step 70: 347.5781, Accuracy: 0.7256\n",
      "Training loss (for one batch) at step 80: 331.9995, Accuracy: 0.7158\n",
      "Training loss (for one batch) at step 90: 306.3313, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 100: 311.4246, Accuracy: 0.7119\n",
      "Training loss (for one batch) at step 110: 336.8458, Accuracy: 0.7118\n",
      "---- Training ----\n",
      "Training loss: 102.2513\n",
      "Training acc over epoch: 0.7110\n",
      "---- Validation ----\n",
      "Validation loss: 44.0912\n",
      "Validation acc: 0.6456\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 347.8896, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 339.2719, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 20: 322.4846, Accuracy: 0.6786\n",
      "Training loss (for one batch) at step 30: 302.1073, Accuracy: 0.6978\n",
      "Training loss (for one batch) at step 40: 291.5275, Accuracy: 0.7161\n",
      "Training loss (for one batch) at step 50: 295.9958, Accuracy: 0.7275\n",
      "Training loss (for one batch) at step 60: 311.1839, Accuracy: 0.7326\n",
      "Training loss (for one batch) at step 70: 326.1017, Accuracy: 0.7230\n",
      "Training loss (for one batch) at step 80: 322.6495, Accuracy: 0.7153\n",
      "Training loss (for one batch) at step 90: 331.1934, Accuracy: 0.7108\n",
      "Training loss (for one batch) at step 100: 319.9769, Accuracy: 0.7119\n",
      "Training loss (for one batch) at step 110: 307.7204, Accuracy: 0.7128\n",
      "---- Training ----\n",
      "Training loss: 103.2624\n",
      "Training acc over epoch: 0.7120\n",
      "---- Validation ----\n",
      "Validation loss: 31.2476\n",
      "Validation acc: 0.6475\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 353.7822, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 318.4579, Accuracy: 0.6477\n",
      "Training loss (for one batch) at step 20: 307.7771, Accuracy: 0.6812\n",
      "Training loss (for one batch) at step 30: 319.6187, Accuracy: 0.7029\n",
      "Training loss (for one batch) at step 40: 294.9396, Accuracy: 0.7153\n",
      "Training loss (for one batch) at step 50: 290.3587, Accuracy: 0.7281\n",
      "Training loss (for one batch) at step 60: 300.1141, Accuracy: 0.7353\n",
      "Training loss (for one batch) at step 70: 335.5474, Accuracy: 0.7270\n",
      "Training loss (for one batch) at step 80: 336.2194, Accuracy: 0.7179\n",
      "Training loss (for one batch) at step 90: 333.0027, Accuracy: 0.7132\n",
      "Training loss (for one batch) at step 100: 317.9388, Accuracy: 0.7126\n",
      "Training loss (for one batch) at step 110: 318.1158, Accuracy: 0.7140\n",
      "---- Training ----\n",
      "Training loss: 124.3347\n",
      "Training acc over epoch: 0.7130\n",
      "---- Validation ----\n",
      "Validation loss: 40.1026\n",
      "Validation acc: 0.6663\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 354.1622, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 332.9890, Accuracy: 0.6669\n",
      "Training loss (for one batch) at step 20: 286.7632, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 30: 300.2140, Accuracy: 0.7069\n",
      "Training loss (for one batch) at step 40: 307.8846, Accuracy: 0.7197\n",
      "Training loss (for one batch) at step 50: 297.7538, Accuracy: 0.7307\n",
      "Training loss (for one batch) at step 60: 300.9166, Accuracy: 0.7357\n",
      "Training loss (for one batch) at step 70: 324.6432, Accuracy: 0.7265\n",
      "Training loss (for one batch) at step 80: 336.4782, Accuracy: 0.7160\n",
      "Training loss (for one batch) at step 90: 308.0538, Accuracy: 0.7144\n",
      "Training loss (for one batch) at step 100: 304.3218, Accuracy: 0.7157\n",
      "Training loss (for one batch) at step 110: 329.1225, Accuracy: 0.7145\n",
      "---- Training ----\n",
      "Training loss: 97.5865\n",
      "Training acc over epoch: 0.7151\n",
      "---- Validation ----\n",
      "Validation loss: 40.1837\n",
      "Validation acc: 0.6628\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 369.2448, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 345.5971, Accuracy: 0.6655\n",
      "Training loss (for one batch) at step 20: 304.0285, Accuracy: 0.6838\n",
      "Training loss (for one batch) at step 30: 292.5935, Accuracy: 0.7004\n",
      "Training loss (for one batch) at step 40: 305.4291, Accuracy: 0.7189\n",
      "Training loss (for one batch) at step 50: 299.2843, Accuracy: 0.7325\n",
      "Training loss (for one batch) at step 60: 308.2372, Accuracy: 0.7387\n",
      "Training loss (for one batch) at step 70: 342.5549, Accuracy: 0.7296\n",
      "Training loss (for one batch) at step 80: 321.8821, Accuracy: 0.7179\n",
      "Training loss (for one batch) at step 90: 299.6087, Accuracy: 0.7155\n",
      "Training loss (for one batch) at step 100: 316.8410, Accuracy: 0.7157\n",
      "Training loss (for one batch) at step 110: 320.5011, Accuracy: 0.7164\n",
      "---- Training ----\n",
      "Training loss: 106.4722\n",
      "Training acc over epoch: 0.7151\n",
      "---- Validation ----\n",
      "Validation loss: 46.0598\n",
      "Validation acc: 0.6518\n",
      "Time taken: 10.64s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 352.3231, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 325.5026, Accuracy: 0.6541\n",
      "Training loss (for one batch) at step 20: 328.1961, Accuracy: 0.6760\n",
      "Training loss (for one batch) at step 30: 319.2217, Accuracy: 0.7014\n",
      "Training loss (for one batch) at step 40: 304.5975, Accuracy: 0.7203\n",
      "Training loss (for one batch) at step 50: 300.2347, Accuracy: 0.7305\n",
      "Training loss (for one batch) at step 60: 311.3564, Accuracy: 0.7373\n",
      "Training loss (for one batch) at step 70: 355.8800, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 80: 322.9061, Accuracy: 0.7181\n",
      "Training loss (for one batch) at step 90: 303.9975, Accuracy: 0.7161\n",
      "Training loss (for one batch) at step 100: 311.9958, Accuracy: 0.7156\n",
      "Training loss (for one batch) at step 110: 324.8549, Accuracy: 0.7158\n",
      "---- Training ----\n",
      "Training loss: 92.3042\n",
      "Training acc over epoch: 0.7151\n",
      "---- Validation ----\n",
      "Validation loss: 55.9508\n",
      "Validation acc: 0.6593\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 345.5574, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 344.2597, Accuracy: 0.6612\n",
      "Training loss (for one batch) at step 20: 299.3604, Accuracy: 0.6849\n",
      "Training loss (for one batch) at step 30: 312.6024, Accuracy: 0.7044\n",
      "Training loss (for one batch) at step 40: 296.9342, Accuracy: 0.7182\n",
      "Training loss (for one batch) at step 50: 307.4825, Accuracy: 0.7307\n",
      "Training loss (for one batch) at step 60: 311.7630, Accuracy: 0.7348\n",
      "Training loss (for one batch) at step 70: 327.6843, Accuracy: 0.7269\n",
      "Training loss (for one batch) at step 80: 362.1800, Accuracy: 0.7173\n",
      "Training loss (for one batch) at step 90: 310.5828, Accuracy: 0.7133\n",
      "Training loss (for one batch) at step 100: 307.4707, Accuracy: 0.7140\n",
      "Training loss (for one batch) at step 110: 298.1481, Accuracy: 0.7148\n",
      "---- Training ----\n",
      "Training loss: 93.3063\n",
      "Training acc over epoch: 0.7143\n",
      "---- Validation ----\n",
      "Validation loss: 76.8989\n",
      "Validation acc: 0.6545\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 339.9353, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 318.8325, Accuracy: 0.6506\n",
      "Training loss (for one batch) at step 20: 303.7021, Accuracy: 0.6808\n",
      "Training loss (for one batch) at step 30: 309.3328, Accuracy: 0.7029\n",
      "Training loss (for one batch) at step 40: 307.7154, Accuracy: 0.7170\n",
      "Training loss (for one batch) at step 50: 296.7722, Accuracy: 0.7284\n",
      "Training loss (for one batch) at step 60: 295.2213, Accuracy: 0.7336\n",
      "Training loss (for one batch) at step 70: 331.2217, Accuracy: 0.7255\n",
      "Training loss (for one batch) at step 80: 359.6641, Accuracy: 0.7158\n",
      "Training loss (for one batch) at step 90: 312.2312, Accuracy: 0.7128\n",
      "Training loss (for one batch) at step 100: 304.7019, Accuracy: 0.7138\n",
      "Training loss (for one batch) at step 110: 299.2112, Accuracy: 0.7128\n",
      "---- Training ----\n",
      "Training loss: 96.3829\n",
      "Training acc over epoch: 0.7125\n",
      "---- Validation ----\n",
      "Validation loss: 47.1235\n",
      "Validation acc: 0.6677\n",
      "Time taken: 10.61s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 353.3742, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 347.0436, Accuracy: 0.6541\n",
      "Training loss (for one batch) at step 20: 301.8974, Accuracy: 0.6830\n",
      "Training loss (for one batch) at step 30: 297.4001, Accuracy: 0.7046\n",
      "Training loss (for one batch) at step 40: 298.4462, Accuracy: 0.7191\n",
      "Training loss (for one batch) at step 50: 305.1683, Accuracy: 0.7305\n",
      "Training loss (for one batch) at step 60: 315.6900, Accuracy: 0.7367\n",
      "Training loss (for one batch) at step 70: 310.7242, Accuracy: 0.7261\n",
      "Training loss (for one batch) at step 80: 325.0247, Accuracy: 0.7192\n",
      "Training loss (for one batch) at step 90: 295.0860, Accuracy: 0.7165\n",
      "Training loss (for one batch) at step 100: 302.9163, Accuracy: 0.7165\n",
      "Training loss (for one batch) at step 110: 297.2534, Accuracy: 0.7158\n",
      "---- Training ----\n",
      "Training loss: 97.6346\n",
      "Training acc over epoch: 0.7146\n",
      "---- Validation ----\n",
      "Validation loss: 43.2548\n",
      "Validation acc: 0.6593\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 344.4968, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 335.8033, Accuracy: 0.6371\n",
      "Training loss (for one batch) at step 20: 304.2961, Accuracy: 0.6749\n",
      "Training loss (for one batch) at step 30: 300.3845, Accuracy: 0.6968\n",
      "Training loss (for one batch) at step 40: 297.3873, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 50: 298.5443, Accuracy: 0.7272\n",
      "Training loss (for one batch) at step 60: 293.1305, Accuracy: 0.7332\n",
      "Training loss (for one batch) at step 70: 321.7439, Accuracy: 0.7252\n",
      "Training loss (for one batch) at step 80: 349.0242, Accuracy: 0.7156\n",
      "Training loss (for one batch) at step 90: 310.9378, Accuracy: 0.7127\n",
      "Training loss (for one batch) at step 100: 312.4283, Accuracy: 0.7128\n",
      "Training loss (for one batch) at step 110: 306.5797, Accuracy: 0.7140\n",
      "---- Training ----\n",
      "Training loss: 101.0755\n",
      "Training acc over epoch: 0.7129\n",
      "---- Validation ----\n",
      "Validation loss: 34.5052\n",
      "Validation acc: 0.6427\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 349.7744, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 320.6270, Accuracy: 0.6278\n",
      "Training loss (for one batch) at step 20: 287.4648, Accuracy: 0.6756\n",
      "Training loss (for one batch) at step 30: 303.3916, Accuracy: 0.7026\n",
      "Training loss (for one batch) at step 40: 309.8374, Accuracy: 0.7218\n",
      "Training loss (for one batch) at step 50: 303.0755, Accuracy: 0.7328\n",
      "Training loss (for one batch) at step 60: 310.5295, Accuracy: 0.7369\n",
      "Training loss (for one batch) at step 70: 315.5170, Accuracy: 0.7246\n",
      "Training loss (for one batch) at step 80: 336.4192, Accuracy: 0.7164\n",
      "Training loss (for one batch) at step 90: 315.3651, Accuracy: 0.7131\n",
      "Training loss (for one batch) at step 100: 313.7061, Accuracy: 0.7140\n",
      "Training loss (for one batch) at step 110: 308.8463, Accuracy: 0.7158\n",
      "---- Training ----\n",
      "Training loss: 90.5419\n",
      "Training acc over epoch: 0.7140\n",
      "---- Validation ----\n",
      "Validation loss: 30.6632\n",
      "Validation acc: 0.6663\n",
      "Time taken: 10.51s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 350.9620, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 307.3031, Accuracy: 0.6598\n",
      "Training loss (for one batch) at step 20: 333.3488, Accuracy: 0.6856\n",
      "Training loss (for one batch) at step 30: 297.2266, Accuracy: 0.7059\n",
      "Training loss (for one batch) at step 40: 295.6136, Accuracy: 0.7197\n",
      "Training loss (for one batch) at step 50: 290.0504, Accuracy: 0.7309\n",
      "Training loss (for one batch) at step 60: 316.9206, Accuracy: 0.7357\n",
      "Training loss (for one batch) at step 70: 320.4385, Accuracy: 0.7271\n",
      "Training loss (for one batch) at step 80: 322.5454, Accuracy: 0.7154\n",
      "Training loss (for one batch) at step 90: 298.6165, Accuracy: 0.7140\n",
      "Training loss (for one batch) at step 100: 294.0730, Accuracy: 0.7155\n",
      "Training loss (for one batch) at step 110: 299.5435, Accuracy: 0.7157\n",
      "---- Training ----\n",
      "Training loss: 119.4505\n",
      "Training acc over epoch: 0.7137\n",
      "---- Validation ----\n",
      "Validation loss: 43.1328\n",
      "Validation acc: 0.6677\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 339.2560, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 318.2294, Accuracy: 0.6570\n",
      "Training loss (for one batch) at step 20: 300.8828, Accuracy: 0.6775\n",
      "Training loss (for one batch) at step 30: 291.5516, Accuracy: 0.7004\n",
      "Training loss (for one batch) at step 40: 278.4138, Accuracy: 0.7165\n",
      "Training loss (for one batch) at step 50: 319.1930, Accuracy: 0.7287\n",
      "Training loss (for one batch) at step 60: 302.9252, Accuracy: 0.7342\n",
      "Training loss (for one batch) at step 70: 320.4291, Accuracy: 0.7257\n",
      "Training loss (for one batch) at step 80: 318.0975, Accuracy: 0.7176\n",
      "Training loss (for one batch) at step 90: 289.6472, Accuracy: 0.7136\n",
      "Training loss (for one batch) at step 100: 301.7556, Accuracy: 0.7140\n",
      "Training loss (for one batch) at step 110: 298.9183, Accuracy: 0.7154\n",
      "---- Training ----\n",
      "Training loss: 99.5316\n",
      "Training acc over epoch: 0.7140\n",
      "---- Validation ----\n",
      "Validation loss: 41.8729\n",
      "Validation acc: 0.6513\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 329.1793, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 338.5811, Accuracy: 0.6612\n",
      "Training loss (for one batch) at step 20: 298.1394, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 30: 294.9934, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 40: 285.5851, Accuracy: 0.7174\n",
      "Training loss (for one batch) at step 50: 279.6311, Accuracy: 0.7331\n",
      "Training loss (for one batch) at step 60: 307.9728, Accuracy: 0.7376\n",
      "Training loss (for one batch) at step 70: 327.2173, Accuracy: 0.7281\n",
      "Training loss (for one batch) at step 80: 331.3658, Accuracy: 0.7169\n",
      "Training loss (for one batch) at step 90: 305.0419, Accuracy: 0.7139\n",
      "Training loss (for one batch) at step 100: 302.5741, Accuracy: 0.7157\n",
      "Training loss (for one batch) at step 110: 305.6703, Accuracy: 0.7165\n",
      "---- Training ----\n",
      "Training loss: 101.7874\n",
      "Training acc over epoch: 0.7150\n",
      "---- Validation ----\n",
      "Validation loss: 49.0446\n",
      "Validation acc: 0.6488\n",
      "Time taken: 10.54s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 324.2806, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 315.4391, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 20: 301.4865, Accuracy: 0.6916\n",
      "Training loss (for one batch) at step 30: 309.0191, Accuracy: 0.7142\n",
      "Training loss (for one batch) at step 40: 285.4946, Accuracy: 0.7241\n",
      "Training loss (for one batch) at step 50: 291.8264, Accuracy: 0.7347\n",
      "Training loss (for one batch) at step 60: 326.8716, Accuracy: 0.7369\n",
      "Training loss (for one batch) at step 70: 322.8298, Accuracy: 0.7295\n",
      "Training loss (for one batch) at step 80: 338.7860, Accuracy: 0.7186\n",
      "Training loss (for one batch) at step 90: 311.9663, Accuracy: 0.7153\n",
      "Training loss (for one batch) at step 100: 314.7950, Accuracy: 0.7159\n",
      "Training loss (for one batch) at step 110: 323.2607, Accuracy: 0.7161\n",
      "---- Training ----\n",
      "Training loss: 103.4843\n",
      "Training acc over epoch: 0.7162\n",
      "---- Validation ----\n",
      "Validation loss: 53.6384\n",
      "Validation acc: 0.6526\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 320.2622, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 304.2402, Accuracy: 0.6584\n",
      "Training loss (for one batch) at step 20: 302.3943, Accuracy: 0.6849\n",
      "Training loss (for one batch) at step 30: 292.1251, Accuracy: 0.7072\n",
      "Training loss (for one batch) at step 40: 271.5683, Accuracy: 0.7229\n",
      "Training loss (for one batch) at step 50: 299.4808, Accuracy: 0.7339\n",
      "Training loss (for one batch) at step 60: 299.4535, Accuracy: 0.7373\n",
      "Training loss (for one batch) at step 70: 327.6604, Accuracy: 0.7273\n",
      "Training loss (for one batch) at step 80: 317.4493, Accuracy: 0.7173\n",
      "Training loss (for one batch) at step 90: 318.3075, Accuracy: 0.7144\n",
      "Training loss (for one batch) at step 100: 295.4145, Accuracy: 0.7160\n",
      "Training loss (for one batch) at step 110: 302.8176, Accuracy: 0.7159\n",
      "---- Training ----\n",
      "Training loss: 106.2315\n",
      "Training acc over epoch: 0.7151\n",
      "---- Validation ----\n",
      "Validation loss: 60.2348\n",
      "Validation acc: 0.6521\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 331.0028, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 311.0399, Accuracy: 0.6506\n",
      "Training loss (for one batch) at step 20: 284.2738, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 30: 309.1978, Accuracy: 0.7011\n",
      "Training loss (for one batch) at step 40: 288.5049, Accuracy: 0.7174\n",
      "Training loss (for one batch) at step 50: 286.9611, Accuracy: 0.7312\n",
      "Training loss (for one batch) at step 60: 309.3873, Accuracy: 0.7363\n",
      "Training loss (for one batch) at step 70: 336.2774, Accuracy: 0.7280\n",
      "Training loss (for one batch) at step 80: 298.0436, Accuracy: 0.7187\n",
      "Training loss (for one batch) at step 90: 303.5337, Accuracy: 0.7148\n",
      "Training loss (for one batch) at step 100: 283.3314, Accuracy: 0.7161\n",
      "Training loss (for one batch) at step 110: 293.4292, Accuracy: 0.7173\n",
      "---- Training ----\n",
      "Training loss: 103.7927\n",
      "Training acc over epoch: 0.7157\n",
      "---- Validation ----\n",
      "Validation loss: 42.6372\n",
      "Validation acc: 0.6462\n",
      "Time taken: 10.88s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 324.2232, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 319.1837, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 20: 294.5750, Accuracy: 0.6789\n",
      "Training loss (for one batch) at step 30: 302.1624, Accuracy: 0.7061\n",
      "Training loss (for one batch) at step 40: 287.5934, Accuracy: 0.7235\n",
      "Training loss (for one batch) at step 50: 286.8002, Accuracy: 0.7336\n",
      "Training loss (for one batch) at step 60: 317.7195, Accuracy: 0.7387\n",
      "Training loss (for one batch) at step 70: 311.4852, Accuracy: 0.7283\n",
      "Training loss (for one batch) at step 80: 336.8940, Accuracy: 0.7176\n",
      "Training loss (for one batch) at step 90: 299.8611, Accuracy: 0.7146\n",
      "Training loss (for one batch) at step 100: 304.0252, Accuracy: 0.7157\n",
      "Training loss (for one batch) at step 110: 306.3330, Accuracy: 0.7155\n",
      "---- Training ----\n",
      "Training loss: 110.2706\n",
      "Training acc over epoch: 0.7155\n",
      "---- Validation ----\n",
      "Validation loss: 34.3718\n",
      "Validation acc: 0.6443\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 332.0051, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 312.6378, Accuracy: 0.6470\n",
      "Training loss (for one batch) at step 20: 313.1436, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 30: 279.7540, Accuracy: 0.7044\n",
      "Training loss (for one batch) at step 40: 304.4201, Accuracy: 0.7172\n",
      "Training loss (for one batch) at step 50: 290.5994, Accuracy: 0.7295\n",
      "Training loss (for one batch) at step 60: 304.1283, Accuracy: 0.7358\n",
      "Training loss (for one batch) at step 70: 348.7754, Accuracy: 0.7249\n",
      "Training loss (for one batch) at step 80: 332.2660, Accuracy: 0.7177\n",
      "Training loss (for one batch) at step 90: 297.1138, Accuracy: 0.7125\n",
      "Training loss (for one batch) at step 100: 292.6162, Accuracy: 0.7145\n",
      "Training loss (for one batch) at step 110: 297.7207, Accuracy: 0.7141\n",
      "---- Training ----\n",
      "Training loss: 94.5964\n",
      "Training acc over epoch: 0.7134\n",
      "---- Validation ----\n",
      "Validation loss: 35.3150\n",
      "Validation acc: 0.6585\n",
      "Time taken: 10.57s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 324.3596, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 324.5342, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 20: 287.8359, Accuracy: 0.6935\n",
      "Training loss (for one batch) at step 30: 301.9405, Accuracy: 0.7089\n",
      "Training loss (for one batch) at step 40: 300.9061, Accuracy: 0.7243\n",
      "Training loss (for one batch) at step 50: 286.6631, Accuracy: 0.7364\n",
      "Training loss (for one batch) at step 60: 296.5310, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 70: 303.8470, Accuracy: 0.7313\n",
      "Training loss (for one batch) at step 80: 318.9189, Accuracy: 0.7197\n",
      "Training loss (for one batch) at step 90: 296.7030, Accuracy: 0.7178\n",
      "Training loss (for one batch) at step 100: 279.3533, Accuracy: 0.7181\n",
      "Training loss (for one batch) at step 110: 300.3441, Accuracy: 0.7192\n",
      "---- Training ----\n",
      "Training loss: 105.8136\n",
      "Training acc over epoch: 0.7174\n",
      "---- Validation ----\n",
      "Validation loss: 39.6633\n",
      "Validation acc: 0.6550\n",
      "Time taken: 10.56s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 322.2041, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 313.2103, Accuracy: 0.6747\n",
      "Training loss (for one batch) at step 20: 307.0835, Accuracy: 0.6864\n",
      "Training loss (for one batch) at step 30: 290.9041, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 40: 281.5153, Accuracy: 0.7199\n",
      "Training loss (for one batch) at step 50: 282.9236, Accuracy: 0.7302\n",
      "Training loss (for one batch) at step 60: 294.0407, Accuracy: 0.7360\n",
      "Training loss (for one batch) at step 70: 315.4669, Accuracy: 0.7256\n",
      "Training loss (for one batch) at step 80: 333.5470, Accuracy: 0.7166\n",
      "Training loss (for one batch) at step 90: 289.8709, Accuracy: 0.7149\n",
      "Training loss (for one batch) at step 100: 285.3237, Accuracy: 0.7172\n",
      "Training loss (for one batch) at step 110: 311.2114, Accuracy: 0.7159\n",
      "---- Training ----\n",
      "Training loss: 121.6286\n",
      "Training acc over epoch: 0.7150\n",
      "---- Validation ----\n",
      "Validation loss: 50.2844\n",
      "Validation acc: 0.6585\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 328.7579, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 325.8190, Accuracy: 0.6612\n",
      "Training loss (for one batch) at step 20: 303.5488, Accuracy: 0.6830\n",
      "Training loss (for one batch) at step 30: 280.6392, Accuracy: 0.7029\n",
      "Training loss (for one batch) at step 40: 287.4467, Accuracy: 0.7165\n",
      "Training loss (for one batch) at step 50: 317.0018, Accuracy: 0.7301\n",
      "Training loss (for one batch) at step 60: 298.8759, Accuracy: 0.7394\n",
      "Training loss (for one batch) at step 70: 302.7667, Accuracy: 0.7274\n",
      "Training loss (for one batch) at step 80: 313.4587, Accuracy: 0.7150\n",
      "Training loss (for one batch) at step 90: 293.4113, Accuracy: 0.7121\n",
      "Training loss (for one batch) at step 100: 285.8386, Accuracy: 0.7149\n",
      "Training loss (for one batch) at step 110: 294.3822, Accuracy: 0.7157\n",
      "---- Training ----\n",
      "Training loss: 118.2088\n",
      "Training acc over epoch: 0.7149\n",
      "---- Validation ----\n",
      "Validation loss: 49.7151\n",
      "Validation acc: 0.6480\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 326.4433, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 323.1650, Accuracy: 0.6555\n",
      "Training loss (for one batch) at step 20: 301.2823, Accuracy: 0.6812\n",
      "Training loss (for one batch) at step 30: 282.6508, Accuracy: 0.7069\n",
      "Training loss (for one batch) at step 40: 288.9671, Accuracy: 0.7222\n",
      "Training loss (for one batch) at step 50: 316.1160, Accuracy: 0.7330\n",
      "Training loss (for one batch) at step 60: 293.1671, Accuracy: 0.7349\n",
      "Training loss (for one batch) at step 70: 321.6206, Accuracy: 0.7258\n",
      "Training loss (for one batch) at step 80: 311.9180, Accuracy: 0.7134\n",
      "Training loss (for one batch) at step 90: 304.1437, Accuracy: 0.7110\n",
      "Training loss (for one batch) at step 100: 293.4939, Accuracy: 0.7143\n",
      "Training loss (for one batch) at step 110: 302.5579, Accuracy: 0.7150\n",
      "---- Training ----\n",
      "Training loss: 99.4793\n",
      "Training acc over epoch: 0.7141\n",
      "---- Validation ----\n",
      "Validation loss: 46.5275\n",
      "Validation acc: 0.6475\n",
      "Time taken: 10.74s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 319.7699, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 313.9732, Accuracy: 0.6420\n",
      "Training loss (for one batch) at step 20: 307.1837, Accuracy: 0.6823\n",
      "Training loss (for one batch) at step 30: 297.7110, Accuracy: 0.7059\n",
      "Training loss (for one batch) at step 40: 284.2537, Accuracy: 0.7182\n",
      "Training loss (for one batch) at step 50: 305.7540, Accuracy: 0.7324\n",
      "Training loss (for one batch) at step 60: 274.1035, Accuracy: 0.7378\n",
      "Training loss (for one batch) at step 70: 307.4503, Accuracy: 0.7280\n",
      "Training loss (for one batch) at step 80: 363.6989, Accuracy: 0.7186\n",
      "Training loss (for one batch) at step 90: 273.8601, Accuracy: 0.7139\n",
      "Training loss (for one batch) at step 100: 307.3755, Accuracy: 0.7164\n",
      "Training loss (for one batch) at step 110: 284.5522, Accuracy: 0.7167\n",
      "---- Training ----\n",
      "Training loss: 91.5151\n",
      "Training acc over epoch: 0.7152\n",
      "---- Validation ----\n",
      "Validation loss: 56.3573\n",
      "Validation acc: 0.6553\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 310.1320, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 324.7808, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 20: 306.2534, Accuracy: 0.6782\n",
      "Training loss (for one batch) at step 30: 294.0490, Accuracy: 0.7074\n",
      "Training loss (for one batch) at step 40: 274.3924, Accuracy: 0.7229\n",
      "Training loss (for one batch) at step 50: 292.7332, Accuracy: 0.7319\n",
      "Training loss (for one batch) at step 60: 297.4866, Accuracy: 0.7377\n",
      "Training loss (for one batch) at step 70: 323.1899, Accuracy: 0.7279\n",
      "Training loss (for one batch) at step 80: 302.4200, Accuracy: 0.7177\n",
      "Training loss (for one batch) at step 90: 302.8713, Accuracy: 0.7150\n",
      "Training loss (for one batch) at step 100: 288.2863, Accuracy: 0.7166\n",
      "Training loss (for one batch) at step 110: 287.7491, Accuracy: 0.7145\n",
      "---- Training ----\n",
      "Training loss: 123.3033\n",
      "Training acc over epoch: 0.7151\n",
      "---- Validation ----\n",
      "Validation loss: 42.4973\n",
      "Validation acc: 0.6437\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 326.2764, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 321.7545, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 20: 296.6499, Accuracy: 0.6808\n",
      "Training loss (for one batch) at step 30: 298.7327, Accuracy: 0.7054\n",
      "Training loss (for one batch) at step 40: 275.6141, Accuracy: 0.7208\n",
      "Training loss (for one batch) at step 50: 287.6510, Accuracy: 0.7341\n",
      "Training loss (for one batch) at step 60: 289.5230, Accuracy: 0.7401\n",
      "Training loss (for one batch) at step 70: 319.1308, Accuracy: 0.7328\n",
      "Training loss (for one batch) at step 80: 311.7167, Accuracy: 0.7200\n",
      "Training loss (for one batch) at step 90: 290.4817, Accuracy: 0.7162\n",
      "Training loss (for one batch) at step 100: 275.2441, Accuracy: 0.7195\n",
      "Training loss (for one batch) at step 110: 292.9702, Accuracy: 0.7192\n",
      "---- Training ----\n",
      "Training loss: 107.1961\n",
      "Training acc over epoch: 0.7174\n",
      "---- Validation ----\n",
      "Validation loss: 49.7737\n",
      "Validation acc: 0.6462\n",
      "Time taken: 10.78s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 313.4392, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 320.9552, Accuracy: 0.6371\n",
      "Training loss (for one batch) at step 20: 302.7278, Accuracy: 0.6767\n",
      "Training loss (for one batch) at step 30: 273.3751, Accuracy: 0.7036\n",
      "Training loss (for one batch) at step 40: 273.1150, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 50: 270.8805, Accuracy: 0.7313\n",
      "Training loss (for one batch) at step 60: 311.4394, Accuracy: 0.7366\n",
      "Training loss (for one batch) at step 70: 318.2031, Accuracy: 0.7282\n",
      "Training loss (for one batch) at step 80: 302.6155, Accuracy: 0.7166\n",
      "Training loss (for one batch) at step 90: 281.8087, Accuracy: 0.7127\n",
      "Training loss (for one batch) at step 100: 292.7885, Accuracy: 0.7143\n",
      "Training loss (for one batch) at step 110: 285.1920, Accuracy: 0.7158\n",
      "---- Training ----\n",
      "Training loss: 90.6946\n",
      "Training acc over epoch: 0.7145\n",
      "---- Validation ----\n",
      "Validation loss: 38.1899\n",
      "Validation acc: 0.6558\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 343.7405, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 316.9430, Accuracy: 0.6697\n",
      "Training loss (for one batch) at step 20: 290.7437, Accuracy: 0.6894\n",
      "Training loss (for one batch) at step 30: 278.9148, Accuracy: 0.7087\n",
      "Training loss (for one batch) at step 40: 294.9805, Accuracy: 0.7229\n",
      "Training loss (for one batch) at step 50: 271.9819, Accuracy: 0.7348\n",
      "Training loss (for one batch) at step 60: 291.5941, Accuracy: 0.7374\n",
      "Training loss (for one batch) at step 70: 304.7138, Accuracy: 0.7283\n",
      "Training loss (for one batch) at step 80: 310.4632, Accuracy: 0.7165\n",
      "Training loss (for one batch) at step 90: 281.8701, Accuracy: 0.7141\n",
      "Training loss (for one batch) at step 100: 298.2256, Accuracy: 0.7167\n",
      "Training loss (for one batch) at step 110: 320.8200, Accuracy: 0.7157\n",
      "---- Training ----\n",
      "Training loss: 99.0582\n",
      "Training acc over epoch: 0.7147\n",
      "---- Validation ----\n",
      "Validation loss: 53.7914\n",
      "Validation acc: 0.6577\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 327.4524, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 285.5271, Accuracy: 0.6605\n",
      "Training loss (for one batch) at step 20: 295.1014, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 30: 282.6363, Accuracy: 0.7082\n",
      "Training loss (for one batch) at step 40: 279.0413, Accuracy: 0.7203\n",
      "Training loss (for one batch) at step 50: 284.3540, Accuracy: 0.7328\n",
      "Training loss (for one batch) at step 60: 285.3978, Accuracy: 0.7398\n",
      "Training loss (for one batch) at step 70: 335.1812, Accuracy: 0.7280\n",
      "Training loss (for one batch) at step 80: 313.5559, Accuracy: 0.7183\n",
      "Training loss (for one batch) at step 90: 288.0540, Accuracy: 0.7159\n",
      "Training loss (for one batch) at step 100: 289.0405, Accuracy: 0.7159\n",
      "Training loss (for one batch) at step 110: 305.9751, Accuracy: 0.7171\n",
      "---- Training ----\n",
      "Training loss: 119.9624\n",
      "Training acc over epoch: 0.7161\n",
      "---- Validation ----\n",
      "Validation loss: 67.4854\n",
      "Validation acc: 0.6486\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 334.7749, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 331.6179, Accuracy: 0.6619\n",
      "Training loss (for one batch) at step 20: 289.7920, Accuracy: 0.6812\n",
      "Training loss (for one batch) at step 30: 280.2391, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 40: 292.8917, Accuracy: 0.7258\n",
      "Training loss (for one batch) at step 50: 270.0549, Accuracy: 0.7358\n",
      "Training loss (for one batch) at step 60: 295.8960, Accuracy: 0.7400\n",
      "Training loss (for one batch) at step 70: 320.4169, Accuracy: 0.7314\n",
      "Training loss (for one batch) at step 80: 292.5118, Accuracy: 0.7217\n",
      "Training loss (for one batch) at step 90: 302.7647, Accuracy: 0.7177\n",
      "Training loss (for one batch) at step 100: 279.3820, Accuracy: 0.7185\n",
      "Training loss (for one batch) at step 110: 291.0551, Accuracy: 0.7182\n",
      "---- Training ----\n",
      "Training loss: 96.4970\n",
      "Training acc over epoch: 0.7180\n",
      "---- Validation ----\n",
      "Validation loss: 39.4584\n",
      "Validation acc: 0.6515\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 308.3049, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 312.1433, Accuracy: 0.6634\n",
      "Training loss (for one batch) at step 20: 302.3084, Accuracy: 0.6834\n",
      "Training loss (for one batch) at step 30: 267.2763, Accuracy: 0.7094\n",
      "Training loss (for one batch) at step 40: 290.2426, Accuracy: 0.7264\n",
      "Training loss (for one batch) at step 50: 281.0796, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 60: 295.3937, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 70: 305.0757, Accuracy: 0.7315\n",
      "Training loss (for one batch) at step 80: 320.4694, Accuracy: 0.7215\n",
      "Training loss (for one batch) at step 90: 301.4884, Accuracy: 0.7192\n",
      "Training loss (for one batch) at step 100: 300.9919, Accuracy: 0.7184\n",
      "Training loss (for one batch) at step 110: 299.6746, Accuracy: 0.7192\n",
      "---- Training ----\n",
      "Training loss: 105.1270\n",
      "Training acc over epoch: 0.7184\n",
      "---- Validation ----\n",
      "Validation loss: 47.8836\n",
      "Validation acc: 0.6577\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 316.6089, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 306.4062, Accuracy: 0.6548\n",
      "Training loss (for one batch) at step 20: 291.8132, Accuracy: 0.6815\n",
      "Training loss (for one batch) at step 30: 285.8374, Accuracy: 0.7112\n",
      "Training loss (for one batch) at step 40: 276.4689, Accuracy: 0.7224\n",
      "Training loss (for one batch) at step 50: 277.4386, Accuracy: 0.7361\n",
      "Training loss (for one batch) at step 60: 286.4830, Accuracy: 0.7400\n",
      "Training loss (for one batch) at step 70: 291.5684, Accuracy: 0.7285\n",
      "Training loss (for one batch) at step 80: 295.7607, Accuracy: 0.7194\n",
      "Training loss (for one batch) at step 90: 292.9709, Accuracy: 0.7177\n",
      "Training loss (for one batch) at step 100: 290.5868, Accuracy: 0.7178\n",
      "Training loss (for one batch) at step 110: 309.7497, Accuracy: 0.7183\n",
      "---- Training ----\n",
      "Training loss: 96.3942\n",
      "Training acc over epoch: 0.7176\n",
      "---- Validation ----\n",
      "Validation loss: 71.7320\n",
      "Validation acc: 0.6556\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 315.1831, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 313.7421, Accuracy: 0.6420\n",
      "Training loss (for one batch) at step 20: 286.1446, Accuracy: 0.6853\n",
      "Training loss (for one batch) at step 30: 289.3896, Accuracy: 0.7064\n",
      "Training loss (for one batch) at step 40: 277.9535, Accuracy: 0.7231\n",
      "Training loss (for one batch) at step 50: 290.8216, Accuracy: 0.7350\n",
      "Training loss (for one batch) at step 60: 277.8774, Accuracy: 0.7390\n",
      "Training loss (for one batch) at step 70: 314.0601, Accuracy: 0.7287\n",
      "Training loss (for one batch) at step 80: 305.5755, Accuracy: 0.7187\n",
      "Training loss (for one batch) at step 90: 286.3441, Accuracy: 0.7152\n",
      "Training loss (for one batch) at step 100: 286.5442, Accuracy: 0.7171\n",
      "Training loss (for one batch) at step 110: 281.2311, Accuracy: 0.7178\n",
      "---- Training ----\n",
      "Training loss: 92.2616\n",
      "Training acc over epoch: 0.7172\n",
      "---- Validation ----\n",
      "Validation loss: 40.3478\n",
      "Validation acc: 0.6376\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 312.9697, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 294.6304, Accuracy: 0.6463\n",
      "Training loss (for one batch) at step 20: 285.9330, Accuracy: 0.6830\n",
      "Training loss (for one batch) at step 30: 263.3128, Accuracy: 0.7054\n",
      "Training loss (for one batch) at step 40: 271.8720, Accuracy: 0.7208\n",
      "Training loss (for one batch) at step 50: 274.6725, Accuracy: 0.7330\n",
      "Training loss (for one batch) at step 60: 285.8179, Accuracy: 0.7367\n",
      "Training loss (for one batch) at step 70: 313.4524, Accuracy: 0.7271\n",
      "Training loss (for one batch) at step 80: 289.6485, Accuracy: 0.7197\n",
      "Training loss (for one batch) at step 90: 292.0859, Accuracy: 0.7152\n",
      "Training loss (for one batch) at step 100: 291.9304, Accuracy: 0.7149\n",
      "Training loss (for one batch) at step 110: 294.2899, Accuracy: 0.7152\n",
      "---- Training ----\n",
      "Training loss: 101.6860\n",
      "Training acc over epoch: 0.7154\n",
      "---- Validation ----\n",
      "Validation loss: 33.3272\n",
      "Validation acc: 0.6580\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 323.3038, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 295.3479, Accuracy: 0.6385\n",
      "Training loss (for one batch) at step 20: 284.5281, Accuracy: 0.6734\n",
      "Training loss (for one batch) at step 30: 309.1018, Accuracy: 0.7039\n",
      "Training loss (for one batch) at step 40: 268.8399, Accuracy: 0.7207\n",
      "Training loss (for one batch) at step 50: 275.7366, Accuracy: 0.7322\n",
      "Training loss (for one batch) at step 60: 275.8922, Accuracy: 0.7358\n",
      "Training loss (for one batch) at step 70: 297.5320, Accuracy: 0.7276\n",
      "Training loss (for one batch) at step 80: 304.7342, Accuracy: 0.7169\n",
      "Training loss (for one batch) at step 90: 294.6852, Accuracy: 0.7145\n",
      "Training loss (for one batch) at step 100: 289.9682, Accuracy: 0.7165\n",
      "Training loss (for one batch) at step 110: 285.1198, Accuracy: 0.7174\n",
      "---- Training ----\n",
      "Training loss: 93.0146\n",
      "Training acc over epoch: 0.7168\n",
      "---- Validation ----\n",
      "Validation loss: 48.9610\n",
      "Validation acc: 0.6553\n",
      "Time taken: 10.51s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 331.0193, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 307.0143, Accuracy: 0.6648\n",
      "Training loss (for one batch) at step 20: 280.1035, Accuracy: 0.6830\n",
      "Training loss (for one batch) at step 30: 263.6543, Accuracy: 0.7079\n",
      "Training loss (for one batch) at step 40: 253.7771, Accuracy: 0.7233\n",
      "Training loss (for one batch) at step 50: 281.0427, Accuracy: 0.7350\n",
      "Training loss (for one batch) at step 60: 293.9139, Accuracy: 0.7398\n",
      "Training loss (for one batch) at step 70: 318.6440, Accuracy: 0.7296\n",
      "Training loss (for one batch) at step 80: 293.6105, Accuracy: 0.7191\n",
      "Training loss (for one batch) at step 90: 289.5141, Accuracy: 0.7161\n",
      "Training loss (for one batch) at step 100: 281.6015, Accuracy: 0.7174\n",
      "Training loss (for one batch) at step 110: 282.6496, Accuracy: 0.7180\n",
      "---- Training ----\n",
      "Training loss: 107.8910\n",
      "Training acc over epoch: 0.7173\n",
      "---- Validation ----\n",
      "Validation loss: 55.6634\n",
      "Validation acc: 0.6545\n",
      "Time taken: 10.88s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 326.4337, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 329.1391, Accuracy: 0.6541\n",
      "Training loss (for one batch) at step 20: 298.4266, Accuracy: 0.6789\n",
      "Training loss (for one batch) at step 30: 284.0669, Accuracy: 0.7034\n",
      "Training loss (for one batch) at step 40: 291.4295, Accuracy: 0.7214\n",
      "Training loss (for one batch) at step 50: 274.1952, Accuracy: 0.7321\n",
      "Training loss (for one batch) at step 60: 282.0092, Accuracy: 0.7392\n",
      "Training loss (for one batch) at step 70: 326.1849, Accuracy: 0.7287\n",
      "Training loss (for one batch) at step 80: 312.0906, Accuracy: 0.7185\n",
      "Training loss (for one batch) at step 90: 298.2510, Accuracy: 0.7134\n",
      "Training loss (for one batch) at step 100: 283.1776, Accuracy: 0.7169\n",
      "Training loss (for one batch) at step 110: 305.1822, Accuracy: 0.7174\n",
      "---- Training ----\n",
      "Training loss: 99.5419\n",
      "Training acc over epoch: 0.7156\n",
      "---- Validation ----\n",
      "Validation loss: 56.7703\n",
      "Validation acc: 0.6475\n",
      "Time taken: 10.16s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 336.8847, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 303.0274, Accuracy: 0.6626\n",
      "Training loss (for one batch) at step 20: 273.1848, Accuracy: 0.6860\n",
      "Training loss (for one batch) at step 30: 289.9554, Accuracy: 0.7046\n",
      "Training loss (for one batch) at step 40: 289.5650, Accuracy: 0.7220\n",
      "Training loss (for one batch) at step 50: 271.7534, Accuracy: 0.7342\n",
      "Training loss (for one batch) at step 60: 275.8173, Accuracy: 0.7367\n",
      "Training loss (for one batch) at step 70: 307.9393, Accuracy: 0.7282\n",
      "Training loss (for one batch) at step 80: 303.8932, Accuracy: 0.7152\n",
      "Training loss (for one batch) at step 90: 282.0910, Accuracy: 0.7140\n",
      "Training loss (for one batch) at step 100: 304.8236, Accuracy: 0.7147\n",
      "Training loss (for one batch) at step 110: 285.7265, Accuracy: 0.7151\n",
      "---- Training ----\n",
      "Training loss: 105.5992\n",
      "Training acc over epoch: 0.7145\n",
      "---- Validation ----\n",
      "Validation loss: 51.8828\n",
      "Validation acc: 0.6515\n",
      "Time taken: 10.65s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 341.9496, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 319.6393, Accuracy: 0.6662\n",
      "Training loss (for one batch) at step 20: 296.1035, Accuracy: 0.6912\n",
      "Training loss (for one batch) at step 30: 293.7793, Accuracy: 0.7094\n",
      "Training loss (for one batch) at step 40: 267.4884, Accuracy: 0.7243\n",
      "Training loss (for one batch) at step 50: 271.3861, Accuracy: 0.7373\n",
      "Training loss (for one batch) at step 60: 292.2634, Accuracy: 0.7404\n",
      "Training loss (for one batch) at step 70: 293.1201, Accuracy: 0.7309\n",
      "Training loss (for one batch) at step 80: 280.8812, Accuracy: 0.7208\n",
      "Training loss (for one batch) at step 90: 284.0906, Accuracy: 0.7177\n",
      "Training loss (for one batch) at step 100: 288.8329, Accuracy: 0.7198\n",
      "Training loss (for one batch) at step 110: 283.2890, Accuracy: 0.7187\n",
      "---- Training ----\n",
      "Training loss: 94.5149\n",
      "Training acc over epoch: 0.7176\n",
      "---- Validation ----\n",
      "Validation loss: 42.1924\n",
      "Validation acc: 0.6631\n",
      "Time taken: 10.72s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 307.4549, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 297.7454, Accuracy: 0.6705\n",
      "Training loss (for one batch) at step 20: 281.5834, Accuracy: 0.6812\n",
      "Training loss (for one batch) at step 30: 258.4875, Accuracy: 0.7056\n",
      "Training loss (for one batch) at step 40: 273.4592, Accuracy: 0.7229\n",
      "Training loss (for one batch) at step 50: 284.2887, Accuracy: 0.7358\n",
      "Training loss (for one batch) at step 60: 294.1819, Accuracy: 0.7414\n",
      "Training loss (for one batch) at step 70: 304.6959, Accuracy: 0.7302\n",
      "Training loss (for one batch) at step 80: 284.0270, Accuracy: 0.7211\n",
      "Training loss (for one batch) at step 90: 306.0555, Accuracy: 0.7193\n",
      "Training loss (for one batch) at step 100: 288.4348, Accuracy: 0.7191\n",
      "Training loss (for one batch) at step 110: 279.2646, Accuracy: 0.7192\n",
      "---- Training ----\n",
      "Training loss: 96.1642\n",
      "Training acc over epoch: 0.7177\n",
      "---- Validation ----\n",
      "Validation loss: 62.3343\n",
      "Validation acc: 0.6515\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 304.3549, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 322.8467, Accuracy: 0.6612\n",
      "Training loss (for one batch) at step 20: 265.6684, Accuracy: 0.6864\n",
      "Training loss (for one batch) at step 30: 276.3359, Accuracy: 0.7056\n",
      "Training loss (for one batch) at step 40: 290.5742, Accuracy: 0.7226\n",
      "Training loss (for one batch) at step 50: 268.5298, Accuracy: 0.7347\n",
      "Training loss (for one batch) at step 60: 297.7796, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 70: 312.7482, Accuracy: 0.7293\n",
      "Training loss (for one batch) at step 80: 316.6096, Accuracy: 0.7211\n",
      "Training loss (for one batch) at step 90: 290.1087, Accuracy: 0.7165\n",
      "Training loss (for one batch) at step 100: 290.4807, Accuracy: 0.7170\n",
      "Training loss (for one batch) at step 110: 279.5112, Accuracy: 0.7180\n",
      "---- Training ----\n",
      "Training loss: 112.7557\n",
      "Training acc over epoch: 0.7178\n",
      "---- Validation ----\n",
      "Validation loss: 35.0695\n",
      "Validation acc: 0.6609\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 309.1139, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 321.6270, Accuracy: 0.6548\n",
      "Training loss (for one batch) at step 20: 281.6445, Accuracy: 0.6894\n",
      "Training loss (for one batch) at step 30: 291.0850, Accuracy: 0.7092\n",
      "Training loss (for one batch) at step 40: 282.6378, Accuracy: 0.7231\n",
      "Training loss (for one batch) at step 50: 292.8309, Accuracy: 0.7339\n",
      "Training loss (for one batch) at step 60: 272.9387, Accuracy: 0.7396\n",
      "Training loss (for one batch) at step 70: 313.2075, Accuracy: 0.7303\n",
      "Training loss (for one batch) at step 80: 295.5852, Accuracy: 0.7203\n",
      "Training loss (for one batch) at step 90: 289.3762, Accuracy: 0.7163\n",
      "Training loss (for one batch) at step 100: 273.4872, Accuracy: 0.7167\n",
      "Training loss (for one batch) at step 110: 310.3510, Accuracy: 0.7179\n",
      "---- Training ----\n",
      "Training loss: 93.3375\n",
      "Training acc over epoch: 0.7184\n",
      "---- Validation ----\n",
      "Validation loss: 42.5305\n",
      "Validation acc: 0.6494\n",
      "Time taken: 10.54s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 320.4580, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 312.6597, Accuracy: 0.6570\n",
      "Training loss (for one batch) at step 20: 276.2118, Accuracy: 0.6838\n",
      "Training loss (for one batch) at step 30: 304.2554, Accuracy: 0.7014\n",
      "Training loss (for one batch) at step 40: 291.0003, Accuracy: 0.7208\n",
      "Training loss (for one batch) at step 50: 257.6002, Accuracy: 0.7318\n",
      "Training loss (for one batch) at step 60: 280.7578, Accuracy: 0.7377\n",
      "Training loss (for one batch) at step 70: 299.9765, Accuracy: 0.7260\n",
      "Training loss (for one batch) at step 80: 299.4532, Accuracy: 0.7168\n",
      "Training loss (for one batch) at step 90: 289.7574, Accuracy: 0.7151\n",
      "Training loss (for one batch) at step 100: 283.0168, Accuracy: 0.7153\n",
      "Training loss (for one batch) at step 110: 297.1779, Accuracy: 0.7179\n",
      "---- Training ----\n",
      "Training loss: 104.2048\n",
      "Training acc over epoch: 0.7166\n",
      "---- Validation ----\n",
      "Validation loss: 49.8547\n",
      "Validation acc: 0.6566\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 312.6068, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 306.7186, Accuracy: 0.6413\n",
      "Training loss (for one batch) at step 20: 283.0861, Accuracy: 0.6749\n",
      "Training loss (for one batch) at step 30: 271.9437, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 40: 275.4028, Accuracy: 0.7159\n",
      "Training loss (for one batch) at step 50: 277.6419, Accuracy: 0.7296\n",
      "Training loss (for one batch) at step 60: 313.1297, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 70: 292.8217, Accuracy: 0.7278\n",
      "Training loss (for one batch) at step 80: 304.6278, Accuracy: 0.7191\n",
      "Training loss (for one batch) at step 90: 272.4899, Accuracy: 0.7172\n",
      "Training loss (for one batch) at step 100: 265.0767, Accuracy: 0.7177\n",
      "Training loss (for one batch) at step 110: 273.5234, Accuracy: 0.7182\n",
      "---- Training ----\n",
      "Training loss: 91.8955\n",
      "Training acc over epoch: 0.7174\n",
      "---- Validation ----\n",
      "Validation loss: 47.1520\n",
      "Validation acc: 0.6341\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 293.5988, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 303.4031, Accuracy: 0.6577\n",
      "Training loss (for one batch) at step 20: 273.4662, Accuracy: 0.6890\n",
      "Training loss (for one batch) at step 30: 285.9807, Accuracy: 0.7051\n",
      "Training loss (for one batch) at step 40: 263.8088, Accuracy: 0.7212\n",
      "Training loss (for one batch) at step 50: 274.4604, Accuracy: 0.7339\n",
      "Training loss (for one batch) at step 60: 267.6003, Accuracy: 0.7389\n",
      "Training loss (for one batch) at step 70: 310.9258, Accuracy: 0.7288\n",
      "Training loss (for one batch) at step 80: 325.2886, Accuracy: 0.7177\n",
      "Training loss (for one batch) at step 90: 278.9339, Accuracy: 0.7137\n",
      "Training loss (for one batch) at step 100: 290.7847, Accuracy: 0.7153\n",
      "Training loss (for one batch) at step 110: 279.3841, Accuracy: 0.7147\n",
      "---- Training ----\n",
      "Training loss: 92.9201\n",
      "Training acc over epoch: 0.7140\n",
      "---- Validation ----\n",
      "Validation loss: 60.1268\n",
      "Validation acc: 0.6523\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 308.4193, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 317.5289, Accuracy: 0.6520\n",
      "Training loss (for one batch) at step 20: 282.2438, Accuracy: 0.6808\n",
      "Training loss (for one batch) at step 30: 272.8347, Accuracy: 0.7026\n",
      "Training loss (for one batch) at step 40: 276.0929, Accuracy: 0.7178\n",
      "Training loss (for one batch) at step 50: 296.7222, Accuracy: 0.7292\n",
      "Training loss (for one batch) at step 60: 297.0076, Accuracy: 0.7349\n",
      "Training loss (for one batch) at step 70: 299.1841, Accuracy: 0.7272\n",
      "Training loss (for one batch) at step 80: 305.3251, Accuracy: 0.7172\n",
      "Training loss (for one batch) at step 90: 264.8600, Accuracy: 0.7148\n",
      "Training loss (for one batch) at step 100: 302.0346, Accuracy: 0.7157\n",
      "Training loss (for one batch) at step 110: 279.2175, Accuracy: 0.7154\n",
      "---- Training ----\n",
      "Training loss: 98.6064\n",
      "Training acc over epoch: 0.7151\n",
      "---- Validation ----\n",
      "Validation loss: 30.3049\n",
      "Validation acc: 0.6505\n",
      "Time taken: 10.37s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.6556\n",
      "Validation AUC: 0.6586\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 602.4993, Accuracy: 0.3750\n",
      "Training loss (for one batch) at step 10: 607.4537, Accuracy: 0.5064\n",
      "Training loss (for one batch) at step 20: 571.5861, Accuracy: 0.4963\n",
      "Training loss (for one batch) at step 30: 521.8425, Accuracy: 0.5169\n",
      "Training loss (for one batch) at step 40: 497.9319, Accuracy: 0.5103\n",
      "Training loss (for one batch) at step 50: 501.4322, Accuracy: 0.5106\n",
      "Training loss (for one batch) at step 60: 467.8735, Accuracy: 0.5086\n",
      "Training loss (for one batch) at step 70: 473.5103, Accuracy: 0.5100\n",
      "Training loss (for one batch) at step 80: 464.3617, Accuracy: 0.5113\n",
      "Training loss (for one batch) at step 90: 470.9391, Accuracy: 0.5085\n",
      "Training loss (for one batch) at step 100: 460.7297, Accuracy: 0.5080\n",
      "Training loss (for one batch) at step 110: 464.6620, Accuracy: 0.5108\n",
      "---- Training ----\n",
      "Training loss: 144.7425\n",
      "Training acc over epoch: 0.5085\n",
      "---- Validation ----\n",
      "Validation loss: 34.9587\n",
      "Validation acc: 0.4876\n",
      "Time taken: 12.25s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 453.0874, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 453.5144, Accuracy: 0.5199\n",
      "Training loss (for one batch) at step 20: 454.3813, Accuracy: 0.5201\n",
      "Training loss (for one batch) at step 30: 450.4301, Accuracy: 0.5232\n",
      "Training loss (for one batch) at step 40: 449.7925, Accuracy: 0.5238\n",
      "Training loss (for one batch) at step 50: 453.9481, Accuracy: 0.5202\n",
      "Training loss (for one batch) at step 60: 451.7562, Accuracy: 0.5177\n",
      "Training loss (for one batch) at step 70: 448.7914, Accuracy: 0.5168\n",
      "Training loss (for one batch) at step 80: 450.0374, Accuracy: 0.5144\n",
      "Training loss (for one batch) at step 90: 446.7520, Accuracy: 0.5137\n",
      "Training loss (for one batch) at step 100: 451.6818, Accuracy: 0.5153\n",
      "Training loss (for one batch) at step 110: 443.8671, Accuracy: 0.5162\n",
      "---- Training ----\n",
      "Training loss: 140.0208\n",
      "Training acc over epoch: 0.5158\n",
      "---- Validation ----\n",
      "Validation loss: 34.8700\n",
      "Validation acc: 0.5134\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 446.6051, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 447.8140, Accuracy: 0.5114\n",
      "Training loss (for one batch) at step 20: 450.2444, Accuracy: 0.5153\n",
      "Training loss (for one batch) at step 30: 447.1495, Accuracy: 0.5106\n",
      "Training loss (for one batch) at step 40: 445.0762, Accuracy: 0.5061\n",
      "Training loss (for one batch) at step 50: 448.3330, Accuracy: 0.5075\n",
      "Training loss (for one batch) at step 60: 445.4287, Accuracy: 0.5123\n",
      "Training loss (for one batch) at step 70: 444.9613, Accuracy: 0.5193\n",
      "Training loss (for one batch) at step 80: 447.5037, Accuracy: 0.5195\n",
      "Training loss (for one batch) at step 90: 447.3021, Accuracy: 0.5229\n",
      "Training loss (for one batch) at step 100: 445.8006, Accuracy: 0.5209\n",
      "Training loss (for one batch) at step 110: 445.5715, Accuracy: 0.5229\n",
      "---- Training ----\n",
      "Training loss: 138.6099\n",
      "Training acc over epoch: 0.5227\n",
      "---- Validation ----\n",
      "Validation loss: 34.4618\n",
      "Validation acc: 0.5328\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 443.9278, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 445.3315, Accuracy: 0.5213\n",
      "Training loss (for one batch) at step 20: 442.9974, Accuracy: 0.5231\n",
      "Training loss (for one batch) at step 30: 446.1182, Accuracy: 0.5280\n",
      "Training loss (for one batch) at step 40: 443.8023, Accuracy: 0.5252\n",
      "Training loss (for one batch) at step 50: 444.3514, Accuracy: 0.5274\n",
      "Training loss (for one batch) at step 60: 444.1002, Accuracy: 0.5301\n",
      "Training loss (for one batch) at step 70: 445.4947, Accuracy: 0.5317\n",
      "Training loss (for one batch) at step 80: 442.4933, Accuracy: 0.5325\n",
      "Training loss (for one batch) at step 90: 445.2005, Accuracy: 0.5344\n",
      "Training loss (for one batch) at step 100: 444.5378, Accuracy: 0.5347\n",
      "Training loss (for one batch) at step 110: 444.4332, Accuracy: 0.5358\n",
      "---- Training ----\n",
      "Training loss: 138.1542\n",
      "Training acc over epoch: 0.5354\n",
      "---- Validation ----\n",
      "Validation loss: 34.6058\n",
      "Validation acc: 0.5529\n",
      "Time taken: 10.51s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 443.3175, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 442.8833, Accuracy: 0.5476\n",
      "Training loss (for one batch) at step 20: 442.2355, Accuracy: 0.5353\n",
      "Training loss (for one batch) at step 30: 441.6026, Accuracy: 0.5318\n",
      "Training loss (for one batch) at step 40: 443.6985, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 50: 445.6147, Accuracy: 0.5346\n",
      "Training loss (for one batch) at step 60: 440.4344, Accuracy: 0.5375\n",
      "Training loss (for one batch) at step 70: 445.0475, Accuracy: 0.5429\n",
      "Training loss (for one batch) at step 80: 443.1438, Accuracy: 0.5464\n",
      "Training loss (for one batch) at step 90: 442.2054, Accuracy: 0.5448\n",
      "Training loss (for one batch) at step 100: 442.9330, Accuracy: 0.5473\n",
      "Training loss (for one batch) at step 110: 443.0784, Accuracy: 0.5486\n",
      "---- Training ----\n",
      "Training loss: 138.2773\n",
      "Training acc over epoch: 0.5490\n",
      "---- Validation ----\n",
      "Validation loss: 34.6853\n",
      "Validation acc: 0.5677\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 444.8575, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 442.4720, Accuracy: 0.5433\n",
      "Training loss (for one batch) at step 20: 441.0343, Accuracy: 0.5406\n",
      "Training loss (for one batch) at step 30: 442.2581, Accuracy: 0.5449\n",
      "Training loss (for one batch) at step 40: 443.2116, Accuracy: 0.5478\n",
      "Training loss (for one batch) at step 50: 442.1255, Accuracy: 0.5565\n",
      "Training loss (for one batch) at step 60: 442.1295, Accuracy: 0.5574\n",
      "Training loss (for one batch) at step 70: 442.5718, Accuracy: 0.5656\n",
      "Training loss (for one batch) at step 80: 440.9029, Accuracy: 0.5693\n",
      "Training loss (for one batch) at step 90: 440.8165, Accuracy: 0.5701\n",
      "Training loss (for one batch) at step 100: 443.5279, Accuracy: 0.5684\n",
      "Training loss (for one batch) at step 110: 441.2646, Accuracy: 0.5696\n",
      "---- Training ----\n",
      "Training loss: 140.1212\n",
      "Training acc over epoch: 0.5717\n",
      "---- Validation ----\n",
      "Validation loss: 34.6934\n",
      "Validation acc: 0.6134\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 441.3011, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 441.4733, Accuracy: 0.5852\n",
      "Training loss (for one batch) at step 20: 442.1898, Accuracy: 0.5629\n",
      "Training loss (for one batch) at step 30: 442.3419, Accuracy: 0.5688\n",
      "Training loss (for one batch) at step 40: 442.1554, Accuracy: 0.5667\n",
      "Training loss (for one batch) at step 50: 439.4340, Accuracy: 0.5755\n",
      "Training loss (for one batch) at step 60: 437.4085, Accuracy: 0.5795\n",
      "Training loss (for one batch) at step 70: 446.6469, Accuracy: 0.5830\n",
      "Training loss (for one batch) at step 80: 443.1963, Accuracy: 0.5864\n",
      "Training loss (for one batch) at step 90: 443.1364, Accuracy: 0.5850\n",
      "Training loss (for one batch) at step 100: 443.5459, Accuracy: 0.5812\n",
      "Training loss (for one batch) at step 110: 442.8336, Accuracy: 0.5807\n",
      "---- Training ----\n",
      "Training loss: 139.6104\n",
      "Training acc over epoch: 0.5800\n",
      "---- Validation ----\n",
      "Validation loss: 34.4454\n",
      "Validation acc: 0.5919\n",
      "Time taken: 10.63s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 443.7047, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 441.4255, Accuracy: 0.5724\n",
      "Training loss (for one batch) at step 20: 442.1308, Accuracy: 0.5595\n",
      "Training loss (for one batch) at step 30: 441.1289, Accuracy: 0.5559\n",
      "Training loss (for one batch) at step 40: 441.0718, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 50: 438.7397, Accuracy: 0.5792\n",
      "Training loss (for one batch) at step 60: 436.7942, Accuracy: 0.5852\n",
      "Training loss (for one batch) at step 70: 442.7443, Accuracy: 0.5895\n",
      "Training loss (for one batch) at step 80: 439.9169, Accuracy: 0.5912\n",
      "Training loss (for one batch) at step 90: 440.8305, Accuracy: 0.5891\n",
      "Training loss (for one batch) at step 100: 442.7988, Accuracy: 0.5879\n",
      "Training loss (for one batch) at step 110: 438.7697, Accuracy: 0.5883\n",
      "---- Training ----\n",
      "Training loss: 137.3441\n",
      "Training acc over epoch: 0.5889\n",
      "---- Validation ----\n",
      "Validation loss: 34.6223\n",
      "Validation acc: 0.5505\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 444.3030, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 442.1372, Accuracy: 0.5668\n",
      "Training loss (for one batch) at step 20: 441.6765, Accuracy: 0.5484\n",
      "Training loss (for one batch) at step 30: 439.4576, Accuracy: 0.5491\n",
      "Training loss (for one batch) at step 40: 433.9290, Accuracy: 0.5633\n",
      "Training loss (for one batch) at step 50: 437.5456, Accuracy: 0.5702\n",
      "Training loss (for one batch) at step 60: 438.1461, Accuracy: 0.5772\n",
      "Training loss (for one batch) at step 70: 437.6801, Accuracy: 0.5841\n",
      "Training loss (for one batch) at step 80: 438.7412, Accuracy: 0.5803\n",
      "Training loss (for one batch) at step 90: 435.7620, Accuracy: 0.5777\n",
      "Training loss (for one batch) at step 100: 437.9030, Accuracy: 0.5772\n",
      "Training loss (for one batch) at step 110: 439.9943, Accuracy: 0.5805\n",
      "---- Training ----\n",
      "Training loss: 137.6491\n",
      "Training acc over epoch: 0.5822\n",
      "---- Validation ----\n",
      "Validation loss: 34.9630\n",
      "Validation acc: 0.5720\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 439.0615, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 436.4382, Accuracy: 0.5561\n",
      "Training loss (for one batch) at step 20: 438.1626, Accuracy: 0.5588\n",
      "Training loss (for one batch) at step 30: 439.2998, Accuracy: 0.5630\n",
      "Training loss (for one batch) at step 40: 433.4706, Accuracy: 0.5701\n",
      "Training loss (for one batch) at step 50: 430.7352, Accuracy: 0.5836\n",
      "Training loss (for one batch) at step 60: 442.5382, Accuracy: 0.5914\n",
      "Training loss (for one batch) at step 70: 441.0923, Accuracy: 0.5944\n",
      "Training loss (for one batch) at step 80: 446.0201, Accuracy: 0.5937\n",
      "Training loss (for one batch) at step 90: 434.6482, Accuracy: 0.5923\n",
      "Training loss (for one batch) at step 100: 427.6539, Accuracy: 0.5936\n",
      "Training loss (for one batch) at step 110: 440.5510, Accuracy: 0.5962\n",
      "---- Training ----\n",
      "Training loss: 137.5008\n",
      "Training acc over epoch: 0.5965\n",
      "---- Validation ----\n",
      "Validation loss: 33.9044\n",
      "Validation acc: 0.5817\n",
      "Time taken: 10.85s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 443.8382, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 440.0325, Accuracy: 0.5824\n",
      "Training loss (for one batch) at step 20: 440.0368, Accuracy: 0.5565\n",
      "Training loss (for one batch) at step 30: 435.7463, Accuracy: 0.5638\n",
      "Training loss (for one batch) at step 40: 435.9926, Accuracy: 0.5768\n",
      "Training loss (for one batch) at step 50: 431.7104, Accuracy: 0.5866\n",
      "Training loss (for one batch) at step 60: 437.7726, Accuracy: 0.5918\n",
      "Training loss (for one batch) at step 70: 442.1150, Accuracy: 0.6000\n",
      "Training loss (for one batch) at step 80: 436.0164, Accuracy: 0.5986\n",
      "Training loss (for one batch) at step 90: 441.7339, Accuracy: 0.5942\n",
      "Training loss (for one batch) at step 100: 433.2466, Accuracy: 0.5948\n",
      "Training loss (for one batch) at step 110: 430.6216, Accuracy: 0.5964\n",
      "---- Training ----\n",
      "Training loss: 142.4308\n",
      "Training acc over epoch: 0.5973\n",
      "---- Validation ----\n",
      "Validation loss: 34.0775\n",
      "Validation acc: 0.5913\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 437.3508, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 440.0005, Accuracy: 0.5930\n",
      "Training loss (for one batch) at step 20: 437.9204, Accuracy: 0.5885\n",
      "Training loss (for one batch) at step 30: 437.2974, Accuracy: 0.5910\n",
      "Training loss (for one batch) at step 40: 436.0334, Accuracy: 0.5928\n",
      "Training loss (for one batch) at step 50: 429.3403, Accuracy: 0.5994\n",
      "Training loss (for one batch) at step 60: 432.7017, Accuracy: 0.6063\n",
      "Training loss (for one batch) at step 70: 430.0233, Accuracy: 0.6085\n",
      "Training loss (for one batch) at step 80: 442.1364, Accuracy: 0.6061\n",
      "Training loss (for one batch) at step 90: 436.5868, Accuracy: 0.6034\n",
      "Training loss (for one batch) at step 100: 430.6379, Accuracy: 0.6030\n",
      "Training loss (for one batch) at step 110: 436.5978, Accuracy: 0.6052\n",
      "---- Training ----\n",
      "Training loss: 136.8720\n",
      "Training acc over epoch: 0.6064\n",
      "---- Validation ----\n",
      "Validation loss: 35.3529\n",
      "Validation acc: 0.5435\n",
      "Time taken: 11.12s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 441.9363, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 440.0046, Accuracy: 0.5760\n",
      "Training loss (for one batch) at step 20: 434.5988, Accuracy: 0.5670\n",
      "Training loss (for one batch) at step 30: 430.2684, Accuracy: 0.5789\n",
      "Training loss (for one batch) at step 40: 422.3021, Accuracy: 0.5899\n",
      "Training loss (for one batch) at step 50: 424.6015, Accuracy: 0.5976\n",
      "Training loss (for one batch) at step 60: 436.2951, Accuracy: 0.6032\n",
      "Training loss (for one batch) at step 70: 437.8528, Accuracy: 0.6013\n",
      "Training loss (for one batch) at step 80: 436.0431, Accuracy: 0.5975\n",
      "Training loss (for one batch) at step 90: 442.0607, Accuracy: 0.5978\n",
      "Training loss (for one batch) at step 100: 429.7484, Accuracy: 0.5987\n",
      "Training loss (for one batch) at step 110: 435.2904, Accuracy: 0.5969\n",
      "---- Training ----\n",
      "Training loss: 135.8782\n",
      "Training acc over epoch: 0.5953\n",
      "---- Validation ----\n",
      "Validation loss: 34.5783\n",
      "Validation acc: 0.5938\n",
      "Time taken: 10.74s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 434.1525, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 435.2239, Accuracy: 0.5923\n",
      "Training loss (for one batch) at step 20: 435.6573, Accuracy: 0.5956\n",
      "Training loss (for one batch) at step 30: 427.0088, Accuracy: 0.5990\n",
      "Training loss (for one batch) at step 40: 419.7623, Accuracy: 0.6069\n",
      "Training loss (for one batch) at step 50: 420.6201, Accuracy: 0.6097\n",
      "Training loss (for one batch) at step 60: 425.5311, Accuracy: 0.6142\n",
      "Training loss (for one batch) at step 70: 437.6538, Accuracy: 0.6133\n",
      "Training loss (for one batch) at step 80: 429.6031, Accuracy: 0.6152\n",
      "Training loss (for one batch) at step 90: 434.2059, Accuracy: 0.6098\n",
      "Training loss (for one batch) at step 100: 423.0891, Accuracy: 0.6083\n",
      "Training loss (for one batch) at step 110: 433.8716, Accuracy: 0.6102\n",
      "---- Training ----\n",
      "Training loss: 134.6264\n",
      "Training acc over epoch: 0.6109\n",
      "---- Validation ----\n",
      "Validation loss: 34.4994\n",
      "Validation acc: 0.6335\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 432.8279, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 433.5398, Accuracy: 0.6229\n",
      "Training loss (for one batch) at step 20: 443.3775, Accuracy: 0.6109\n",
      "Training loss (for one batch) at step 30: 425.0504, Accuracy: 0.6167\n",
      "Training loss (for one batch) at step 40: 420.0113, Accuracy: 0.6185\n",
      "Training loss (for one batch) at step 50: 412.3796, Accuracy: 0.6291\n",
      "Training loss (for one batch) at step 60: 425.2165, Accuracy: 0.6346\n",
      "Training loss (for one batch) at step 70: 432.6279, Accuracy: 0.6381\n",
      "Training loss (for one batch) at step 80: 429.3658, Accuracy: 0.6350\n",
      "Training loss (for one batch) at step 90: 428.5446, Accuracy: 0.6308\n",
      "Training loss (for one batch) at step 100: 431.8242, Accuracy: 0.6277\n",
      "Training loss (for one batch) at step 110: 425.7331, Accuracy: 0.6286\n",
      "---- Training ----\n",
      "Training loss: 133.5895\n",
      "Training acc over epoch: 0.6288\n",
      "---- Validation ----\n",
      "Validation loss: 33.0298\n",
      "Validation acc: 0.6034\n",
      "Time taken: 10.51s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 444.0315, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 434.1191, Accuracy: 0.6278\n",
      "Training loss (for one batch) at step 20: 428.1301, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 30: 430.8294, Accuracy: 0.6220\n",
      "Training loss (for one batch) at step 40: 422.1585, Accuracy: 0.6261\n",
      "Training loss (for one batch) at step 50: 414.3736, Accuracy: 0.6333\n",
      "Training loss (for one batch) at step 60: 435.3144, Accuracy: 0.6422\n",
      "Training loss (for one batch) at step 70: 418.7575, Accuracy: 0.6427\n",
      "Training loss (for one batch) at step 80: 431.0320, Accuracy: 0.6396\n",
      "Training loss (for one batch) at step 90: 429.7742, Accuracy: 0.6347\n",
      "Training loss (for one batch) at step 100: 418.8518, Accuracy: 0.6325\n",
      "Training loss (for one batch) at step 110: 419.9343, Accuracy: 0.6344\n",
      "---- Training ----\n",
      "Training loss: 134.3451\n",
      "Training acc over epoch: 0.6332\n",
      "---- Validation ----\n",
      "Validation loss: 35.4008\n",
      "Validation acc: 0.6300\n",
      "Time taken: 10.58s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 439.7085, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 428.2166, Accuracy: 0.6072\n",
      "Training loss (for one batch) at step 20: 433.9895, Accuracy: 0.6179\n",
      "Training loss (for one batch) at step 30: 421.7915, Accuracy: 0.6323\n",
      "Training loss (for one batch) at step 40: 412.6428, Accuracy: 0.6393\n",
      "Training loss (for one batch) at step 50: 411.0124, Accuracy: 0.6474\n",
      "Training loss (for one batch) at step 60: 421.9380, Accuracy: 0.6493\n",
      "Training loss (for one batch) at step 70: 432.3387, Accuracy: 0.6491\n",
      "Training loss (for one batch) at step 80: 427.1236, Accuracy: 0.6440\n",
      "Training loss (for one batch) at step 90: 422.2351, Accuracy: 0.6413\n",
      "Training loss (for one batch) at step 100: 420.9221, Accuracy: 0.6400\n",
      "Training loss (for one batch) at step 110: 422.9014, Accuracy: 0.6415\n",
      "---- Training ----\n",
      "Training loss: 134.1282\n",
      "Training acc over epoch: 0.6413\n",
      "---- Validation ----\n",
      "Validation loss: 35.4383\n",
      "Validation acc: 0.6282\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 439.4486, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 423.2167, Accuracy: 0.6371\n",
      "Training loss (for one batch) at step 20: 430.6754, Accuracy: 0.6429\n",
      "Training loss (for one batch) at step 30: 415.4942, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 40: 413.2745, Accuracy: 0.6498\n",
      "Training loss (for one batch) at step 50: 405.3549, Accuracy: 0.6569\n",
      "Training loss (for one batch) at step 60: 421.5181, Accuracy: 0.6598\n",
      "Training loss (for one batch) at step 70: 430.0405, Accuracy: 0.6583\n",
      "Training loss (for one batch) at step 80: 425.6004, Accuracy: 0.6533\n",
      "Training loss (for one batch) at step 90: 426.4988, Accuracy: 0.6487\n",
      "Training loss (for one batch) at step 100: 428.6768, Accuracy: 0.6457\n",
      "Training loss (for one batch) at step 110: 422.9107, Accuracy: 0.6463\n",
      "---- Training ----\n",
      "Training loss: 130.8032\n",
      "Training acc over epoch: 0.6468\n",
      "---- Validation ----\n",
      "Validation loss: 36.1023\n",
      "Validation acc: 0.6400\n",
      "Time taken: 10.40s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 431.6662, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 431.2236, Accuracy: 0.6349\n",
      "Training loss (for one batch) at step 20: 429.7989, Accuracy: 0.6503\n",
      "Training loss (for one batch) at step 30: 425.2324, Accuracy: 0.6568\n",
      "Training loss (for one batch) at step 40: 422.0720, Accuracy: 0.6648\n",
      "Training loss (for one batch) at step 50: 420.3173, Accuracy: 0.6746\n",
      "Training loss (for one batch) at step 60: 437.3275, Accuracy: 0.6751\n",
      "Training loss (for one batch) at step 70: 422.8976, Accuracy: 0.6774\n",
      "Training loss (for one batch) at step 80: 428.7636, Accuracy: 0.6732\n",
      "Training loss (for one batch) at step 90: 421.6013, Accuracy: 0.6699\n",
      "Training loss (for one batch) at step 100: 405.9339, Accuracy: 0.6668\n",
      "Training loss (for one batch) at step 110: 428.4112, Accuracy: 0.6665\n",
      "---- Training ----\n",
      "Training loss: 131.6501\n",
      "Training acc over epoch: 0.6672\n",
      "---- Validation ----\n",
      "Validation loss: 32.7995\n",
      "Validation acc: 0.6652\n",
      "Time taken: 10.60s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 436.2321, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 424.1067, Accuracy: 0.6534\n",
      "Training loss (for one batch) at step 20: 421.2215, Accuracy: 0.6570\n",
      "Training loss (for one batch) at step 30: 417.7234, Accuracy: 0.6608\n",
      "Training loss (for one batch) at step 40: 395.6732, Accuracy: 0.6671\n",
      "Training loss (for one batch) at step 50: 399.0125, Accuracy: 0.6785\n",
      "Training loss (for one batch) at step 60: 423.8107, Accuracy: 0.6837\n",
      "Training loss (for one batch) at step 70: 418.7881, Accuracy: 0.6839\n",
      "Training loss (for one batch) at step 80: 428.1864, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 90: 420.8061, Accuracy: 0.6763\n",
      "Training loss (for one batch) at step 100: 404.6694, Accuracy: 0.6706\n",
      "Training loss (for one batch) at step 110: 421.0645, Accuracy: 0.6712\n",
      "---- Training ----\n",
      "Training loss: 133.4327\n",
      "Training acc over epoch: 0.6716\n",
      "---- Validation ----\n",
      "Validation loss: 32.2281\n",
      "Validation acc: 0.6682\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 434.9299, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 422.9967, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 20: 429.6358, Accuracy: 0.6696\n",
      "Training loss (for one batch) at step 30: 406.5428, Accuracy: 0.6792\n",
      "Training loss (for one batch) at step 40: 407.0812, Accuracy: 0.6791\n",
      "Training loss (for one batch) at step 50: 389.5169, Accuracy: 0.6878\n",
      "Training loss (for one batch) at step 60: 412.1761, Accuracy: 0.6942\n",
      "Training loss (for one batch) at step 70: 418.3136, Accuracy: 0.6933\n",
      "Training loss (for one batch) at step 80: 414.8208, Accuracy: 0.6899\n",
      "Training loss (for one batch) at step 90: 412.0166, Accuracy: 0.6844\n",
      "Training loss (for one batch) at step 100: 408.1017, Accuracy: 0.6795\n",
      "Training loss (for one batch) at step 110: 415.0129, Accuracy: 0.6788\n",
      "---- Training ----\n",
      "Training loss: 128.8794\n",
      "Training acc over epoch: 0.6796\n",
      "---- Validation ----\n",
      "Validation loss: 34.3691\n",
      "Validation acc: 0.6701\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 436.2737, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 423.2374, Accuracy: 0.6754\n",
      "Training loss (for one batch) at step 20: 425.3001, Accuracy: 0.6652\n",
      "Training loss (for one batch) at step 30: 399.1234, Accuracy: 0.6736\n",
      "Training loss (for one batch) at step 40: 389.6169, Accuracy: 0.6787\n",
      "Training loss (for one batch) at step 50: 401.8630, Accuracy: 0.6877\n",
      "Training loss (for one batch) at step 60: 408.6880, Accuracy: 0.6962\n",
      "Training loss (for one batch) at step 70: 421.0594, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 80: 426.9323, Accuracy: 0.6903\n",
      "Training loss (for one batch) at step 90: 412.3542, Accuracy: 0.6840\n",
      "Training loss (for one batch) at step 100: 419.8317, Accuracy: 0.6810\n",
      "Training loss (for one batch) at step 110: 415.7441, Accuracy: 0.6833\n",
      "---- Training ----\n",
      "Training loss: 123.7940\n",
      "Training acc over epoch: 0.6826\n",
      "---- Validation ----\n",
      "Validation loss: 35.5875\n",
      "Validation acc: 0.6803\n",
      "Time taken: 13.44s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 438.8309, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 414.1493, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 20: 411.0388, Accuracy: 0.6856\n",
      "Training loss (for one batch) at step 30: 410.4938, Accuracy: 0.6862\n",
      "Training loss (for one batch) at step 40: 395.3718, Accuracy: 0.6946\n",
      "Training loss (for one batch) at step 50: 389.3664, Accuracy: 0.7002\n",
      "Training loss (for one batch) at step 60: 406.4327, Accuracy: 0.7059\n",
      "Training loss (for one batch) at step 70: 431.2993, Accuracy: 0.7061\n",
      "Training loss (for one batch) at step 80: 421.3086, Accuracy: 0.7005\n",
      "Training loss (for one batch) at step 90: 412.2729, Accuracy: 0.6973\n",
      "Training loss (for one batch) at step 100: 412.0232, Accuracy: 0.6948\n",
      "Training loss (for one batch) at step 110: 412.1362, Accuracy: 0.6956\n",
      "---- Training ----\n",
      "Training loss: 133.9290\n",
      "Training acc over epoch: 0.6959\n",
      "---- Validation ----\n",
      "Validation loss: 34.7584\n",
      "Validation acc: 0.6343\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 432.6326, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 418.0881, Accuracy: 0.6818\n",
      "Training loss (for one batch) at step 20: 397.4309, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 30: 414.1452, Accuracy: 0.6903\n",
      "Training loss (for one batch) at step 40: 412.3105, Accuracy: 0.6963\n",
      "Training loss (for one batch) at step 50: 385.5389, Accuracy: 0.7093\n",
      "Training loss (for one batch) at step 60: 412.5328, Accuracy: 0.7131\n",
      "Training loss (for one batch) at step 70: 411.5109, Accuracy: 0.7130\n",
      "Training loss (for one batch) at step 80: 431.8855, Accuracy: 0.7097\n",
      "Training loss (for one batch) at step 90: 417.9777, Accuracy: 0.7057\n",
      "Training loss (for one batch) at step 100: 405.7892, Accuracy: 0.7041\n",
      "Training loss (for one batch) at step 110: 407.4222, Accuracy: 0.7052\n",
      "---- Training ----\n",
      "Training loss: 135.6429\n",
      "Training acc over epoch: 0.7053\n",
      "---- Validation ----\n",
      "Validation loss: 36.0514\n",
      "Validation acc: 0.6808\n",
      "Time taken: 10.59s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 429.3571, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 412.8797, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 20: 407.9337, Accuracy: 0.6801\n",
      "Training loss (for one batch) at step 30: 387.5343, Accuracy: 0.6908\n",
      "Training loss (for one batch) at step 40: 388.9104, Accuracy: 0.7001\n",
      "Training loss (for one batch) at step 50: 395.3051, Accuracy: 0.7033\n",
      "Training loss (for one batch) at step 60: 398.6792, Accuracy: 0.7108\n",
      "Training loss (for one batch) at step 70: 422.1710, Accuracy: 0.7118\n",
      "Training loss (for one batch) at step 80: 429.4152, Accuracy: 0.7039\n",
      "Training loss (for one batch) at step 90: 414.7839, Accuracy: 0.7006\n",
      "Training loss (for one batch) at step 100: 400.3326, Accuracy: 0.6977\n",
      "Training loss (for one batch) at step 110: 401.6339, Accuracy: 0.6981\n",
      "---- Training ----\n",
      "Training loss: 133.0146\n",
      "Training acc over epoch: 0.6977\n",
      "---- Validation ----\n",
      "Validation loss: 33.9632\n",
      "Validation acc: 0.6824\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 419.8795, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 417.4139, Accuracy: 0.6832\n",
      "Training loss (for one batch) at step 20: 401.2193, Accuracy: 0.6983\n",
      "Training loss (for one batch) at step 30: 392.2150, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 40: 391.0557, Accuracy: 0.7130\n",
      "Training loss (for one batch) at step 50: 375.0412, Accuracy: 0.7177\n",
      "Training loss (for one batch) at step 60: 392.8162, Accuracy: 0.7234\n",
      "Training loss (for one batch) at step 70: 409.2128, Accuracy: 0.7202\n",
      "Training loss (for one batch) at step 80: 407.8338, Accuracy: 0.7134\n",
      "Training loss (for one batch) at step 90: 407.3144, Accuracy: 0.7094\n",
      "Training loss (for one batch) at step 100: 394.1408, Accuracy: 0.7068\n",
      "Training loss (for one batch) at step 110: 400.7137, Accuracy: 0.7076\n",
      "---- Training ----\n",
      "Training loss: 133.2407\n",
      "Training acc over epoch: 0.7084\n",
      "---- Validation ----\n",
      "Validation loss: 33.4413\n",
      "Validation acc: 0.6647\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 421.9701, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 407.6440, Accuracy: 0.6847\n",
      "Training loss (for one batch) at step 20: 404.7881, Accuracy: 0.6897\n",
      "Training loss (for one batch) at step 30: 389.4471, Accuracy: 0.7049\n",
      "Training loss (for one batch) at step 40: 382.7778, Accuracy: 0.7134\n",
      "Training loss (for one batch) at step 50: 383.2070, Accuracy: 0.7207\n",
      "Training loss (for one batch) at step 60: 400.4478, Accuracy: 0.7254\n",
      "Training loss (for one batch) at step 70: 400.6314, Accuracy: 0.7226\n",
      "Training loss (for one batch) at step 80: 408.7472, Accuracy: 0.7164\n",
      "Training loss (for one batch) at step 90: 398.4558, Accuracy: 0.7128\n",
      "Training loss (for one batch) at step 100: 398.6176, Accuracy: 0.7100\n",
      "Training loss (for one batch) at step 110: 389.6840, Accuracy: 0.7121\n",
      "---- Training ----\n",
      "Training loss: 123.2245\n",
      "Training acc over epoch: 0.7112\n",
      "---- Validation ----\n",
      "Validation loss: 31.7101\n",
      "Validation acc: 0.6642\n",
      "Time taken: 10.67s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 405.8746, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 412.4385, Accuracy: 0.7102\n",
      "Training loss (for one batch) at step 20: 400.4460, Accuracy: 0.6975\n",
      "Training loss (for one batch) at step 30: 398.4821, Accuracy: 0.7044\n",
      "Training loss (for one batch) at step 40: 384.7377, Accuracy: 0.7113\n",
      "Training loss (for one batch) at step 50: 377.9377, Accuracy: 0.7192\n",
      "Training loss (for one batch) at step 60: 395.1814, Accuracy: 0.7239\n",
      "Training loss (for one batch) at step 70: 406.4114, Accuracy: 0.7203\n",
      "Training loss (for one batch) at step 80: 399.1635, Accuracy: 0.7176\n",
      "Training loss (for one batch) at step 90: 392.2778, Accuracy: 0.7155\n",
      "Training loss (for one batch) at step 100: 387.8842, Accuracy: 0.7126\n",
      "Training loss (for one batch) at step 110: 400.7079, Accuracy: 0.7133\n",
      "---- Training ----\n",
      "Training loss: 121.9433\n",
      "Training acc over epoch: 0.7129\n",
      "---- Validation ----\n",
      "Validation loss: 34.2645\n",
      "Validation acc: 0.6625\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 409.3570, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 404.3945, Accuracy: 0.6932\n",
      "Training loss (for one batch) at step 20: 394.3050, Accuracy: 0.7098\n",
      "Training loss (for one batch) at step 30: 386.1621, Accuracy: 0.7210\n",
      "Training loss (for one batch) at step 40: 370.0132, Accuracy: 0.7288\n",
      "Training loss (for one batch) at step 50: 368.6393, Accuracy: 0.7374\n",
      "Training loss (for one batch) at step 60: 389.2669, Accuracy: 0.7378\n",
      "Training loss (for one batch) at step 70: 390.1681, Accuracy: 0.7356\n",
      "Training loss (for one batch) at step 80: 403.6281, Accuracy: 0.7273\n",
      "Training loss (for one batch) at step 90: 384.9266, Accuracy: 0.7226\n",
      "Training loss (for one batch) at step 100: 379.0287, Accuracy: 0.7221\n",
      "Training loss (for one batch) at step 110: 408.5182, Accuracy: 0.7235\n",
      "---- Training ----\n",
      "Training loss: 131.2503\n",
      "Training acc over epoch: 0.7225\n",
      "---- Validation ----\n",
      "Validation loss: 42.1506\n",
      "Validation acc: 0.6736\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 427.2122, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 401.6858, Accuracy: 0.6754\n",
      "Training loss (for one batch) at step 20: 384.7059, Accuracy: 0.6860\n",
      "Training loss (for one batch) at step 30: 368.3569, Accuracy: 0.7021\n",
      "Training loss (for one batch) at step 40: 357.0980, Accuracy: 0.7087\n",
      "Training loss (for one batch) at step 50: 363.8091, Accuracy: 0.7194\n",
      "Training loss (for one batch) at step 60: 362.3169, Accuracy: 0.7277\n",
      "Training loss (for one batch) at step 70: 401.1462, Accuracy: 0.7229\n",
      "Training loss (for one batch) at step 80: 375.4298, Accuracy: 0.7180\n",
      "Training loss (for one batch) at step 90: 378.7112, Accuracy: 0.7143\n",
      "Training loss (for one batch) at step 100: 370.8843, Accuracy: 0.7129\n",
      "Training loss (for one batch) at step 110: 400.2485, Accuracy: 0.7126\n",
      "---- Training ----\n",
      "Training loss: 126.9119\n",
      "Training acc over epoch: 0.7127\n",
      "---- Validation ----\n",
      "Validation loss: 35.2114\n",
      "Validation acc: 0.6736\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 401.3237, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 404.8014, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 20: 407.6123, Accuracy: 0.6957\n",
      "Training loss (for one batch) at step 30: 372.5605, Accuracy: 0.7087\n",
      "Training loss (for one batch) at step 40: 358.2157, Accuracy: 0.7201\n",
      "Training loss (for one batch) at step 50: 360.8326, Accuracy: 0.7287\n",
      "Training loss (for one batch) at step 60: 387.5414, Accuracy: 0.7376\n",
      "Training loss (for one batch) at step 70: 397.2497, Accuracy: 0.7318\n",
      "Training loss (for one batch) at step 80: 387.4111, Accuracy: 0.7250\n",
      "Training loss (for one batch) at step 90: 386.0203, Accuracy: 0.7213\n",
      "Training loss (for one batch) at step 100: 376.0353, Accuracy: 0.7206\n",
      "Training loss (for one batch) at step 110: 377.7416, Accuracy: 0.7207\n",
      "---- Training ----\n",
      "Training loss: 119.6279\n",
      "Training acc over epoch: 0.7208\n",
      "---- Validation ----\n",
      "Validation loss: 39.9175\n",
      "Validation acc: 0.6685\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 396.8574, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 384.2274, Accuracy: 0.6740\n",
      "Training loss (for one batch) at step 20: 389.5805, Accuracy: 0.6998\n",
      "Training loss (for one batch) at step 30: 382.2884, Accuracy: 0.7145\n",
      "Training loss (for one batch) at step 40: 368.8388, Accuracy: 0.7237\n",
      "Training loss (for one batch) at step 50: 357.8081, Accuracy: 0.7298\n",
      "Training loss (for one batch) at step 60: 385.1818, Accuracy: 0.7357\n",
      "Training loss (for one batch) at step 70: 389.1046, Accuracy: 0.7325\n",
      "Training loss (for one batch) at step 80: 399.3301, Accuracy: 0.7274\n",
      "Training loss (for one batch) at step 90: 378.9909, Accuracy: 0.7235\n",
      "Training loss (for one batch) at step 100: 376.2383, Accuracy: 0.7225\n",
      "Training loss (for one batch) at step 110: 369.8621, Accuracy: 0.7223\n",
      "---- Training ----\n",
      "Training loss: 127.7969\n",
      "Training acc over epoch: 0.7215\n",
      "---- Validation ----\n",
      "Validation loss: 34.1889\n",
      "Validation acc: 0.6634\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 408.7785, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 401.9975, Accuracy: 0.6825\n",
      "Training loss (for one batch) at step 20: 407.7526, Accuracy: 0.7054\n",
      "Training loss (for one batch) at step 30: 373.9348, Accuracy: 0.7145\n",
      "Training loss (for one batch) at step 40: 355.8254, Accuracy: 0.7275\n",
      "Training loss (for one batch) at step 50: 357.9638, Accuracy: 0.7373\n",
      "Training loss (for one batch) at step 60: 374.5643, Accuracy: 0.7400\n",
      "Training loss (for one batch) at step 70: 389.2821, Accuracy: 0.7356\n",
      "Training loss (for one batch) at step 80: 394.5747, Accuracy: 0.7303\n",
      "Training loss (for one batch) at step 90: 379.9208, Accuracy: 0.7275\n",
      "Training loss (for one batch) at step 100: 368.5678, Accuracy: 0.7249\n",
      "Training loss (for one batch) at step 110: 369.7678, Accuracy: 0.7249\n",
      "---- Training ----\n",
      "Training loss: 125.3814\n",
      "Training acc over epoch: 0.7256\n",
      "---- Validation ----\n",
      "Validation loss: 35.3363\n",
      "Validation acc: 0.6803\n",
      "Time taken: 10.62s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 378.7962, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 391.1239, Accuracy: 0.6939\n",
      "Training loss (for one batch) at step 20: 390.5736, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 30: 365.9771, Accuracy: 0.7261\n",
      "Training loss (for one batch) at step 40: 365.6381, Accuracy: 0.7313\n",
      "Training loss (for one batch) at step 50: 350.6584, Accuracy: 0.7400\n",
      "Training loss (for one batch) at step 60: 373.0372, Accuracy: 0.7432\n",
      "Training loss (for one batch) at step 70: 390.6776, Accuracy: 0.7388\n",
      "Training loss (for one batch) at step 80: 383.3565, Accuracy: 0.7324\n",
      "Training loss (for one batch) at step 90: 379.8967, Accuracy: 0.7291\n",
      "Training loss (for one batch) at step 100: 375.1543, Accuracy: 0.7280\n",
      "Training loss (for one batch) at step 110: 364.8591, Accuracy: 0.7283\n",
      "---- Training ----\n",
      "Training loss: 108.7473\n",
      "Training acc over epoch: 0.7274\n",
      "---- Validation ----\n",
      "Validation loss: 36.9695\n",
      "Validation acc: 0.6502\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 399.2920, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 380.0811, Accuracy: 0.6939\n",
      "Training loss (for one batch) at step 20: 378.1298, Accuracy: 0.7094\n",
      "Training loss (for one batch) at step 30: 357.8406, Accuracy: 0.7235\n",
      "Training loss (for one batch) at step 40: 365.3120, Accuracy: 0.7327\n",
      "Training loss (for one batch) at step 50: 351.8221, Accuracy: 0.7400\n",
      "Training loss (for one batch) at step 60: 377.5393, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 70: 377.2199, Accuracy: 0.7366\n",
      "Training loss (for one batch) at step 80: 383.7377, Accuracy: 0.7288\n",
      "Training loss (for one batch) at step 90: 370.5041, Accuracy: 0.7254\n",
      "Training loss (for one batch) at step 100: 349.6034, Accuracy: 0.7225\n",
      "Training loss (for one batch) at step 110: 361.8682, Accuracy: 0.7229\n",
      "---- Training ----\n",
      "Training loss: 115.3781\n",
      "Training acc over epoch: 0.7218\n",
      "---- Validation ----\n",
      "Validation loss: 39.1171\n",
      "Validation acc: 0.6591\n",
      "Time taken: 10.40s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 400.9651, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 385.2035, Accuracy: 0.6946\n",
      "Training loss (for one batch) at step 20: 364.3466, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 30: 356.9211, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 40: 341.9973, Accuracy: 0.7269\n",
      "Training loss (for one batch) at step 50: 358.8772, Accuracy: 0.7348\n",
      "Training loss (for one batch) at step 60: 367.9605, Accuracy: 0.7404\n",
      "Training loss (for one batch) at step 70: 389.6935, Accuracy: 0.7355\n",
      "Training loss (for one batch) at step 80: 380.8124, Accuracy: 0.7250\n",
      "Training loss (for one batch) at step 90: 357.9030, Accuracy: 0.7230\n",
      "Training loss (for one batch) at step 100: 355.0082, Accuracy: 0.7225\n",
      "Training loss (for one batch) at step 110: 382.2357, Accuracy: 0.7230\n",
      "---- Training ----\n",
      "Training loss: 117.4612\n",
      "Training acc over epoch: 0.7218\n",
      "---- Validation ----\n",
      "Validation loss: 34.5288\n",
      "Validation acc: 0.6655\n",
      "Time taken: 10.51s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 395.9232, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 384.7605, Accuracy: 0.6932\n",
      "Training loss (for one batch) at step 20: 358.4825, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 30: 362.7351, Accuracy: 0.7296\n",
      "Training loss (for one batch) at step 40: 366.9745, Accuracy: 0.7346\n",
      "Training loss (for one batch) at step 50: 354.1181, Accuracy: 0.7425\n",
      "Training loss (for one batch) at step 60: 354.7612, Accuracy: 0.7476\n",
      "Training loss (for one batch) at step 70: 375.7918, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 80: 363.8474, Accuracy: 0.7349\n",
      "Training loss (for one batch) at step 90: 360.6569, Accuracy: 0.7302\n",
      "Training loss (for one batch) at step 100: 351.5833, Accuracy: 0.7291\n",
      "Training loss (for one batch) at step 110: 374.5323, Accuracy: 0.7289\n",
      "---- Training ----\n",
      "Training loss: 116.8457\n",
      "Training acc over epoch: 0.7282\n",
      "---- Validation ----\n",
      "Validation loss: 40.0142\n",
      "Validation acc: 0.6854\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 397.4267, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 391.7674, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 20: 363.1092, Accuracy: 0.7068\n",
      "Training loss (for one batch) at step 30: 356.4771, Accuracy: 0.7162\n",
      "Training loss (for one batch) at step 40: 353.0089, Accuracy: 0.7239\n",
      "Training loss (for one batch) at step 50: 369.3453, Accuracy: 0.7324\n",
      "Training loss (for one batch) at step 60: 366.1541, Accuracy: 0.7360\n",
      "Training loss (for one batch) at step 70: 385.5935, Accuracy: 0.7315\n",
      "Training loss (for one batch) at step 80: 397.0146, Accuracy: 0.7269\n",
      "Training loss (for one batch) at step 90: 348.5142, Accuracy: 0.7255\n",
      "Training loss (for one batch) at step 100: 348.5951, Accuracy: 0.7254\n",
      "Training loss (for one batch) at step 110: 359.7347, Accuracy: 0.7256\n",
      "---- Training ----\n",
      "Training loss: 111.4430\n",
      "Training acc over epoch: 0.7233\n",
      "---- Validation ----\n",
      "Validation loss: 37.4810\n",
      "Validation acc: 0.6857\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 378.5049, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 366.5875, Accuracy: 0.7010\n",
      "Training loss (for one batch) at step 20: 357.6890, Accuracy: 0.7132\n",
      "Training loss (for one batch) at step 30: 366.4681, Accuracy: 0.7215\n",
      "Training loss (for one batch) at step 40: 350.2356, Accuracy: 0.7290\n",
      "Training loss (for one batch) at step 50: 344.7725, Accuracy: 0.7396\n",
      "Training loss (for one batch) at step 60: 345.3374, Accuracy: 0.7428\n",
      "Training loss (for one batch) at step 70: 369.5779, Accuracy: 0.7361\n",
      "Training loss (for one batch) at step 80: 383.4825, Accuracy: 0.7311\n",
      "Training loss (for one batch) at step 90: 355.6404, Accuracy: 0.7265\n",
      "Training loss (for one batch) at step 100: 354.1938, Accuracy: 0.7263\n",
      "Training loss (for one batch) at step 110: 353.5237, Accuracy: 0.7263\n",
      "---- Training ----\n",
      "Training loss: 115.5940\n",
      "Training acc over epoch: 0.7258\n",
      "---- Validation ----\n",
      "Validation loss: 34.2282\n",
      "Validation acc: 0.6623\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 367.2003, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 365.2071, Accuracy: 0.6903\n",
      "Training loss (for one batch) at step 20: 358.8203, Accuracy: 0.7147\n",
      "Training loss (for one batch) at step 30: 351.5653, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 40: 343.2483, Accuracy: 0.7369\n",
      "Training loss (for one batch) at step 50: 340.8426, Accuracy: 0.7451\n",
      "Training loss (for one batch) at step 60: 352.6546, Accuracy: 0.7494\n",
      "Training loss (for one batch) at step 70: 357.9913, Accuracy: 0.7391\n",
      "Training loss (for one batch) at step 80: 384.6816, Accuracy: 0.7332\n",
      "Training loss (for one batch) at step 90: 364.4254, Accuracy: 0.7315\n",
      "Training loss (for one batch) at step 100: 345.6623, Accuracy: 0.7287\n",
      "Training loss (for one batch) at step 110: 354.3956, Accuracy: 0.7287\n",
      "---- Training ----\n",
      "Training loss: 110.7376\n",
      "Training acc over epoch: 0.7276\n",
      "---- Validation ----\n",
      "Validation loss: 40.6336\n",
      "Validation acc: 0.6714\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 370.0511, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 362.2303, Accuracy: 0.6889\n",
      "Training loss (for one batch) at step 20: 360.0126, Accuracy: 0.7042\n",
      "Training loss (for one batch) at step 30: 352.6771, Accuracy: 0.7225\n",
      "Training loss (for one batch) at step 40: 338.3838, Accuracy: 0.7315\n",
      "Training loss (for one batch) at step 50: 340.7235, Accuracy: 0.7385\n",
      "Training loss (for one batch) at step 60: 333.2042, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 70: 358.4389, Accuracy: 0.7364\n",
      "Training loss (for one batch) at step 80: 378.2021, Accuracy: 0.7293\n",
      "Training loss (for one batch) at step 90: 352.8151, Accuracy: 0.7285\n",
      "Training loss (for one batch) at step 100: 341.6761, Accuracy: 0.7280\n",
      "Training loss (for one batch) at step 110: 369.0207, Accuracy: 0.7273\n",
      "---- Training ----\n",
      "Training loss: 118.7203\n",
      "Training acc over epoch: 0.7272\n",
      "---- Validation ----\n",
      "Validation loss: 39.5390\n",
      "Validation acc: 0.6510\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 371.7450, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 375.5791, Accuracy: 0.6932\n",
      "Training loss (for one batch) at step 20: 350.2380, Accuracy: 0.7202\n",
      "Training loss (for one batch) at step 30: 341.4113, Accuracy: 0.7273\n",
      "Training loss (for one batch) at step 40: 332.7064, Accuracy: 0.7378\n",
      "Training loss (for one batch) at step 50: 348.4951, Accuracy: 0.7434\n",
      "Training loss (for one batch) at step 60: 353.6019, Accuracy: 0.7460\n",
      "Training loss (for one batch) at step 70: 367.9101, Accuracy: 0.7400\n",
      "Training loss (for one batch) at step 80: 380.8670, Accuracy: 0.7322\n",
      "Training loss (for one batch) at step 90: 367.6178, Accuracy: 0.7307\n",
      "Training loss (for one batch) at step 100: 360.2862, Accuracy: 0.7293\n",
      "Training loss (for one batch) at step 110: 365.3613, Accuracy: 0.7290\n",
      "---- Training ----\n",
      "Training loss: 118.9399\n",
      "Training acc over epoch: 0.7290\n",
      "---- Validation ----\n",
      "Validation loss: 41.6103\n",
      "Validation acc: 0.6652\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 378.8523, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 370.8412, Accuracy: 0.6946\n",
      "Training loss (for one batch) at step 20: 349.6922, Accuracy: 0.7150\n",
      "Training loss (for one batch) at step 30: 328.1018, Accuracy: 0.7316\n",
      "Training loss (for one batch) at step 40: 344.5106, Accuracy: 0.7380\n",
      "Training loss (for one batch) at step 50: 328.5253, Accuracy: 0.7466\n",
      "Training loss (for one batch) at step 60: 356.6219, Accuracy: 0.7512\n",
      "Training loss (for one batch) at step 70: 359.9406, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 80: 384.2660, Accuracy: 0.7343\n",
      "Training loss (for one batch) at step 90: 354.7650, Accuracy: 0.7323\n",
      "Training loss (for one batch) at step 100: 343.3049, Accuracy: 0.7305\n",
      "Training loss (for one batch) at step 110: 364.1650, Accuracy: 0.7299\n",
      "---- Training ----\n",
      "Training loss: 109.3561\n",
      "Training acc over epoch: 0.7282\n",
      "---- Validation ----\n",
      "Validation loss: 35.6481\n",
      "Validation acc: 0.6792\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 365.8899, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 367.3766, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 20: 354.1935, Accuracy: 0.7191\n",
      "Training loss (for one batch) at step 30: 342.9695, Accuracy: 0.7298\n",
      "Training loss (for one batch) at step 40: 339.2690, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 50: 328.3834, Accuracy: 0.7488\n",
      "Training loss (for one batch) at step 60: 344.9297, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 70: 358.0433, Accuracy: 0.7421\n",
      "Training loss (for one batch) at step 80: 358.6764, Accuracy: 0.7342\n",
      "Training loss (for one batch) at step 90: 347.9487, Accuracy: 0.7339\n",
      "Training loss (for one batch) at step 100: 336.0924, Accuracy: 0.7315\n",
      "Training loss (for one batch) at step 110: 347.4825, Accuracy: 0.7308\n",
      "---- Training ----\n",
      "Training loss: 119.5735\n",
      "Training acc over epoch: 0.7298\n",
      "---- Validation ----\n",
      "Validation loss: 34.9971\n",
      "Validation acc: 0.6709\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 389.3987, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 353.2141, Accuracy: 0.7010\n",
      "Training loss (for one batch) at step 20: 343.6918, Accuracy: 0.7135\n",
      "Training loss (for one batch) at step 30: 353.4872, Accuracy: 0.7293\n",
      "Training loss (for one batch) at step 40: 333.5951, Accuracy: 0.7395\n",
      "Training loss (for one batch) at step 50: 324.3009, Accuracy: 0.7495\n",
      "Training loss (for one batch) at step 60: 323.3337, Accuracy: 0.7526\n",
      "Training loss (for one batch) at step 70: 369.3190, Accuracy: 0.7445\n",
      "Training loss (for one batch) at step 80: 357.3113, Accuracy: 0.7356\n",
      "Training loss (for one batch) at step 90: 343.6590, Accuracy: 0.7329\n",
      "Training loss (for one batch) at step 100: 325.4296, Accuracy: 0.7333\n",
      "Training loss (for one batch) at step 110: 350.5337, Accuracy: 0.7324\n",
      "---- Training ----\n",
      "Training loss: 109.2401\n",
      "Training acc over epoch: 0.7316\n",
      "---- Validation ----\n",
      "Validation loss: 40.2665\n",
      "Validation acc: 0.6803\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 354.7379, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 360.7647, Accuracy: 0.6925\n",
      "Training loss (for one batch) at step 20: 335.3232, Accuracy: 0.7206\n",
      "Training loss (for one batch) at step 30: 336.7943, Accuracy: 0.7301\n",
      "Training loss (for one batch) at step 40: 320.3480, Accuracy: 0.7389\n",
      "Training loss (for one batch) at step 50: 332.3585, Accuracy: 0.7483\n",
      "Training loss (for one batch) at step 60: 332.0611, Accuracy: 0.7538\n",
      "Training loss (for one batch) at step 70: 364.2968, Accuracy: 0.7469\n",
      "Training loss (for one batch) at step 80: 373.9478, Accuracy: 0.7380\n",
      "Training loss (for one batch) at step 90: 351.0785, Accuracy: 0.7331\n",
      "Training loss (for one batch) at step 100: 348.4169, Accuracy: 0.7330\n",
      "Training loss (for one batch) at step 110: 350.7491, Accuracy: 0.7320\n",
      "---- Training ----\n",
      "Training loss: 106.3050\n",
      "Training acc over epoch: 0.7321\n",
      "---- Validation ----\n",
      "Validation loss: 31.4545\n",
      "Validation acc: 0.6822\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 361.6146, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 359.5040, Accuracy: 0.6932\n",
      "Training loss (for one batch) at step 20: 346.3688, Accuracy: 0.7098\n",
      "Training loss (for one batch) at step 30: 346.3369, Accuracy: 0.7293\n",
      "Training loss (for one batch) at step 40: 333.7563, Accuracy: 0.7388\n",
      "Training loss (for one batch) at step 50: 335.6262, Accuracy: 0.7483\n",
      "Training loss (for one batch) at step 60: 329.1396, Accuracy: 0.7532\n",
      "Training loss (for one batch) at step 70: 355.4606, Accuracy: 0.7463\n",
      "Training loss (for one batch) at step 80: 374.8051, Accuracy: 0.7369\n",
      "Training loss (for one batch) at step 90: 349.7264, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 100: 324.4992, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 110: 342.9215, Accuracy: 0.7340\n",
      "---- Training ----\n",
      "Training loss: 105.7304\n",
      "Training acc over epoch: 0.7335\n",
      "---- Validation ----\n",
      "Validation loss: 48.6149\n",
      "Validation acc: 0.6486\n",
      "Time taken: 10.70s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 366.9436, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 347.8048, Accuracy: 0.6960\n",
      "Training loss (for one batch) at step 20: 335.1816, Accuracy: 0.7135\n",
      "Training loss (for one batch) at step 30: 330.6780, Accuracy: 0.7324\n",
      "Training loss (for one batch) at step 40: 338.9079, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 50: 333.8542, Accuracy: 0.7506\n",
      "Training loss (for one batch) at step 60: 336.5611, Accuracy: 0.7563\n",
      "Training loss (for one batch) at step 70: 367.2965, Accuracy: 0.7461\n",
      "Training loss (for one batch) at step 80: 371.8163, Accuracy: 0.7375\n",
      "Training loss (for one batch) at step 90: 350.3787, Accuracy: 0.7354\n",
      "Training loss (for one batch) at step 100: 336.2818, Accuracy: 0.7322\n",
      "Training loss (for one batch) at step 110: 326.6814, Accuracy: 0.7332\n",
      "---- Training ----\n",
      "Training loss: 109.5714\n",
      "Training acc over epoch: 0.7321\n",
      "---- Validation ----\n",
      "Validation loss: 39.5574\n",
      "Validation acc: 0.6779\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 352.1530, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 353.8896, Accuracy: 0.6854\n",
      "Training loss (for one batch) at step 20: 338.6335, Accuracy: 0.7184\n",
      "Training loss (for one batch) at step 30: 346.2844, Accuracy: 0.7326\n",
      "Training loss (for one batch) at step 40: 337.9391, Accuracy: 0.7407\n",
      "Training loss (for one batch) at step 50: 337.0927, Accuracy: 0.7471\n",
      "Training loss (for one batch) at step 60: 337.6364, Accuracy: 0.7512\n",
      "Training loss (for one batch) at step 70: 354.7438, Accuracy: 0.7430\n",
      "Training loss (for one batch) at step 80: 349.1378, Accuracy: 0.7349\n",
      "Training loss (for one batch) at step 90: 337.9427, Accuracy: 0.7340\n",
      "Training loss (for one batch) at step 100: 336.6344, Accuracy: 0.7350\n",
      "Training loss (for one batch) at step 110: 347.3320, Accuracy: 0.7339\n",
      "---- Training ----\n",
      "Training loss: 107.0350\n",
      "Training acc over epoch: 0.7326\n",
      "---- Validation ----\n",
      "Validation loss: 34.9497\n",
      "Validation acc: 0.6658\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 358.5625, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 358.8005, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 20: 337.5177, Accuracy: 0.7161\n",
      "Training loss (for one batch) at step 30: 346.3087, Accuracy: 0.7253\n",
      "Training loss (for one batch) at step 40: 313.4021, Accuracy: 0.7353\n",
      "Training loss (for one batch) at step 50: 325.0546, Accuracy: 0.7465\n",
      "Training loss (for one batch) at step 60: 338.2304, Accuracy: 0.7491\n",
      "Training loss (for one batch) at step 70: 372.8675, Accuracy: 0.7425\n",
      "Training loss (for one batch) at step 80: 357.3184, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 90: 347.4759, Accuracy: 0.7325\n",
      "Training loss (for one batch) at step 100: 328.8812, Accuracy: 0.7332\n",
      "Training loss (for one batch) at step 110: 327.5775, Accuracy: 0.7344\n",
      "---- Training ----\n",
      "Training loss: 116.4052\n",
      "Training acc over epoch: 0.7335\n",
      "---- Validation ----\n",
      "Validation loss: 60.4595\n",
      "Validation acc: 0.6746\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 370.7791, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 351.5347, Accuracy: 0.6847\n",
      "Training loss (for one batch) at step 20: 321.2546, Accuracy: 0.7143\n",
      "Training loss (for one batch) at step 30: 332.0450, Accuracy: 0.7276\n",
      "Training loss (for one batch) at step 40: 327.9080, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 50: 314.1979, Accuracy: 0.7460\n",
      "Training loss (for one batch) at step 60: 328.0081, Accuracy: 0.7518\n",
      "Training loss (for one batch) at step 70: 369.3741, Accuracy: 0.7459\n",
      "Training loss (for one batch) at step 80: 349.4652, Accuracy: 0.7363\n",
      "Training loss (for one batch) at step 90: 338.6301, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 100: 337.2656, Accuracy: 0.7350\n",
      "Training loss (for one batch) at step 110: 346.5233, Accuracy: 0.7330\n",
      "---- Training ----\n",
      "Training loss: 125.1662\n",
      "Training acc over epoch: 0.7319\n",
      "---- Validation ----\n",
      "Validation loss: 41.7176\n",
      "Validation acc: 0.6687\n",
      "Time taken: 10.40s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 352.7155, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 340.9282, Accuracy: 0.6925\n",
      "Training loss (for one batch) at step 20: 345.2563, Accuracy: 0.7195\n",
      "Training loss (for one batch) at step 30: 306.9239, Accuracy: 0.7424\n",
      "Training loss (for one batch) at step 40: 308.3303, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 50: 316.6945, Accuracy: 0.7552\n",
      "Training loss (for one batch) at step 60: 331.8286, Accuracy: 0.7551\n",
      "Training loss (for one batch) at step 70: 335.3558, Accuracy: 0.7471\n",
      "Training loss (for one batch) at step 80: 351.8633, Accuracy: 0.7393\n",
      "Training loss (for one batch) at step 90: 328.8411, Accuracy: 0.7368\n",
      "Training loss (for one batch) at step 100: 312.3325, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 110: 320.1727, Accuracy: 0.7341\n",
      "---- Training ----\n",
      "Training loss: 106.7572\n",
      "Training acc over epoch: 0.7335\n",
      "---- Validation ----\n",
      "Validation loss: 41.1886\n",
      "Validation acc: 0.6671\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 347.8429, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 347.7449, Accuracy: 0.7081\n",
      "Training loss (for one batch) at step 20: 341.1664, Accuracy: 0.7202\n",
      "Training loss (for one batch) at step 30: 324.6439, Accuracy: 0.7392\n",
      "Training loss (for one batch) at step 40: 312.9146, Accuracy: 0.7504\n",
      "Training loss (for one batch) at step 50: 321.4301, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 60: 306.4683, Accuracy: 0.7547\n",
      "Training loss (for one batch) at step 70: 350.9213, Accuracy: 0.7483\n",
      "Training loss (for one batch) at step 80: 348.9971, Accuracy: 0.7384\n",
      "Training loss (for one batch) at step 90: 325.9931, Accuracy: 0.7365\n",
      "Training loss (for one batch) at step 100: 322.1188, Accuracy: 0.7346\n",
      "Training loss (for one batch) at step 110: 330.3127, Accuracy: 0.7347\n",
      "---- Training ----\n",
      "Training loss: 98.2488\n",
      "Training acc over epoch: 0.7329\n",
      "---- Validation ----\n",
      "Validation loss: 47.4459\n",
      "Validation acc: 0.6738\n",
      "Time taken: 12.49s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 365.8281, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 343.0558, Accuracy: 0.6839\n",
      "Training loss (for one batch) at step 20: 336.3353, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 30: 315.1646, Accuracy: 0.7377\n",
      "Training loss (for one batch) at step 40: 307.9700, Accuracy: 0.7452\n",
      "Training loss (for one batch) at step 50: 321.6507, Accuracy: 0.7531\n",
      "Training loss (for one batch) at step 60: 333.7015, Accuracy: 0.7564\n",
      "Training loss (for one batch) at step 70: 336.8347, Accuracy: 0.7472\n",
      "Training loss (for one batch) at step 80: 373.2078, Accuracy: 0.7367\n",
      "Training loss (for one batch) at step 90: 324.2665, Accuracy: 0.7348\n",
      "Training loss (for one batch) at step 100: 320.9078, Accuracy: 0.7341\n",
      "Training loss (for one batch) at step 110: 343.9262, Accuracy: 0.7325\n",
      "---- Training ----\n",
      "Training loss: 92.1390\n",
      "Training acc over epoch: 0.7323\n",
      "---- Validation ----\n",
      "Validation loss: 42.4056\n",
      "Validation acc: 0.6757\n",
      "Time taken: 10.70s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 374.2396, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 360.4372, Accuracy: 0.6960\n",
      "Training loss (for one batch) at step 20: 324.4853, Accuracy: 0.7161\n",
      "Training loss (for one batch) at step 30: 324.1246, Accuracy: 0.7339\n",
      "Training loss (for one batch) at step 40: 323.0192, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 50: 306.8250, Accuracy: 0.7517\n",
      "Training loss (for one batch) at step 60: 321.2932, Accuracy: 0.7564\n",
      "Training loss (for one batch) at step 70: 350.6671, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 80: 334.0155, Accuracy: 0.7377\n",
      "Training loss (for one batch) at step 90: 348.5817, Accuracy: 0.7353\n",
      "Training loss (for one batch) at step 100: 323.6442, Accuracy: 0.7358\n",
      "Training loss (for one batch) at step 110: 318.5369, Accuracy: 0.7350\n",
      "---- Training ----\n",
      "Training loss: 115.9405\n",
      "Training acc over epoch: 0.7337\n",
      "---- Validation ----\n",
      "Validation loss: 33.5887\n",
      "Validation acc: 0.6857\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 342.6994, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 340.1053, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 20: 337.0470, Accuracy: 0.7173\n",
      "Training loss (for one batch) at step 30: 322.2812, Accuracy: 0.7316\n",
      "Training loss (for one batch) at step 40: 297.8779, Accuracy: 0.7456\n",
      "Training loss (for one batch) at step 50: 312.0699, Accuracy: 0.7505\n",
      "Training loss (for one batch) at step 60: 343.0552, Accuracy: 0.7540\n",
      "Training loss (for one batch) at step 70: 330.6257, Accuracy: 0.7485\n",
      "Training loss (for one batch) at step 80: 351.2859, Accuracy: 0.7379\n",
      "Training loss (for one batch) at step 90: 336.2056, Accuracy: 0.7349\n",
      "Training loss (for one batch) at step 100: 324.2773, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 110: 326.0413, Accuracy: 0.7346\n",
      "---- Training ----\n",
      "Training loss: 106.3019\n",
      "Training acc over epoch: 0.7325\n",
      "---- Validation ----\n",
      "Validation loss: 43.0026\n",
      "Validation acc: 0.6771\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 368.8339, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 335.0386, Accuracy: 0.7003\n",
      "Training loss (for one batch) at step 20: 326.1262, Accuracy: 0.7202\n",
      "Training loss (for one batch) at step 30: 325.5824, Accuracy: 0.7399\n",
      "Training loss (for one batch) at step 40: 346.1392, Accuracy: 0.7477\n",
      "Training loss (for one batch) at step 50: 320.3398, Accuracy: 0.7572\n",
      "Training loss (for one batch) at step 60: 321.9199, Accuracy: 0.7599\n",
      "Training loss (for one batch) at step 70: 338.5789, Accuracy: 0.7537\n",
      "Training loss (for one batch) at step 80: 349.2213, Accuracy: 0.7411\n",
      "Training loss (for one batch) at step 90: 323.2822, Accuracy: 0.7400\n",
      "Training loss (for one batch) at step 100: 351.2014, Accuracy: 0.7373\n",
      "Training loss (for one batch) at step 110: 319.2473, Accuracy: 0.7375\n",
      "---- Training ----\n",
      "Training loss: 114.7472\n",
      "Training acc over epoch: 0.7362\n",
      "---- Validation ----\n",
      "Validation loss: 44.2850\n",
      "Validation acc: 0.6677\n",
      "Time taken: 10.58s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 340.2274, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 344.5424, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 20: 328.8969, Accuracy: 0.7135\n",
      "Training loss (for one batch) at step 30: 323.5614, Accuracy: 0.7339\n",
      "Training loss (for one batch) at step 40: 300.7672, Accuracy: 0.7407\n",
      "Training loss (for one batch) at step 50: 304.8932, Accuracy: 0.7514\n",
      "Training loss (for one batch) at step 60: 326.7129, Accuracy: 0.7523\n",
      "Training loss (for one batch) at step 70: 339.5068, Accuracy: 0.7461\n",
      "Training loss (for one batch) at step 80: 339.5542, Accuracy: 0.7381\n",
      "Training loss (for one batch) at step 90: 322.3466, Accuracy: 0.7350\n",
      "Training loss (for one batch) at step 100: 311.3122, Accuracy: 0.7343\n",
      "Training loss (for one batch) at step 110: 315.9535, Accuracy: 0.7348\n",
      "---- Training ----\n",
      "Training loss: 103.4912\n",
      "Training acc over epoch: 0.7340\n",
      "---- Validation ----\n",
      "Validation loss: 39.9475\n",
      "Validation acc: 0.6795\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 362.0158, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 342.7961, Accuracy: 0.7017\n",
      "Training loss (for one batch) at step 20: 310.2156, Accuracy: 0.7195\n",
      "Training loss (for one batch) at step 30: 332.1801, Accuracy: 0.7374\n",
      "Training loss (for one batch) at step 40: 312.9813, Accuracy: 0.7485\n",
      "Training loss (for one batch) at step 50: 303.0717, Accuracy: 0.7532\n",
      "Training loss (for one batch) at step 60: 321.8711, Accuracy: 0.7558\n",
      "Training loss (for one batch) at step 70: 361.7669, Accuracy: 0.7494\n",
      "Training loss (for one batch) at step 80: 334.8759, Accuracy: 0.7390\n",
      "Training loss (for one batch) at step 90: 318.8355, Accuracy: 0.7374\n",
      "Training loss (for one batch) at step 100: 328.0294, Accuracy: 0.7364\n",
      "Training loss (for one batch) at step 110: 321.2610, Accuracy: 0.7384\n",
      "---- Training ----\n",
      "Training loss: 108.4665\n",
      "Training acc over epoch: 0.7366\n",
      "---- Validation ----\n",
      "Validation loss: 41.9001\n",
      "Validation acc: 0.6625\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 333.3952, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 338.2471, Accuracy: 0.6896\n",
      "Training loss (for one batch) at step 20: 322.7784, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 30: 308.5641, Accuracy: 0.7356\n",
      "Training loss (for one batch) at step 40: 308.5555, Accuracy: 0.7435\n",
      "Training loss (for one batch) at step 50: 288.8436, Accuracy: 0.7503\n",
      "Training loss (for one batch) at step 60: 325.8718, Accuracy: 0.7568\n",
      "Training loss (for one batch) at step 70: 333.8782, Accuracy: 0.7490\n",
      "Training loss (for one batch) at step 80: 350.1721, Accuracy: 0.7380\n",
      "Training loss (for one batch) at step 90: 326.7726, Accuracy: 0.7353\n",
      "Training loss (for one batch) at step 100: 320.3415, Accuracy: 0.7335\n",
      "Training loss (for one batch) at step 110: 303.4065, Accuracy: 0.7325\n",
      "---- Training ----\n",
      "Training loss: 114.8472\n",
      "Training acc over epoch: 0.7317\n",
      "---- Validation ----\n",
      "Validation loss: 43.9997\n",
      "Validation acc: 0.6639\n",
      "Time taken: 10.51s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 376.2661, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 335.8959, Accuracy: 0.6974\n",
      "Training loss (for one batch) at step 20: 326.8338, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 30: 293.9039, Accuracy: 0.7326\n",
      "Training loss (for one batch) at step 40: 317.8084, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 50: 309.3914, Accuracy: 0.7498\n",
      "Training loss (for one batch) at step 60: 308.5233, Accuracy: 0.7545\n",
      "Training loss (for one batch) at step 70: 347.7257, Accuracy: 0.7475\n",
      "Training loss (for one batch) at step 80: 330.3196, Accuracy: 0.7374\n",
      "Training loss (for one batch) at step 90: 332.2954, Accuracy: 0.7346\n",
      "Training loss (for one batch) at step 100: 325.0001, Accuracy: 0.7333\n",
      "Training loss (for one batch) at step 110: 331.6140, Accuracy: 0.7328\n",
      "---- Training ----\n",
      "Training loss: 115.9703\n",
      "Training acc over epoch: 0.7327\n",
      "---- Validation ----\n",
      "Validation loss: 33.6824\n",
      "Validation acc: 0.6682\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 346.7585, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 355.6646, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 20: 322.4654, Accuracy: 0.7154\n",
      "Training loss (for one batch) at step 30: 310.6023, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 40: 304.6369, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 50: 315.7823, Accuracy: 0.7538\n",
      "Training loss (for one batch) at step 60: 314.1806, Accuracy: 0.7602\n",
      "Training loss (for one batch) at step 70: 344.4701, Accuracy: 0.7497\n",
      "Training loss (for one batch) at step 80: 355.5833, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 90: 318.4000, Accuracy: 0.7361\n",
      "Training loss (for one batch) at step 100: 312.3814, Accuracy: 0.7348\n",
      "Training loss (for one batch) at step 110: 340.6165, Accuracy: 0.7346\n",
      "---- Training ----\n",
      "Training loss: 93.8805\n",
      "Training acc over epoch: 0.7343\n",
      "---- Validation ----\n",
      "Validation loss: 47.0266\n",
      "Validation acc: 0.6625\n",
      "Time taken: 10.40s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 356.7801, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 332.8358, Accuracy: 0.6918\n",
      "Training loss (for one batch) at step 20: 319.0302, Accuracy: 0.7154\n",
      "Training loss (for one batch) at step 30: 324.6688, Accuracy: 0.7286\n",
      "Training loss (for one batch) at step 40: 305.9603, Accuracy: 0.7441\n",
      "Training loss (for one batch) at step 50: 299.6541, Accuracy: 0.7538\n",
      "Training loss (for one batch) at step 60: 306.9888, Accuracy: 0.7576\n",
      "Training loss (for one batch) at step 70: 334.7535, Accuracy: 0.7493\n",
      "Training loss (for one batch) at step 80: 351.6547, Accuracy: 0.7413\n",
      "Training loss (for one batch) at step 90: 318.6105, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 100: 321.6506, Accuracy: 0.7372\n",
      "Training loss (for one batch) at step 110: 318.8421, Accuracy: 0.7362\n",
      "---- Training ----\n",
      "Training loss: 102.8911\n",
      "Training acc over epoch: 0.7357\n",
      "---- Validation ----\n",
      "Validation loss: 51.5095\n",
      "Validation acc: 0.6652\n",
      "Time taken: 10.56s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 339.9359, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 316.3489, Accuracy: 0.6911\n",
      "Training loss (for one batch) at step 20: 352.6706, Accuracy: 0.7195\n",
      "Training loss (for one batch) at step 30: 315.7927, Accuracy: 0.7303\n",
      "Training loss (for one batch) at step 40: 296.8914, Accuracy: 0.7416\n",
      "Training loss (for one batch) at step 50: 321.4684, Accuracy: 0.7491\n",
      "Training loss (for one batch) at step 60: 330.1368, Accuracy: 0.7546\n",
      "Training loss (for one batch) at step 70: 323.2007, Accuracy: 0.7479\n",
      "Training loss (for one batch) at step 80: 327.7648, Accuracy: 0.7383\n",
      "Training loss (for one batch) at step 90: 316.4614, Accuracy: 0.7361\n",
      "Training loss (for one batch) at step 100: 319.4110, Accuracy: 0.7372\n",
      "Training loss (for one batch) at step 110: 324.2436, Accuracy: 0.7370\n",
      "---- Training ----\n",
      "Training loss: 109.2442\n",
      "Training acc over epoch: 0.7352\n",
      "---- Validation ----\n",
      "Validation loss: 57.0286\n",
      "Validation acc: 0.6647\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 319.1609, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 336.2499, Accuracy: 0.6911\n",
      "Training loss (for one batch) at step 20: 314.8410, Accuracy: 0.7251\n",
      "Training loss (for one batch) at step 30: 332.2538, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 40: 293.3131, Accuracy: 0.7504\n",
      "Training loss (for one batch) at step 50: 309.7607, Accuracy: 0.7567\n",
      "Training loss (for one batch) at step 60: 317.2248, Accuracy: 0.7590\n",
      "Training loss (for one batch) at step 70: 322.0733, Accuracy: 0.7498\n",
      "Training loss (for one batch) at step 80: 324.0070, Accuracy: 0.7416\n",
      "Training loss (for one batch) at step 90: 332.6557, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 100: 327.5809, Accuracy: 0.7374\n",
      "Training loss (for one batch) at step 110: 320.9827, Accuracy: 0.7358\n",
      "---- Training ----\n",
      "Training loss: 105.9473\n",
      "Training acc over epoch: 0.7346\n",
      "---- Validation ----\n",
      "Validation loss: 41.9205\n",
      "Validation acc: 0.6591\n",
      "Time taken: 10.62s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 329.7723, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 330.9214, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 20: 315.3309, Accuracy: 0.7206\n",
      "Training loss (for one batch) at step 30: 302.2021, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 40: 307.4652, Accuracy: 0.7490\n",
      "Training loss (for one batch) at step 50: 307.2372, Accuracy: 0.7564\n",
      "Training loss (for one batch) at step 60: 319.3256, Accuracy: 0.7618\n",
      "Training loss (for one batch) at step 70: 319.8849, Accuracy: 0.7518\n",
      "Training loss (for one batch) at step 80: 344.7628, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 90: 316.9991, Accuracy: 0.7379\n",
      "Training loss (for one batch) at step 100: 296.3687, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 110: 312.8603, Accuracy: 0.7380\n",
      "---- Training ----\n",
      "Training loss: 97.7060\n",
      "Training acc over epoch: 0.7368\n",
      "---- Validation ----\n",
      "Validation loss: 35.5245\n",
      "Validation acc: 0.6666\n",
      "Time taken: 10.74s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 343.2665, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 320.6432, Accuracy: 0.6932\n",
      "Training loss (for one batch) at step 20: 319.2246, Accuracy: 0.7221\n",
      "Training loss (for one batch) at step 30: 315.2535, Accuracy: 0.7334\n",
      "Training loss (for one batch) at step 40: 301.2499, Accuracy: 0.7389\n",
      "Training loss (for one batch) at step 50: 289.7968, Accuracy: 0.7492\n",
      "Training loss (for one batch) at step 60: 321.2396, Accuracy: 0.7553\n",
      "Training loss (for one batch) at step 70: 327.5003, Accuracy: 0.7467\n",
      "Training loss (for one batch) at step 80: 335.4128, Accuracy: 0.7364\n",
      "Training loss (for one batch) at step 90: 327.1142, Accuracy: 0.7337\n",
      "Training loss (for one batch) at step 100: 304.8845, Accuracy: 0.7345\n",
      "Training loss (for one batch) at step 110: 328.5446, Accuracy: 0.7337\n",
      "---- Training ----\n",
      "Training loss: 106.1039\n",
      "Training acc over epoch: 0.7337\n",
      "---- Validation ----\n",
      "Validation loss: 41.8350\n",
      "Validation acc: 0.6765\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 338.5295, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 320.7642, Accuracy: 0.6960\n",
      "Training loss (for one batch) at step 20: 319.3982, Accuracy: 0.7173\n",
      "Training loss (for one batch) at step 30: 299.9004, Accuracy: 0.7319\n",
      "Training loss (for one batch) at step 40: 297.6869, Accuracy: 0.7391\n",
      "Training loss (for one batch) at step 50: 287.3452, Accuracy: 0.7489\n",
      "Training loss (for one batch) at step 60: 294.8989, Accuracy: 0.7533\n",
      "Training loss (for one batch) at step 70: 337.0005, Accuracy: 0.7477\n",
      "Training loss (for one batch) at step 80: 337.6835, Accuracy: 0.7392\n",
      "Training loss (for one batch) at step 90: 315.4279, Accuracy: 0.7343\n",
      "Training loss (for one batch) at step 100: 313.5896, Accuracy: 0.7358\n",
      "Training loss (for one batch) at step 110: 319.9739, Accuracy: 0.7346\n",
      "---- Training ----\n",
      "Training loss: 94.8720\n",
      "Training acc over epoch: 0.7341\n",
      "---- Validation ----\n",
      "Validation loss: 43.8646\n",
      "Validation acc: 0.6617\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 351.3503, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 328.0609, Accuracy: 0.6896\n",
      "Training loss (for one batch) at step 20: 295.6856, Accuracy: 0.7180\n",
      "Training loss (for one batch) at step 30: 292.7699, Accuracy: 0.7339\n",
      "Training loss (for one batch) at step 40: 298.1701, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 50: 289.6475, Accuracy: 0.7509\n",
      "Training loss (for one batch) at step 60: 333.9067, Accuracy: 0.7537\n",
      "Training loss (for one batch) at step 70: 342.2794, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 80: 325.0769, Accuracy: 0.7377\n",
      "Training loss (for one batch) at step 90: 316.3893, Accuracy: 0.7349\n",
      "Training loss (for one batch) at step 100: 311.5005, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 110: 302.5620, Accuracy: 0.7344\n",
      "---- Training ----\n",
      "Training loss: 103.4122\n",
      "Training acc over epoch: 0.7331\n",
      "---- Validation ----\n",
      "Validation loss: 47.9899\n",
      "Validation acc: 0.6698\n",
      "Time taken: 10.60s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 325.0876, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 347.3452, Accuracy: 0.7003\n",
      "Training loss (for one batch) at step 20: 317.6267, Accuracy: 0.7232\n",
      "Training loss (for one batch) at step 30: 302.3784, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 40: 299.3156, Accuracy: 0.7504\n",
      "Training loss (for one batch) at step 50: 312.6227, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 60: 316.6180, Accuracy: 0.7615\n",
      "Training loss (for one batch) at step 70: 331.2498, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 80: 325.3767, Accuracy: 0.7410\n",
      "Training loss (for one batch) at step 90: 327.0340, Accuracy: 0.7389\n",
      "Training loss (for one batch) at step 100: 317.3629, Accuracy: 0.7365\n",
      "Training loss (for one batch) at step 110: 321.6511, Accuracy: 0.7368\n",
      "---- Training ----\n",
      "Training loss: 113.1720\n",
      "Training acc over epoch: 0.7356\n",
      "---- Validation ----\n",
      "Validation loss: 40.2437\n",
      "Validation acc: 0.6738\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 332.8842, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 330.6938, Accuracy: 0.6825\n",
      "Training loss (for one batch) at step 20: 299.4902, Accuracy: 0.7184\n",
      "Training loss (for one batch) at step 30: 328.2583, Accuracy: 0.7384\n",
      "Training loss (for one batch) at step 40: 292.7595, Accuracy: 0.7471\n",
      "Training loss (for one batch) at step 50: 292.2324, Accuracy: 0.7561\n",
      "Training loss (for one batch) at step 60: 315.1371, Accuracy: 0.7576\n",
      "Training loss (for one batch) at step 70: 335.8479, Accuracy: 0.7498\n",
      "Training loss (for one batch) at step 80: 348.1611, Accuracy: 0.7417\n",
      "Training loss (for one batch) at step 90: 326.0211, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 100: 319.2622, Accuracy: 0.7366\n",
      "Training loss (for one batch) at step 110: 301.2982, Accuracy: 0.7380\n",
      "---- Training ----\n",
      "Training loss: 104.2398\n",
      "Training acc over epoch: 0.7364\n",
      "---- Validation ----\n",
      "Validation loss: 53.5648\n",
      "Validation acc: 0.6752\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 331.0328, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 317.2432, Accuracy: 0.7003\n",
      "Training loss (for one batch) at step 20: 327.8779, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 30: 299.6024, Accuracy: 0.7308\n",
      "Training loss (for one batch) at step 40: 284.0638, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 50: 282.7466, Accuracy: 0.7518\n",
      "Training loss (for one batch) at step 60: 322.6566, Accuracy: 0.7577\n",
      "Training loss (for one batch) at step 70: 313.1588, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 80: 324.9537, Accuracy: 0.7407\n",
      "Training loss (for one batch) at step 90: 314.7715, Accuracy: 0.7363\n",
      "Training loss (for one batch) at step 100: 305.0910, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 110: 297.7728, Accuracy: 0.7358\n",
      "---- Training ----\n",
      "Training loss: 116.6019\n",
      "Training acc over epoch: 0.7343\n",
      "---- Validation ----\n",
      "Validation loss: 44.6701\n",
      "Validation acc: 0.6668\n",
      "Time taken: 10.76s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 338.8142, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 331.6721, Accuracy: 0.7081\n",
      "Training loss (for one batch) at step 20: 301.2867, Accuracy: 0.7254\n",
      "Training loss (for one batch) at step 30: 297.5517, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 40: 283.6509, Accuracy: 0.7511\n",
      "Training loss (for one batch) at step 50: 288.9973, Accuracy: 0.7609\n",
      "Training loss (for one batch) at step 60: 324.7244, Accuracy: 0.7654\n",
      "Training loss (for one batch) at step 70: 324.4714, Accuracy: 0.7541\n",
      "Training loss (for one batch) at step 80: 316.2896, Accuracy: 0.7430\n",
      "Training loss (for one batch) at step 90: 295.9093, Accuracy: 0.7406\n",
      "Training loss (for one batch) at step 100: 297.3796, Accuracy: 0.7400\n",
      "Training loss (for one batch) at step 110: 312.3947, Accuracy: 0.7383\n",
      "---- Training ----\n",
      "Training loss: 99.7569\n",
      "Training acc over epoch: 0.7370\n",
      "---- Validation ----\n",
      "Validation loss: 51.5427\n",
      "Validation acc: 0.6628\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 337.6871, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 315.3603, Accuracy: 0.6932\n",
      "Training loss (for one batch) at step 20: 312.6146, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 30: 308.5183, Accuracy: 0.7371\n",
      "Training loss (for one batch) at step 40: 294.3138, Accuracy: 0.7464\n",
      "Training loss (for one batch) at step 50: 285.0555, Accuracy: 0.7540\n",
      "Training loss (for one batch) at step 60: 309.3494, Accuracy: 0.7595\n",
      "Training loss (for one batch) at step 70: 329.6179, Accuracy: 0.7493\n",
      "Training loss (for one batch) at step 80: 350.9329, Accuracy: 0.7420\n",
      "Training loss (for one batch) at step 90: 319.0016, Accuracy: 0.7384\n",
      "Training loss (for one batch) at step 100: 300.4473, Accuracy: 0.7394\n",
      "Training loss (for one batch) at step 110: 308.8660, Accuracy: 0.7396\n",
      "---- Training ----\n",
      "Training loss: 114.5825\n",
      "Training acc over epoch: 0.7380\n",
      "---- Validation ----\n",
      "Validation loss: 40.2173\n",
      "Validation acc: 0.6550\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 325.3277, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 312.2297, Accuracy: 0.6925\n",
      "Training loss (for one batch) at step 20: 291.8762, Accuracy: 0.7206\n",
      "Training loss (for one batch) at step 30: 307.8732, Accuracy: 0.7349\n",
      "Training loss (for one batch) at step 40: 305.3320, Accuracy: 0.7462\n",
      "Training loss (for one batch) at step 50: 294.9756, Accuracy: 0.7551\n",
      "Training loss (for one batch) at step 60: 309.8354, Accuracy: 0.7597\n",
      "Training loss (for one batch) at step 70: 319.2713, Accuracy: 0.7510\n",
      "Training loss (for one batch) at step 80: 314.8688, Accuracy: 0.7407\n",
      "Training loss (for one batch) at step 90: 299.5141, Accuracy: 0.7373\n",
      "Training loss (for one batch) at step 100: 296.0994, Accuracy: 0.7375\n",
      "Training loss (for one batch) at step 110: 316.7908, Accuracy: 0.7357\n",
      "---- Training ----\n",
      "Training loss: 102.8193\n",
      "Training acc over epoch: 0.7349\n",
      "---- Validation ----\n",
      "Validation loss: 63.3029\n",
      "Validation acc: 0.6647\n",
      "Time taken: 10.60s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 332.4687, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 326.7418, Accuracy: 0.6925\n",
      "Training loss (for one batch) at step 20: 299.4782, Accuracy: 0.7217\n",
      "Training loss (for one batch) at step 30: 294.1162, Accuracy: 0.7366\n",
      "Training loss (for one batch) at step 40: 291.5683, Accuracy: 0.7492\n",
      "Training loss (for one batch) at step 50: 301.0575, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 60: 300.3723, Accuracy: 0.7583\n",
      "Training loss (for one batch) at step 70: 331.9550, Accuracy: 0.7499\n",
      "Training loss (for one batch) at step 80: 329.1308, Accuracy: 0.7410\n",
      "Training loss (for one batch) at step 90: 314.7233, Accuracy: 0.7385\n",
      "Training loss (for one batch) at step 100: 318.6697, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 110: 303.5386, Accuracy: 0.7366\n",
      "---- Training ----\n",
      "Training loss: 98.6171\n",
      "Training acc over epoch: 0.7357\n",
      "---- Validation ----\n",
      "Validation loss: 58.5193\n",
      "Validation acc: 0.6666\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 323.0600, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 333.6079, Accuracy: 0.6982\n",
      "Training loss (for one batch) at step 20: 320.0125, Accuracy: 0.7154\n",
      "Training loss (for one batch) at step 30: 309.3833, Accuracy: 0.7303\n",
      "Training loss (for one batch) at step 40: 292.8000, Accuracy: 0.7414\n",
      "Training loss (for one batch) at step 50: 290.2784, Accuracy: 0.7508\n",
      "Training loss (for one batch) at step 60: 288.9717, Accuracy: 0.7535\n",
      "Training loss (for one batch) at step 70: 318.2995, Accuracy: 0.7479\n",
      "Training loss (for one batch) at step 80: 337.1458, Accuracy: 0.7394\n",
      "Training loss (for one batch) at step 90: 302.0064, Accuracy: 0.7358\n",
      "Training loss (for one batch) at step 100: 311.3286, Accuracy: 0.7349\n",
      "Training loss (for one batch) at step 110: 306.7319, Accuracy: 0.7344\n",
      "---- Training ----\n",
      "Training loss: 98.2990\n",
      "Training acc over epoch: 0.7339\n",
      "---- Validation ----\n",
      "Validation loss: 33.9045\n",
      "Validation acc: 0.6797\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 318.8121, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 316.2087, Accuracy: 0.6918\n",
      "Training loss (for one batch) at step 20: 310.2626, Accuracy: 0.7236\n",
      "Training loss (for one batch) at step 30: 287.0011, Accuracy: 0.7331\n",
      "Training loss (for one batch) at step 40: 289.0860, Accuracy: 0.7471\n",
      "Training loss (for one batch) at step 50: 284.8131, Accuracy: 0.7541\n",
      "Training loss (for one batch) at step 60: 294.4106, Accuracy: 0.7576\n",
      "Training loss (for one batch) at step 70: 327.0497, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 80: 341.6374, Accuracy: 0.7408\n",
      "Training loss (for one batch) at step 90: 311.8254, Accuracy: 0.7376\n",
      "Training loss (for one batch) at step 100: 308.1895, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 110: 317.0697, Accuracy: 0.7378\n",
      "---- Training ----\n",
      "Training loss: 106.1275\n",
      "Training acc over epoch: 0.7358\n",
      "---- Validation ----\n",
      "Validation loss: 46.0446\n",
      "Validation acc: 0.6556\n",
      "Time taken: 10.61s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 338.7335, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 330.9596, Accuracy: 0.7017\n",
      "Training loss (for one batch) at step 20: 306.0764, Accuracy: 0.7210\n",
      "Training loss (for one batch) at step 30: 294.5615, Accuracy: 0.7364\n",
      "Training loss (for one batch) at step 40: 289.6433, Accuracy: 0.7466\n",
      "Training loss (for one batch) at step 50: 291.1055, Accuracy: 0.7544\n",
      "Training loss (for one batch) at step 60: 291.7903, Accuracy: 0.7601\n",
      "Training loss (for one batch) at step 70: 314.8866, Accuracy: 0.7524\n",
      "Training loss (for one batch) at step 80: 314.5159, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 90: 311.8595, Accuracy: 0.7373\n",
      "Training loss (for one batch) at step 100: 308.9754, Accuracy: 0.7385\n",
      "Training loss (for one batch) at step 110: 300.6991, Accuracy: 0.7378\n",
      "---- Training ----\n",
      "Training loss: 97.5860\n",
      "Training acc over epoch: 0.7363\n",
      "---- Validation ----\n",
      "Validation loss: 44.6585\n",
      "Validation acc: 0.6593\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 333.0250, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 324.9790, Accuracy: 0.6960\n",
      "Training loss (for one batch) at step 20: 319.3139, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 30: 314.2251, Accuracy: 0.7389\n",
      "Training loss (for one batch) at step 40: 282.4797, Accuracy: 0.7473\n",
      "Training loss (for one batch) at step 50: 294.0775, Accuracy: 0.7584\n",
      "Training loss (for one batch) at step 60: 306.9295, Accuracy: 0.7622\n",
      "Training loss (for one batch) at step 70: 315.9286, Accuracy: 0.7546\n",
      "Training loss (for one batch) at step 80: 314.1935, Accuracy: 0.7414\n",
      "Training loss (for one batch) at step 90: 309.5633, Accuracy: 0.7374\n",
      "Training loss (for one batch) at step 100: 306.0899, Accuracy: 0.7379\n",
      "Training loss (for one batch) at step 110: 314.2316, Accuracy: 0.7378\n",
      "---- Training ----\n",
      "Training loss: 105.0735\n",
      "Training acc over epoch: 0.7361\n",
      "---- Validation ----\n",
      "Validation loss: 59.6883\n",
      "Validation acc: 0.6604\n",
      "Time taken: 10.60s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 322.7961, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 321.0952, Accuracy: 0.6960\n",
      "Training loss (for one batch) at step 20: 300.4539, Accuracy: 0.7173\n",
      "Training loss (for one batch) at step 30: 298.6853, Accuracy: 0.7316\n",
      "Training loss (for one batch) at step 40: 279.9810, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 50: 274.1738, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 60: 291.2699, Accuracy: 0.7570\n",
      "Training loss (for one batch) at step 70: 333.8845, Accuracy: 0.7490\n",
      "Training loss (for one batch) at step 80: 316.1728, Accuracy: 0.7385\n",
      "Training loss (for one batch) at step 90: 292.3236, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 100: 290.2705, Accuracy: 0.7363\n",
      "Training loss (for one batch) at step 110: 309.2522, Accuracy: 0.7350\n",
      "---- Training ----\n",
      "Training loss: 94.1210\n",
      "Training acc over epoch: 0.7339\n",
      "---- Validation ----\n",
      "Validation loss: 48.8946\n",
      "Validation acc: 0.6580\n",
      "Time taken: 10.80s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 329.1227, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 321.3462, Accuracy: 0.6847\n",
      "Training loss (for one batch) at step 20: 280.9422, Accuracy: 0.7132\n",
      "Training loss (for one batch) at step 30: 280.9098, Accuracy: 0.7371\n",
      "Training loss (for one batch) at step 40: 291.3789, Accuracy: 0.7483\n",
      "Training loss (for one batch) at step 50: 270.0409, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 60: 291.6766, Accuracy: 0.7581\n",
      "Training loss (for one batch) at step 70: 317.6479, Accuracy: 0.7494\n",
      "Training loss (for one batch) at step 80: 325.7410, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 90: 315.7615, Accuracy: 0.7355\n",
      "Training loss (for one batch) at step 100: 315.9991, Accuracy: 0.7363\n",
      "Training loss (for one batch) at step 110: 300.1002, Accuracy: 0.7368\n",
      "---- Training ----\n",
      "Training loss: 105.5700\n",
      "Training acc over epoch: 0.7355\n",
      "---- Validation ----\n",
      "Validation loss: 41.5458\n",
      "Validation acc: 0.6623\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 347.0769, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 318.0218, Accuracy: 0.6960\n",
      "Training loss (for one batch) at step 20: 320.5031, Accuracy: 0.7262\n",
      "Training loss (for one batch) at step 30: 289.7813, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 40: 294.1268, Accuracy: 0.7485\n",
      "Training loss (for one batch) at step 50: 305.4656, Accuracy: 0.7560\n",
      "Training loss (for one batch) at step 60: 295.7636, Accuracy: 0.7590\n",
      "Training loss (for one batch) at step 70: 327.7412, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 80: 294.1601, Accuracy: 0.7425\n",
      "Training loss (for one batch) at step 90: 306.1325, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 100: 327.9979, Accuracy: 0.7379\n",
      "Training loss (for one batch) at step 110: 308.7627, Accuracy: 0.7380\n",
      "---- Training ----\n",
      "Training loss: 86.3469\n",
      "Training acc over epoch: 0.7368\n",
      "---- Validation ----\n",
      "Validation loss: 53.0533\n",
      "Validation acc: 0.6730\n",
      "Time taken: 10.96s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 322.6317, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 355.1917, Accuracy: 0.6889\n",
      "Training loss (for one batch) at step 20: 287.7854, Accuracy: 0.7176\n",
      "Training loss (for one batch) at step 30: 291.6834, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 40: 296.0526, Accuracy: 0.7439\n",
      "Training loss (for one batch) at step 50: 281.1528, Accuracy: 0.7531\n",
      "Training loss (for one batch) at step 60: 303.6919, Accuracy: 0.7563\n",
      "Training loss (for one batch) at step 70: 333.4470, Accuracy: 0.7497\n",
      "Training loss (for one batch) at step 80: 321.8274, Accuracy: 0.7410\n",
      "Training loss (for one batch) at step 90: 304.8638, Accuracy: 0.7383\n",
      "Training loss (for one batch) at step 100: 295.6409, Accuracy: 0.7379\n",
      "Training loss (for one batch) at step 110: 298.5104, Accuracy: 0.7378\n",
      "---- Training ----\n",
      "Training loss: 92.6653\n",
      "Training acc over epoch: 0.7358\n",
      "---- Validation ----\n",
      "Validation loss: 42.3745\n",
      "Validation acc: 0.6754\n",
      "Time taken: 10.56s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 357.8443, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 321.2021, Accuracy: 0.6925\n",
      "Training loss (for one batch) at step 20: 299.6702, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 30: 289.5551, Accuracy: 0.7324\n",
      "Training loss (for one batch) at step 40: 282.6457, Accuracy: 0.7431\n",
      "Training loss (for one batch) at step 50: 268.8687, Accuracy: 0.7523\n",
      "Training loss (for one batch) at step 60: 311.8445, Accuracy: 0.7549\n",
      "Training loss (for one batch) at step 70: 304.1768, Accuracy: 0.7477\n",
      "Training loss (for one batch) at step 80: 325.0007, Accuracy: 0.7389\n",
      "Training loss (for one batch) at step 90: 281.8022, Accuracy: 0.7361\n",
      "Training loss (for one batch) at step 100: 286.4529, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 110: 298.1222, Accuracy: 0.7366\n",
      "---- Training ----\n",
      "Training loss: 120.5545\n",
      "Training acc over epoch: 0.7362\n",
      "---- Validation ----\n",
      "Validation loss: 39.9756\n",
      "Validation acc: 0.6687\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 331.6265, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 326.1050, Accuracy: 0.6861\n",
      "Training loss (for one batch) at step 20: 306.2423, Accuracy: 0.7195\n",
      "Training loss (for one batch) at step 30: 293.2064, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 40: 291.8681, Accuracy: 0.7477\n",
      "Training loss (for one batch) at step 50: 305.4291, Accuracy: 0.7555\n",
      "Training loss (for one batch) at step 60: 301.1718, Accuracy: 0.7613\n",
      "Training loss (for one batch) at step 70: 310.6988, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 80: 327.2114, Accuracy: 0.7431\n",
      "Training loss (for one batch) at step 90: 299.2246, Accuracy: 0.7413\n",
      "Training loss (for one batch) at step 100: 297.1750, Accuracy: 0.7408\n",
      "Training loss (for one batch) at step 110: 305.8613, Accuracy: 0.7409\n",
      "---- Training ----\n",
      "Training loss: 102.7510\n",
      "Training acc over epoch: 0.7393\n",
      "---- Validation ----\n",
      "Validation loss: 55.9760\n",
      "Validation acc: 0.6625\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 315.9153, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 317.0583, Accuracy: 0.7017\n",
      "Training loss (for one batch) at step 20: 295.2107, Accuracy: 0.7236\n",
      "Training loss (for one batch) at step 30: 293.8281, Accuracy: 0.7404\n",
      "Training loss (for one batch) at step 40: 281.5229, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 50: 303.0476, Accuracy: 0.7590\n",
      "Training loss (for one batch) at step 60: 294.0052, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 70: 318.4018, Accuracy: 0.7553\n",
      "Training loss (for one batch) at step 80: 323.0106, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 90: 304.1788, Accuracy: 0.7407\n",
      "Training loss (for one batch) at step 100: 290.2373, Accuracy: 0.7398\n",
      "Training loss (for one batch) at step 110: 290.1115, Accuracy: 0.7393\n",
      "---- Training ----\n",
      "Training loss: 96.1621\n",
      "Training acc over epoch: 0.7375\n",
      "---- Validation ----\n",
      "Validation loss: 48.7510\n",
      "Validation acc: 0.6677\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 326.6230, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 299.1570, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 20: 303.1417, Accuracy: 0.7217\n",
      "Training loss (for one batch) at step 30: 297.6824, Accuracy: 0.7366\n",
      "Training loss (for one batch) at step 40: 286.7017, Accuracy: 0.7426\n",
      "Training loss (for one batch) at step 50: 300.0178, Accuracy: 0.7537\n",
      "Training loss (for one batch) at step 60: 291.0388, Accuracy: 0.7587\n",
      "Training loss (for one batch) at step 70: 321.9160, Accuracy: 0.7501\n",
      "Training loss (for one batch) at step 80: 306.8102, Accuracy: 0.7419\n",
      "Training loss (for one batch) at step 90: 316.4659, Accuracy: 0.7383\n",
      "Training loss (for one batch) at step 100: 312.7950, Accuracy: 0.7374\n",
      "Training loss (for one batch) at step 110: 318.7325, Accuracy: 0.7378\n",
      "---- Training ----\n",
      "Training loss: 110.8573\n",
      "Training acc over epoch: 0.7370\n",
      "---- Validation ----\n",
      "Validation loss: 48.9713\n",
      "Validation acc: 0.6585\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 341.6456, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 313.8028, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 20: 276.7001, Accuracy: 0.7210\n",
      "Training loss (for one batch) at step 30: 300.2019, Accuracy: 0.7361\n",
      "Training loss (for one batch) at step 40: 308.7112, Accuracy: 0.7494\n",
      "Training loss (for one batch) at step 50: 287.9464, Accuracy: 0.7574\n",
      "Training loss (for one batch) at step 60: 280.8172, Accuracy: 0.7591\n",
      "Training loss (for one batch) at step 70: 321.6889, Accuracy: 0.7518\n",
      "Training loss (for one batch) at step 80: 316.7891, Accuracy: 0.7442\n",
      "Training loss (for one batch) at step 90: 296.8702, Accuracy: 0.7400\n",
      "Training loss (for one batch) at step 100: 302.9190, Accuracy: 0.7401\n",
      "Training loss (for one batch) at step 110: 298.9398, Accuracy: 0.7395\n",
      "---- Training ----\n",
      "Training loss: 97.1901\n",
      "Training acc over epoch: 0.7387\n",
      "---- Validation ----\n",
      "Validation loss: 48.9711\n",
      "Validation acc: 0.6499\n",
      "Time taken: 10.59s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 324.4956, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 327.3983, Accuracy: 0.6790\n",
      "Training loss (for one batch) at step 20: 303.4197, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 30: 285.0112, Accuracy: 0.7331\n",
      "Training loss (for one batch) at step 40: 278.1515, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 50: 278.4429, Accuracy: 0.7520\n",
      "Training loss (for one batch) at step 60: 277.8182, Accuracy: 0.7555\n",
      "Training loss (for one batch) at step 70: 323.4849, Accuracy: 0.7486\n",
      "Training loss (for one batch) at step 80: 315.2672, Accuracy: 0.7399\n",
      "Training loss (for one batch) at step 90: 300.2305, Accuracy: 0.7375\n",
      "Training loss (for one batch) at step 100: 307.2948, Accuracy: 0.7375\n",
      "Training loss (for one batch) at step 110: 301.4646, Accuracy: 0.7372\n",
      "---- Training ----\n",
      "Training loss: 99.0110\n",
      "Training acc over epoch: 0.7369\n",
      "---- Validation ----\n",
      "Validation loss: 73.5706\n",
      "Validation acc: 0.6609\n",
      "Time taken: 10.60s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 296.9217, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 304.3208, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 20: 297.4226, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 30: 296.4180, Accuracy: 0.7374\n",
      "Training loss (for one batch) at step 40: 274.4185, Accuracy: 0.7483\n",
      "Training loss (for one batch) at step 50: 280.9225, Accuracy: 0.7546\n",
      "Training loss (for one batch) at step 60: 290.6890, Accuracy: 0.7581\n",
      "Training loss (for one batch) at step 70: 318.5930, Accuracy: 0.7508\n",
      "Training loss (for one batch) at step 80: 304.4591, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 90: 307.7611, Accuracy: 0.7395\n",
      "Training loss (for one batch) at step 100: 281.5565, Accuracy: 0.7396\n",
      "Training loss (for one batch) at step 110: 306.3123, Accuracy: 0.7389\n",
      "---- Training ----\n",
      "Training loss: 99.3497\n",
      "Training acc over epoch: 0.7379\n",
      "---- Validation ----\n",
      "Validation loss: 39.7755\n",
      "Validation acc: 0.6642\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 301.6009, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 318.3237, Accuracy: 0.6989\n",
      "Training loss (for one batch) at step 20: 291.4502, Accuracy: 0.7221\n",
      "Training loss (for one batch) at step 30: 283.0255, Accuracy: 0.7354\n",
      "Training loss (for one batch) at step 40: 287.8368, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 50: 299.3328, Accuracy: 0.7551\n",
      "Training loss (for one batch) at step 60: 283.8504, Accuracy: 0.7569\n",
      "Training loss (for one batch) at step 70: 307.8336, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 80: 326.5859, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 90: 287.7534, Accuracy: 0.7371\n",
      "Training loss (for one batch) at step 100: 303.8354, Accuracy: 0.7375\n",
      "Training loss (for one batch) at step 110: 302.0179, Accuracy: 0.7368\n",
      "---- Training ----\n",
      "Training loss: 98.5045\n",
      "Training acc over epoch: 0.7368\n",
      "---- Validation ----\n",
      "Validation loss: 32.3165\n",
      "Validation acc: 0.6663\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 318.6541, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 329.8898, Accuracy: 0.6918\n",
      "Training loss (for one batch) at step 20: 285.2108, Accuracy: 0.7184\n",
      "Training loss (for one batch) at step 30: 270.9679, Accuracy: 0.7364\n",
      "Training loss (for one batch) at step 40: 279.4956, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 50: 306.8022, Accuracy: 0.7569\n",
      "Training loss (for one batch) at step 60: 297.8890, Accuracy: 0.7609\n",
      "Training loss (for one batch) at step 70: 319.7896, Accuracy: 0.7507\n",
      "Training loss (for one batch) at step 80: 304.9940, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 90: 286.0782, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 100: 294.5887, Accuracy: 0.7379\n",
      "Training loss (for one batch) at step 110: 284.6068, Accuracy: 0.7377\n",
      "---- Training ----\n",
      "Training loss: 97.6040\n",
      "Training acc over epoch: 0.7368\n",
      "---- Validation ----\n",
      "Validation loss: 62.9533\n",
      "Validation acc: 0.6744\n",
      "Time taken: 10.99s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 329.2361, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 331.6345, Accuracy: 0.6974\n",
      "Training loss (for one batch) at step 20: 298.4966, Accuracy: 0.7225\n",
      "Training loss (for one batch) at step 30: 291.7495, Accuracy: 0.7354\n",
      "Training loss (for one batch) at step 40: 281.2667, Accuracy: 0.7466\n",
      "Training loss (for one batch) at step 50: 304.7530, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 60: 302.9709, Accuracy: 0.7555\n",
      "Training loss (for one batch) at step 70: 309.4742, Accuracy: 0.7487\n",
      "Training loss (for one batch) at step 80: 294.6327, Accuracy: 0.7411\n",
      "Training loss (for one batch) at step 90: 283.2737, Accuracy: 0.7380\n",
      "Training loss (for one batch) at step 100: 301.6983, Accuracy: 0.7380\n",
      "Training loss (for one batch) at step 110: 301.1038, Accuracy: 0.7357\n",
      "---- Training ----\n",
      "Training loss: 96.7944\n",
      "Training acc over epoch: 0.7361\n",
      "---- Validation ----\n",
      "Validation loss: 57.5309\n",
      "Validation acc: 0.6585\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 312.9264, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 311.6642, Accuracy: 0.6946\n",
      "Training loss (for one batch) at step 20: 320.2271, Accuracy: 0.7258\n",
      "Training loss (for one batch) at step 30: 307.8295, Accuracy: 0.7417\n",
      "Training loss (for one batch) at step 40: 297.2210, Accuracy: 0.7487\n",
      "Training loss (for one batch) at step 50: 277.7531, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 60: 291.9581, Accuracy: 0.7568\n",
      "Training loss (for one batch) at step 70: 305.2399, Accuracy: 0.7491\n",
      "Training loss (for one batch) at step 80: 300.2452, Accuracy: 0.7398\n",
      "Training loss (for one batch) at step 90: 308.7141, Accuracy: 0.7377\n",
      "Training loss (for one batch) at step 100: 309.2162, Accuracy: 0.7364\n",
      "Training loss (for one batch) at step 110: 286.2843, Accuracy: 0.7371\n",
      "---- Training ----\n",
      "Training loss: 103.1279\n",
      "Training acc over epoch: 0.7365\n",
      "---- Validation ----\n",
      "Validation loss: 55.9147\n",
      "Validation acc: 0.6714\n",
      "Time taken: 10.51s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 326.5643, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 338.6261, Accuracy: 0.6996\n",
      "Training loss (for one batch) at step 20: 278.6865, Accuracy: 0.7217\n",
      "Training loss (for one batch) at step 30: 277.3085, Accuracy: 0.7399\n",
      "Training loss (for one batch) at step 40: 293.7445, Accuracy: 0.7508\n",
      "Training loss (for one batch) at step 50: 275.3612, Accuracy: 0.7589\n",
      "Training loss (for one batch) at step 60: 288.6342, Accuracy: 0.7632\n",
      "Training loss (for one batch) at step 70: 320.1313, Accuracy: 0.7539\n",
      "Training loss (for one batch) at step 80: 299.8083, Accuracy: 0.7453\n",
      "Training loss (for one batch) at step 90: 301.1891, Accuracy: 0.7401\n",
      "Training loss (for one batch) at step 100: 289.5819, Accuracy: 0.7400\n",
      "Training loss (for one batch) at step 110: 292.9950, Accuracy: 0.7397\n",
      "---- Training ----\n",
      "Training loss: 97.8763\n",
      "Training acc over epoch: 0.7391\n",
      "---- Validation ----\n",
      "Validation loss: 67.3199\n",
      "Validation acc: 0.6682\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 342.9317, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 301.5945, Accuracy: 0.6776\n",
      "Training loss (for one batch) at step 20: 289.7872, Accuracy: 0.7173\n",
      "Training loss (for one batch) at step 30: 273.6410, Accuracy: 0.7339\n",
      "Training loss (for one batch) at step 40: 274.8681, Accuracy: 0.7445\n",
      "Training loss (for one batch) at step 50: 283.5690, Accuracy: 0.7498\n",
      "Training loss (for one batch) at step 60: 306.6407, Accuracy: 0.7563\n",
      "Training loss (for one batch) at step 70: 307.0716, Accuracy: 0.7482\n",
      "Training loss (for one batch) at step 80: 321.2989, Accuracy: 0.7376\n",
      "Training loss (for one batch) at step 90: 271.1149, Accuracy: 0.7339\n",
      "Training loss (for one batch) at step 100: 298.5995, Accuracy: 0.7338\n",
      "Training loss (for one batch) at step 110: 309.4848, Accuracy: 0.7340\n",
      "---- Training ----\n",
      "Training loss: 92.1836\n",
      "Training acc over epoch: 0.7345\n",
      "---- Validation ----\n",
      "Validation loss: 48.9222\n",
      "Validation acc: 0.6572\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 319.3543, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 303.3631, Accuracy: 0.6754\n",
      "Training loss (for one batch) at step 20: 291.0813, Accuracy: 0.7076\n",
      "Training loss (for one batch) at step 30: 300.6181, Accuracy: 0.7288\n",
      "Training loss (for one batch) at step 40: 280.7854, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 50: 294.9035, Accuracy: 0.7469\n",
      "Training loss (for one batch) at step 60: 305.1283, Accuracy: 0.7536\n",
      "Training loss (for one batch) at step 70: 287.6589, Accuracy: 0.7470\n",
      "Training loss (for one batch) at step 80: 317.6840, Accuracy: 0.7402\n",
      "Training loss (for one batch) at step 90: 282.8167, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 100: 289.7353, Accuracy: 0.7361\n",
      "Training loss (for one batch) at step 110: 288.8096, Accuracy: 0.7360\n",
      "---- Training ----\n",
      "Training loss: 100.2555\n",
      "Training acc over epoch: 0.7351\n",
      "---- Validation ----\n",
      "Validation loss: 60.3232\n",
      "Validation acc: 0.6722\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 304.2900, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 298.8222, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 20: 273.0911, Accuracy: 0.7106\n",
      "Training loss (for one batch) at step 30: 295.9304, Accuracy: 0.7369\n",
      "Training loss (for one batch) at step 40: 287.5231, Accuracy: 0.7511\n",
      "Training loss (for one batch) at step 50: 273.2001, Accuracy: 0.7563\n",
      "Training loss (for one batch) at step 60: 296.1569, Accuracy: 0.7599\n",
      "Training loss (for one batch) at step 70: 296.2993, Accuracy: 0.7510\n",
      "Training loss (for one batch) at step 80: 300.2541, Accuracy: 0.7432\n",
      "Training loss (for one batch) at step 90: 274.0553, Accuracy: 0.7402\n",
      "Training loss (for one batch) at step 100: 289.8318, Accuracy: 0.7404\n",
      "Training loss (for one batch) at step 110: 291.0398, Accuracy: 0.7389\n",
      "---- Training ----\n",
      "Training loss: 94.1676\n",
      "Training acc over epoch: 0.7388\n",
      "---- Validation ----\n",
      "Validation loss: 53.6097\n",
      "Validation acc: 0.6604\n",
      "Time taken: 10.58s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 306.2568, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 314.6125, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 20: 299.8667, Accuracy: 0.7158\n",
      "Training loss (for one batch) at step 30: 316.9001, Accuracy: 0.7392\n",
      "Training loss (for one batch) at step 40: 282.7421, Accuracy: 0.7460\n",
      "Training loss (for one batch) at step 50: 278.2654, Accuracy: 0.7547\n",
      "Training loss (for one batch) at step 60: 304.3462, Accuracy: 0.7613\n",
      "Training loss (for one batch) at step 70: 318.1751, Accuracy: 0.7512\n",
      "Training loss (for one batch) at step 80: 303.0046, Accuracy: 0.7417\n",
      "Training loss (for one batch) at step 90: 292.4593, Accuracy: 0.7388\n",
      "Training loss (for one batch) at step 100: 290.8481, Accuracy: 0.7392\n",
      "Training loss (for one batch) at step 110: 296.2645, Accuracy: 0.7391\n",
      "---- Training ----\n",
      "Training loss: 113.1396\n",
      "Training acc over epoch: 0.7377\n",
      "---- Validation ----\n",
      "Validation loss: 58.4559\n",
      "Validation acc: 0.6658\n",
      "Time taken: 10.35s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.6651\n",
      "Validation AUC: 0.6689\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 670.9489, Accuracy: 0.4688\n",
      "Training loss (for one batch) at step 10: 580.4990, Accuracy: 0.4893\n",
      "Training loss (for one batch) at step 20: 552.9230, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 30: 513.3492, Accuracy: 0.5055\n",
      "Training loss (for one batch) at step 40: 507.0436, Accuracy: 0.5057\n",
      "Training loss (for one batch) at step 50: 502.5324, Accuracy: 0.5069\n",
      "Training loss (for one batch) at step 60: 484.1464, Accuracy: 0.5068\n",
      "Training loss (for one batch) at step 70: 484.7224, Accuracy: 0.5097\n",
      "Training loss (for one batch) at step 80: 487.4322, Accuracy: 0.5068\n",
      "Training loss (for one batch) at step 90: 465.6000, Accuracy: 0.5060\n",
      "Training loss (for one batch) at step 100: 476.0539, Accuracy: 0.5059\n",
      "Training loss (for one batch) at step 110: 462.1617, Accuracy: 0.5061\n",
      "---- Training ----\n",
      "Training loss: 149.4908\n",
      "Training acc over epoch: 0.5051\n",
      "---- Validation ----\n",
      "Validation loss: 33.9998\n",
      "Validation acc: 0.5134\n",
      "Time taken: 12.19s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 468.5639, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 458.8385, Accuracy: 0.5291\n",
      "Training loss (for one batch) at step 20: 457.6022, Accuracy: 0.5201\n",
      "Training loss (for one batch) at step 30: 449.9278, Accuracy: 0.5204\n",
      "Training loss (for one batch) at step 40: 454.2090, Accuracy: 0.5166\n",
      "Training loss (for one batch) at step 50: 452.1196, Accuracy: 0.5179\n",
      "Training loss (for one batch) at step 60: 450.8768, Accuracy: 0.5177\n",
      "Training loss (for one batch) at step 70: 451.6125, Accuracy: 0.5231\n",
      "Training loss (for one batch) at step 80: 446.0517, Accuracy: 0.5243\n",
      "Training loss (for one batch) at step 90: 451.5287, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 100: 457.4591, Accuracy: 0.5236\n",
      "Training loss (for one batch) at step 110: 444.8511, Accuracy: 0.5251\n",
      "---- Training ----\n",
      "Training loss: 138.2023\n",
      "Training acc over epoch: 0.5255\n",
      "---- Validation ----\n",
      "Validation loss: 33.7417\n",
      "Validation acc: 0.5137\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 448.2725, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 445.3442, Accuracy: 0.5206\n",
      "Training loss (for one batch) at step 20: 442.3522, Accuracy: 0.5242\n",
      "Training loss (for one batch) at step 30: 446.7310, Accuracy: 0.5252\n",
      "Training loss (for one batch) at step 40: 447.6686, Accuracy: 0.5240\n",
      "Training loss (for one batch) at step 50: 445.5558, Accuracy: 0.5228\n",
      "Training loss (for one batch) at step 60: 443.6678, Accuracy: 0.5273\n",
      "Training loss (for one batch) at step 70: 445.3642, Accuracy: 0.5346\n",
      "Training loss (for one batch) at step 80: 445.2664, Accuracy: 0.5366\n",
      "Training loss (for one batch) at step 90: 445.2754, Accuracy: 0.5393\n",
      "Training loss (for one batch) at step 100: 445.9440, Accuracy: 0.5363\n",
      "Training loss (for one batch) at step 110: 447.9966, Accuracy: 0.5355\n",
      "---- Training ----\n",
      "Training loss: 138.9535\n",
      "Training acc over epoch: 0.5365\n",
      "---- Validation ----\n",
      "Validation loss: 33.8217\n",
      "Validation acc: 0.5266\n",
      "Time taken: 10.63s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 451.5210, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 444.6671, Accuracy: 0.5284\n",
      "Training loss (for one batch) at step 20: 443.1399, Accuracy: 0.5372\n",
      "Training loss (for one batch) at step 30: 441.0490, Accuracy: 0.5433\n",
      "Training loss (for one batch) at step 40: 446.6100, Accuracy: 0.5433\n",
      "Training loss (for one batch) at step 50: 440.2625, Accuracy: 0.5434\n",
      "Training loss (for one batch) at step 60: 441.0002, Accuracy: 0.5443\n",
      "Training loss (for one batch) at step 70: 447.3870, Accuracy: 0.5485\n",
      "Training loss (for one batch) at step 80: 447.4116, Accuracy: 0.5493\n",
      "Training loss (for one batch) at step 90: 441.7444, Accuracy: 0.5466\n",
      "Training loss (for one batch) at step 100: 445.4493, Accuracy: 0.5460\n",
      "Training loss (for one batch) at step 110: 441.8737, Accuracy: 0.5460\n",
      "---- Training ----\n",
      "Training loss: 139.4790\n",
      "Training acc over epoch: 0.5476\n",
      "---- Validation ----\n",
      "Validation loss: 34.1870\n",
      "Validation acc: 0.5707\n",
      "Time taken: 10.72s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 445.5344, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 442.2789, Accuracy: 0.5582\n",
      "Training loss (for one batch) at step 20: 438.6552, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 30: 444.5469, Accuracy: 0.5534\n",
      "Training loss (for one batch) at step 40: 442.3738, Accuracy: 0.5589\n",
      "Training loss (for one batch) at step 50: 444.5169, Accuracy: 0.5593\n",
      "Training loss (for one batch) at step 60: 442.7931, Accuracy: 0.5647\n",
      "Training loss (for one batch) at step 70: 445.3447, Accuracy: 0.5686\n",
      "Training loss (for one batch) at step 80: 445.3040, Accuracy: 0.5706\n",
      "Training loss (for one batch) at step 90: 442.3129, Accuracy: 0.5664\n",
      "Training loss (for one batch) at step 100: 442.4050, Accuracy: 0.5651\n",
      "Training loss (for one batch) at step 110: 442.5356, Accuracy: 0.5678\n",
      "---- Training ----\n",
      "Training loss: 137.9110\n",
      "Training acc over epoch: 0.5696\n",
      "---- Validation ----\n",
      "Validation loss: 33.4970\n",
      "Validation acc: 0.5733\n",
      "Time taken: 10.40s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 442.8683, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 443.5700, Accuracy: 0.5526\n",
      "Training loss (for one batch) at step 20: 442.7441, Accuracy: 0.5487\n",
      "Training loss (for one batch) at step 30: 443.1196, Accuracy: 0.5572\n",
      "Training loss (for one batch) at step 40: 438.5508, Accuracy: 0.5690\n",
      "Training loss (for one batch) at step 50: 441.1934, Accuracy: 0.5722\n",
      "Training loss (for one batch) at step 60: 440.2026, Accuracy: 0.5739\n",
      "Training loss (for one batch) at step 70: 440.8048, Accuracy: 0.5786\n",
      "Training loss (for one batch) at step 80: 442.4620, Accuracy: 0.5778\n",
      "Training loss (for one batch) at step 90: 442.1751, Accuracy: 0.5734\n",
      "Training loss (for one batch) at step 100: 442.1826, Accuracy: 0.5714\n",
      "Training loss (for one batch) at step 110: 443.1342, Accuracy: 0.5727\n",
      "---- Training ----\n",
      "Training loss: 137.8484\n",
      "Training acc over epoch: 0.5742\n",
      "---- Validation ----\n",
      "Validation loss: 33.9841\n",
      "Validation acc: 0.5935\n",
      "Time taken: 10.79s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 443.4319, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 441.9030, Accuracy: 0.5675\n",
      "Training loss (for one batch) at step 20: 439.3341, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 30: 438.1146, Accuracy: 0.5786\n",
      "Training loss (for one batch) at step 40: 438.0366, Accuracy: 0.5783\n",
      "Training loss (for one batch) at step 50: 437.6629, Accuracy: 0.5855\n",
      "Training loss (for one batch) at step 60: 438.7525, Accuracy: 0.5875\n",
      "Training loss (for one batch) at step 70: 441.4675, Accuracy: 0.5924\n",
      "Training loss (for one batch) at step 80: 445.0892, Accuracy: 0.5895\n",
      "Training loss (for one batch) at step 90: 438.6060, Accuracy: 0.5864\n",
      "Training loss (for one batch) at step 100: 432.9836, Accuracy: 0.5849\n",
      "Training loss (for one batch) at step 110: 441.2432, Accuracy: 0.5890\n",
      "---- Training ----\n",
      "Training loss: 138.3319\n",
      "Training acc over epoch: 0.5892\n",
      "---- Validation ----\n",
      "Validation loss: 34.4670\n",
      "Validation acc: 0.5680\n",
      "Time taken: 10.58s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 442.5839, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 437.0585, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 20: 439.1106, Accuracy: 0.5606\n",
      "Training loss (for one batch) at step 30: 440.7347, Accuracy: 0.5665\n",
      "Training loss (for one batch) at step 40: 431.3207, Accuracy: 0.5753\n",
      "Training loss (for one batch) at step 50: 441.9011, Accuracy: 0.5820\n",
      "Training loss (for one batch) at step 60: 432.2852, Accuracy: 0.5858\n",
      "Training loss (for one batch) at step 70: 441.3690, Accuracy: 0.5877\n",
      "Training loss (for one batch) at step 80: 438.4498, Accuracy: 0.5829\n",
      "Training loss (for one batch) at step 90: 435.7898, Accuracy: 0.5769\n",
      "Training loss (for one batch) at step 100: 435.4228, Accuracy: 0.5742\n",
      "Training loss (for one batch) at step 110: 442.2594, Accuracy: 0.5769\n",
      "---- Training ----\n",
      "Training loss: 134.5749\n",
      "Training acc over epoch: 0.5770\n",
      "---- Validation ----\n",
      "Validation loss: 34.8237\n",
      "Validation acc: 0.5758\n",
      "Time taken: 10.68s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 442.8792, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 439.6429, Accuracy: 0.5575\n",
      "Training loss (for one batch) at step 20: 439.0671, Accuracy: 0.5465\n",
      "Training loss (for one batch) at step 30: 438.7108, Accuracy: 0.5570\n",
      "Training loss (for one batch) at step 40: 432.2699, Accuracy: 0.5554\n",
      "Training loss (for one batch) at step 50: 435.3470, Accuracy: 0.5620\n",
      "Training loss (for one batch) at step 60: 431.1859, Accuracy: 0.5748\n",
      "Training loss (for one batch) at step 70: 438.0977, Accuracy: 0.5777\n",
      "Training loss (for one batch) at step 80: 438.5091, Accuracy: 0.5784\n",
      "Training loss (for one batch) at step 90: 436.0919, Accuracy: 0.5718\n",
      "Training loss (for one batch) at step 100: 436.7361, Accuracy: 0.5705\n",
      "Training loss (for one batch) at step 110: 438.2030, Accuracy: 0.5729\n",
      "---- Training ----\n",
      "Training loss: 136.8620\n",
      "Training acc over epoch: 0.5725\n",
      "---- Validation ----\n",
      "Validation loss: 34.8519\n",
      "Validation acc: 0.5505\n",
      "Time taken: 10.65s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 438.2591, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 431.2767, Accuracy: 0.5668\n",
      "Training loss (for one batch) at step 20: 436.2355, Accuracy: 0.5647\n",
      "Training loss (for one batch) at step 30: 433.8443, Accuracy: 0.5685\n",
      "Training loss (for one batch) at step 40: 437.5625, Accuracy: 0.5646\n",
      "Training loss (for one batch) at step 50: 424.4927, Accuracy: 0.5718\n",
      "Training loss (for one batch) at step 60: 440.2147, Accuracy: 0.5743\n",
      "Training loss (for one batch) at step 70: 442.6040, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 80: 430.7751, Accuracy: 0.5747\n",
      "Training loss (for one batch) at step 90: 433.9013, Accuracy: 0.5693\n",
      "Training loss (for one batch) at step 100: 428.2076, Accuracy: 0.5680\n",
      "Training loss (for one batch) at step 110: 432.4402, Accuracy: 0.5693\n",
      "---- Training ----\n",
      "Training loss: 137.7534\n",
      "Training acc over epoch: 0.5689\n",
      "---- Validation ----\n",
      "Validation loss: 34.4348\n",
      "Validation acc: 0.5314\n",
      "Time taken: 10.75s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 434.2552, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 435.5455, Accuracy: 0.5362\n",
      "Training loss (for one batch) at step 20: 436.9108, Accuracy: 0.5502\n",
      "Training loss (for one batch) at step 30: 429.3247, Accuracy: 0.5439\n",
      "Training loss (for one batch) at step 40: 432.4482, Accuracy: 0.5471\n",
      "Training loss (for one batch) at step 50: 427.7662, Accuracy: 0.5499\n",
      "Training loss (for one batch) at step 60: 427.1533, Accuracy: 0.5590\n",
      "Training loss (for one batch) at step 70: 441.2358, Accuracy: 0.5613\n",
      "Training loss (for one batch) at step 80: 435.5352, Accuracy: 0.5585\n",
      "Training loss (for one batch) at step 90: 438.1186, Accuracy: 0.5528\n",
      "Training loss (for one batch) at step 100: 429.0922, Accuracy: 0.5509\n",
      "Training loss (for one batch) at step 110: 436.4316, Accuracy: 0.5516\n",
      "---- Training ----\n",
      "Training loss: 139.0154\n",
      "Training acc over epoch: 0.5502\n",
      "---- Validation ----\n",
      "Validation loss: 33.8757\n",
      "Validation acc: 0.5406\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 438.8446, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 431.3117, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 20: 437.2400, Accuracy: 0.5413\n",
      "Training loss (for one batch) at step 30: 422.0916, Accuracy: 0.5444\n",
      "Training loss (for one batch) at step 40: 426.0848, Accuracy: 0.5509\n",
      "Training loss (for one batch) at step 50: 420.9261, Accuracy: 0.5585\n",
      "Training loss (for one batch) at step 60: 432.4235, Accuracy: 0.5654\n",
      "Training loss (for one batch) at step 70: 437.5347, Accuracy: 0.5710\n",
      "Training loss (for one batch) at step 80: 434.3375, Accuracy: 0.5666\n",
      "Training loss (for one batch) at step 90: 431.0847, Accuracy: 0.5582\n",
      "Training loss (for one batch) at step 100: 430.8690, Accuracy: 0.5578\n",
      "Training loss (for one batch) at step 110: 429.1623, Accuracy: 0.5579\n",
      "---- Training ----\n",
      "Training loss: 139.6948\n",
      "Training acc over epoch: 0.5578\n",
      "---- Validation ----\n",
      "Validation loss: 34.3874\n",
      "Validation acc: 0.5419\n",
      "Time taken: 10.59s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 434.8979, Accuracy: 0.4766\n",
      "Training loss (for one batch) at step 10: 434.1188, Accuracy: 0.5419\n",
      "Training loss (for one batch) at step 20: 432.8737, Accuracy: 0.5432\n",
      "Training loss (for one batch) at step 30: 423.7294, Accuracy: 0.5439\n",
      "Training loss (for one batch) at step 40: 425.2783, Accuracy: 0.5484\n",
      "Training loss (for one batch) at step 50: 414.9633, Accuracy: 0.5576\n",
      "Training loss (for one batch) at step 60: 433.7661, Accuracy: 0.5622\n",
      "Training loss (for one batch) at step 70: 432.6854, Accuracy: 0.5694\n",
      "Training loss (for one batch) at step 80: 429.9892, Accuracy: 0.5677\n",
      "Training loss (for one batch) at step 90: 434.2065, Accuracy: 0.5603\n",
      "Training loss (for one batch) at step 100: 423.0536, Accuracy: 0.5606\n",
      "Training loss (for one batch) at step 110: 432.6263, Accuracy: 0.5617\n",
      "---- Training ----\n",
      "Training loss: 133.0647\n",
      "Training acc over epoch: 0.5621\n",
      "---- Validation ----\n",
      "Validation loss: 33.6135\n",
      "Validation acc: 0.5478\n",
      "Time taken: 10.78s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 432.5573, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 428.3511, Accuracy: 0.5561\n",
      "Training loss (for one batch) at step 20: 431.7270, Accuracy: 0.5487\n",
      "Training loss (for one batch) at step 30: 422.2797, Accuracy: 0.5504\n",
      "Training loss (for one batch) at step 40: 421.8486, Accuracy: 0.5549\n",
      "Training loss (for one batch) at step 50: 410.6060, Accuracy: 0.5588\n",
      "Training loss (for one batch) at step 60: 426.9023, Accuracy: 0.5675\n",
      "Training loss (for one batch) at step 70: 427.1694, Accuracy: 0.5691\n",
      "Training loss (for one batch) at step 80: 438.8605, Accuracy: 0.5656\n",
      "Training loss (for one batch) at step 90: 430.2462, Accuracy: 0.5602\n",
      "Training loss (for one batch) at step 100: 422.9577, Accuracy: 0.5595\n",
      "Training loss (for one batch) at step 110: 426.3730, Accuracy: 0.5610\n",
      "---- Training ----\n",
      "Training loss: 132.1445\n",
      "Training acc over epoch: 0.5614\n",
      "---- Validation ----\n",
      "Validation loss: 35.6547\n",
      "Validation acc: 0.5301\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 429.5187, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 429.8081, Accuracy: 0.5412\n",
      "Training loss (for one batch) at step 20: 429.7152, Accuracy: 0.5420\n",
      "Training loss (for one batch) at step 30: 420.0659, Accuracy: 0.5449\n",
      "Training loss (for one batch) at step 40: 419.8766, Accuracy: 0.5518\n",
      "Training loss (for one batch) at step 50: 406.3969, Accuracy: 0.5628\n",
      "Training loss (for one batch) at step 60: 423.9687, Accuracy: 0.5707\n",
      "Training loss (for one batch) at step 70: 433.8982, Accuracy: 0.5724\n",
      "Training loss (for one batch) at step 80: 422.6455, Accuracy: 0.5685\n",
      "Training loss (for one batch) at step 90: 428.6319, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 100: 415.8969, Accuracy: 0.5626\n",
      "Training loss (for one batch) at step 110: 421.0153, Accuracy: 0.5639\n",
      "---- Training ----\n",
      "Training loss: 135.9710\n",
      "Training acc over epoch: 0.5643\n",
      "---- Validation ----\n",
      "Validation loss: 34.6239\n",
      "Validation acc: 0.5387\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 439.2476, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 423.1513, Accuracy: 0.5320\n",
      "Training loss (for one batch) at step 20: 440.4901, Accuracy: 0.5294\n",
      "Training loss (for one batch) at step 30: 421.2593, Accuracy: 0.5338\n",
      "Training loss (for one batch) at step 40: 414.8056, Accuracy: 0.5461\n",
      "Training loss (for one batch) at step 50: 408.4696, Accuracy: 0.5565\n",
      "Training loss (for one batch) at step 60: 429.5727, Accuracy: 0.5649\n",
      "Training loss (for one batch) at step 70: 426.9863, Accuracy: 0.5681\n",
      "Training loss (for one batch) at step 80: 420.7580, Accuracy: 0.5645\n",
      "Training loss (for one batch) at step 90: 421.2037, Accuracy: 0.5632\n",
      "Training loss (for one batch) at step 100: 413.9083, Accuracy: 0.5627\n",
      "Training loss (for one batch) at step 110: 420.1797, Accuracy: 0.5628\n",
      "---- Training ----\n",
      "Training loss: 128.9346\n",
      "Training acc over epoch: 0.5626\n",
      "---- Validation ----\n",
      "Validation loss: 36.7004\n",
      "Validation acc: 0.5226\n",
      "Time taken: 10.58s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 430.5876, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 414.6429, Accuracy: 0.5419\n",
      "Training loss (for one batch) at step 20: 423.8791, Accuracy: 0.5428\n",
      "Training loss (for one batch) at step 30: 417.9879, Accuracy: 0.5491\n",
      "Training loss (for one batch) at step 40: 410.3724, Accuracy: 0.5589\n",
      "Training loss (for one batch) at step 50: 402.6217, Accuracy: 0.5671\n",
      "Training loss (for one batch) at step 60: 402.3759, Accuracy: 0.5751\n",
      "Training loss (for one batch) at step 70: 427.7032, Accuracy: 0.5768\n",
      "Training loss (for one batch) at step 80: 417.7399, Accuracy: 0.5715\n",
      "Training loss (for one batch) at step 90: 421.7715, Accuracy: 0.5670\n",
      "Training loss (for one batch) at step 100: 414.4766, Accuracy: 0.5671\n",
      "Training loss (for one batch) at step 110: 425.2661, Accuracy: 0.5683\n",
      "---- Training ----\n",
      "Training loss: 131.4719\n",
      "Training acc over epoch: 0.5687\n",
      "---- Validation ----\n",
      "Validation loss: 33.3762\n",
      "Validation acc: 0.5449\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 439.6956, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 419.9065, Accuracy: 0.5369\n",
      "Training loss (for one batch) at step 20: 419.6549, Accuracy: 0.5324\n",
      "Training loss (for one batch) at step 30: 420.9448, Accuracy: 0.5454\n",
      "Training loss (for one batch) at step 40: 399.2290, Accuracy: 0.5518\n",
      "Training loss (for one batch) at step 50: 403.1451, Accuracy: 0.5622\n",
      "Training loss (for one batch) at step 60: 410.7864, Accuracy: 0.5692\n",
      "Training loss (for one batch) at step 70: 429.8877, Accuracy: 0.5726\n",
      "Training loss (for one batch) at step 80: 423.9040, Accuracy: 0.5669\n",
      "Training loss (for one batch) at step 90: 417.1653, Accuracy: 0.5645\n",
      "Training loss (for one batch) at step 100: 405.2624, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 110: 424.5344, Accuracy: 0.5637\n",
      "---- Training ----\n",
      "Training loss: 125.0425\n",
      "Training acc over epoch: 0.5633\n",
      "---- Validation ----\n",
      "Validation loss: 32.7475\n",
      "Validation acc: 0.5535\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 428.4787, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 424.5485, Accuracy: 0.5589\n",
      "Training loss (for one batch) at step 20: 425.7580, Accuracy: 0.5487\n",
      "Training loss (for one batch) at step 30: 411.6724, Accuracy: 0.5600\n",
      "Training loss (for one batch) at step 40: 406.7309, Accuracy: 0.5686\n",
      "Training loss (for one batch) at step 50: 394.9124, Accuracy: 0.5774\n",
      "Training loss (for one batch) at step 60: 412.2848, Accuracy: 0.5845\n",
      "Training loss (for one batch) at step 70: 418.3160, Accuracy: 0.5844\n",
      "Training loss (for one batch) at step 80: 421.5324, Accuracy: 0.5782\n",
      "Training loss (for one batch) at step 90: 413.5620, Accuracy: 0.5725\n",
      "Training loss (for one batch) at step 100: 409.7325, Accuracy: 0.5743\n",
      "Training loss (for one batch) at step 110: 420.4838, Accuracy: 0.5740\n",
      "---- Training ----\n",
      "Training loss: 128.5539\n",
      "Training acc over epoch: 0.5736\n",
      "---- Validation ----\n",
      "Validation loss: 31.4499\n",
      "Validation acc: 0.5462\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 429.6841, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 407.2159, Accuracy: 0.5774\n",
      "Training loss (for one batch) at step 20: 413.5674, Accuracy: 0.5666\n",
      "Training loss (for one batch) at step 30: 404.8892, Accuracy: 0.5701\n",
      "Training loss (for one batch) at step 40: 402.0329, Accuracy: 0.5795\n",
      "Training loss (for one batch) at step 50: 396.8535, Accuracy: 0.5818\n",
      "Training loss (for one batch) at step 60: 416.7747, Accuracy: 0.5890\n",
      "Training loss (for one batch) at step 70: 417.8524, Accuracy: 0.5887\n",
      "Training loss (for one batch) at step 80: 428.9773, Accuracy: 0.5836\n",
      "Training loss (for one batch) at step 90: 413.9090, Accuracy: 0.5810\n",
      "Training loss (for one batch) at step 100: 408.6936, Accuracy: 0.5825\n",
      "Training loss (for one batch) at step 110: 419.0688, Accuracy: 0.5826\n",
      "---- Training ----\n",
      "Training loss: 135.7786\n",
      "Training acc over epoch: 0.5814\n",
      "---- Validation ----\n",
      "Validation loss: 33.8726\n",
      "Validation acc: 0.5564\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 425.2507, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 413.7421, Accuracy: 0.5426\n",
      "Training loss (for one batch) at step 20: 417.0043, Accuracy: 0.5491\n",
      "Training loss (for one batch) at step 30: 405.2442, Accuracy: 0.5610\n",
      "Training loss (for one batch) at step 40: 391.1723, Accuracy: 0.5692\n",
      "Training loss (for one batch) at step 50: 395.1144, Accuracy: 0.5809\n",
      "Training loss (for one batch) at step 60: 398.7151, Accuracy: 0.5871\n",
      "Training loss (for one batch) at step 70: 413.5842, Accuracy: 0.5885\n",
      "Training loss (for one batch) at step 80: 419.6426, Accuracy: 0.5815\n",
      "Training loss (for one batch) at step 90: 413.0872, Accuracy: 0.5780\n",
      "Training loss (for one batch) at step 100: 406.9503, Accuracy: 0.5762\n",
      "Training loss (for one batch) at step 110: 409.0178, Accuracy: 0.5773\n",
      "---- Training ----\n",
      "Training loss: 130.3181\n",
      "Training acc over epoch: 0.5768\n",
      "---- Validation ----\n",
      "Validation loss: 33.8751\n",
      "Validation acc: 0.5779\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 430.3692, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 414.3219, Accuracy: 0.5511\n",
      "Training loss (for one batch) at step 20: 417.7587, Accuracy: 0.5577\n",
      "Training loss (for one batch) at step 30: 401.1448, Accuracy: 0.5663\n",
      "Training loss (for one batch) at step 40: 386.1225, Accuracy: 0.5737\n",
      "Training loss (for one batch) at step 50: 391.0179, Accuracy: 0.5792\n",
      "Training loss (for one batch) at step 60: 395.5822, Accuracy: 0.5880\n",
      "Training loss (for one batch) at step 70: 418.9686, Accuracy: 0.5893\n",
      "Training loss (for one batch) at step 80: 423.4384, Accuracy: 0.5834\n",
      "Training loss (for one batch) at step 90: 412.8979, Accuracy: 0.5810\n",
      "Training loss (for one batch) at step 100: 406.9929, Accuracy: 0.5801\n",
      "Training loss (for one batch) at step 110: 400.5792, Accuracy: 0.5797\n",
      "---- Training ----\n",
      "Training loss: 127.2004\n",
      "Training acc over epoch: 0.5795\n",
      "---- Validation ----\n",
      "Validation loss: 36.0294\n",
      "Validation acc: 0.5562\n",
      "Time taken: 10.68s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 415.0746, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 411.8547, Accuracy: 0.5604\n",
      "Training loss (for one batch) at step 20: 406.5058, Accuracy: 0.5554\n",
      "Training loss (for one batch) at step 30: 409.2719, Accuracy: 0.5696\n",
      "Training loss (for one batch) at step 40: 391.5157, Accuracy: 0.5791\n",
      "Training loss (for one batch) at step 50: 381.9076, Accuracy: 0.5864\n",
      "Training loss (for one batch) at step 60: 391.7136, Accuracy: 0.5907\n",
      "Training loss (for one batch) at step 70: 409.5830, Accuracy: 0.5928\n",
      "Training loss (for one batch) at step 80: 397.6248, Accuracy: 0.5883\n",
      "Training loss (for one batch) at step 90: 408.5573, Accuracy: 0.5834\n",
      "Training loss (for one batch) at step 100: 385.8196, Accuracy: 0.5811\n",
      "Training loss (for one batch) at step 110: 398.0203, Accuracy: 0.5826\n",
      "---- Training ----\n",
      "Training loss: 127.6969\n",
      "Training acc over epoch: 0.5820\n",
      "---- Validation ----\n",
      "Validation loss: 32.8470\n",
      "Validation acc: 0.5572\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 408.0298, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 407.2138, Accuracy: 0.5597\n",
      "Training loss (for one batch) at step 20: 410.4730, Accuracy: 0.5636\n",
      "Training loss (for one batch) at step 30: 390.1534, Accuracy: 0.5774\n",
      "Training loss (for one batch) at step 40: 393.1028, Accuracy: 0.5854\n",
      "Training loss (for one batch) at step 50: 393.5590, Accuracy: 0.5954\n",
      "Training loss (for one batch) at step 60: 384.9740, Accuracy: 0.5972\n",
      "Training loss (for one batch) at step 70: 408.1569, Accuracy: 0.5962\n",
      "Training loss (for one batch) at step 80: 405.7778, Accuracy: 0.5910\n",
      "Training loss (for one batch) at step 90: 392.1740, Accuracy: 0.5878\n",
      "Training loss (for one batch) at step 100: 391.4126, Accuracy: 0.5863\n",
      "Training loss (for one batch) at step 110: 412.9248, Accuracy: 0.5868\n",
      "---- Training ----\n",
      "Training loss: 129.1593\n",
      "Training acc over epoch: 0.5864\n",
      "---- Validation ----\n",
      "Validation loss: 35.4231\n",
      "Validation acc: 0.5486\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 418.5944, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 398.3078, Accuracy: 0.5476\n",
      "Training loss (for one batch) at step 20: 398.6786, Accuracy: 0.5621\n",
      "Training loss (for one batch) at step 30: 389.5493, Accuracy: 0.5769\n",
      "Training loss (for one batch) at step 40: 390.2341, Accuracy: 0.5875\n",
      "Training loss (for one batch) at step 50: 377.9760, Accuracy: 0.5950\n",
      "Training loss (for one batch) at step 60: 382.5286, Accuracy: 0.5993\n",
      "Training loss (for one batch) at step 70: 414.2927, Accuracy: 0.5956\n",
      "Training loss (for one batch) at step 80: 394.0727, Accuracy: 0.5910\n",
      "Training loss (for one batch) at step 90: 389.3737, Accuracy: 0.5876\n",
      "Training loss (for one batch) at step 100: 388.3476, Accuracy: 0.5872\n",
      "Training loss (for one batch) at step 110: 403.7084, Accuracy: 0.5883\n",
      "---- Training ----\n",
      "Training loss: 123.2051\n",
      "Training acc over epoch: 0.5864\n",
      "---- Validation ----\n",
      "Validation loss: 36.4323\n",
      "Validation acc: 0.5602\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 403.7998, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 403.0918, Accuracy: 0.5568\n",
      "Training loss (for one batch) at step 20: 399.8330, Accuracy: 0.5614\n",
      "Training loss (for one batch) at step 30: 382.0333, Accuracy: 0.5827\n",
      "Training loss (for one batch) at step 40: 385.5216, Accuracy: 0.5896\n",
      "Training loss (for one batch) at step 50: 377.0059, Accuracy: 0.5970\n",
      "Training loss (for one batch) at step 60: 377.7360, Accuracy: 0.6037\n",
      "Training loss (for one batch) at step 70: 409.3078, Accuracy: 0.6017\n",
      "Training loss (for one batch) at step 80: 404.3429, Accuracy: 0.5944\n",
      "Training loss (for one batch) at step 90: 395.2245, Accuracy: 0.5904\n",
      "Training loss (for one batch) at step 100: 378.1136, Accuracy: 0.5896\n",
      "Training loss (for one batch) at step 110: 387.8097, Accuracy: 0.5903\n",
      "---- Training ----\n",
      "Training loss: 119.2857\n",
      "Training acc over epoch: 0.5898\n",
      "---- Validation ----\n",
      "Validation loss: 39.3790\n",
      "Validation acc: 0.5685\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 406.2402, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 404.1859, Accuracy: 0.5675\n",
      "Training loss (for one batch) at step 20: 384.7103, Accuracy: 0.5725\n",
      "Training loss (for one batch) at step 30: 374.3863, Accuracy: 0.5814\n",
      "Training loss (for one batch) at step 40: 370.3983, Accuracy: 0.5917\n",
      "Training loss (for one batch) at step 50: 375.4445, Accuracy: 0.5985\n",
      "Training loss (for one batch) at step 60: 379.4081, Accuracy: 0.6034\n",
      "Training loss (for one batch) at step 70: 396.5395, Accuracy: 0.5994\n",
      "Training loss (for one batch) at step 80: 401.1292, Accuracy: 0.5949\n",
      "Training loss (for one batch) at step 90: 389.5585, Accuracy: 0.5907\n",
      "Training loss (for one batch) at step 100: 388.0161, Accuracy: 0.5891\n",
      "Training loss (for one batch) at step 110: 381.6194, Accuracy: 0.5909\n",
      "---- Training ----\n",
      "Training loss: 116.0934\n",
      "Training acc over epoch: 0.5903\n",
      "---- Validation ----\n",
      "Validation loss: 39.4917\n",
      "Validation acc: 0.5822\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 406.9427, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 388.8187, Accuracy: 0.5646\n",
      "Training loss (for one batch) at step 20: 372.4714, Accuracy: 0.5766\n",
      "Training loss (for one batch) at step 30: 366.3500, Accuracy: 0.5925\n",
      "Training loss (for one batch) at step 40: 378.0588, Accuracy: 0.6004\n",
      "Training loss (for one batch) at step 50: 355.1768, Accuracy: 0.6083\n",
      "Training loss (for one batch) at step 60: 382.9034, Accuracy: 0.6105\n",
      "Training loss (for one batch) at step 70: 405.7744, Accuracy: 0.6033\n",
      "Training loss (for one batch) at step 80: 384.6768, Accuracy: 0.5983\n",
      "Training loss (for one batch) at step 90: 384.6620, Accuracy: 0.5958\n",
      "Training loss (for one batch) at step 100: 371.8694, Accuracy: 0.5961\n",
      "Training loss (for one batch) at step 110: 381.9112, Accuracy: 0.5958\n",
      "---- Training ----\n",
      "Training loss: 131.3672\n",
      "Training acc over epoch: 0.5950\n",
      "---- Validation ----\n",
      "Validation loss: 41.3980\n",
      "Validation acc: 0.5723\n",
      "Time taken: 10.51s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 392.6455, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 398.9148, Accuracy: 0.5511\n",
      "Training loss (for one batch) at step 20: 378.7680, Accuracy: 0.5722\n",
      "Training loss (for one batch) at step 30: 376.7385, Accuracy: 0.5915\n",
      "Training loss (for one batch) at step 40: 378.9104, Accuracy: 0.5949\n",
      "Training loss (for one batch) at step 50: 370.0900, Accuracy: 0.6006\n",
      "Training loss (for one batch) at step 60: 371.4526, Accuracy: 0.6068\n",
      "Training loss (for one batch) at step 70: 401.9263, Accuracy: 0.6038\n",
      "Training loss (for one batch) at step 80: 389.9299, Accuracy: 0.5964\n",
      "Training loss (for one batch) at step 90: 383.8733, Accuracy: 0.5945\n",
      "Training loss (for one batch) at step 100: 374.8799, Accuracy: 0.5943\n",
      "Training loss (for one batch) at step 110: 378.2329, Accuracy: 0.5934\n",
      "---- Training ----\n",
      "Training loss: 121.3853\n",
      "Training acc over epoch: 0.5936\n",
      "---- Validation ----\n",
      "Validation loss: 40.0730\n",
      "Validation acc: 0.5680\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 398.1706, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 391.9338, Accuracy: 0.5682\n",
      "Training loss (for one batch) at step 20: 386.8538, Accuracy: 0.5766\n",
      "Training loss (for one batch) at step 30: 364.4200, Accuracy: 0.5862\n",
      "Training loss (for one batch) at step 40: 360.5225, Accuracy: 0.5886\n",
      "Training loss (for one batch) at step 50: 368.8625, Accuracy: 0.5977\n",
      "Training loss (for one batch) at step 60: 370.8724, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 70: 382.4626, Accuracy: 0.5967\n",
      "Training loss (for one batch) at step 80: 384.2963, Accuracy: 0.5924\n",
      "Training loss (for one batch) at step 90: 371.4974, Accuracy: 0.5907\n",
      "Training loss (for one batch) at step 100: 379.2967, Accuracy: 0.5910\n",
      "Training loss (for one batch) at step 110: 376.2486, Accuracy: 0.5913\n",
      "---- Training ----\n",
      "Training loss: 115.4502\n",
      "Training acc over epoch: 0.5906\n",
      "---- Validation ----\n",
      "Validation loss: 41.0254\n",
      "Validation acc: 0.5591\n",
      "Time taken: 10.51s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 402.5089, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 392.5919, Accuracy: 0.5440\n",
      "Training loss (for one batch) at step 20: 371.9033, Accuracy: 0.5807\n",
      "Training loss (for one batch) at step 30: 367.6883, Accuracy: 0.5900\n",
      "Training loss (for one batch) at step 40: 363.6593, Accuracy: 0.5960\n",
      "Training loss (for one batch) at step 50: 357.1540, Accuracy: 0.6003\n",
      "Training loss (for one batch) at step 60: 363.8199, Accuracy: 0.6057\n",
      "Training loss (for one batch) at step 70: 383.6746, Accuracy: 0.6004\n",
      "Training loss (for one batch) at step 80: 396.7397, Accuracy: 0.5963\n",
      "Training loss (for one batch) at step 90: 365.4888, Accuracy: 0.5907\n",
      "Training loss (for one batch) at step 100: 363.3685, Accuracy: 0.5919\n",
      "Training loss (for one batch) at step 110: 375.8264, Accuracy: 0.5930\n",
      "---- Training ----\n",
      "Training loss: 113.6355\n",
      "Training acc over epoch: 0.5930\n",
      "---- Validation ----\n",
      "Validation loss: 35.7560\n",
      "Validation acc: 0.5653\n",
      "Time taken: 10.67s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 388.1266, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 373.6543, Accuracy: 0.5561\n",
      "Training loss (for one batch) at step 20: 369.1212, Accuracy: 0.5770\n",
      "Training loss (for one batch) at step 30: 368.2087, Accuracy: 0.5912\n",
      "Training loss (for one batch) at step 40: 371.8826, Accuracy: 0.6018\n",
      "Training loss (for one batch) at step 50: 364.2756, Accuracy: 0.6065\n",
      "Training loss (for one batch) at step 60: 375.3280, Accuracy: 0.6098\n",
      "Training loss (for one batch) at step 70: 389.6809, Accuracy: 0.6048\n",
      "Training loss (for one batch) at step 80: 378.1948, Accuracy: 0.5989\n",
      "Training loss (for one batch) at step 90: 364.5430, Accuracy: 0.5959\n",
      "Training loss (for one batch) at step 100: 368.9985, Accuracy: 0.5972\n",
      "Training loss (for one batch) at step 110: 382.9869, Accuracy: 0.5950\n",
      "---- Training ----\n",
      "Training loss: 116.8578\n",
      "Training acc over epoch: 0.5950\n",
      "---- Validation ----\n",
      "Validation loss: 44.1182\n",
      "Validation acc: 0.5586\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 387.4812, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 378.6022, Accuracy: 0.5476\n",
      "Training loss (for one batch) at step 20: 383.4948, Accuracy: 0.5725\n",
      "Training loss (for one batch) at step 30: 363.2278, Accuracy: 0.5955\n",
      "Training loss (for one batch) at step 40: 363.9787, Accuracy: 0.6033\n",
      "Training loss (for one batch) at step 50: 361.3142, Accuracy: 0.6078\n",
      "Training loss (for one batch) at step 60: 354.8018, Accuracy: 0.6148\n",
      "Training loss (for one batch) at step 70: 391.7342, Accuracy: 0.6088\n",
      "Training loss (for one batch) at step 80: 379.7563, Accuracy: 0.6013\n",
      "Training loss (for one batch) at step 90: 367.3393, Accuracy: 0.5986\n",
      "Training loss (for one batch) at step 100: 379.2683, Accuracy: 0.5981\n",
      "Training loss (for one batch) at step 110: 359.7987, Accuracy: 0.5973\n",
      "---- Training ----\n",
      "Training loss: 105.5862\n",
      "Training acc over epoch: 0.5963\n",
      "---- Validation ----\n",
      "Validation loss: 43.8148\n",
      "Validation acc: 0.5486\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 394.2432, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 369.4099, Accuracy: 0.5504\n",
      "Training loss (for one batch) at step 20: 356.8954, Accuracy: 0.5792\n",
      "Training loss (for one batch) at step 30: 376.2461, Accuracy: 0.5930\n",
      "Training loss (for one batch) at step 40: 345.9346, Accuracy: 0.6010\n",
      "Training loss (for one batch) at step 50: 341.0918, Accuracy: 0.6072\n",
      "Training loss (for one batch) at step 60: 354.0579, Accuracy: 0.6098\n",
      "Training loss (for one batch) at step 70: 391.4186, Accuracy: 0.6065\n",
      "Training loss (for one batch) at step 80: 371.9612, Accuracy: 0.5995\n",
      "Training loss (for one batch) at step 90: 361.4226, Accuracy: 0.5985\n",
      "Training loss (for one batch) at step 100: 347.8978, Accuracy: 0.5982\n",
      "Training loss (for one batch) at step 110: 368.0853, Accuracy: 0.5968\n",
      "---- Training ----\n",
      "Training loss: 123.1857\n",
      "Training acc over epoch: 0.5985\n",
      "---- Validation ----\n",
      "Validation loss: 39.3059\n",
      "Validation acc: 0.5736\n",
      "Time taken: 10.89s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 383.7784, Accuracy: 0.4844\n",
      "Training loss (for one batch) at step 10: 366.0672, Accuracy: 0.5476\n",
      "Training loss (for one batch) at step 20: 387.7776, Accuracy: 0.5699\n",
      "Training loss (for one batch) at step 30: 355.8571, Accuracy: 0.5844\n",
      "Training loss (for one batch) at step 40: 365.2846, Accuracy: 0.5932\n",
      "Training loss (for one batch) at step 50: 354.2774, Accuracy: 0.6036\n",
      "Training loss (for one batch) at step 60: 367.8437, Accuracy: 0.6067\n",
      "Training loss (for one batch) at step 70: 374.2688, Accuracy: 0.6032\n",
      "Training loss (for one batch) at step 80: 371.0926, Accuracy: 0.5957\n",
      "Training loss (for one batch) at step 90: 348.6584, Accuracy: 0.5939\n",
      "Training loss (for one batch) at step 100: 343.6604, Accuracy: 0.5935\n",
      "Training loss (for one batch) at step 110: 363.0812, Accuracy: 0.5931\n",
      "---- Training ----\n",
      "Training loss: 107.2979\n",
      "Training acc over epoch: 0.5930\n",
      "---- Validation ----\n",
      "Validation loss: 37.4945\n",
      "Validation acc: 0.5508\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 374.9093, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 359.3451, Accuracy: 0.5661\n",
      "Training loss (for one batch) at step 20: 361.3425, Accuracy: 0.5915\n",
      "Training loss (for one batch) at step 30: 351.4710, Accuracy: 0.6038\n",
      "Training loss (for one batch) at step 40: 346.8520, Accuracy: 0.6067\n",
      "Training loss (for one batch) at step 50: 341.8422, Accuracy: 0.6134\n",
      "Training loss (for one batch) at step 60: 361.4214, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 70: 365.0145, Accuracy: 0.6084\n",
      "Training loss (for one batch) at step 80: 384.9900, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 90: 362.1335, Accuracy: 0.5980\n",
      "Training loss (for one batch) at step 100: 372.5565, Accuracy: 0.5975\n",
      "Training loss (for one batch) at step 110: 360.5806, Accuracy: 0.5987\n",
      "---- Training ----\n",
      "Training loss: 111.9478\n",
      "Training acc over epoch: 0.5976\n",
      "---- Validation ----\n",
      "Validation loss: 44.5095\n",
      "Validation acc: 0.5830\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 373.3558, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 369.5941, Accuracy: 0.5483\n",
      "Training loss (for one batch) at step 20: 357.1169, Accuracy: 0.5800\n",
      "Training loss (for one batch) at step 30: 347.5889, Accuracy: 0.6001\n",
      "Training loss (for one batch) at step 40: 328.7486, Accuracy: 0.6080\n",
      "Training loss (for one batch) at step 50: 355.3815, Accuracy: 0.6118\n",
      "Training loss (for one batch) at step 60: 358.0849, Accuracy: 0.6113\n",
      "Training loss (for one batch) at step 70: 376.3263, Accuracy: 0.6050\n",
      "Training loss (for one batch) at step 80: 380.3296, Accuracy: 0.5992\n",
      "Training loss (for one batch) at step 90: 352.2727, Accuracy: 0.5975\n",
      "Training loss (for one batch) at step 100: 360.3717, Accuracy: 0.5982\n",
      "Training loss (for one batch) at step 110: 364.7439, Accuracy: 0.5985\n",
      "---- Training ----\n",
      "Training loss: 117.1413\n",
      "Training acc over epoch: 0.5979\n",
      "---- Validation ----\n",
      "Validation loss: 38.2199\n",
      "Validation acc: 0.5631\n",
      "Time taken: 10.59s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 364.3639, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 363.5417, Accuracy: 0.5575\n",
      "Training loss (for one batch) at step 20: 344.1545, Accuracy: 0.5751\n",
      "Training loss (for one batch) at step 30: 344.8730, Accuracy: 0.5945\n",
      "Training loss (for one batch) at step 40: 346.1170, Accuracy: 0.6018\n",
      "Training loss (for one batch) at step 50: 343.8725, Accuracy: 0.6068\n",
      "Training loss (for one batch) at step 60: 344.3412, Accuracy: 0.6099\n",
      "Training loss (for one batch) at step 70: 371.6944, Accuracy: 0.6052\n",
      "Training loss (for one batch) at step 80: 385.0946, Accuracy: 0.5978\n",
      "Training loss (for one batch) at step 90: 359.3445, Accuracy: 0.5946\n",
      "Training loss (for one batch) at step 100: 357.3269, Accuracy: 0.5943\n",
      "Training loss (for one batch) at step 110: 359.3885, Accuracy: 0.5937\n",
      "---- Training ----\n",
      "Training loss: 111.2967\n",
      "Training acc over epoch: 0.5948\n",
      "---- Validation ----\n",
      "Validation loss: 31.5162\n",
      "Validation acc: 0.5591\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 381.1420, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 365.7733, Accuracy: 0.5504\n",
      "Training loss (for one batch) at step 20: 349.2031, Accuracy: 0.5751\n",
      "Training loss (for one batch) at step 30: 347.3311, Accuracy: 0.5978\n",
      "Training loss (for one batch) at step 40: 336.1697, Accuracy: 0.5998\n",
      "Training loss (for one batch) at step 50: 325.9521, Accuracy: 0.6074\n",
      "Training loss (for one batch) at step 60: 344.3649, Accuracy: 0.6110\n",
      "Training loss (for one batch) at step 70: 383.8580, Accuracy: 0.6078\n",
      "Training loss (for one batch) at step 80: 391.4691, Accuracy: 0.6013\n",
      "Training loss (for one batch) at step 90: 346.8776, Accuracy: 0.5984\n",
      "Training loss (for one batch) at step 100: 339.0829, Accuracy: 0.5965\n",
      "Training loss (for one batch) at step 110: 351.4227, Accuracy: 0.5984\n",
      "---- Training ----\n",
      "Training loss: 109.1545\n",
      "Training acc over epoch: 0.5984\n",
      "---- Validation ----\n",
      "Validation loss: 41.0660\n",
      "Validation acc: 0.5648\n",
      "Time taken: 12.43s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 361.7351, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 359.7504, Accuracy: 0.5710\n",
      "Training loss (for one batch) at step 20: 350.1433, Accuracy: 0.5818\n",
      "Training loss (for one batch) at step 30: 340.3645, Accuracy: 0.5965\n",
      "Training loss (for one batch) at step 40: 342.8498, Accuracy: 0.6025\n",
      "Training loss (for one batch) at step 50: 334.5577, Accuracy: 0.6059\n",
      "Training loss (for one batch) at step 60: 344.4927, Accuracy: 0.6119\n",
      "Training loss (for one batch) at step 70: 379.9659, Accuracy: 0.6061\n",
      "Training loss (for one batch) at step 80: 371.4525, Accuracy: 0.5987\n",
      "Training loss (for one batch) at step 90: 346.7459, Accuracy: 0.5963\n",
      "Training loss (for one batch) at step 100: 345.6175, Accuracy: 0.5970\n",
      "Training loss (for one batch) at step 110: 357.0063, Accuracy: 0.5984\n",
      "---- Training ----\n",
      "Training loss: 119.5505\n",
      "Training acc over epoch: 0.5979\n",
      "---- Validation ----\n",
      "Validation loss: 36.7401\n",
      "Validation acc: 0.5782\n",
      "Time taken: 10.76s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 377.7098, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 371.8626, Accuracy: 0.5518\n",
      "Training loss (for one batch) at step 20: 350.3213, Accuracy: 0.5737\n",
      "Training loss (for one batch) at step 30: 341.2855, Accuracy: 0.6001\n",
      "Training loss (for one batch) at step 40: 335.5224, Accuracy: 0.6050\n",
      "Training loss (for one batch) at step 50: 345.5249, Accuracy: 0.6104\n",
      "Training loss (for one batch) at step 60: 356.9277, Accuracy: 0.6127\n",
      "Training loss (for one batch) at step 70: 381.4044, Accuracy: 0.6045\n",
      "Training loss (for one batch) at step 80: 362.4391, Accuracy: 0.5978\n",
      "Training loss (for one batch) at step 90: 336.5962, Accuracy: 0.5958\n",
      "Training loss (for one batch) at step 100: 347.6956, Accuracy: 0.5956\n",
      "Training loss (for one batch) at step 110: 355.8588, Accuracy: 0.5964\n",
      "---- Training ----\n",
      "Training loss: 117.2466\n",
      "Training acc over epoch: 0.5958\n",
      "---- Validation ----\n",
      "Validation loss: 36.7127\n",
      "Validation acc: 0.5755\n",
      "Time taken: 10.56s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 354.7434, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 358.1467, Accuracy: 0.5526\n",
      "Training loss (for one batch) at step 20: 349.9095, Accuracy: 0.5763\n",
      "Training loss (for one batch) at step 30: 339.2439, Accuracy: 0.5917\n",
      "Training loss (for one batch) at step 40: 325.1760, Accuracy: 0.5972\n",
      "Training loss (for one batch) at step 50: 335.4005, Accuracy: 0.6043\n",
      "Training loss (for one batch) at step 60: 346.3708, Accuracy: 0.6103\n",
      "Training loss (for one batch) at step 70: 374.1979, Accuracy: 0.6042\n",
      "Training loss (for one batch) at step 80: 360.4981, Accuracy: 0.5971\n",
      "Training loss (for one batch) at step 90: 345.9752, Accuracy: 0.5925\n",
      "Training loss (for one batch) at step 100: 354.6128, Accuracy: 0.5955\n",
      "Training loss (for one batch) at step 110: 339.8746, Accuracy: 0.5938\n",
      "---- Training ----\n",
      "Training loss: 106.9347\n",
      "Training acc over epoch: 0.5950\n",
      "---- Validation ----\n",
      "Validation loss: 43.3762\n",
      "Validation acc: 0.5690\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 357.8864, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 356.1157, Accuracy: 0.5426\n",
      "Training loss (for one batch) at step 20: 361.4603, Accuracy: 0.5766\n",
      "Training loss (for one batch) at step 30: 338.4144, Accuracy: 0.5953\n",
      "Training loss (for one batch) at step 40: 323.4510, Accuracy: 0.6012\n",
      "Training loss (for one batch) at step 50: 332.9881, Accuracy: 0.6100\n",
      "Training loss (for one batch) at step 60: 355.3922, Accuracy: 0.6112\n",
      "Training loss (for one batch) at step 70: 369.0595, Accuracy: 0.6030\n",
      "Training loss (for one batch) at step 80: 351.7791, Accuracy: 0.5967\n",
      "Training loss (for one batch) at step 90: 335.9289, Accuracy: 0.5968\n",
      "Training loss (for one batch) at step 100: 340.3818, Accuracy: 0.5965\n",
      "Training loss (for one batch) at step 110: 344.2814, Accuracy: 0.5982\n",
      "---- Training ----\n",
      "Training loss: 113.1142\n",
      "Training acc over epoch: 0.5976\n",
      "---- Validation ----\n",
      "Validation loss: 41.2379\n",
      "Validation acc: 0.5575\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 357.4879, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 351.6366, Accuracy: 0.5597\n",
      "Training loss (for one batch) at step 20: 346.3018, Accuracy: 0.5763\n",
      "Training loss (for one batch) at step 30: 341.2886, Accuracy: 0.5950\n",
      "Training loss (for one batch) at step 40: 331.9760, Accuracy: 0.6048\n",
      "Training loss (for one batch) at step 50: 337.8416, Accuracy: 0.6104\n",
      "Training loss (for one batch) at step 60: 335.0636, Accuracy: 0.6128\n",
      "Training loss (for one batch) at step 70: 374.5829, Accuracy: 0.6065\n",
      "Training loss (for one batch) at step 80: 350.7499, Accuracy: 0.5992\n",
      "Training loss (for one batch) at step 90: 346.9649, Accuracy: 0.5972\n",
      "Training loss (for one batch) at step 100: 333.7831, Accuracy: 0.5961\n",
      "Training loss (for one batch) at step 110: 350.8304, Accuracy: 0.5942\n",
      "---- Training ----\n",
      "Training loss: 107.7728\n",
      "Training acc over epoch: 0.5938\n",
      "---- Validation ----\n",
      "Validation loss: 39.3955\n",
      "Validation acc: 0.5556\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 361.9020, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 336.2794, Accuracy: 0.5462\n",
      "Training loss (for one batch) at step 20: 321.5991, Accuracy: 0.5711\n",
      "Training loss (for one batch) at step 30: 342.7740, Accuracy: 0.5854\n",
      "Training loss (for one batch) at step 40: 342.9421, Accuracy: 0.5978\n",
      "Training loss (for one batch) at step 50: 308.8964, Accuracy: 0.6057\n",
      "Training loss (for one batch) at step 60: 350.7658, Accuracy: 0.6095\n",
      "Training loss (for one batch) at step 70: 340.0405, Accuracy: 0.6042\n",
      "Training loss (for one batch) at step 80: 352.3638, Accuracy: 0.5984\n",
      "Training loss (for one batch) at step 90: 332.6691, Accuracy: 0.5956\n",
      "Training loss (for one batch) at step 100: 348.5815, Accuracy: 0.5961\n",
      "Training loss (for one batch) at step 110: 335.4379, Accuracy: 0.5959\n",
      "---- Training ----\n",
      "Training loss: 120.1398\n",
      "Training acc over epoch: 0.5952\n",
      "---- Validation ----\n",
      "Validation loss: 45.1960\n",
      "Validation acc: 0.5661\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 345.6695, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 358.9885, Accuracy: 0.5639\n",
      "Training loss (for one batch) at step 20: 334.2563, Accuracy: 0.5748\n",
      "Training loss (for one batch) at step 30: 327.3759, Accuracy: 0.6006\n",
      "Training loss (for one batch) at step 40: 332.7283, Accuracy: 0.6044\n",
      "Training loss (for one batch) at step 50: 323.9316, Accuracy: 0.6129\n",
      "Training loss (for one batch) at step 60: 339.2056, Accuracy: 0.6157\n",
      "Training loss (for one batch) at step 70: 359.0550, Accuracy: 0.6063\n",
      "Training loss (for one batch) at step 80: 365.6866, Accuracy: 0.5986\n",
      "Training loss (for one batch) at step 90: 344.5361, Accuracy: 0.5969\n",
      "Training loss (for one batch) at step 100: 330.7909, Accuracy: 0.5975\n",
      "Training loss (for one batch) at step 110: 353.1465, Accuracy: 0.5980\n",
      "---- Training ----\n",
      "Training loss: 104.2626\n",
      "Training acc over epoch: 0.5964\n",
      "---- Validation ----\n",
      "Validation loss: 37.2406\n",
      "Validation acc: 0.5502\n",
      "Time taken: 10.65s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 374.7599, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 342.8611, Accuracy: 0.5433\n",
      "Training loss (for one batch) at step 20: 328.1862, Accuracy: 0.5711\n",
      "Training loss (for one batch) at step 30: 314.3162, Accuracy: 0.5953\n",
      "Training loss (for one batch) at step 40: 314.6609, Accuracy: 0.6004\n",
      "Training loss (for one batch) at step 50: 315.4398, Accuracy: 0.6075\n",
      "Training loss (for one batch) at step 60: 321.3964, Accuracy: 0.6062\n",
      "Training loss (for one batch) at step 70: 354.1039, Accuracy: 0.6040\n",
      "Training loss (for one batch) at step 80: 364.2376, Accuracy: 0.5981\n",
      "Training loss (for one batch) at step 90: 328.5011, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 100: 321.2818, Accuracy: 0.5947\n",
      "Training loss (for one batch) at step 110: 336.7791, Accuracy: 0.5957\n",
      "---- Training ----\n",
      "Training loss: 116.7596\n",
      "Training acc over epoch: 0.5946\n",
      "---- Validation ----\n",
      "Validation loss: 33.3624\n",
      "Validation acc: 0.5709\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 354.6316, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 366.6497, Accuracy: 0.5597\n",
      "Training loss (for one batch) at step 20: 334.3338, Accuracy: 0.5818\n",
      "Training loss (for one batch) at step 30: 316.6947, Accuracy: 0.5968\n",
      "Training loss (for one batch) at step 40: 332.6111, Accuracy: 0.6025\n",
      "Training loss (for one batch) at step 50: 338.4850, Accuracy: 0.6088\n",
      "Training loss (for one batch) at step 60: 322.9935, Accuracy: 0.6130\n",
      "Training loss (for one batch) at step 70: 338.6773, Accuracy: 0.6063\n",
      "Training loss (for one batch) at step 80: 367.0779, Accuracy: 0.6003\n",
      "Training loss (for one batch) at step 90: 336.7548, Accuracy: 0.5969\n",
      "Training loss (for one batch) at step 100: 321.0345, Accuracy: 0.5973\n",
      "Training loss (for one batch) at step 110: 345.1395, Accuracy: 0.5972\n",
      "---- Training ----\n",
      "Training loss: 110.8395\n",
      "Training acc over epoch: 0.5961\n",
      "---- Validation ----\n",
      "Validation loss: 39.8395\n",
      "Validation acc: 0.5650\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 360.1276, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 363.4745, Accuracy: 0.5575\n",
      "Training loss (for one batch) at step 20: 323.5966, Accuracy: 0.5818\n",
      "Training loss (for one batch) at step 30: 323.5515, Accuracy: 0.5927\n",
      "Training loss (for one batch) at step 40: 323.5392, Accuracy: 0.5947\n",
      "Training loss (for one batch) at step 50: 326.2368, Accuracy: 0.6045\n",
      "Training loss (for one batch) at step 60: 338.9641, Accuracy: 0.6077\n",
      "Training loss (for one batch) at step 70: 368.2681, Accuracy: 0.6027\n",
      "Training loss (for one batch) at step 80: 333.2134, Accuracy: 0.5947\n",
      "Training loss (for one batch) at step 90: 339.8055, Accuracy: 0.5926\n",
      "Training loss (for one batch) at step 100: 333.8566, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 110: 321.3803, Accuracy: 0.5957\n",
      "---- Training ----\n",
      "Training loss: 107.6200\n",
      "Training acc over epoch: 0.5942\n",
      "---- Validation ----\n",
      "Validation loss: 44.8837\n",
      "Validation acc: 0.5548\n",
      "Time taken: 10.99s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 363.2241, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 342.4229, Accuracy: 0.5696\n",
      "Training loss (for one batch) at step 20: 333.4182, Accuracy: 0.5856\n",
      "Training loss (for one batch) at step 30: 318.0484, Accuracy: 0.6033\n",
      "Training loss (for one batch) at step 40: 320.9674, Accuracy: 0.6050\n",
      "Training loss (for one batch) at step 50: 322.2442, Accuracy: 0.6101\n",
      "Training loss (for one batch) at step 60: 322.5888, Accuracy: 0.6128\n",
      "Training loss (for one batch) at step 70: 356.9511, Accuracy: 0.6056\n",
      "Training loss (for one batch) at step 80: 360.1804, Accuracy: 0.5975\n",
      "Training loss (for one batch) at step 90: 327.8376, Accuracy: 0.5950\n",
      "Training loss (for one batch) at step 100: 329.9750, Accuracy: 0.5959\n",
      "Training loss (for one batch) at step 110: 353.9572, Accuracy: 0.5961\n",
      "---- Training ----\n",
      "Training loss: 106.2792\n",
      "Training acc over epoch: 0.5968\n",
      "---- Validation ----\n",
      "Validation loss: 39.4036\n",
      "Validation acc: 0.5650\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 365.8924, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 347.2327, Accuracy: 0.5497\n",
      "Training loss (for one batch) at step 20: 332.1976, Accuracy: 0.5763\n",
      "Training loss (for one batch) at step 30: 312.9097, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 40: 315.5753, Accuracy: 0.5985\n",
      "Training loss (for one batch) at step 50: 319.9226, Accuracy: 0.6036\n",
      "Training loss (for one batch) at step 60: 317.2902, Accuracy: 0.6082\n",
      "Training loss (for one batch) at step 70: 340.3613, Accuracy: 0.6031\n",
      "Training loss (for one batch) at step 80: 350.9682, Accuracy: 0.5958\n",
      "Training loss (for one batch) at step 90: 328.3528, Accuracy: 0.5963\n",
      "Training loss (for one batch) at step 100: 313.0030, Accuracy: 0.5976\n",
      "Training loss (for one batch) at step 110: 336.3187, Accuracy: 0.5973\n",
      "---- Training ----\n",
      "Training loss: 93.9265\n",
      "Training acc over epoch: 0.5964\n",
      "---- Validation ----\n",
      "Validation loss: 48.4916\n",
      "Validation acc: 0.5760\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 363.9120, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 350.7825, Accuracy: 0.5526\n",
      "Training loss (for one batch) at step 20: 320.3437, Accuracy: 0.5770\n",
      "Training loss (for one batch) at step 30: 319.1545, Accuracy: 0.5927\n",
      "Training loss (for one batch) at step 40: 322.5269, Accuracy: 0.5993\n",
      "Training loss (for one batch) at step 50: 308.2069, Accuracy: 0.6062\n",
      "Training loss (for one batch) at step 60: 336.5002, Accuracy: 0.6108\n",
      "Training loss (for one batch) at step 70: 362.9806, Accuracy: 0.6055\n",
      "Training loss (for one batch) at step 80: 350.0882, Accuracy: 0.5972\n",
      "Training loss (for one batch) at step 90: 327.0003, Accuracy: 0.5958\n",
      "Training loss (for one batch) at step 100: 323.4605, Accuracy: 0.5962\n",
      "Training loss (for one batch) at step 110: 338.4685, Accuracy: 0.5977\n",
      "---- Training ----\n",
      "Training loss: 108.9871\n",
      "Training acc over epoch: 0.5956\n",
      "---- Validation ----\n",
      "Validation loss: 71.0640\n",
      "Validation acc: 0.5712\n",
      "Time taken: 12.60s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 342.6840, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 328.9077, Accuracy: 0.5646\n",
      "Training loss (for one batch) at step 20: 335.6471, Accuracy: 0.5800\n",
      "Training loss (for one batch) at step 30: 310.0435, Accuracy: 0.5925\n",
      "Training loss (for one batch) at step 40: 334.9936, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 50: 316.5966, Accuracy: 0.6089\n",
      "Training loss (for one batch) at step 60: 335.2181, Accuracy: 0.6132\n",
      "Training loss (for one batch) at step 70: 354.9287, Accuracy: 0.6061\n",
      "Training loss (for one batch) at step 80: 326.9832, Accuracy: 0.5965\n",
      "Training loss (for one batch) at step 90: 330.8460, Accuracy: 0.5942\n",
      "Training loss (for one batch) at step 100: 324.9022, Accuracy: 0.5947\n",
      "Training loss (for one batch) at step 110: 337.5279, Accuracy: 0.5956\n",
      "---- Training ----\n",
      "Training loss: 126.2088\n",
      "Training acc over epoch: 0.5951\n",
      "---- Validation ----\n",
      "Validation loss: 37.3872\n",
      "Validation acc: 0.5688\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 356.6171, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 349.3119, Accuracy: 0.5511\n",
      "Training loss (for one batch) at step 20: 307.8701, Accuracy: 0.5729\n",
      "Training loss (for one batch) at step 30: 333.0569, Accuracy: 0.5970\n",
      "Training loss (for one batch) at step 40: 312.7744, Accuracy: 0.6002\n",
      "Training loss (for one batch) at step 50: 313.3001, Accuracy: 0.6062\n",
      "Training loss (for one batch) at step 60: 330.8558, Accuracy: 0.6124\n",
      "Training loss (for one batch) at step 70: 334.3898, Accuracy: 0.6068\n",
      "Training loss (for one batch) at step 80: 339.9256, Accuracy: 0.5982\n",
      "Training loss (for one batch) at step 90: 330.7710, Accuracy: 0.5948\n",
      "Training loss (for one batch) at step 100: 326.2876, Accuracy: 0.5952\n",
      "Training loss (for one batch) at step 110: 326.5534, Accuracy: 0.5947\n",
      "---- Training ----\n",
      "Training loss: 100.1749\n",
      "Training acc over epoch: 0.5948\n",
      "---- Validation ----\n",
      "Validation loss: 56.2037\n",
      "Validation acc: 0.5656\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 355.4128, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 329.4968, Accuracy: 0.5724\n",
      "Training loss (for one batch) at step 20: 328.6725, Accuracy: 0.5804\n",
      "Training loss (for one batch) at step 30: 320.2504, Accuracy: 0.5953\n",
      "Training loss (for one batch) at step 40: 316.9667, Accuracy: 0.5998\n",
      "Training loss (for one batch) at step 50: 314.2346, Accuracy: 0.6109\n",
      "Training loss (for one batch) at step 60: 326.9265, Accuracy: 0.6119\n",
      "Training loss (for one batch) at step 70: 329.8725, Accuracy: 0.6042\n",
      "Training loss (for one batch) at step 80: 335.3554, Accuracy: 0.5964\n",
      "Training loss (for one batch) at step 90: 347.1180, Accuracy: 0.5937\n",
      "Training loss (for one batch) at step 100: 321.5856, Accuracy: 0.5948\n",
      "Training loss (for one batch) at step 110: 331.4088, Accuracy: 0.5947\n",
      "---- Training ----\n",
      "Training loss: 121.0386\n",
      "Training acc over epoch: 0.5932\n",
      "---- Validation ----\n",
      "Validation loss: 50.0304\n",
      "Validation acc: 0.5688\n",
      "Time taken: 10.54s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 344.3821, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 351.4412, Accuracy: 0.5554\n",
      "Training loss (for one batch) at step 20: 322.3187, Accuracy: 0.5740\n",
      "Training loss (for one batch) at step 30: 323.7722, Accuracy: 0.5948\n",
      "Training loss (for one batch) at step 40: 325.7018, Accuracy: 0.5995\n",
      "Training loss (for one batch) at step 50: 294.3576, Accuracy: 0.6068\n",
      "Training loss (for one batch) at step 60: 344.0503, Accuracy: 0.6107\n",
      "Training loss (for one batch) at step 70: 335.4805, Accuracy: 0.6051\n",
      "Training loss (for one batch) at step 80: 338.2318, Accuracy: 0.5963\n",
      "Training loss (for one batch) at step 90: 330.6836, Accuracy: 0.5968\n",
      "Training loss (for one batch) at step 100: 325.9745, Accuracy: 0.5976\n",
      "Training loss (for one batch) at step 110: 339.8702, Accuracy: 0.5953\n",
      "---- Training ----\n",
      "Training loss: 102.8612\n",
      "Training acc over epoch: 0.5957\n",
      "---- Validation ----\n",
      "Validation loss: 49.8773\n",
      "Validation acc: 0.5626\n",
      "Time taken: 10.98s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 341.5279, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 335.4431, Accuracy: 0.5646\n",
      "Training loss (for one batch) at step 20: 317.9119, Accuracy: 0.5811\n",
      "Training loss (for one batch) at step 30: 323.9642, Accuracy: 0.5927\n",
      "Training loss (for one batch) at step 40: 323.2074, Accuracy: 0.6027\n",
      "Training loss (for one batch) at step 50: 294.3601, Accuracy: 0.6086\n",
      "Training loss (for one batch) at step 60: 333.3797, Accuracy: 0.6109\n",
      "Training loss (for one batch) at step 70: 326.0476, Accuracy: 0.6044\n",
      "Training loss (for one batch) at step 80: 331.0881, Accuracy: 0.5965\n",
      "Training loss (for one batch) at step 90: 317.8131, Accuracy: 0.5931\n",
      "Training loss (for one batch) at step 100: 308.6504, Accuracy: 0.5951\n",
      "Training loss (for one batch) at step 110: 351.9196, Accuracy: 0.5944\n",
      "---- Training ----\n",
      "Training loss: 108.6625\n",
      "Training acc over epoch: 0.5940\n",
      "---- Validation ----\n",
      "Validation loss: 45.9973\n",
      "Validation acc: 0.5658\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 350.9823, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 334.9330, Accuracy: 0.5483\n",
      "Training loss (for one batch) at step 20: 312.3901, Accuracy: 0.5796\n",
      "Training loss (for one batch) at step 30: 331.8990, Accuracy: 0.5980\n",
      "Training loss (for one batch) at step 40: 293.8682, Accuracy: 0.6002\n",
      "Training loss (for one batch) at step 50: 302.0096, Accuracy: 0.6066\n",
      "Training loss (for one batch) at step 60: 319.1063, Accuracy: 0.6101\n",
      "Training loss (for one batch) at step 70: 321.8794, Accuracy: 0.6076\n",
      "Training loss (for one batch) at step 80: 342.3791, Accuracy: 0.5986\n",
      "Training loss (for one batch) at step 90: 303.2494, Accuracy: 0.5953\n",
      "Training loss (for one batch) at step 100: 318.5505, Accuracy: 0.5944\n",
      "Training loss (for one batch) at step 110: 319.1756, Accuracy: 0.5945\n",
      "---- Training ----\n",
      "Training loss: 103.1582\n",
      "Training acc over epoch: 0.5957\n",
      "---- Validation ----\n",
      "Validation loss: 43.0526\n",
      "Validation acc: 0.5610\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 346.7144, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 321.5672, Accuracy: 0.5582\n",
      "Training loss (for one batch) at step 20: 311.8246, Accuracy: 0.5878\n",
      "Training loss (for one batch) at step 30: 313.2843, Accuracy: 0.5995\n",
      "Training loss (for one batch) at step 40: 309.0460, Accuracy: 0.6054\n",
      "Training loss (for one batch) at step 50: 328.7122, Accuracy: 0.6088\n",
      "Training loss (for one batch) at step 60: 325.5022, Accuracy: 0.6126\n",
      "Training loss (for one batch) at step 70: 318.8685, Accuracy: 0.6071\n",
      "Training loss (for one batch) at step 80: 337.6072, Accuracy: 0.5987\n",
      "Training loss (for one batch) at step 90: 307.1016, Accuracy: 0.5972\n",
      "Training loss (for one batch) at step 100: 307.1022, Accuracy: 0.5961\n",
      "Training loss (for one batch) at step 110: 324.5985, Accuracy: 0.5960\n",
      "---- Training ----\n",
      "Training loss: 113.3478\n",
      "Training acc over epoch: 0.5961\n",
      "---- Validation ----\n",
      "Validation loss: 33.8898\n",
      "Validation acc: 0.5688\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 315.6076, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 334.3601, Accuracy: 0.5504\n",
      "Training loss (for one batch) at step 20: 321.8375, Accuracy: 0.5774\n",
      "Training loss (for one batch) at step 30: 318.5093, Accuracy: 0.5902\n",
      "Training loss (for one batch) at step 40: 307.6743, Accuracy: 0.5949\n",
      "Training loss (for one batch) at step 50: 319.8326, Accuracy: 0.6011\n",
      "Training loss (for one batch) at step 60: 320.5024, Accuracy: 0.6058\n",
      "Training loss (for one batch) at step 70: 351.6260, Accuracy: 0.5984\n",
      "Training loss (for one batch) at step 80: 333.3299, Accuracy: 0.5909\n",
      "Training loss (for one batch) at step 90: 313.3872, Accuracy: 0.5887\n",
      "Training loss (for one batch) at step 100: 312.8339, Accuracy: 0.5889\n",
      "Training loss (for one batch) at step 110: 321.3291, Accuracy: 0.5904\n",
      "---- Training ----\n",
      "Training loss: 111.2141\n",
      "Training acc over epoch: 0.5893\n",
      "---- Validation ----\n",
      "Validation loss: 46.7173\n",
      "Validation acc: 0.5731\n",
      "Time taken: 10.57s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 326.1978, Accuracy: 0.4844\n",
      "Training loss (for one batch) at step 10: 318.0934, Accuracy: 0.5497\n",
      "Training loss (for one batch) at step 20: 304.2705, Accuracy: 0.5677\n",
      "Training loss (for one batch) at step 30: 323.3474, Accuracy: 0.5897\n",
      "Training loss (for one batch) at step 40: 324.8230, Accuracy: 0.5962\n",
      "Training loss (for one batch) at step 50: 291.2112, Accuracy: 0.6063\n",
      "Training loss (for one batch) at step 60: 321.8032, Accuracy: 0.6067\n",
      "Training loss (for one batch) at step 70: 312.0331, Accuracy: 0.6008\n",
      "Training loss (for one batch) at step 80: 352.4601, Accuracy: 0.5921\n",
      "Training loss (for one batch) at step 90: 319.1224, Accuracy: 0.5912\n",
      "Training loss (for one batch) at step 100: 314.0144, Accuracy: 0.5913\n",
      "Training loss (for one batch) at step 110: 317.7312, Accuracy: 0.5933\n",
      "---- Training ----\n",
      "Training loss: 110.9300\n",
      "Training acc over epoch: 0.5925\n",
      "---- Validation ----\n",
      "Validation loss: 52.0947\n",
      "Validation acc: 0.5535\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 335.0000, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 321.1311, Accuracy: 0.5632\n",
      "Training loss (for one batch) at step 20: 309.6310, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 30: 304.5220, Accuracy: 0.5965\n",
      "Training loss (for one batch) at step 40: 307.6354, Accuracy: 0.6038\n",
      "Training loss (for one batch) at step 50: 292.6747, Accuracy: 0.6072\n",
      "Training loss (for one batch) at step 60: 318.2084, Accuracy: 0.6110\n",
      "Training loss (for one batch) at step 70: 333.0303, Accuracy: 0.6038\n",
      "Training loss (for one batch) at step 80: 328.4134, Accuracy: 0.5975\n",
      "Training loss (for one batch) at step 90: 305.6889, Accuracy: 0.5937\n",
      "Training loss (for one batch) at step 100: 317.6665, Accuracy: 0.5958\n",
      "Training loss (for one batch) at step 110: 338.7427, Accuracy: 0.5959\n",
      "---- Training ----\n",
      "Training loss: 110.0473\n",
      "Training acc over epoch: 0.5958\n",
      "---- Validation ----\n",
      "Validation loss: 41.8636\n",
      "Validation acc: 0.5725\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 321.2352, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 341.3187, Accuracy: 0.5440\n",
      "Training loss (for one batch) at step 20: 311.4747, Accuracy: 0.5737\n",
      "Training loss (for one batch) at step 30: 304.7397, Accuracy: 0.5935\n",
      "Training loss (for one batch) at step 40: 311.1356, Accuracy: 0.6000\n",
      "Training loss (for one batch) at step 50: 306.1983, Accuracy: 0.6100\n",
      "Training loss (for one batch) at step 60: 332.5620, Accuracy: 0.6113\n",
      "Training loss (for one batch) at step 70: 313.6214, Accuracy: 0.6022\n",
      "Training loss (for one batch) at step 80: 338.3431, Accuracy: 0.5970\n",
      "Training loss (for one batch) at step 90: 313.7745, Accuracy: 0.5947\n",
      "Training loss (for one batch) at step 100: 329.5729, Accuracy: 0.5941\n",
      "Training loss (for one batch) at step 110: 310.8514, Accuracy: 0.5935\n",
      "---- Training ----\n",
      "Training loss: 109.5781\n",
      "Training acc over epoch: 0.5938\n",
      "---- Validation ----\n",
      "Validation loss: 50.4629\n",
      "Validation acc: 0.5626\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 327.6259, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 317.1462, Accuracy: 0.5632\n",
      "Training loss (for one batch) at step 20: 327.0829, Accuracy: 0.5804\n",
      "Training loss (for one batch) at step 30: 309.9352, Accuracy: 0.5917\n",
      "Training loss (for one batch) at step 40: 321.3211, Accuracy: 0.6004\n",
      "Training loss (for one batch) at step 50: 319.3976, Accuracy: 0.6092\n",
      "Training loss (for one batch) at step 60: 322.7902, Accuracy: 0.6116\n",
      "Training loss (for one batch) at step 70: 326.3111, Accuracy: 0.6024\n",
      "Training loss (for one batch) at step 80: 331.5016, Accuracy: 0.5949\n",
      "Training loss (for one batch) at step 90: 312.2725, Accuracy: 0.5913\n",
      "Training loss (for one batch) at step 100: 330.8484, Accuracy: 0.5935\n",
      "Training loss (for one batch) at step 110: 312.6102, Accuracy: 0.5938\n",
      "---- Training ----\n",
      "Training loss: 97.2933\n",
      "Training acc over epoch: 0.5937\n",
      "---- Validation ----\n",
      "Validation loss: 41.9780\n",
      "Validation acc: 0.5736\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 337.4734, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 321.0327, Accuracy: 0.5355\n",
      "Training loss (for one batch) at step 20: 307.7080, Accuracy: 0.5707\n",
      "Training loss (for one batch) at step 30: 304.9358, Accuracy: 0.5869\n",
      "Training loss (for one batch) at step 40: 307.5618, Accuracy: 0.5966\n",
      "Training loss (for one batch) at step 50: 301.0531, Accuracy: 0.6028\n",
      "Training loss (for one batch) at step 60: 313.4770, Accuracy: 0.6067\n",
      "Training loss (for one batch) at step 70: 342.2780, Accuracy: 0.6010\n",
      "Training loss (for one batch) at step 80: 328.5084, Accuracy: 0.5924\n",
      "Training loss (for one batch) at step 90: 314.1858, Accuracy: 0.5903\n",
      "Training loss (for one batch) at step 100: 317.8877, Accuracy: 0.5910\n",
      "Training loss (for one batch) at step 110: 303.1607, Accuracy: 0.5925\n",
      "---- Training ----\n",
      "Training loss: 103.6313\n",
      "Training acc over epoch: 0.5922\n",
      "---- Validation ----\n",
      "Validation loss: 37.6972\n",
      "Validation acc: 0.5688\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 351.2715, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 326.8436, Accuracy: 0.5518\n",
      "Training loss (for one batch) at step 20: 301.7067, Accuracy: 0.5763\n",
      "Training loss (for one batch) at step 30: 311.2854, Accuracy: 0.6013\n",
      "Training loss (for one batch) at step 40: 316.9882, Accuracy: 0.6035\n",
      "Training loss (for one batch) at step 50: 313.0922, Accuracy: 0.6083\n",
      "Training loss (for one batch) at step 60: 322.5276, Accuracy: 0.6108\n",
      "Training loss (for one batch) at step 70: 314.3738, Accuracy: 0.6035\n",
      "Training loss (for one batch) at step 80: 335.0057, Accuracy: 0.5943\n",
      "Training loss (for one batch) at step 90: 307.8183, Accuracy: 0.5939\n",
      "Training loss (for one batch) at step 100: 306.5739, Accuracy: 0.5968\n",
      "Training loss (for one batch) at step 110: 303.4695, Accuracy: 0.5959\n",
      "---- Training ----\n",
      "Training loss: 98.0835\n",
      "Training acc over epoch: 0.5950\n",
      "---- Validation ----\n",
      "Validation loss: 40.3924\n",
      "Validation acc: 0.5618\n",
      "Time taken: 12.46s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 335.2428, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 340.3344, Accuracy: 0.5433\n",
      "Training loss (for one batch) at step 20: 304.1075, Accuracy: 0.5699\n",
      "Training loss (for one batch) at step 30: 305.0927, Accuracy: 0.5847\n",
      "Training loss (for one batch) at step 40: 307.6704, Accuracy: 0.5949\n",
      "Training loss (for one batch) at step 50: 301.2849, Accuracy: 0.6040\n",
      "Training loss (for one batch) at step 60: 302.7857, Accuracy: 0.6067\n",
      "Training loss (for one batch) at step 70: 337.4166, Accuracy: 0.5999\n",
      "Training loss (for one batch) at step 80: 312.7438, Accuracy: 0.5942\n",
      "Training loss (for one batch) at step 90: 306.4670, Accuracy: 0.5889\n",
      "Training loss (for one batch) at step 100: 306.7737, Accuracy: 0.5905\n",
      "Training loss (for one batch) at step 110: 328.8901, Accuracy: 0.5907\n",
      "---- Training ----\n",
      "Training loss: 97.4134\n",
      "Training acc over epoch: 0.5903\n",
      "---- Validation ----\n",
      "Validation loss: 62.4249\n",
      "Validation acc: 0.5825\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 330.7499, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 311.8787, Accuracy: 0.5518\n",
      "Training loss (for one batch) at step 20: 308.5736, Accuracy: 0.5759\n",
      "Training loss (for one batch) at step 30: 306.0775, Accuracy: 0.5945\n",
      "Training loss (for one batch) at step 40: 314.3651, Accuracy: 0.5998\n",
      "Training loss (for one batch) at step 50: 290.4784, Accuracy: 0.6049\n",
      "Training loss (for one batch) at step 60: 316.7968, Accuracy: 0.6096\n",
      "Training loss (for one batch) at step 70: 322.1283, Accuracy: 0.6051\n",
      "Training loss (for one batch) at step 80: 341.3604, Accuracy: 0.5951\n",
      "Training loss (for one batch) at step 90: 307.2331, Accuracy: 0.5919\n",
      "Training loss (for one batch) at step 100: 311.2748, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 110: 307.9386, Accuracy: 0.5945\n",
      "---- Training ----\n",
      "Training loss: 106.4207\n",
      "Training acc over epoch: 0.5939\n",
      "---- Validation ----\n",
      "Validation loss: 41.1231\n",
      "Validation acc: 0.5742\n",
      "Time taken: 10.40s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 330.1626, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 314.0693, Accuracy: 0.5589\n",
      "Training loss (for one batch) at step 20: 299.4456, Accuracy: 0.5718\n",
      "Training loss (for one batch) at step 30: 296.8171, Accuracy: 0.5915\n",
      "Training loss (for one batch) at step 40: 289.6648, Accuracy: 0.6002\n",
      "Training loss (for one batch) at step 50: 294.3139, Accuracy: 0.6048\n",
      "Training loss (for one batch) at step 60: 312.3185, Accuracy: 0.6096\n",
      "Training loss (for one batch) at step 70: 318.8372, Accuracy: 0.6028\n",
      "Training loss (for one batch) at step 80: 332.4758, Accuracy: 0.5948\n",
      "Training loss (for one batch) at step 90: 302.0297, Accuracy: 0.5919\n",
      "Training loss (for one batch) at step 100: 312.2702, Accuracy: 0.5937\n",
      "Training loss (for one batch) at step 110: 343.5490, Accuracy: 0.5921\n",
      "---- Training ----\n",
      "Training loss: 99.2124\n",
      "Training acc over epoch: 0.5923\n",
      "---- Validation ----\n",
      "Validation loss: 43.5073\n",
      "Validation acc: 0.5653\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 325.4937, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 320.9846, Accuracy: 0.5518\n",
      "Training loss (for one batch) at step 20: 304.3794, Accuracy: 0.5796\n",
      "Training loss (for one batch) at step 30: 321.9026, Accuracy: 0.5968\n",
      "Training loss (for one batch) at step 40: 306.1161, Accuracy: 0.5989\n",
      "Training loss (for one batch) at step 50: 326.7556, Accuracy: 0.6072\n",
      "Training loss (for one batch) at step 60: 302.8519, Accuracy: 0.6092\n",
      "Training loss (for one batch) at step 70: 340.6414, Accuracy: 0.6045\n",
      "Training loss (for one batch) at step 80: 329.4467, Accuracy: 0.5962\n",
      "Training loss (for one batch) at step 90: 308.8380, Accuracy: 0.5939\n",
      "Training loss (for one batch) at step 100: 304.9737, Accuracy: 0.5954\n",
      "Training loss (for one batch) at step 110: 318.6969, Accuracy: 0.5956\n",
      "---- Training ----\n",
      "Training loss: 106.7032\n",
      "Training acc over epoch: 0.5946\n",
      "---- Validation ----\n",
      "Validation loss: 43.1315\n",
      "Validation acc: 0.5605\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 334.9033, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 321.8032, Accuracy: 0.5362\n",
      "Training loss (for one batch) at step 20: 315.1163, Accuracy: 0.5629\n",
      "Training loss (for one batch) at step 30: 303.2816, Accuracy: 0.5814\n",
      "Training loss (for one batch) at step 40: 297.3026, Accuracy: 0.5958\n",
      "Training loss (for one batch) at step 50: 296.5402, Accuracy: 0.6011\n",
      "Training loss (for one batch) at step 60: 313.0582, Accuracy: 0.6078\n",
      "Training loss (for one batch) at step 70: 303.3862, Accuracy: 0.6010\n",
      "Training loss (for one batch) at step 80: 314.2917, Accuracy: 0.5932\n",
      "Training loss (for one batch) at step 90: 313.5867, Accuracy: 0.5889\n",
      "Training loss (for one batch) at step 100: 306.8516, Accuracy: 0.5905\n",
      "Training loss (for one batch) at step 110: 309.9344, Accuracy: 0.5899\n",
      "---- Training ----\n",
      "Training loss: 100.5107\n",
      "Training acc over epoch: 0.5905\n",
      "---- Validation ----\n",
      "Validation loss: 39.5882\n",
      "Validation acc: 0.5656\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 327.6218, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 331.8746, Accuracy: 0.5518\n",
      "Training loss (for one batch) at step 20: 296.4059, Accuracy: 0.5744\n",
      "Training loss (for one batch) at step 30: 288.2073, Accuracy: 0.5963\n",
      "Training loss (for one batch) at step 40: 293.4347, Accuracy: 0.5998\n",
      "Training loss (for one batch) at step 50: 303.1912, Accuracy: 0.6072\n",
      "Training loss (for one batch) at step 60: 308.1139, Accuracy: 0.6113\n",
      "Training loss (for one batch) at step 70: 310.6792, Accuracy: 0.6029\n",
      "Training loss (for one batch) at step 80: 331.2908, Accuracy: 0.5948\n",
      "Training loss (for one batch) at step 90: 314.0177, Accuracy: 0.5901\n",
      "Training loss (for one batch) at step 100: 291.1654, Accuracy: 0.5917\n",
      "Training loss (for one batch) at step 110: 303.2142, Accuracy: 0.5930\n",
      "---- Training ----\n",
      "Training loss: 100.6028\n",
      "Training acc over epoch: 0.5922\n",
      "---- Validation ----\n",
      "Validation loss: 44.3137\n",
      "Validation acc: 0.5648\n",
      "Time taken: 10.83s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 320.0387, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 325.9446, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 20: 295.1989, Accuracy: 0.5729\n",
      "Training loss (for one batch) at step 30: 297.3524, Accuracy: 0.5922\n",
      "Training loss (for one batch) at step 40: 296.5393, Accuracy: 0.6008\n",
      "Training loss (for one batch) at step 50: 299.3311, Accuracy: 0.6068\n",
      "Training loss (for one batch) at step 60: 307.8086, Accuracy: 0.6118\n",
      "Training loss (for one batch) at step 70: 343.2837, Accuracy: 0.6040\n",
      "Training loss (for one batch) at step 80: 307.3396, Accuracy: 0.5956\n",
      "Training loss (for one batch) at step 90: 337.1775, Accuracy: 0.5917\n",
      "Training loss (for one batch) at step 100: 320.1895, Accuracy: 0.5931\n",
      "Training loss (for one batch) at step 110: 311.4194, Accuracy: 0.5932\n",
      "---- Training ----\n",
      "Training loss: 122.3050\n",
      "Training acc over epoch: 0.5926\n",
      "---- Validation ----\n",
      "Validation loss: 32.2464\n",
      "Validation acc: 0.5510\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 332.0097, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 315.7733, Accuracy: 0.5284\n",
      "Training loss (for one batch) at step 20: 294.7176, Accuracy: 0.5725\n",
      "Training loss (for one batch) at step 30: 303.9577, Accuracy: 0.5885\n",
      "Training loss (for one batch) at step 40: 290.1659, Accuracy: 0.5972\n",
      "Training loss (for one batch) at step 50: 294.3237, Accuracy: 0.6063\n",
      "Training loss (for one batch) at step 60: 303.4013, Accuracy: 0.6069\n",
      "Training loss (for one batch) at step 70: 335.5978, Accuracy: 0.5995\n",
      "Training loss (for one batch) at step 80: 327.1912, Accuracy: 0.5921\n",
      "Training loss (for one batch) at step 90: 305.5504, Accuracy: 0.5904\n",
      "Training loss (for one batch) at step 100: 290.7623, Accuracy: 0.5924\n",
      "Training loss (for one batch) at step 110: 313.7812, Accuracy: 0.5918\n",
      "---- Training ----\n",
      "Training loss: 97.7223\n",
      "Training acc over epoch: 0.5921\n",
      "---- Validation ----\n",
      "Validation loss: 48.7240\n",
      "Validation acc: 0.5669\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 314.7097, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 315.2298, Accuracy: 0.5689\n",
      "Training loss (for one batch) at step 20: 319.2739, Accuracy: 0.5725\n",
      "Training loss (for one batch) at step 30: 300.6354, Accuracy: 0.5892\n",
      "Training loss (for one batch) at step 40: 298.9141, Accuracy: 0.5972\n",
      "Training loss (for one batch) at step 50: 305.7323, Accuracy: 0.6059\n",
      "Training loss (for one batch) at step 60: 286.2022, Accuracy: 0.6084\n",
      "Training loss (for one batch) at step 70: 323.9036, Accuracy: 0.6026\n",
      "Training loss (for one batch) at step 80: 322.3319, Accuracy: 0.5955\n",
      "Training loss (for one batch) at step 90: 313.9001, Accuracy: 0.5927\n",
      "Training loss (for one batch) at step 100: 305.9267, Accuracy: 0.5931\n",
      "Training loss (for one batch) at step 110: 324.4472, Accuracy: 0.5933\n",
      "---- Training ----\n",
      "Training loss: 99.0282\n",
      "Training acc over epoch: 0.5922\n",
      "---- Validation ----\n",
      "Validation loss: 34.2145\n",
      "Validation acc: 0.5725\n",
      "Time taken: 11.71s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 327.1618, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 308.9221, Accuracy: 0.5455\n",
      "Training loss (for one batch) at step 20: 293.9224, Accuracy: 0.5696\n",
      "Training loss (for one batch) at step 30: 314.1967, Accuracy: 0.5854\n",
      "Training loss (for one batch) at step 40: 287.5956, Accuracy: 0.5957\n",
      "Training loss (for one batch) at step 50: 283.5271, Accuracy: 0.6017\n",
      "Training loss (for one batch) at step 60: 312.2535, Accuracy: 0.6066\n",
      "Training loss (for one batch) at step 70: 319.1094, Accuracy: 0.5988\n",
      "Training loss (for one batch) at step 80: 316.7478, Accuracy: 0.5923\n",
      "Training loss (for one batch) at step 90: 313.1038, Accuracy: 0.5919\n",
      "Training loss (for one batch) at step 100: 308.4291, Accuracy: 0.5915\n",
      "Training loss (for one batch) at step 110: 316.8623, Accuracy: 0.5927\n",
      "---- Training ----\n",
      "Training loss: 100.7902\n",
      "Training acc over epoch: 0.5921\n",
      "---- Validation ----\n",
      "Validation loss: 50.9037\n",
      "Validation acc: 0.5693\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 321.9077, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 316.8303, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 20: 295.7165, Accuracy: 0.5711\n",
      "Training loss (for one batch) at step 30: 290.0145, Accuracy: 0.5930\n",
      "Training loss (for one batch) at step 40: 293.5959, Accuracy: 0.6004\n",
      "Training loss (for one batch) at step 50: 304.3969, Accuracy: 0.6085\n",
      "Training loss (for one batch) at step 60: 304.7722, Accuracy: 0.6091\n",
      "Training loss (for one batch) at step 70: 315.8630, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 80: 324.5927, Accuracy: 0.5944\n",
      "Training loss (for one batch) at step 90: 317.2360, Accuracy: 0.5907\n",
      "Training loss (for one batch) at step 100: 286.4256, Accuracy: 0.5913\n",
      "Training loss (for one batch) at step 110: 300.9225, Accuracy: 0.5928\n",
      "---- Training ----\n",
      "Training loss: 110.1243\n",
      "Training acc over epoch: 0.5921\n",
      "---- Validation ----\n",
      "Validation loss: 41.1535\n",
      "Validation acc: 0.5578\n",
      "Time taken: 10.80s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 316.2869, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 313.1008, Accuracy: 0.5554\n",
      "Training loss (for one batch) at step 20: 296.1685, Accuracy: 0.5722\n",
      "Training loss (for one batch) at step 30: 300.9681, Accuracy: 0.5927\n",
      "Training loss (for one batch) at step 40: 283.8178, Accuracy: 0.5979\n",
      "Training loss (for one batch) at step 50: 286.1509, Accuracy: 0.6051\n",
      "Training loss (for one batch) at step 60: 319.4528, Accuracy: 0.6078\n",
      "Training loss (for one batch) at step 70: 302.7306, Accuracy: 0.5976\n",
      "Training loss (for one batch) at step 80: 312.9204, Accuracy: 0.5921\n",
      "Training loss (for one batch) at step 90: 289.9298, Accuracy: 0.5900\n",
      "Training loss (for one batch) at step 100: 304.9832, Accuracy: 0.5913\n",
      "Training loss (for one batch) at step 110: 317.0236, Accuracy: 0.5928\n",
      "---- Training ----\n",
      "Training loss: 107.7345\n",
      "Training acc over epoch: 0.5928\n",
      "---- Validation ----\n",
      "Validation loss: 51.0568\n",
      "Validation acc: 0.5699\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 299.8620, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 322.3806, Accuracy: 0.5518\n",
      "Training loss (for one batch) at step 20: 291.0931, Accuracy: 0.5658\n",
      "Training loss (for one batch) at step 30: 280.3276, Accuracy: 0.5847\n",
      "Training loss (for one batch) at step 40: 297.0846, Accuracy: 0.5924\n",
      "Training loss (for one batch) at step 50: 296.0370, Accuracy: 0.6002\n",
      "Training loss (for one batch) at step 60: 290.5419, Accuracy: 0.6050\n",
      "Training loss (for one batch) at step 70: 312.4388, Accuracy: 0.6008\n",
      "Training loss (for one batch) at step 80: 309.2709, Accuracy: 0.5909\n",
      "Training loss (for one batch) at step 90: 300.5265, Accuracy: 0.5877\n",
      "Training loss (for one batch) at step 100: 294.5876, Accuracy: 0.5879\n",
      "Training loss (for one batch) at step 110: 301.3156, Accuracy: 0.5890\n",
      "---- Training ----\n",
      "Training loss: 100.8499\n",
      "Training acc over epoch: 0.5893\n",
      "---- Validation ----\n",
      "Validation loss: 41.1190\n",
      "Validation acc: 0.5755\n",
      "Time taken: 10.64s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 312.4882, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 315.4850, Accuracy: 0.5298\n",
      "Training loss (for one batch) at step 20: 285.3186, Accuracy: 0.5751\n",
      "Training loss (for one batch) at step 30: 281.9258, Accuracy: 0.5849\n",
      "Training loss (for one batch) at step 40: 289.7789, Accuracy: 0.5958\n",
      "Training loss (for one batch) at step 50: 286.3899, Accuracy: 0.6068\n",
      "Training loss (for one batch) at step 60: 311.8619, Accuracy: 0.6064\n",
      "Training loss (for one batch) at step 70: 325.1649, Accuracy: 0.5990\n",
      "Training loss (for one batch) at step 80: 333.8332, Accuracy: 0.5924\n",
      "Training loss (for one batch) at step 90: 303.7945, Accuracy: 0.5890\n",
      "Training loss (for one batch) at step 100: 312.9464, Accuracy: 0.5918\n",
      "Training loss (for one batch) at step 110: 316.6028, Accuracy: 0.5925\n",
      "---- Training ----\n",
      "Training loss: 93.8402\n",
      "Training acc over epoch: 0.5903\n",
      "---- Validation ----\n",
      "Validation loss: 38.7619\n",
      "Validation acc: 0.5610\n",
      "Time taken: 10.68s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 327.3976, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 321.5074, Accuracy: 0.5419\n",
      "Training loss (for one batch) at step 20: 295.8057, Accuracy: 0.5725\n",
      "Training loss (for one batch) at step 30: 294.5385, Accuracy: 0.5862\n",
      "Training loss (for one batch) at step 40: 306.9561, Accuracy: 0.5966\n",
      "Training loss (for one batch) at step 50: 290.1293, Accuracy: 0.6057\n",
      "Training loss (for one batch) at step 60: 309.7184, Accuracy: 0.6110\n",
      "Training loss (for one batch) at step 70: 316.3595, Accuracy: 0.6033\n",
      "Training loss (for one batch) at step 80: 311.5062, Accuracy: 0.5955\n",
      "Training loss (for one batch) at step 90: 303.5823, Accuracy: 0.5933\n",
      "Training loss (for one batch) at step 100: 302.5689, Accuracy: 0.5952\n",
      "Training loss (for one batch) at step 110: 303.9104, Accuracy: 0.5948\n",
      "---- Training ----\n",
      "Training loss: 99.7666\n",
      "Training acc over epoch: 0.5940\n",
      "---- Validation ----\n",
      "Validation loss: 50.5984\n",
      "Validation acc: 0.5634\n",
      "Time taken: 11.07s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 316.3850, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 312.4958, Accuracy: 0.5511\n",
      "Training loss (for one batch) at step 20: 295.3039, Accuracy: 0.5818\n",
      "Training loss (for one batch) at step 30: 286.2124, Accuracy: 0.5925\n",
      "Training loss (for one batch) at step 40: 293.7405, Accuracy: 0.5979\n",
      "Training loss (for one batch) at step 50: 275.2113, Accuracy: 0.6051\n",
      "Training loss (for one batch) at step 60: 301.5770, Accuracy: 0.6055\n",
      "Training loss (for one batch) at step 70: 303.8159, Accuracy: 0.6018\n",
      "Training loss (for one batch) at step 80: 306.1021, Accuracy: 0.5936\n",
      "Training loss (for one batch) at step 90: 302.8639, Accuracy: 0.5897\n",
      "Training loss (for one batch) at step 100: 298.3687, Accuracy: 0.5916\n",
      "Training loss (for one batch) at step 110: 295.9203, Accuracy: 0.5915\n",
      "---- Training ----\n",
      "Training loss: 97.5429\n",
      "Training acc over epoch: 0.5911\n",
      "---- Validation ----\n",
      "Validation loss: 37.8403\n",
      "Validation acc: 0.5648\n",
      "Time taken: 11.21s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 307.6258, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 313.4758, Accuracy: 0.5497\n",
      "Training loss (for one batch) at step 20: 294.2907, Accuracy: 0.5636\n",
      "Training loss (for one batch) at step 30: 291.4069, Accuracy: 0.5877\n",
      "Training loss (for one batch) at step 40: 288.4439, Accuracy: 0.5960\n",
      "Training loss (for one batch) at step 50: 291.9275, Accuracy: 0.6034\n",
      "Training loss (for one batch) at step 60: 310.4455, Accuracy: 0.6084\n",
      "Training loss (for one batch) at step 70: 302.3589, Accuracy: 0.6048\n",
      "Training loss (for one batch) at step 80: 318.6623, Accuracy: 0.5944\n",
      "Training loss (for one batch) at step 90: 312.4400, Accuracy: 0.5923\n",
      "Training loss (for one batch) at step 100: 309.7321, Accuracy: 0.5934\n",
      "Training loss (for one batch) at step 110: 316.7142, Accuracy: 0.5940\n",
      "---- Training ----\n",
      "Training loss: 117.0165\n",
      "Training acc over epoch: 0.5924\n",
      "---- Validation ----\n",
      "Validation loss: 38.8444\n",
      "Validation acc: 0.5629\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 326.0525, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 316.6042, Accuracy: 0.5277\n",
      "Training loss (for one batch) at step 20: 291.1984, Accuracy: 0.5588\n",
      "Training loss (for one batch) at step 30: 287.8542, Accuracy: 0.5852\n",
      "Training loss (for one batch) at step 40: 308.9616, Accuracy: 0.5981\n",
      "Training loss (for one batch) at step 50: 327.9555, Accuracy: 0.6045\n",
      "Training loss (for one batch) at step 60: 307.9041, Accuracy: 0.6060\n",
      "Training loss (for one batch) at step 70: 313.9962, Accuracy: 0.5995\n",
      "Training loss (for one batch) at step 80: 334.0352, Accuracy: 0.5923\n",
      "Training loss (for one batch) at step 90: 294.0744, Accuracy: 0.5910\n",
      "Training loss (for one batch) at step 100: 317.9949, Accuracy: 0.5934\n",
      "Training loss (for one batch) at step 110: 299.9260, Accuracy: 0.5935\n",
      "---- Training ----\n",
      "Training loss: 92.9486\n",
      "Training acc over epoch: 0.5926\n",
      "---- Validation ----\n",
      "Validation loss: 38.8563\n",
      "Validation acc: 0.5798\n",
      "Time taken: 10.73s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 312.9772, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 316.8281, Accuracy: 0.5554\n",
      "Training loss (for one batch) at step 20: 304.3636, Accuracy: 0.5621\n",
      "Training loss (for one batch) at step 30: 283.7972, Accuracy: 0.5874\n",
      "Training loss (for one batch) at step 40: 303.2254, Accuracy: 0.5957\n",
      "Training loss (for one batch) at step 50: 273.0727, Accuracy: 0.6014\n",
      "Training loss (for one batch) at step 60: 308.5497, Accuracy: 0.6035\n",
      "Training loss (for one batch) at step 70: 304.4666, Accuracy: 0.5997\n",
      "Training loss (for one batch) at step 80: 304.8181, Accuracy: 0.5917\n",
      "Training loss (for one batch) at step 90: 317.6674, Accuracy: 0.5891\n",
      "Training loss (for one batch) at step 100: 289.9876, Accuracy: 0.5893\n",
      "Training loss (for one batch) at step 110: 306.5979, Accuracy: 0.5920\n",
      "---- Training ----\n",
      "Training loss: 106.3100\n",
      "Training acc over epoch: 0.5912\n",
      "---- Validation ----\n",
      "Validation loss: 34.5001\n",
      "Validation acc: 0.5653\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 335.5231, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 315.0511, Accuracy: 0.5327\n",
      "Training loss (for one batch) at step 20: 285.5293, Accuracy: 0.5647\n",
      "Training loss (for one batch) at step 30: 298.8113, Accuracy: 0.5867\n",
      "Training loss (for one batch) at step 40: 281.8938, Accuracy: 0.5960\n",
      "Training loss (for one batch) at step 50: 304.8008, Accuracy: 0.6046\n",
      "Training loss (for one batch) at step 60: 335.3383, Accuracy: 0.6072\n",
      "Training loss (for one batch) at step 70: 294.0971, Accuracy: 0.6019\n",
      "Training loss (for one batch) at step 80: 318.8102, Accuracy: 0.5940\n",
      "Training loss (for one batch) at step 90: 303.7565, Accuracy: 0.5907\n",
      "Training loss (for one batch) at step 100: 297.4698, Accuracy: 0.5916\n",
      "Training loss (for one batch) at step 110: 298.0461, Accuracy: 0.5910\n",
      "---- Training ----\n",
      "Training loss: 108.6018\n",
      "Training acc over epoch: 0.5920\n",
      "---- Validation ----\n",
      "Validation loss: 34.0365\n",
      "Validation acc: 0.5607\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 314.6340, Accuracy: 0.4922\n",
      "Training loss (for one batch) at step 10: 314.1209, Accuracy: 0.5341\n",
      "Training loss (for one batch) at step 20: 327.4160, Accuracy: 0.5632\n",
      "Training loss (for one batch) at step 30: 291.2763, Accuracy: 0.5849\n",
      "Training loss (for one batch) at step 40: 289.5597, Accuracy: 0.5978\n",
      "Training loss (for one batch) at step 50: 305.0217, Accuracy: 0.6060\n",
      "Training loss (for one batch) at step 60: 305.2432, Accuracy: 0.6067\n",
      "Training loss (for one batch) at step 70: 312.0679, Accuracy: 0.6008\n",
      "Training loss (for one batch) at step 80: 338.9294, Accuracy: 0.5931\n",
      "Training loss (for one batch) at step 90: 304.2030, Accuracy: 0.5910\n",
      "Training loss (for one batch) at step 100: 306.7726, Accuracy: 0.5920\n",
      "Training loss (for one batch) at step 110: 306.8793, Accuracy: 0.5924\n",
      "---- Training ----\n",
      "Training loss: 113.6090\n",
      "Training acc over epoch: 0.5919\n",
      "---- Validation ----\n",
      "Validation loss: 38.5932\n",
      "Validation acc: 0.5610\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 309.5262, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 316.6746, Accuracy: 0.5518\n",
      "Training loss (for one batch) at step 20: 299.7506, Accuracy: 0.5785\n",
      "Training loss (for one batch) at step 30: 285.7501, Accuracy: 0.5907\n",
      "Training loss (for one batch) at step 40: 287.6021, Accuracy: 0.6027\n",
      "Training loss (for one batch) at step 50: 272.0576, Accuracy: 0.6104\n",
      "Training loss (for one batch) at step 60: 301.5576, Accuracy: 0.6124\n",
      "Training loss (for one batch) at step 70: 311.1421, Accuracy: 0.6041\n",
      "Training loss (for one batch) at step 80: 309.0521, Accuracy: 0.5957\n",
      "Training loss (for one batch) at step 90: 285.9117, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 100: 299.3908, Accuracy: 0.5948\n",
      "Training loss (for one batch) at step 110: 298.3936, Accuracy: 0.5950\n",
      "---- Training ----\n",
      "Training loss: 101.6309\n",
      "Training acc over epoch: 0.5948\n",
      "---- Validation ----\n",
      "Validation loss: 60.3617\n",
      "Validation acc: 0.5720\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 319.6105, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 300.1710, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 20: 287.5819, Accuracy: 0.5804\n",
      "Training loss (for one batch) at step 30: 291.3201, Accuracy: 0.5925\n",
      "Training loss (for one batch) at step 40: 280.8857, Accuracy: 0.5949\n",
      "Training loss (for one batch) at step 50: 271.4227, Accuracy: 0.6029\n",
      "Training loss (for one batch) at step 60: 304.5865, Accuracy: 0.6054\n",
      "Training loss (for one batch) at step 70: 320.5367, Accuracy: 0.6000\n",
      "Training loss (for one batch) at step 80: 305.8542, Accuracy: 0.5927\n",
      "Training loss (for one batch) at step 90: 289.2629, Accuracy: 0.5912\n",
      "Training loss (for one batch) at step 100: 308.1803, Accuracy: 0.5897\n",
      "Training loss (for one batch) at step 110: 297.1844, Accuracy: 0.5914\n",
      "---- Training ----\n",
      "Training loss: 117.2413\n",
      "Training acc over epoch: 0.5907\n",
      "---- Validation ----\n",
      "Validation loss: 77.4558\n",
      "Validation acc: 0.5728\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 306.3767, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 299.8637, Accuracy: 0.5447\n",
      "Training loss (for one batch) at step 20: 281.0096, Accuracy: 0.5751\n",
      "Training loss (for one batch) at step 30: 292.3660, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 40: 306.5946, Accuracy: 0.5997\n",
      "Training loss (for one batch) at step 50: 305.6961, Accuracy: 0.6092\n",
      "Training loss (for one batch) at step 60: 272.6593, Accuracy: 0.6117\n",
      "Training loss (for one batch) at step 70: 298.7928, Accuracy: 0.6048\n",
      "Training loss (for one batch) at step 80: 304.0351, Accuracy: 0.5956\n",
      "Training loss (for one batch) at step 90: 316.4358, Accuracy: 0.5907\n",
      "Training loss (for one batch) at step 100: 317.3162, Accuracy: 0.5902\n",
      "Training loss (for one batch) at step 110: 295.8411, Accuracy: 0.5916\n",
      "---- Training ----\n",
      "Training loss: 105.0418\n",
      "Training acc over epoch: 0.5920\n",
      "---- Validation ----\n",
      "Validation loss: 51.8265\n",
      "Validation acc: 0.5790\n",
      "Time taken: 10.68s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 303.6534, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 310.1269, Accuracy: 0.5490\n",
      "Training loss (for one batch) at step 20: 286.4394, Accuracy: 0.5655\n",
      "Training loss (for one batch) at step 30: 288.2335, Accuracy: 0.5905\n",
      "Training loss (for one batch) at step 40: 298.9846, Accuracy: 0.5974\n",
      "Training loss (for one batch) at step 50: 291.2716, Accuracy: 0.6040\n",
      "Training loss (for one batch) at step 60: 304.5509, Accuracy: 0.6095\n",
      "Training loss (for one batch) at step 70: 301.6260, Accuracy: 0.6024\n",
      "Training loss (for one batch) at step 80: 301.7906, Accuracy: 0.5944\n",
      "Training loss (for one batch) at step 90: 317.7746, Accuracy: 0.5894\n",
      "Training loss (for one batch) at step 100: 289.6698, Accuracy: 0.5924\n",
      "Training loss (for one batch) at step 110: 300.1952, Accuracy: 0.5910\n",
      "---- Training ----\n",
      "Training loss: 94.9890\n",
      "Training acc over epoch: 0.5915\n",
      "---- Validation ----\n",
      "Validation loss: 47.8825\n",
      "Validation acc: 0.5712\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 302.9738, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 317.0636, Accuracy: 0.5419\n",
      "Training loss (for one batch) at step 20: 308.3926, Accuracy: 0.5748\n",
      "Training loss (for one batch) at step 30: 297.3133, Accuracy: 0.5882\n",
      "Training loss (for one batch) at step 40: 294.8711, Accuracy: 0.5983\n",
      "Training loss (for one batch) at step 50: 282.5502, Accuracy: 0.6040\n",
      "Training loss (for one batch) at step 60: 291.5475, Accuracy: 0.6080\n",
      "Training loss (for one batch) at step 70: 298.7400, Accuracy: 0.6021\n",
      "Training loss (for one batch) at step 80: 292.9074, Accuracy: 0.5960\n",
      "Training loss (for one batch) at step 90: 280.7952, Accuracy: 0.5941\n",
      "Training loss (for one batch) at step 100: 302.6930, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 110: 287.1176, Accuracy: 0.5945\n",
      "---- Training ----\n",
      "Training loss: 112.5630\n",
      "Training acc over epoch: 0.5942\n",
      "---- Validation ----\n",
      "Validation loss: 45.6056\n",
      "Validation acc: 0.5553\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 321.0888, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 296.4930, Accuracy: 0.5433\n",
      "Training loss (for one batch) at step 20: 278.1973, Accuracy: 0.5696\n",
      "Training loss (for one batch) at step 30: 279.0876, Accuracy: 0.5925\n",
      "Training loss (for one batch) at step 40: 281.9142, Accuracy: 0.5955\n",
      "Training loss (for one batch) at step 50: 284.8192, Accuracy: 0.6078\n",
      "Training loss (for one batch) at step 60: 291.2693, Accuracy: 0.6107\n",
      "Training loss (for one batch) at step 70: 312.1136, Accuracy: 0.6005\n",
      "Training loss (for one batch) at step 80: 293.8909, Accuracy: 0.5940\n",
      "Training loss (for one batch) at step 90: 292.5385, Accuracy: 0.5911\n",
      "Training loss (for one batch) at step 100: 285.5706, Accuracy: 0.5917\n",
      "Training loss (for one batch) at step 110: 294.1058, Accuracy: 0.5926\n",
      "---- Training ----\n",
      "Training loss: 90.5412\n",
      "Training acc over epoch: 0.5921\n",
      "---- Validation ----\n",
      "Validation loss: 65.0043\n",
      "Validation acc: 0.5680\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 321.9987, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 312.5905, Accuracy: 0.5341\n",
      "Training loss (for one batch) at step 20: 275.6172, Accuracy: 0.5688\n",
      "Training loss (for one batch) at step 30: 277.9605, Accuracy: 0.5915\n",
      "Training loss (for one batch) at step 40: 280.8056, Accuracy: 0.5966\n",
      "Training loss (for one batch) at step 50: 281.6731, Accuracy: 0.6025\n",
      "Training loss (for one batch) at step 60: 299.9973, Accuracy: 0.6071\n",
      "Training loss (for one batch) at step 70: 304.0102, Accuracy: 0.5988\n",
      "Training loss (for one batch) at step 80: 298.5464, Accuracy: 0.5918\n",
      "Training loss (for one batch) at step 90: 287.8257, Accuracy: 0.5914\n",
      "Training loss (for one batch) at step 100: 284.1950, Accuracy: 0.5921\n",
      "Training loss (for one batch) at step 110: 304.0601, Accuracy: 0.5915\n",
      "---- Training ----\n",
      "Training loss: 98.0830\n",
      "Training acc over epoch: 0.5910\n",
      "---- Validation ----\n",
      "Validation loss: 42.7011\n",
      "Validation acc: 0.5631\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 317.7360, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 303.4788, Accuracy: 0.5597\n",
      "Training loss (for one batch) at step 20: 307.9119, Accuracy: 0.5733\n",
      "Training loss (for one batch) at step 30: 281.7038, Accuracy: 0.5917\n",
      "Training loss (for one batch) at step 40: 288.0768, Accuracy: 0.6004\n",
      "Training loss (for one batch) at step 50: 282.4075, Accuracy: 0.6052\n",
      "Training loss (for one batch) at step 60: 291.7436, Accuracy: 0.6080\n",
      "Training loss (for one batch) at step 70: 308.3045, Accuracy: 0.6010\n",
      "Training loss (for one batch) at step 80: 310.6127, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 90: 281.5244, Accuracy: 0.5915\n",
      "Training loss (for one batch) at step 100: 290.0651, Accuracy: 0.5921\n",
      "Training loss (for one batch) at step 110: 288.1472, Accuracy: 0.5938\n",
      "---- Training ----\n",
      "Training loss: 94.7020\n",
      "Training acc over epoch: 0.5912\n",
      "---- Validation ----\n",
      "Validation loss: 52.4001\n",
      "Validation acc: 0.5776\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 323.7400, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 293.9549, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 20: 297.7870, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 30: 275.2363, Accuracy: 0.5892\n",
      "Training loss (for one batch) at step 40: 271.1313, Accuracy: 0.6004\n",
      "Training loss (for one batch) at step 50: 270.1576, Accuracy: 0.6089\n",
      "Training loss (for one batch) at step 60: 299.8348, Accuracy: 0.6084\n",
      "Training loss (for one batch) at step 70: 301.4086, Accuracy: 0.6019\n",
      "Training loss (for one batch) at step 80: 295.7722, Accuracy: 0.5932\n",
      "Training loss (for one batch) at step 90: 280.0015, Accuracy: 0.5912\n",
      "Training loss (for one batch) at step 100: 281.0916, Accuracy: 0.5921\n",
      "Training loss (for one batch) at step 110: 294.0814, Accuracy: 0.5934\n",
      "---- Training ----\n",
      "Training loss: 116.0249\n",
      "Training acc over epoch: 0.5922\n",
      "---- Validation ----\n",
      "Validation loss: 45.6277\n",
      "Validation acc: 0.5699\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 305.8522, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 300.9157, Accuracy: 0.5249\n",
      "Training loss (for one batch) at step 20: 288.3880, Accuracy: 0.5629\n",
      "Training loss (for one batch) at step 30: 290.2613, Accuracy: 0.5872\n",
      "Training loss (for one batch) at step 40: 283.0765, Accuracy: 0.5972\n",
      "Training loss (for one batch) at step 50: 281.8398, Accuracy: 0.6052\n",
      "Training loss (for one batch) at step 60: 280.5408, Accuracy: 0.6098\n",
      "Training loss (for one batch) at step 70: 296.4363, Accuracy: 0.6038\n",
      "Training loss (for one batch) at step 80: 332.8300, Accuracy: 0.5957\n",
      "Training loss (for one batch) at step 90: 285.6462, Accuracy: 0.5939\n",
      "Training loss (for one batch) at step 100: 279.6307, Accuracy: 0.5948\n",
      "Training loss (for one batch) at step 110: 292.5075, Accuracy: 0.5953\n",
      "---- Training ----\n",
      "Training loss: 97.4849\n",
      "Training acc over epoch: 0.5938\n",
      "---- Validation ----\n",
      "Validation loss: 50.2853\n",
      "Validation acc: 0.5591\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 312.1729, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 308.6454, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 20: 284.4598, Accuracy: 0.5718\n",
      "Training loss (for one batch) at step 30: 281.3948, Accuracy: 0.5900\n",
      "Training loss (for one batch) at step 40: 292.8781, Accuracy: 0.5970\n",
      "Training loss (for one batch) at step 50: 296.7368, Accuracy: 0.6043\n",
      "Training loss (for one batch) at step 60: 304.0582, Accuracy: 0.6073\n",
      "Training loss (for one batch) at step 70: 283.7489, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 80: 318.5170, Accuracy: 0.5943\n",
      "Training loss (for one batch) at step 90: 286.7844, Accuracy: 0.5893\n",
      "Training loss (for one batch) at step 100: 293.1829, Accuracy: 0.5891\n",
      "Training loss (for one batch) at step 110: 284.3806, Accuracy: 0.5898\n",
      "---- Training ----\n",
      "Training loss: 93.9744\n",
      "Training acc over epoch: 0.5895\n",
      "---- Validation ----\n",
      "Validation loss: 62.5389\n",
      "Validation acc: 0.5607\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 296.5189, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 297.9550, Accuracy: 0.5376\n",
      "Training loss (for one batch) at step 20: 284.9435, Accuracy: 0.5685\n",
      "Training loss (for one batch) at step 30: 270.7494, Accuracy: 0.5882\n",
      "Training loss (for one batch) at step 40: 274.9250, Accuracy: 0.5993\n",
      "Training loss (for one batch) at step 50: 280.4403, Accuracy: 0.6083\n",
      "Training loss (for one batch) at step 60: 285.0087, Accuracy: 0.6108\n",
      "Training loss (for one batch) at step 70: 291.6967, Accuracy: 0.6019\n",
      "Training loss (for one batch) at step 80: 290.7537, Accuracy: 0.5911\n",
      "Training loss (for one batch) at step 90: 288.7504, Accuracy: 0.5903\n",
      "Training loss (for one batch) at step 100: 301.6036, Accuracy: 0.5910\n",
      "Training loss (for one batch) at step 110: 297.5526, Accuracy: 0.5909\n",
      "---- Training ----\n",
      "Training loss: 108.3585\n",
      "Training acc over epoch: 0.5902\n",
      "---- Validation ----\n",
      "Validation loss: 58.6482\n",
      "Validation acc: 0.5615\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 302.6469, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 313.9937, Accuracy: 0.5433\n",
      "Training loss (for one batch) at step 20: 287.4725, Accuracy: 0.5699\n",
      "Training loss (for one batch) at step 30: 275.4022, Accuracy: 0.5885\n",
      "Training loss (for one batch) at step 40: 310.9995, Accuracy: 0.5972\n",
      "Training loss (for one batch) at step 50: 301.1441, Accuracy: 0.6045\n",
      "Training loss (for one batch) at step 60: 303.1071, Accuracy: 0.6090\n",
      "Training loss (for one batch) at step 70: 302.6707, Accuracy: 0.5997\n",
      "Training loss (for one batch) at step 80: 314.0327, Accuracy: 0.5942\n",
      "Training loss (for one batch) at step 90: 304.9808, Accuracy: 0.5895\n",
      "Training loss (for one batch) at step 100: 300.5173, Accuracy: 0.5912\n",
      "Training loss (for one batch) at step 110: 293.7409, Accuracy: 0.5919\n",
      "---- Training ----\n",
      "Training loss: 99.6229\n",
      "Training acc over epoch: 0.5906\n",
      "---- Validation ----\n",
      "Validation loss: 50.4790\n",
      "Validation acc: 0.5685\n",
      "Time taken: 10.34s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.5522\n",
      "Validation AUC: 0.5595\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 610.8853, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 606.1340, Accuracy: 0.5064\n",
      "Training loss (for one batch) at step 20: 553.6683, Accuracy: 0.5037\n",
      "Training loss (for one batch) at step 30: 531.0156, Accuracy: 0.4952\n",
      "Training loss (for one batch) at step 40: 512.7643, Accuracy: 0.5002\n",
      "Training loss (for one batch) at step 50: 513.4720, Accuracy: 0.5005\n",
      "Training loss (for one batch) at step 60: 479.1627, Accuracy: 0.5006\n",
      "Training loss (for one batch) at step 70: 491.1385, Accuracy: 0.4986\n",
      "Training loss (for one batch) at step 80: 476.8503, Accuracy: 0.5002\n",
      "Training loss (for one batch) at step 90: 487.7534, Accuracy: 0.4985\n",
      "Training loss (for one batch) at step 100: 466.9946, Accuracy: 0.5005\n",
      "Training loss (for one batch) at step 110: 466.2692, Accuracy: 0.5013\n",
      "---- Training ----\n",
      "Training loss: 143.5549\n",
      "Training acc over epoch: 0.5020\n",
      "---- Validation ----\n",
      "Validation loss: 33.8080\n",
      "Validation acc: 0.5134\n",
      "Time taken: 12.27s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 464.1121, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 463.9890, Accuracy: 0.4936\n",
      "Training loss (for one batch) at step 20: 454.3132, Accuracy: 0.5015\n",
      "Training loss (for one batch) at step 30: 454.1363, Accuracy: 0.5071\n",
      "Training loss (for one batch) at step 40: 462.1850, Accuracy: 0.4992\n",
      "Training loss (for one batch) at step 50: 454.8938, Accuracy: 0.5012\n",
      "Training loss (for one batch) at step 60: 451.2126, Accuracy: 0.5069\n",
      "Training loss (for one batch) at step 70: 445.8675, Accuracy: 0.5100\n",
      "Training loss (for one batch) at step 80: 450.5764, Accuracy: 0.5075\n",
      "Training loss (for one batch) at step 90: 447.8536, Accuracy: 0.5086\n",
      "Training loss (for one batch) at step 100: 448.1895, Accuracy: 0.5099\n",
      "Training loss (for one batch) at step 110: 446.6881, Accuracy: 0.5099\n",
      "---- Training ----\n",
      "Training loss: 140.5301\n",
      "Training acc over epoch: 0.5105\n",
      "---- Validation ----\n",
      "Validation loss: 34.5728\n",
      "Validation acc: 0.5121\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 450.1046, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 449.4773, Accuracy: 0.5433\n",
      "Training loss (for one batch) at step 20: 446.8957, Accuracy: 0.5242\n",
      "Training loss (for one batch) at step 30: 446.7459, Accuracy: 0.5290\n",
      "Training loss (for one batch) at step 40: 446.6192, Accuracy: 0.5324\n",
      "Training loss (for one batch) at step 50: 445.4849, Accuracy: 0.5351\n",
      "Training loss (for one batch) at step 60: 446.3916, Accuracy: 0.5352\n",
      "Training loss (for one batch) at step 70: 446.6375, Accuracy: 0.5344\n",
      "Training loss (for one batch) at step 80: 445.4437, Accuracy: 0.5365\n",
      "Training loss (for one batch) at step 90: 445.1629, Accuracy: 0.5363\n",
      "Training loss (for one batch) at step 100: 449.9950, Accuracy: 0.5348\n",
      "Training loss (for one batch) at step 110: 446.9504, Accuracy: 0.5358\n",
      "---- Training ----\n",
      "Training loss: 138.6312\n",
      "Training acc over epoch: 0.5363\n",
      "---- Validation ----\n",
      "Validation loss: 34.5840\n",
      "Validation acc: 0.5427\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 443.6860, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 443.9246, Accuracy: 0.5199\n",
      "Training loss (for one batch) at step 20: 443.0747, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 30: 444.6463, Accuracy: 0.5439\n",
      "Training loss (for one batch) at step 40: 444.0059, Accuracy: 0.5495\n",
      "Training loss (for one batch) at step 50: 446.4258, Accuracy: 0.5530\n",
      "Training loss (for one batch) at step 60: 444.2067, Accuracy: 0.5569\n",
      "Training loss (for one batch) at step 70: 445.2675, Accuracy: 0.5577\n",
      "Training loss (for one batch) at step 80: 446.1933, Accuracy: 0.5607\n",
      "Training loss (for one batch) at step 90: 443.8434, Accuracy: 0.5628\n",
      "Training loss (for one batch) at step 100: 442.0106, Accuracy: 0.5609\n",
      "Training loss (for one batch) at step 110: 444.3428, Accuracy: 0.5607\n",
      "---- Training ----\n",
      "Training loss: 139.2025\n",
      "Training acc over epoch: 0.5630\n",
      "---- Validation ----\n",
      "Validation loss: 35.1154\n",
      "Validation acc: 0.5825\n",
      "Time taken: 10.67s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 445.3522, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 444.0592, Accuracy: 0.5646\n",
      "Training loss (for one batch) at step 20: 443.3354, Accuracy: 0.5714\n",
      "Training loss (for one batch) at step 30: 444.7778, Accuracy: 0.5761\n",
      "Training loss (for one batch) at step 40: 442.7208, Accuracy: 0.5798\n",
      "Training loss (for one batch) at step 50: 443.5291, Accuracy: 0.5777\n",
      "Training loss (for one batch) at step 60: 443.1446, Accuracy: 0.5767\n",
      "Training loss (for one batch) at step 70: 443.1346, Accuracy: 0.5844\n",
      "Training loss (for one batch) at step 80: 443.2643, Accuracy: 0.5852\n",
      "Training loss (for one batch) at step 90: 443.5336, Accuracy: 0.5847\n",
      "Training loss (for one batch) at step 100: 442.1190, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 110: 441.6840, Accuracy: 0.5864\n",
      "---- Training ----\n",
      "Training loss: 138.4738\n",
      "Training acc over epoch: 0.5875\n",
      "---- Validation ----\n",
      "Validation loss: 34.7456\n",
      "Validation acc: 0.6386\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 442.5888, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 443.8167, Accuracy: 0.6072\n",
      "Training loss (for one batch) at step 20: 446.6643, Accuracy: 0.6090\n",
      "Training loss (for one batch) at step 30: 436.9893, Accuracy: 0.6091\n",
      "Training loss (for one batch) at step 40: 443.4669, Accuracy: 0.6084\n",
      "Training loss (for one batch) at step 50: 439.8153, Accuracy: 0.6137\n",
      "Training loss (for one batch) at step 60: 439.5561, Accuracy: 0.6180\n",
      "Training loss (for one batch) at step 70: 443.0292, Accuracy: 0.6186\n",
      "Training loss (for one batch) at step 80: 442.6313, Accuracy: 0.6208\n",
      "Training loss (for one batch) at step 90: 443.1275, Accuracy: 0.6189\n",
      "Training loss (for one batch) at step 100: 441.2922, Accuracy: 0.6163\n",
      "Training loss (for one batch) at step 110: 441.4411, Accuracy: 0.6153\n",
      "---- Training ----\n",
      "Training loss: 139.4431\n",
      "Training acc over epoch: 0.6153\n",
      "---- Validation ----\n",
      "Validation loss: 35.0198\n",
      "Validation acc: 0.6480\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 443.2817, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 442.9113, Accuracy: 0.6364\n",
      "Training loss (for one batch) at step 20: 443.5557, Accuracy: 0.6280\n",
      "Training loss (for one batch) at step 30: 440.1844, Accuracy: 0.6305\n",
      "Training loss (for one batch) at step 40: 440.4928, Accuracy: 0.6279\n",
      "Training loss (for one batch) at step 50: 436.2245, Accuracy: 0.6294\n",
      "Training loss (for one batch) at step 60: 439.4505, Accuracy: 0.6277\n",
      "Training loss (for one batch) at step 70: 446.2686, Accuracy: 0.6302\n",
      "Training loss (for one batch) at step 80: 439.3294, Accuracy: 0.6296\n",
      "Training loss (for one batch) at step 90: 444.1820, Accuracy: 0.6268\n",
      "Training loss (for one batch) at step 100: 439.7523, Accuracy: 0.6238\n",
      "Training loss (for one batch) at step 110: 442.3571, Accuracy: 0.6285\n",
      "---- Training ----\n",
      "Training loss: 138.2653\n",
      "Training acc over epoch: 0.6311\n",
      "---- Validation ----\n",
      "Validation loss: 34.2821\n",
      "Validation acc: 0.6609\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 444.0660, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 448.4408, Accuracy: 0.6669\n",
      "Training loss (for one batch) at step 20: 438.3244, Accuracy: 0.6455\n",
      "Training loss (for one batch) at step 30: 432.1107, Accuracy: 0.6368\n",
      "Training loss (for one batch) at step 40: 438.7795, Accuracy: 0.6454\n",
      "Training loss (for one batch) at step 50: 439.6475, Accuracy: 0.6475\n",
      "Training loss (for one batch) at step 60: 438.2463, Accuracy: 0.6451\n",
      "Training loss (for one batch) at step 70: 443.4434, Accuracy: 0.6446\n",
      "Training loss (for one batch) at step 80: 440.6625, Accuracy: 0.6448\n",
      "Training loss (for one batch) at step 90: 442.1692, Accuracy: 0.6389\n",
      "Training loss (for one batch) at step 100: 439.0924, Accuracy: 0.6364\n",
      "Training loss (for one batch) at step 110: 439.6590, Accuracy: 0.6400\n",
      "---- Training ----\n",
      "Training loss: 140.8331\n",
      "Training acc over epoch: 0.6413\n",
      "---- Validation ----\n",
      "Validation loss: 34.6325\n",
      "Validation acc: 0.6910\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 439.4588, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 441.5347, Accuracy: 0.6705\n",
      "Training loss (for one batch) at step 20: 436.6183, Accuracy: 0.6514\n",
      "Training loss (for one batch) at step 30: 431.6104, Accuracy: 0.6585\n",
      "Training loss (for one batch) at step 40: 437.7694, Accuracy: 0.6585\n",
      "Training loss (for one batch) at step 50: 444.3055, Accuracy: 0.6613\n",
      "Training loss (for one batch) at step 60: 427.3012, Accuracy: 0.6575\n",
      "Training loss (for one batch) at step 70: 438.8189, Accuracy: 0.6610\n",
      "Training loss (for one batch) at step 80: 441.8281, Accuracy: 0.6605\n",
      "Training loss (for one batch) at step 90: 441.2761, Accuracy: 0.6541\n",
      "Training loss (for one batch) at step 100: 437.5223, Accuracy: 0.6538\n",
      "Training loss (for one batch) at step 110: 444.6830, Accuracy: 0.6563\n",
      "---- Training ----\n",
      "Training loss: 137.8669\n",
      "Training acc over epoch: 0.6569\n",
      "---- Validation ----\n",
      "Validation loss: 35.3321\n",
      "Validation acc: 0.6843\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 444.5914, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 440.4136, Accuracy: 0.6605\n",
      "Training loss (for one batch) at step 20: 437.9028, Accuracy: 0.6473\n",
      "Training loss (for one batch) at step 30: 440.3531, Accuracy: 0.6421\n",
      "Training loss (for one batch) at step 40: 433.0313, Accuracy: 0.6540\n",
      "Training loss (for one batch) at step 50: 428.8651, Accuracy: 0.6636\n",
      "Training loss (for one batch) at step 60: 435.8690, Accuracy: 0.6675\n",
      "Training loss (for one batch) at step 70: 445.5432, Accuracy: 0.6703\n",
      "Training loss (for one batch) at step 80: 440.9018, Accuracy: 0.6665\n",
      "Training loss (for one batch) at step 90: 437.6144, Accuracy: 0.6623\n",
      "Training loss (for one batch) at step 100: 439.3877, Accuracy: 0.6590\n",
      "Training loss (for one batch) at step 110: 434.7193, Accuracy: 0.6605\n",
      "---- Training ----\n",
      "Training loss: 137.2467\n",
      "Training acc over epoch: 0.6615\n",
      "---- Validation ----\n",
      "Validation loss: 34.8286\n",
      "Validation acc: 0.6867\n",
      "Time taken: 10.78s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 443.9590, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 444.3306, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 20: 440.5548, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 30: 433.6631, Accuracy: 0.6754\n",
      "Training loss (for one batch) at step 40: 438.1104, Accuracy: 0.6837\n",
      "Training loss (for one batch) at step 50: 421.1961, Accuracy: 0.6950\n",
      "Training loss (for one batch) at step 60: 429.2322, Accuracy: 0.6985\n",
      "Training loss (for one batch) at step 70: 441.3070, Accuracy: 0.6978\n",
      "Training loss (for one batch) at step 80: 443.1720, Accuracy: 0.6943\n",
      "Training loss (for one batch) at step 90: 436.1175, Accuracy: 0.6894\n",
      "Training loss (for one batch) at step 100: 437.5405, Accuracy: 0.6853\n",
      "Training loss (for one batch) at step 110: 433.8356, Accuracy: 0.6873\n",
      "---- Training ----\n",
      "Training loss: 137.1205\n",
      "Training acc over epoch: 0.6861\n",
      "---- Validation ----\n",
      "Validation loss: 33.8777\n",
      "Validation acc: 0.6881\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 446.0332, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 443.0194, Accuracy: 0.7131\n",
      "Training loss (for one batch) at step 20: 433.6230, Accuracy: 0.6894\n",
      "Training loss (for one batch) at step 30: 431.4454, Accuracy: 0.6938\n",
      "Training loss (for one batch) at step 40: 429.6223, Accuracy: 0.6940\n",
      "Training loss (for one batch) at step 50: 425.8552, Accuracy: 0.6982\n",
      "Training loss (for one batch) at step 60: 433.0884, Accuracy: 0.7058\n",
      "Training loss (for one batch) at step 70: 445.3934, Accuracy: 0.7110\n",
      "Training loss (for one batch) at step 80: 438.4291, Accuracy: 0.7106\n",
      "Training loss (for one batch) at step 90: 436.4245, Accuracy: 0.7059\n",
      "Training loss (for one batch) at step 100: 423.6026, Accuracy: 0.7060\n",
      "Training loss (for one batch) at step 110: 435.5370, Accuracy: 0.7078\n",
      "---- Training ----\n",
      "Training loss: 137.4917\n",
      "Training acc over epoch: 0.7077\n",
      "---- Validation ----\n",
      "Validation loss: 35.1544\n",
      "Validation acc: 0.6644\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 444.0655, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 440.3561, Accuracy: 0.6903\n",
      "Training loss (for one batch) at step 20: 437.4998, Accuracy: 0.6756\n",
      "Training loss (for one batch) at step 30: 426.4294, Accuracy: 0.6809\n",
      "Training loss (for one batch) at step 40: 419.5947, Accuracy: 0.6921\n",
      "Training loss (for one batch) at step 50: 436.2774, Accuracy: 0.6978\n",
      "Training loss (for one batch) at step 60: 440.3262, Accuracy: 0.7043\n",
      "Training loss (for one batch) at step 70: 449.6819, Accuracy: 0.7107\n",
      "Training loss (for one batch) at step 80: 437.7059, Accuracy: 0.7070\n",
      "Training loss (for one batch) at step 90: 435.9060, Accuracy: 0.7021\n",
      "Training loss (for one batch) at step 100: 429.7573, Accuracy: 0.7030\n",
      "Training loss (for one batch) at step 110: 437.4835, Accuracy: 0.7050\n",
      "---- Training ----\n",
      "Training loss: 136.9817\n",
      "Training acc over epoch: 0.7073\n",
      "---- Validation ----\n",
      "Validation loss: 35.0465\n",
      "Validation acc: 0.7222\n",
      "Time taken: 10.54s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 436.6879, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 439.4938, Accuracy: 0.7010\n",
      "Training loss (for one batch) at step 20: 438.1423, Accuracy: 0.6942\n",
      "Training loss (for one batch) at step 30: 423.2900, Accuracy: 0.6935\n",
      "Training loss (for one batch) at step 40: 424.6538, Accuracy: 0.7016\n",
      "Training loss (for one batch) at step 50: 413.5497, Accuracy: 0.7088\n",
      "Training loss (for one batch) at step 60: 429.7640, Accuracy: 0.7132\n",
      "Training loss (for one batch) at step 70: 437.2223, Accuracy: 0.7163\n",
      "Training loss (for one batch) at step 80: 439.1451, Accuracy: 0.7132\n",
      "Training loss (for one batch) at step 90: 434.9039, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 100: 429.4569, Accuracy: 0.7119\n",
      "Training loss (for one batch) at step 110: 437.3841, Accuracy: 0.7124\n",
      "---- Training ----\n",
      "Training loss: 137.5628\n",
      "Training acc over epoch: 0.7154\n",
      "---- Validation ----\n",
      "Validation loss: 35.6956\n",
      "Validation acc: 0.7241\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 443.3510, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 430.6206, Accuracy: 0.7259\n",
      "Training loss (for one batch) at step 20: 440.7480, Accuracy: 0.7106\n",
      "Training loss (for one batch) at step 30: 431.3203, Accuracy: 0.7243\n",
      "Training loss (for one batch) at step 40: 418.6831, Accuracy: 0.7275\n",
      "Training loss (for one batch) at step 50: 408.6202, Accuracy: 0.7354\n",
      "Training loss (for one batch) at step 60: 432.5381, Accuracy: 0.7423\n",
      "Training loss (for one batch) at step 70: 425.2051, Accuracy: 0.7456\n",
      "Training loss (for one batch) at step 80: 437.4158, Accuracy: 0.7418\n",
      "Training loss (for one batch) at step 90: 438.3970, Accuracy: 0.7394\n",
      "Training loss (for one batch) at step 100: 431.1161, Accuracy: 0.7390\n",
      "Training loss (for one batch) at step 110: 441.8579, Accuracy: 0.7380\n",
      "---- Training ----\n",
      "Training loss: 139.0310\n",
      "Training acc over epoch: 0.7368\n",
      "---- Validation ----\n",
      "Validation loss: 36.5758\n",
      "Validation acc: 0.7071\n",
      "Time taken: 10.56s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 442.6568, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 435.5730, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 20: 437.7912, Accuracy: 0.7321\n",
      "Training loss (for one batch) at step 30: 424.5290, Accuracy: 0.7356\n",
      "Training loss (for one batch) at step 40: 419.3675, Accuracy: 0.7416\n",
      "Training loss (for one batch) at step 50: 409.2521, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 60: 429.2338, Accuracy: 0.7537\n",
      "Training loss (for one batch) at step 70: 441.5546, Accuracy: 0.7532\n",
      "Training loss (for one batch) at step 80: 433.7031, Accuracy: 0.7498\n",
      "Training loss (for one batch) at step 90: 433.5742, Accuracy: 0.7435\n",
      "Training loss (for one batch) at step 100: 429.6332, Accuracy: 0.7417\n",
      "Training loss (for one batch) at step 110: 433.2401, Accuracy: 0.7423\n",
      "---- Training ----\n",
      "Training loss: 130.4813\n",
      "Training acc over epoch: 0.7427\n",
      "---- Validation ----\n",
      "Validation loss: 34.8910\n",
      "Validation acc: 0.7195\n",
      "Time taken: 10.51s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 440.9251, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 438.3907, Accuracy: 0.7322\n",
      "Training loss (for one batch) at step 20: 437.3017, Accuracy: 0.7243\n",
      "Training loss (for one batch) at step 30: 426.0621, Accuracy: 0.7329\n",
      "Training loss (for one batch) at step 40: 408.9642, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 50: 415.8744, Accuracy: 0.7485\n",
      "Training loss (for one batch) at step 60: 430.8462, Accuracy: 0.7587\n",
      "Training loss (for one batch) at step 70: 439.3933, Accuracy: 0.7599\n",
      "Training loss (for one batch) at step 80: 433.8101, Accuracy: 0.7556\n",
      "Training loss (for one batch) at step 90: 430.3961, Accuracy: 0.7527\n",
      "Training loss (for one batch) at step 100: 419.0932, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 110: 414.0960, Accuracy: 0.7520\n",
      "---- Training ----\n",
      "Training loss: 130.6969\n",
      "Training acc over epoch: 0.7524\n",
      "---- Validation ----\n",
      "Validation loss: 34.7415\n",
      "Validation acc: 0.7080\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 442.6161, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 444.2620, Accuracy: 0.7202\n",
      "Training loss (for one batch) at step 20: 430.6146, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 30: 416.4122, Accuracy: 0.7291\n",
      "Training loss (for one batch) at step 40: 411.1372, Accuracy: 0.7441\n",
      "Training loss (for one batch) at step 50: 394.7146, Accuracy: 0.7544\n",
      "Training loss (for one batch) at step 60: 432.3397, Accuracy: 0.7610\n",
      "Training loss (for one batch) at step 70: 436.1457, Accuracy: 0.7641\n",
      "Training loss (for one batch) at step 80: 430.1491, Accuracy: 0.7584\n",
      "Training loss (for one batch) at step 90: 427.9619, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 100: 423.7337, Accuracy: 0.7519\n",
      "Training loss (for one batch) at step 110: 429.0373, Accuracy: 0.7534\n",
      "---- Training ----\n",
      "Training loss: 130.5918\n",
      "Training acc over epoch: 0.7534\n",
      "---- Validation ----\n",
      "Validation loss: 33.8820\n",
      "Validation acc: 0.7332\n",
      "Time taken: 10.56s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 438.9180, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 431.0996, Accuracy: 0.7337\n",
      "Training loss (for one batch) at step 20: 421.7403, Accuracy: 0.7411\n",
      "Training loss (for one batch) at step 30: 415.7907, Accuracy: 0.7472\n",
      "Training loss (for one batch) at step 40: 397.6201, Accuracy: 0.7572\n",
      "Training loss (for one batch) at step 50: 400.5055, Accuracy: 0.7658\n",
      "Training loss (for one batch) at step 60: 423.4915, Accuracy: 0.7736\n",
      "Training loss (for one batch) at step 70: 435.0693, Accuracy: 0.7764\n",
      "Training loss (for one batch) at step 80: 437.3878, Accuracy: 0.7700\n",
      "Training loss (for one batch) at step 90: 430.4542, Accuracy: 0.7624\n",
      "Training loss (for one batch) at step 100: 416.6013, Accuracy: 0.7594\n",
      "Training loss (for one batch) at step 110: 427.7917, Accuracy: 0.7604\n",
      "---- Training ----\n",
      "Training loss: 131.0099\n",
      "Training acc over epoch: 0.7611\n",
      "---- Validation ----\n",
      "Validation loss: 35.1178\n",
      "Validation acc: 0.7450\n",
      "Time taken: 10.61s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 440.3725, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 436.3272, Accuracy: 0.7564\n",
      "Training loss (for one batch) at step 20: 424.8318, Accuracy: 0.7493\n",
      "Training loss (for one batch) at step 30: 414.2222, Accuracy: 0.7568\n",
      "Training loss (for one batch) at step 40: 414.0704, Accuracy: 0.7626\n",
      "Training loss (for one batch) at step 50: 393.4575, Accuracy: 0.7719\n",
      "Training loss (for one batch) at step 60: 420.9016, Accuracy: 0.7774\n",
      "Training loss (for one batch) at step 70: 434.5243, Accuracy: 0.7757\n",
      "Training loss (for one batch) at step 80: 429.8385, Accuracy: 0.7738\n",
      "Training loss (for one batch) at step 90: 423.6862, Accuracy: 0.7690\n",
      "Training loss (for one batch) at step 100: 413.3591, Accuracy: 0.7666\n",
      "Training loss (for one batch) at step 110: 431.4558, Accuracy: 0.7668\n",
      "---- Training ----\n",
      "Training loss: 131.9480\n",
      "Training acc over epoch: 0.7660\n",
      "---- Validation ----\n",
      "Validation loss: 35.9828\n",
      "Validation acc: 0.7367\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 442.3662, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 437.1737, Accuracy: 0.7486\n",
      "Training loss (for one batch) at step 20: 418.5599, Accuracy: 0.7489\n",
      "Training loss (for one batch) at step 30: 410.8478, Accuracy: 0.7631\n",
      "Training loss (for one batch) at step 40: 399.8692, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 50: 379.4255, Accuracy: 0.7822\n",
      "Training loss (for one batch) at step 60: 403.0181, Accuracy: 0.7918\n",
      "Training loss (for one batch) at step 70: 421.7093, Accuracy: 0.7883\n",
      "Training loss (for one batch) at step 80: 432.9634, Accuracy: 0.7825\n",
      "Training loss (for one batch) at step 90: 422.1118, Accuracy: 0.7784\n",
      "Training loss (for one batch) at step 100: 417.7322, Accuracy: 0.7748\n",
      "Training loss (for one batch) at step 110: 420.9445, Accuracy: 0.7741\n",
      "---- Training ----\n",
      "Training loss: 129.7885\n",
      "Training acc over epoch: 0.7736\n",
      "---- Validation ----\n",
      "Validation loss: 40.3547\n",
      "Validation acc: 0.7217\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 434.7892, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 426.3908, Accuracy: 0.7585\n",
      "Training loss (for one batch) at step 20: 411.3130, Accuracy: 0.7641\n",
      "Training loss (for one batch) at step 30: 406.4109, Accuracy: 0.7757\n",
      "Training loss (for one batch) at step 40: 392.3600, Accuracy: 0.7824\n",
      "Training loss (for one batch) at step 50: 384.6002, Accuracy: 0.7983\n",
      "Training loss (for one batch) at step 60: 400.0790, Accuracy: 0.8028\n",
      "Training loss (for one batch) at step 70: 418.4350, Accuracy: 0.8028\n",
      "Training loss (for one batch) at step 80: 426.4951, Accuracy: 0.7944\n",
      "Training loss (for one batch) at step 90: 423.1268, Accuracy: 0.7855\n",
      "Training loss (for one batch) at step 100: 416.2433, Accuracy: 0.7825\n",
      "Training loss (for one batch) at step 110: 419.4878, Accuracy: 0.7810\n",
      "---- Training ----\n",
      "Training loss: 124.3628\n",
      "Training acc over epoch: 0.7818\n",
      "---- Validation ----\n",
      "Validation loss: 35.4384\n",
      "Validation acc: 0.7348\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 435.6503, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 431.0945, Accuracy: 0.7663\n",
      "Training loss (for one batch) at step 20: 422.2104, Accuracy: 0.7608\n",
      "Training loss (for one batch) at step 30: 403.3436, Accuracy: 0.7689\n",
      "Training loss (for one batch) at step 40: 390.7071, Accuracy: 0.7814\n",
      "Training loss (for one batch) at step 50: 381.7990, Accuracy: 0.7935\n",
      "Training loss (for one batch) at step 60: 400.3736, Accuracy: 0.8012\n",
      "Training loss (for one batch) at step 70: 437.3542, Accuracy: 0.7985\n",
      "Training loss (for one batch) at step 80: 418.8109, Accuracy: 0.7910\n",
      "Training loss (for one batch) at step 90: 416.5891, Accuracy: 0.7835\n",
      "Training loss (for one batch) at step 100: 398.8062, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 110: 411.9011, Accuracy: 0.7785\n",
      "---- Training ----\n",
      "Training loss: 129.5171\n",
      "Training acc over epoch: 0.7771\n",
      "---- Validation ----\n",
      "Validation loss: 36.9430\n",
      "Validation acc: 0.7472\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 434.1919, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 423.6856, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 20: 408.7002, Accuracy: 0.7645\n",
      "Training loss (for one batch) at step 30: 412.8864, Accuracy: 0.7755\n",
      "Training loss (for one batch) at step 40: 398.8499, Accuracy: 0.7856\n",
      "Training loss (for one batch) at step 50: 384.4075, Accuracy: 0.7970\n",
      "Training loss (for one batch) at step 60: 391.4721, Accuracy: 0.8021\n",
      "Training loss (for one batch) at step 70: 407.5768, Accuracy: 0.8003\n",
      "Training loss (for one batch) at step 80: 433.8613, Accuracy: 0.7923\n",
      "Training loss (for one batch) at step 90: 417.6934, Accuracy: 0.7882\n",
      "Training loss (for one batch) at step 100: 395.8298, Accuracy: 0.7875\n",
      "Training loss (for one batch) at step 110: 418.7078, Accuracy: 0.7874\n",
      "---- Training ----\n",
      "Training loss: 134.6675\n",
      "Training acc over epoch: 0.7864\n",
      "---- Validation ----\n",
      "Validation loss: 36.3223\n",
      "Validation acc: 0.7276\n",
      "Time taken: 10.61s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 437.2804, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 425.7342, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 20: 398.0395, Accuracy: 0.7604\n",
      "Training loss (for one batch) at step 30: 392.5008, Accuracy: 0.7772\n",
      "Training loss (for one batch) at step 40: 383.9632, Accuracy: 0.7879\n",
      "Training loss (for one batch) at step 50: 368.0786, Accuracy: 0.8001\n",
      "Training loss (for one batch) at step 60: 387.8058, Accuracy: 0.8088\n",
      "Training loss (for one batch) at step 70: 401.0090, Accuracy: 0.8048\n",
      "Training loss (for one batch) at step 80: 430.0089, Accuracy: 0.7968\n",
      "Training loss (for one batch) at step 90: 401.4687, Accuracy: 0.7918\n",
      "Training loss (for one batch) at step 100: 396.0783, Accuracy: 0.7933\n",
      "Training loss (for one batch) at step 110: 400.4972, Accuracy: 0.7931\n",
      "---- Training ----\n",
      "Training loss: 131.1810\n",
      "Training acc over epoch: 0.7927\n",
      "---- Validation ----\n",
      "Validation loss: 34.5540\n",
      "Validation acc: 0.7469\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 432.4558, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 417.0432, Accuracy: 0.7692\n",
      "Training loss (for one batch) at step 20: 408.2310, Accuracy: 0.7626\n",
      "Training loss (for one batch) at step 30: 388.1254, Accuracy: 0.7780\n",
      "Training loss (for one batch) at step 40: 382.5197, Accuracy: 0.7900\n",
      "Training loss (for one batch) at step 50: 356.7769, Accuracy: 0.8035\n",
      "Training loss (for one batch) at step 60: 370.1797, Accuracy: 0.8115\n",
      "Training loss (for one batch) at step 70: 418.8777, Accuracy: 0.8052\n",
      "Training loss (for one batch) at step 80: 421.9405, Accuracy: 0.7942\n",
      "Training loss (for one batch) at step 90: 385.7912, Accuracy: 0.7892\n",
      "Training loss (for one batch) at step 100: 380.5048, Accuracy: 0.7893\n",
      "Training loss (for one batch) at step 110: 401.4711, Accuracy: 0.7889\n",
      "---- Training ----\n",
      "Training loss: 132.8253\n",
      "Training acc over epoch: 0.7877\n",
      "---- Validation ----\n",
      "Validation loss: 39.2631\n",
      "Validation acc: 0.7200\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 441.2839, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 417.7468, Accuracy: 0.7322\n",
      "Training loss (for one batch) at step 20: 400.5900, Accuracy: 0.7463\n",
      "Training loss (for one batch) at step 30: 376.9786, Accuracy: 0.7724\n",
      "Training loss (for one batch) at step 40: 371.7414, Accuracy: 0.7913\n",
      "Training loss (for one batch) at step 50: 351.1765, Accuracy: 0.8036\n",
      "Training loss (for one batch) at step 60: 400.0898, Accuracy: 0.8101\n",
      "Training loss (for one batch) at step 70: 394.9435, Accuracy: 0.8020\n",
      "Training loss (for one batch) at step 80: 420.2087, Accuracy: 0.7984\n",
      "Training loss (for one batch) at step 90: 390.8126, Accuracy: 0.7929\n",
      "Training loss (for one batch) at step 100: 371.1835, Accuracy: 0.7911\n",
      "Training loss (for one batch) at step 110: 413.8046, Accuracy: 0.7908\n",
      "---- Training ----\n",
      "Training loss: 135.4912\n",
      "Training acc over epoch: 0.7906\n",
      "---- Validation ----\n",
      "Validation loss: 38.2289\n",
      "Validation acc: 0.7512\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 437.4754, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 413.6742, Accuracy: 0.7585\n",
      "Training loss (for one batch) at step 20: 382.5145, Accuracy: 0.7682\n",
      "Training loss (for one batch) at step 30: 373.3627, Accuracy: 0.7843\n",
      "Training loss (for one batch) at step 40: 364.3235, Accuracy: 0.7963\n",
      "Training loss (for one batch) at step 50: 363.4909, Accuracy: 0.8119\n",
      "Training loss (for one batch) at step 60: 381.1109, Accuracy: 0.8169\n",
      "Training loss (for one batch) at step 70: 414.0094, Accuracy: 0.8110\n",
      "Training loss (for one batch) at step 80: 407.7444, Accuracy: 0.8010\n",
      "Training loss (for one batch) at step 90: 397.6398, Accuracy: 0.7934\n",
      "Training loss (for one batch) at step 100: 368.3383, Accuracy: 0.7949\n",
      "Training loss (for one batch) at step 110: 381.6672, Accuracy: 0.7938\n",
      "---- Training ----\n",
      "Training loss: 123.5286\n",
      "Training acc over epoch: 0.7925\n",
      "---- Validation ----\n",
      "Validation loss: 40.8119\n",
      "Validation acc: 0.7286\n",
      "Time taken: 10.59s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 441.1295, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 399.6956, Accuracy: 0.7308\n",
      "Training loss (for one batch) at step 20: 392.3891, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 30: 382.0213, Accuracy: 0.7692\n",
      "Training loss (for one batch) at step 40: 370.5550, Accuracy: 0.7893\n",
      "Training loss (for one batch) at step 50: 370.2209, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 60: 393.1683, Accuracy: 0.8149\n",
      "Training loss (for one batch) at step 70: 408.8666, Accuracy: 0.8106\n",
      "Training loss (for one batch) at step 80: 405.0852, Accuracy: 0.8006\n",
      "Training loss (for one batch) at step 90: 401.9778, Accuracy: 0.7950\n",
      "Training loss (for one batch) at step 100: 383.8789, Accuracy: 0.7975\n",
      "Training loss (for one batch) at step 110: 399.5129, Accuracy: 0.7969\n",
      "---- Training ----\n",
      "Training loss: 123.8871\n",
      "Training acc over epoch: 0.7971\n",
      "---- Validation ----\n",
      "Validation loss: 38.4975\n",
      "Validation acc: 0.7286\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 417.9442, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 408.0133, Accuracy: 0.7592\n",
      "Training loss (for one batch) at step 20: 377.5134, Accuracy: 0.7641\n",
      "Training loss (for one batch) at step 30: 381.1379, Accuracy: 0.7840\n",
      "Training loss (for one batch) at step 40: 362.8177, Accuracy: 0.7967\n",
      "Training loss (for one batch) at step 50: 351.1671, Accuracy: 0.8131\n",
      "Training loss (for one batch) at step 60: 383.9188, Accuracy: 0.8199\n",
      "Training loss (for one batch) at step 70: 409.8894, Accuracy: 0.8134\n",
      "Training loss (for one batch) at step 80: 406.2144, Accuracy: 0.8019\n",
      "Training loss (for one batch) at step 90: 390.4208, Accuracy: 0.7986\n",
      "Training loss (for one batch) at step 100: 367.1960, Accuracy: 0.7982\n",
      "Training loss (for one batch) at step 110: 383.0499, Accuracy: 0.7981\n",
      "---- Training ----\n",
      "Training loss: 125.3611\n",
      "Training acc over epoch: 0.7976\n",
      "---- Validation ----\n",
      "Validation loss: 40.7722\n",
      "Validation acc: 0.7332\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 417.5605, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 411.2013, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 20: 373.5329, Accuracy: 0.7682\n",
      "Training loss (for one batch) at step 30: 372.8804, Accuracy: 0.7828\n",
      "Training loss (for one batch) at step 40: 360.9935, Accuracy: 0.7982\n",
      "Training loss (for one batch) at step 50: 358.2263, Accuracy: 0.8114\n",
      "Training loss (for one batch) at step 60: 393.2019, Accuracy: 0.8186\n",
      "Training loss (for one batch) at step 70: 384.1283, Accuracy: 0.8112\n",
      "Training loss (for one batch) at step 80: 390.3015, Accuracy: 0.8016\n",
      "Training loss (for one batch) at step 90: 373.9697, Accuracy: 0.7993\n",
      "Training loss (for one batch) at step 100: 358.1408, Accuracy: 0.8008\n",
      "Training loss (for one batch) at step 110: 373.0352, Accuracy: 0.8014\n",
      "---- Training ----\n",
      "Training loss: 119.8801\n",
      "Training acc over epoch: 0.8006\n",
      "---- Validation ----\n",
      "Validation loss: 44.8850\n",
      "Validation acc: 0.7410\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 392.2971, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 405.9283, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 20: 367.3456, Accuracy: 0.7593\n",
      "Training loss (for one batch) at step 30: 359.1253, Accuracy: 0.7760\n",
      "Training loss (for one batch) at step 40: 339.0756, Accuracy: 0.7952\n",
      "Training loss (for one batch) at step 50: 334.2274, Accuracy: 0.8088\n",
      "Training loss (for one batch) at step 60: 356.4713, Accuracy: 0.8167\n",
      "Training loss (for one batch) at step 70: 399.5583, Accuracy: 0.8086\n",
      "Training loss (for one batch) at step 80: 400.4257, Accuracy: 0.7994\n",
      "Training loss (for one batch) at step 90: 375.0387, Accuracy: 0.7952\n",
      "Training loss (for one batch) at step 100: 348.4275, Accuracy: 0.7953\n",
      "Training loss (for one batch) at step 110: 381.7288, Accuracy: 0.7976\n",
      "---- Training ----\n",
      "Training loss: 131.4276\n",
      "Training acc over epoch: 0.7968\n",
      "---- Validation ----\n",
      "Validation loss: 43.2060\n",
      "Validation acc: 0.7418\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 403.6584, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 408.7945, Accuracy: 0.7372\n",
      "Training loss (for one batch) at step 20: 376.1851, Accuracy: 0.7545\n",
      "Training loss (for one batch) at step 30: 361.0992, Accuracy: 0.7749\n",
      "Training loss (for one batch) at step 40: 354.0094, Accuracy: 0.7940\n",
      "Training loss (for one batch) at step 50: 333.3182, Accuracy: 0.8056\n",
      "Training loss (for one batch) at step 60: 367.3658, Accuracy: 0.8142\n",
      "Training loss (for one batch) at step 70: 383.4709, Accuracy: 0.8075\n",
      "Training loss (for one batch) at step 80: 398.0831, Accuracy: 0.7983\n",
      "Training loss (for one batch) at step 90: 369.3997, Accuracy: 0.7931\n",
      "Training loss (for one batch) at step 100: 358.0639, Accuracy: 0.7962\n",
      "Training loss (for one batch) at step 110: 356.0428, Accuracy: 0.7972\n",
      "---- Training ----\n",
      "Training loss: 120.6027\n",
      "Training acc over epoch: 0.7972\n",
      "---- Validation ----\n",
      "Validation loss: 40.1516\n",
      "Validation acc: 0.7488\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 406.9071, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 422.5678, Accuracy: 0.7393\n",
      "Training loss (for one batch) at step 20: 366.6985, Accuracy: 0.7604\n",
      "Training loss (for one batch) at step 30: 356.3235, Accuracy: 0.7830\n",
      "Training loss (for one batch) at step 40: 346.7876, Accuracy: 0.7980\n",
      "Training loss (for one batch) at step 50: 343.3387, Accuracy: 0.8148\n",
      "Training loss (for one batch) at step 60: 365.1190, Accuracy: 0.8234\n",
      "Training loss (for one batch) at step 70: 366.3184, Accuracy: 0.8161\n",
      "Training loss (for one batch) at step 80: 383.6789, Accuracy: 0.8036\n",
      "Training loss (for one batch) at step 90: 370.8593, Accuracy: 0.7992\n",
      "Training loss (for one batch) at step 100: 348.0648, Accuracy: 0.8003\n",
      "Training loss (for one batch) at step 110: 375.4282, Accuracy: 0.8005\n",
      "---- Training ----\n",
      "Training loss: 123.0886\n",
      "Training acc over epoch: 0.7994\n",
      "---- Validation ----\n",
      "Validation loss: 45.8101\n",
      "Validation acc: 0.7343\n",
      "Time taken: 10.69s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 414.9916, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 401.4922, Accuracy: 0.7202\n",
      "Training loss (for one batch) at step 20: 346.9279, Accuracy: 0.7593\n",
      "Training loss (for one batch) at step 30: 351.2533, Accuracy: 0.7785\n",
      "Training loss (for one batch) at step 40: 327.3080, Accuracy: 0.7948\n",
      "Training loss (for one batch) at step 50: 316.8904, Accuracy: 0.8102\n",
      "Training loss (for one batch) at step 60: 357.8141, Accuracy: 0.8201\n",
      "Training loss (for one batch) at step 70: 376.7933, Accuracy: 0.8107\n",
      "Training loss (for one batch) at step 80: 394.4949, Accuracy: 0.7991\n",
      "Training loss (for one batch) at step 90: 359.7855, Accuracy: 0.7948\n",
      "Training loss (for one batch) at step 100: 364.1773, Accuracy: 0.7974\n",
      "Training loss (for one batch) at step 110: 361.4597, Accuracy: 0.7996\n",
      "---- Training ----\n",
      "Training loss: 112.5622\n",
      "Training acc over epoch: 0.7994\n",
      "---- Validation ----\n",
      "Validation loss: 35.5787\n",
      "Validation acc: 0.7346\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 409.5409, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 382.9280, Accuracy: 0.7393\n",
      "Training loss (for one batch) at step 20: 355.3331, Accuracy: 0.7630\n",
      "Training loss (for one batch) at step 30: 353.4350, Accuracy: 0.7850\n",
      "Training loss (for one batch) at step 40: 351.3297, Accuracy: 0.8028\n",
      "Training loss (for one batch) at step 50: 328.0390, Accuracy: 0.8194\n",
      "Training loss (for one batch) at step 60: 371.1161, Accuracy: 0.8248\n",
      "Training loss (for one batch) at step 70: 383.2150, Accuracy: 0.8166\n",
      "Training loss (for one batch) at step 80: 392.5663, Accuracy: 0.8036\n",
      "Training loss (for one batch) at step 90: 335.7169, Accuracy: 0.8009\n",
      "Training loss (for one batch) at step 100: 342.0075, Accuracy: 0.8031\n",
      "Training loss (for one batch) at step 110: 363.6714, Accuracy: 0.8028\n",
      "---- Training ----\n",
      "Training loss: 122.2150\n",
      "Training acc over epoch: 0.8006\n",
      "---- Validation ----\n",
      "Validation loss: 38.5416\n",
      "Validation acc: 0.7367\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 402.8267, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 370.9752, Accuracy: 0.7280\n",
      "Training loss (for one batch) at step 20: 352.0313, Accuracy: 0.7582\n",
      "Training loss (for one batch) at step 30: 338.8575, Accuracy: 0.7835\n",
      "Training loss (for one batch) at step 40: 342.9826, Accuracy: 0.8009\n",
      "Training loss (for one batch) at step 50: 346.7136, Accuracy: 0.8157\n",
      "Training loss (for one batch) at step 60: 342.5525, Accuracy: 0.8233\n",
      "Training loss (for one batch) at step 70: 382.4903, Accuracy: 0.8147\n",
      "Training loss (for one batch) at step 80: 395.4727, Accuracy: 0.8007\n",
      "Training loss (for one batch) at step 90: 367.3533, Accuracy: 0.7982\n",
      "Training loss (for one batch) at step 100: 320.6864, Accuracy: 0.8004\n",
      "Training loss (for one batch) at step 110: 352.4983, Accuracy: 0.8022\n",
      "---- Training ----\n",
      "Training loss: 118.5872\n",
      "Training acc over epoch: 0.8012\n",
      "---- Validation ----\n",
      "Validation loss: 38.7074\n",
      "Validation acc: 0.7378\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 388.5322, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 382.9798, Accuracy: 0.7322\n",
      "Training loss (for one batch) at step 20: 337.7086, Accuracy: 0.7608\n",
      "Training loss (for one batch) at step 30: 325.7872, Accuracy: 0.7843\n",
      "Training loss (for one batch) at step 40: 343.0465, Accuracy: 0.8037\n",
      "Training loss (for one batch) at step 50: 314.1650, Accuracy: 0.8168\n",
      "Training loss (for one batch) at step 60: 374.4878, Accuracy: 0.8221\n",
      "Training loss (for one batch) at step 70: 397.0837, Accuracy: 0.8126\n",
      "Training loss (for one batch) at step 80: 371.3167, Accuracy: 0.7970\n",
      "Training loss (for one batch) at step 90: 352.6109, Accuracy: 0.7950\n",
      "Training loss (for one batch) at step 100: 358.7618, Accuracy: 0.7993\n",
      "Training loss (for one batch) at step 110: 354.7024, Accuracy: 0.8007\n",
      "---- Training ----\n",
      "Training loss: 108.4831\n",
      "Training acc over epoch: 0.7999\n",
      "---- Validation ----\n",
      "Validation loss: 39.2013\n",
      "Validation acc: 0.7429\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 409.0122, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 383.1747, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 20: 342.0828, Accuracy: 0.7582\n",
      "Training loss (for one batch) at step 30: 330.0519, Accuracy: 0.7848\n",
      "Training loss (for one batch) at step 40: 352.1370, Accuracy: 0.8011\n",
      "Training loss (for one batch) at step 50: 344.5011, Accuracy: 0.8143\n",
      "Training loss (for one batch) at step 60: 341.2847, Accuracy: 0.8225\n",
      "Training loss (for one batch) at step 70: 384.0667, Accuracy: 0.8113\n",
      "Training loss (for one batch) at step 80: 386.7946, Accuracy: 0.7975\n",
      "Training loss (for one batch) at step 90: 335.8353, Accuracy: 0.7946\n",
      "Training loss (for one batch) at step 100: 346.1357, Accuracy: 0.7964\n",
      "Training loss (for one batch) at step 110: 338.1453, Accuracy: 0.7986\n",
      "---- Training ----\n",
      "Training loss: 113.0437\n",
      "Training acc over epoch: 0.7969\n",
      "---- Validation ----\n",
      "Validation loss: 37.6793\n",
      "Validation acc: 0.7509\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 394.6518, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 377.0106, Accuracy: 0.7393\n",
      "Training loss (for one batch) at step 20: 336.7955, Accuracy: 0.7638\n",
      "Training loss (for one batch) at step 30: 313.9948, Accuracy: 0.7870\n",
      "Training loss (for one batch) at step 40: 325.2171, Accuracy: 0.8037\n",
      "Training loss (for one batch) at step 50: 328.1372, Accuracy: 0.8188\n",
      "Training loss (for one batch) at step 60: 322.4403, Accuracy: 0.8268\n",
      "Training loss (for one batch) at step 70: 343.3392, Accuracy: 0.8148\n",
      "Training loss (for one batch) at step 80: 360.4246, Accuracy: 0.8013\n",
      "Training loss (for one batch) at step 90: 340.1367, Accuracy: 0.7968\n",
      "Training loss (for one batch) at step 100: 341.5879, Accuracy: 0.8004\n",
      "Training loss (for one batch) at step 110: 348.9274, Accuracy: 0.8008\n",
      "---- Training ----\n",
      "Training loss: 107.2808\n",
      "Training acc over epoch: 0.8000\n",
      "---- Validation ----\n",
      "Validation loss: 37.9009\n",
      "Validation acc: 0.7442\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 381.9873, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 359.4898, Accuracy: 0.7273\n",
      "Training loss (for one batch) at step 20: 363.7144, Accuracy: 0.7582\n",
      "Training loss (for one batch) at step 30: 334.1878, Accuracy: 0.7863\n",
      "Training loss (for one batch) at step 40: 321.4229, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 50: 300.2781, Accuracy: 0.8202\n",
      "Training loss (for one batch) at step 60: 313.3870, Accuracy: 0.8276\n",
      "Training loss (for one batch) at step 70: 379.4742, Accuracy: 0.8154\n",
      "Training loss (for one batch) at step 80: 378.8988, Accuracy: 0.8029\n",
      "Training loss (for one batch) at step 90: 353.4836, Accuracy: 0.8004\n",
      "Training loss (for one batch) at step 100: 328.1806, Accuracy: 0.8045\n",
      "Training loss (for one batch) at step 110: 335.4934, Accuracy: 0.8048\n",
      "---- Training ----\n",
      "Training loss: 109.8731\n",
      "Training acc over epoch: 0.8045\n",
      "---- Validation ----\n",
      "Validation loss: 56.9158\n",
      "Validation acc: 0.7364\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 383.4253, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 375.9666, Accuracy: 0.7074\n",
      "Training loss (for one batch) at step 20: 338.2601, Accuracy: 0.7478\n",
      "Training loss (for one batch) at step 30: 331.8024, Accuracy: 0.7719\n",
      "Training loss (for one batch) at step 40: 318.4706, Accuracy: 0.7936\n",
      "Training loss (for one batch) at step 50: 313.1358, Accuracy: 0.8096\n",
      "Training loss (for one batch) at step 60: 333.1735, Accuracy: 0.8188\n",
      "Training loss (for one batch) at step 70: 379.5193, Accuracy: 0.8097\n",
      "Training loss (for one batch) at step 80: 363.9136, Accuracy: 0.7965\n",
      "Training loss (for one batch) at step 90: 344.2798, Accuracy: 0.7941\n",
      "Training loss (for one batch) at step 100: 331.1028, Accuracy: 0.7976\n",
      "Training loss (for one batch) at step 110: 325.6703, Accuracy: 0.8003\n",
      "---- Training ----\n",
      "Training loss: 102.1495\n",
      "Training acc over epoch: 0.7990\n",
      "---- Validation ----\n",
      "Validation loss: 38.9420\n",
      "Validation acc: 0.7528\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 369.8229, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 377.8531, Accuracy: 0.7315\n",
      "Training loss (for one batch) at step 20: 341.6726, Accuracy: 0.7623\n",
      "Training loss (for one batch) at step 30: 312.9541, Accuracy: 0.7926\n",
      "Training loss (for one batch) at step 40: 317.8611, Accuracy: 0.8087\n",
      "Training loss (for one batch) at step 50: 326.0341, Accuracy: 0.8241\n",
      "Training loss (for one batch) at step 60: 339.1842, Accuracy: 0.8291\n",
      "Training loss (for one batch) at step 70: 381.5934, Accuracy: 0.8168\n",
      "Training loss (for one batch) at step 80: 354.3975, Accuracy: 0.8017\n",
      "Training loss (for one batch) at step 90: 343.7480, Accuracy: 0.7974\n",
      "Training loss (for one batch) at step 100: 316.6466, Accuracy: 0.8024\n",
      "Training loss (for one batch) at step 110: 362.0421, Accuracy: 0.8020\n",
      "---- Training ----\n",
      "Training loss: 115.7463\n",
      "Training acc over epoch: 0.8012\n",
      "---- Validation ----\n",
      "Validation loss: 40.4623\n",
      "Validation acc: 0.7469\n",
      "Time taken: 10.56s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 410.7642, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 373.2115, Accuracy: 0.7202\n",
      "Training loss (for one batch) at step 20: 341.8748, Accuracy: 0.7548\n",
      "Training loss (for one batch) at step 30: 313.0477, Accuracy: 0.7865\n",
      "Training loss (for one batch) at step 40: 323.2723, Accuracy: 0.8068\n",
      "Training loss (for one batch) at step 50: 325.3996, Accuracy: 0.8197\n",
      "Training loss (for one batch) at step 60: 336.5805, Accuracy: 0.8289\n",
      "Training loss (for one batch) at step 70: 359.5963, Accuracy: 0.8188\n",
      "Training loss (for one batch) at step 80: 373.7865, Accuracy: 0.8058\n",
      "Training loss (for one batch) at step 90: 334.0800, Accuracy: 0.8035\n",
      "Training loss (for one batch) at step 100: 320.4732, Accuracy: 0.8048\n",
      "Training loss (for one batch) at step 110: 338.2442, Accuracy: 0.8058\n",
      "---- Training ----\n",
      "Training loss: 112.2566\n",
      "Training acc over epoch: 0.8040\n",
      "---- Validation ----\n",
      "Validation loss: 39.1910\n",
      "Validation acc: 0.7464\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 369.4602, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 391.0544, Accuracy: 0.7202\n",
      "Training loss (for one batch) at step 20: 341.0763, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 30: 331.9030, Accuracy: 0.7807\n",
      "Training loss (for one batch) at step 40: 310.5772, Accuracy: 0.8037\n",
      "Training loss (for one batch) at step 50: 312.6549, Accuracy: 0.8182\n",
      "Training loss (for one batch) at step 60: 321.5523, Accuracy: 0.8249\n",
      "Training loss (for one batch) at step 70: 351.8792, Accuracy: 0.8105\n",
      "Training loss (for one batch) at step 80: 357.9795, Accuracy: 0.7978\n",
      "Training loss (for one batch) at step 90: 327.5628, Accuracy: 0.7946\n",
      "Training loss (for one batch) at step 100: 324.8781, Accuracy: 0.7986\n",
      "Training loss (for one batch) at step 110: 343.4107, Accuracy: 0.8014\n",
      "---- Training ----\n",
      "Training loss: 113.5695\n",
      "Training acc over epoch: 0.8006\n",
      "---- Validation ----\n",
      "Validation loss: 44.5205\n",
      "Validation acc: 0.7466\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 367.6127, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 349.2581, Accuracy: 0.7280\n",
      "Training loss (for one batch) at step 20: 318.8043, Accuracy: 0.7533\n",
      "Training loss (for one batch) at step 30: 310.9244, Accuracy: 0.7845\n",
      "Training loss (for one batch) at step 40: 318.9294, Accuracy: 0.8032\n",
      "Training loss (for one batch) at step 50: 296.7688, Accuracy: 0.8194\n",
      "Training loss (for one batch) at step 60: 340.0474, Accuracy: 0.8252\n",
      "Training loss (for one batch) at step 70: 338.6018, Accuracy: 0.8140\n",
      "Training loss (for one batch) at step 80: 367.0756, Accuracy: 0.8005\n",
      "Training loss (for one batch) at step 90: 337.1086, Accuracy: 0.7973\n",
      "Training loss (for one batch) at step 100: 331.0397, Accuracy: 0.8017\n",
      "Training loss (for one batch) at step 110: 351.2617, Accuracy: 0.8027\n",
      "---- Training ----\n",
      "Training loss: 104.1011\n",
      "Training acc over epoch: 0.8018\n",
      "---- Validation ----\n",
      "Validation loss: 57.0688\n",
      "Validation acc: 0.7389\n",
      "Time taken: 10.63s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 377.5234, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 355.9454, Accuracy: 0.6911\n",
      "Training loss (for one batch) at step 20: 328.0743, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 30: 293.7023, Accuracy: 0.7845\n",
      "Training loss (for one batch) at step 40: 299.9541, Accuracy: 0.8060\n",
      "Training loss (for one batch) at step 50: 312.6479, Accuracy: 0.8172\n",
      "Training loss (for one batch) at step 60: 326.9094, Accuracy: 0.8275\n",
      "Training loss (for one batch) at step 70: 359.7112, Accuracy: 0.8180\n",
      "Training loss (for one batch) at step 80: 366.8483, Accuracy: 0.8038\n",
      "Training loss (for one batch) at step 90: 318.1553, Accuracy: 0.7996\n",
      "Training loss (for one batch) at step 100: 321.3177, Accuracy: 0.8057\n",
      "Training loss (for one batch) at step 110: 333.0284, Accuracy: 0.8060\n",
      "---- Training ----\n",
      "Training loss: 105.4734\n",
      "Training acc over epoch: 0.8051\n",
      "---- Validation ----\n",
      "Validation loss: 35.8582\n",
      "Validation acc: 0.7431\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 360.3095, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 362.2637, Accuracy: 0.7216\n",
      "Training loss (for one batch) at step 20: 326.5218, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 30: 331.5974, Accuracy: 0.7881\n",
      "Training loss (for one batch) at step 40: 298.8759, Accuracy: 0.8083\n",
      "Training loss (for one batch) at step 50: 300.4758, Accuracy: 0.8238\n",
      "Training loss (for one batch) at step 60: 307.1447, Accuracy: 0.8295\n",
      "Training loss (for one batch) at step 70: 349.3348, Accuracy: 0.8180\n",
      "Training loss (for one batch) at step 80: 356.6802, Accuracy: 0.8027\n",
      "Training loss (for one batch) at step 90: 311.0372, Accuracy: 0.7975\n",
      "Training loss (for one batch) at step 100: 310.0557, Accuracy: 0.8029\n",
      "Training loss (for one batch) at step 110: 344.4167, Accuracy: 0.8038\n",
      "---- Training ----\n",
      "Training loss: 114.7709\n",
      "Training acc over epoch: 0.8027\n",
      "---- Validation ----\n",
      "Validation loss: 36.5515\n",
      "Validation acc: 0.7327\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 364.6560, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 365.8388, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 20: 327.8929, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 30: 303.8749, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 40: 301.2527, Accuracy: 0.7986\n",
      "Training loss (for one batch) at step 50: 303.9873, Accuracy: 0.8146\n",
      "Training loss (for one batch) at step 60: 345.7729, Accuracy: 0.8221\n",
      "Training loss (for one batch) at step 70: 347.7512, Accuracy: 0.8133\n",
      "Training loss (for one batch) at step 80: 375.0802, Accuracy: 0.8012\n",
      "Training loss (for one batch) at step 90: 340.0466, Accuracy: 0.7993\n",
      "Training loss (for one batch) at step 100: 320.8601, Accuracy: 0.8017\n",
      "Training loss (for one batch) at step 110: 326.9863, Accuracy: 0.8018\n",
      "---- Training ----\n",
      "Training loss: 100.8519\n",
      "Training acc over epoch: 0.8012\n",
      "---- Validation ----\n",
      "Validation loss: 34.6375\n",
      "Validation acc: 0.7346\n",
      "Time taken: 10.69s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 357.5145, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 362.1084, Accuracy: 0.7166\n",
      "Training loss (for one batch) at step 20: 320.4483, Accuracy: 0.7526\n",
      "Training loss (for one batch) at step 30: 301.1996, Accuracy: 0.7855\n",
      "Training loss (for one batch) at step 40: 319.8662, Accuracy: 0.8053\n",
      "Training loss (for one batch) at step 50: 297.1488, Accuracy: 0.8189\n",
      "Training loss (for one batch) at step 60: 309.5785, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 70: 346.4870, Accuracy: 0.8176\n",
      "Training loss (for one batch) at step 80: 382.0509, Accuracy: 0.8036\n",
      "Training loss (for one batch) at step 90: 290.1654, Accuracy: 0.8009\n",
      "Training loss (for one batch) at step 100: 316.8146, Accuracy: 0.8039\n",
      "Training loss (for one batch) at step 110: 314.2343, Accuracy: 0.8046\n",
      "---- Training ----\n",
      "Training loss: 115.3624\n",
      "Training acc over epoch: 0.8043\n",
      "---- Validation ----\n",
      "Validation loss: 45.0337\n",
      "Validation acc: 0.7531\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 358.7119, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 354.3211, Accuracy: 0.7216\n",
      "Training loss (for one batch) at step 20: 308.9628, Accuracy: 0.7571\n",
      "Training loss (for one batch) at step 30: 307.9593, Accuracy: 0.7863\n",
      "Training loss (for one batch) at step 40: 310.8201, Accuracy: 0.8056\n",
      "Training loss (for one batch) at step 50: 295.1210, Accuracy: 0.8208\n",
      "Training loss (for one batch) at step 60: 302.9515, Accuracy: 0.8300\n",
      "Training loss (for one batch) at step 70: 344.9762, Accuracy: 0.8197\n",
      "Training loss (for one batch) at step 80: 346.6518, Accuracy: 0.8072\n",
      "Training loss (for one batch) at step 90: 309.3503, Accuracy: 0.8026\n",
      "Training loss (for one batch) at step 100: 328.1590, Accuracy: 0.8049\n",
      "Training loss (for one batch) at step 110: 318.6159, Accuracy: 0.8073\n",
      "---- Training ----\n",
      "Training loss: 94.7482\n",
      "Training acc over epoch: 0.8061\n",
      "---- Validation ----\n",
      "Validation loss: 51.1352\n",
      "Validation acc: 0.7329\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 378.6591, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 359.2322, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 20: 309.7441, Accuracy: 0.7489\n",
      "Training loss (for one batch) at step 30: 309.1670, Accuracy: 0.7833\n",
      "Training loss (for one batch) at step 40: 295.5490, Accuracy: 0.8051\n",
      "Training loss (for one batch) at step 50: 299.4716, Accuracy: 0.8211\n",
      "Training loss (for one batch) at step 60: 317.2105, Accuracy: 0.8276\n",
      "Training loss (for one batch) at step 70: 377.3340, Accuracy: 0.8160\n",
      "Training loss (for one batch) at step 80: 333.7740, Accuracy: 0.8009\n",
      "Training loss (for one batch) at step 90: 302.9633, Accuracy: 0.7992\n",
      "Training loss (for one batch) at step 100: 319.9019, Accuracy: 0.8016\n",
      "Training loss (for one batch) at step 110: 305.6974, Accuracy: 0.8032\n",
      "---- Training ----\n",
      "Training loss: 108.4224\n",
      "Training acc over epoch: 0.8021\n",
      "---- Validation ----\n",
      "Validation loss: 39.2004\n",
      "Validation acc: 0.7469\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 361.9421, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 360.7752, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 20: 314.4482, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 30: 301.6819, Accuracy: 0.7729\n",
      "Training loss (for one batch) at step 40: 310.3119, Accuracy: 0.7950\n",
      "Training loss (for one batch) at step 50: 285.3733, Accuracy: 0.8099\n",
      "Training loss (for one batch) at step 60: 317.7856, Accuracy: 0.8194\n",
      "Training loss (for one batch) at step 70: 339.9904, Accuracy: 0.8113\n",
      "Training loss (for one batch) at step 80: 333.0067, Accuracy: 0.7967\n",
      "Training loss (for one batch) at step 90: 331.2063, Accuracy: 0.7936\n",
      "Training loss (for one batch) at step 100: 304.6267, Accuracy: 0.7976\n",
      "Training loss (for one batch) at step 110: 336.7763, Accuracy: 0.7993\n",
      "---- Training ----\n",
      "Training loss: 119.0543\n",
      "Training acc over epoch: 0.7990\n",
      "---- Validation ----\n",
      "Validation loss: 40.2320\n",
      "Validation acc: 0.7472\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 361.7840, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 338.5717, Accuracy: 0.7173\n",
      "Training loss (for one batch) at step 20: 313.8322, Accuracy: 0.7560\n",
      "Training loss (for one batch) at step 30: 316.1053, Accuracy: 0.7833\n",
      "Training loss (for one batch) at step 40: 302.4538, Accuracy: 0.8020\n",
      "Training loss (for one batch) at step 50: 289.5377, Accuracy: 0.8179\n",
      "Training loss (for one batch) at step 60: 301.7481, Accuracy: 0.8242\n",
      "Training loss (for one batch) at step 70: 340.9316, Accuracy: 0.8165\n",
      "Training loss (for one batch) at step 80: 347.6972, Accuracy: 0.8034\n",
      "Training loss (for one batch) at step 90: 323.9372, Accuracy: 0.7992\n",
      "Training loss (for one batch) at step 100: 309.4149, Accuracy: 0.8017\n",
      "Training loss (for one batch) at step 110: 318.8414, Accuracy: 0.8036\n",
      "---- Training ----\n",
      "Training loss: 107.1976\n",
      "Training acc over epoch: 0.8025\n",
      "---- Validation ----\n",
      "Validation loss: 54.1610\n",
      "Validation acc: 0.7445\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 353.8041, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 333.9480, Accuracy: 0.7223\n",
      "Training loss (for one batch) at step 20: 299.0352, Accuracy: 0.7582\n",
      "Training loss (for one batch) at step 30: 310.5552, Accuracy: 0.7906\n",
      "Training loss (for one batch) at step 40: 295.4460, Accuracy: 0.8079\n",
      "Training loss (for one batch) at step 50: 298.0315, Accuracy: 0.8209\n",
      "Training loss (for one batch) at step 60: 311.8030, Accuracy: 0.8303\n",
      "Training loss (for one batch) at step 70: 334.0994, Accuracy: 0.8230\n",
      "Training loss (for one batch) at step 80: 367.8563, Accuracy: 0.8070\n",
      "Training loss (for one batch) at step 90: 310.6532, Accuracy: 0.8026\n",
      "Training loss (for one batch) at step 100: 294.8958, Accuracy: 0.8052\n",
      "Training loss (for one batch) at step 110: 314.3518, Accuracy: 0.8045\n",
      "---- Training ----\n",
      "Training loss: 99.6221\n",
      "Training acc over epoch: 0.8041\n",
      "---- Validation ----\n",
      "Validation loss: 45.0153\n",
      "Validation acc: 0.7397\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 357.5753, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 337.0602, Accuracy: 0.7116\n",
      "Training loss (for one batch) at step 20: 306.7303, Accuracy: 0.7459\n",
      "Training loss (for one batch) at step 30: 288.8304, Accuracy: 0.7830\n",
      "Training loss (for one batch) at step 40: 279.6233, Accuracy: 0.8016\n",
      "Training loss (for one batch) at step 50: 287.2860, Accuracy: 0.8208\n",
      "Training loss (for one batch) at step 60: 314.8773, Accuracy: 0.8259\n",
      "Training loss (for one batch) at step 70: 346.1306, Accuracy: 0.8144\n",
      "Training loss (for one batch) at step 80: 338.2224, Accuracy: 0.7997\n",
      "Training loss (for one batch) at step 90: 296.5510, Accuracy: 0.7939\n",
      "Training loss (for one batch) at step 100: 295.8393, Accuracy: 0.7983\n",
      "Training loss (for one batch) at step 110: 319.8475, Accuracy: 0.8003\n",
      "---- Training ----\n",
      "Training loss: 109.6515\n",
      "Training acc over epoch: 0.7988\n",
      "---- Validation ----\n",
      "Validation loss: 37.3981\n",
      "Validation acc: 0.7469\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 377.4583, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 359.3050, Accuracy: 0.7074\n",
      "Training loss (for one batch) at step 20: 305.3351, Accuracy: 0.7440\n",
      "Training loss (for one batch) at step 30: 315.0273, Accuracy: 0.7835\n",
      "Training loss (for one batch) at step 40: 289.3352, Accuracy: 0.8024\n",
      "Training loss (for one batch) at step 50: 283.0100, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 60: 321.7801, Accuracy: 0.8252\n",
      "Training loss (for one batch) at step 70: 355.3308, Accuracy: 0.8138\n",
      "Training loss (for one batch) at step 80: 339.8361, Accuracy: 0.8010\n",
      "Training loss (for one batch) at step 90: 320.8202, Accuracy: 0.7971\n",
      "Training loss (for one batch) at step 100: 308.4859, Accuracy: 0.8025\n",
      "Training loss (for one batch) at step 110: 321.5099, Accuracy: 0.8030\n",
      "---- Training ----\n",
      "Training loss: 102.9753\n",
      "Training acc over epoch: 0.8006\n",
      "---- Validation ----\n",
      "Validation loss: 50.4228\n",
      "Validation acc: 0.7485\n",
      "Time taken: 10.56s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 357.7650, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 323.0648, Accuracy: 0.7074\n",
      "Training loss (for one batch) at step 20: 309.9401, Accuracy: 0.7444\n",
      "Training loss (for one batch) at step 30: 284.1343, Accuracy: 0.7818\n",
      "Training loss (for one batch) at step 40: 312.0387, Accuracy: 0.8003\n",
      "Training loss (for one batch) at step 50: 283.2873, Accuracy: 0.8180\n",
      "Training loss (for one batch) at step 60: 307.4273, Accuracy: 0.8254\n",
      "Training loss (for one batch) at step 70: 336.2379, Accuracy: 0.8151\n",
      "Training loss (for one batch) at step 80: 357.6404, Accuracy: 0.8030\n",
      "Training loss (for one batch) at step 90: 305.1376, Accuracy: 0.7984\n",
      "Training loss (for one batch) at step 100: 295.2722, Accuracy: 0.8017\n",
      "Training loss (for one batch) at step 110: 302.2342, Accuracy: 0.8034\n",
      "---- Training ----\n",
      "Training loss: 109.1284\n",
      "Training acc over epoch: 0.8021\n",
      "---- Validation ----\n",
      "Validation loss: 43.9350\n",
      "Validation acc: 0.7367\n",
      "Time taken: 10.73s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 352.2257, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 353.6109, Accuracy: 0.7017\n",
      "Training loss (for one batch) at step 20: 292.6319, Accuracy: 0.7452\n",
      "Training loss (for one batch) at step 30: 304.4361, Accuracy: 0.7800\n",
      "Training loss (for one batch) at step 40: 317.9572, Accuracy: 0.8043\n",
      "Training loss (for one batch) at step 50: 276.1649, Accuracy: 0.8209\n",
      "Training loss (for one batch) at step 60: 311.6663, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 70: 339.0635, Accuracy: 0.8155\n",
      "Training loss (for one batch) at step 80: 332.9075, Accuracy: 0.8000\n",
      "Training loss (for one batch) at step 90: 293.3850, Accuracy: 0.7963\n",
      "Training loss (for one batch) at step 100: 305.8412, Accuracy: 0.8014\n",
      "Training loss (for one batch) at step 110: 331.2867, Accuracy: 0.8024\n",
      "---- Training ----\n",
      "Training loss: 101.2767\n",
      "Training acc over epoch: 0.8008\n",
      "---- Validation ----\n",
      "Validation loss: 48.0724\n",
      "Validation acc: 0.7335\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 338.4872, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 335.1045, Accuracy: 0.7067\n",
      "Training loss (for one batch) at step 20: 298.0258, Accuracy: 0.7444\n",
      "Training loss (for one batch) at step 30: 294.5852, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 40: 283.3081, Accuracy: 0.8009\n",
      "Training loss (for one batch) at step 50: 283.4270, Accuracy: 0.8176\n",
      "Training loss (for one batch) at step 60: 320.5251, Accuracy: 0.8257\n",
      "Training loss (for one batch) at step 70: 336.4985, Accuracy: 0.8157\n",
      "Training loss (for one batch) at step 80: 335.5967, Accuracy: 0.8036\n",
      "Training loss (for one batch) at step 90: 320.7773, Accuracy: 0.8004\n",
      "Training loss (for one batch) at step 100: 311.6932, Accuracy: 0.8033\n",
      "Training loss (for one batch) at step 110: 328.9722, Accuracy: 0.8052\n",
      "---- Training ----\n",
      "Training loss: 116.2246\n",
      "Training acc over epoch: 0.8028\n",
      "---- Validation ----\n",
      "Validation loss: 40.1024\n",
      "Validation acc: 0.7391\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 344.9828, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 347.2404, Accuracy: 0.6974\n",
      "Training loss (for one batch) at step 20: 313.1962, Accuracy: 0.7355\n",
      "Training loss (for one batch) at step 30: 294.0557, Accuracy: 0.7797\n",
      "Training loss (for one batch) at step 40: 303.8434, Accuracy: 0.8005\n",
      "Training loss (for one batch) at step 50: 281.3381, Accuracy: 0.8185\n",
      "Training loss (for one batch) at step 60: 307.4507, Accuracy: 0.8262\n",
      "Training loss (for one batch) at step 70: 322.9708, Accuracy: 0.8134\n",
      "Training loss (for one batch) at step 80: 330.2421, Accuracy: 0.7958\n",
      "Training loss (for one batch) at step 90: 317.7082, Accuracy: 0.7952\n",
      "Training loss (for one batch) at step 100: 286.8652, Accuracy: 0.7993\n",
      "Training loss (for one batch) at step 110: 312.1568, Accuracy: 0.8000\n",
      "---- Training ----\n",
      "Training loss: 100.7970\n",
      "Training acc over epoch: 0.7999\n",
      "---- Validation ----\n",
      "Validation loss: 33.0876\n",
      "Validation acc: 0.7536\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 372.1946, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 327.9403, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 20: 300.9105, Accuracy: 0.7440\n",
      "Training loss (for one batch) at step 30: 292.8493, Accuracy: 0.7782\n",
      "Training loss (for one batch) at step 40: 288.0236, Accuracy: 0.7995\n",
      "Training loss (for one batch) at step 50: 280.3097, Accuracy: 0.8180\n",
      "Training loss (for one batch) at step 60: 293.4060, Accuracy: 0.8256\n",
      "Training loss (for one batch) at step 70: 327.0497, Accuracy: 0.8167\n",
      "Training loss (for one batch) at step 80: 332.0489, Accuracy: 0.8027\n",
      "Training loss (for one batch) at step 90: 315.6235, Accuracy: 0.7993\n",
      "Training loss (for one batch) at step 100: 298.1716, Accuracy: 0.8033\n",
      "Training loss (for one batch) at step 110: 315.5554, Accuracy: 0.8043\n",
      "---- Training ----\n",
      "Training loss: 104.7333\n",
      "Training acc over epoch: 0.8031\n",
      "---- Validation ----\n",
      "Validation loss: 52.9580\n",
      "Validation acc: 0.7397\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 333.6820, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 346.9659, Accuracy: 0.7067\n",
      "Training loss (for one batch) at step 20: 322.5669, Accuracy: 0.7507\n",
      "Training loss (for one batch) at step 30: 316.0782, Accuracy: 0.7848\n",
      "Training loss (for one batch) at step 40: 300.3492, Accuracy: 0.8045\n",
      "Training loss (for one batch) at step 50: 300.2226, Accuracy: 0.8186\n",
      "Training loss (for one batch) at step 60: 315.0859, Accuracy: 0.8252\n",
      "Training loss (for one batch) at step 70: 334.8581, Accuracy: 0.8134\n",
      "Training loss (for one batch) at step 80: 329.3290, Accuracy: 0.8005\n",
      "Training loss (for one batch) at step 90: 295.1339, Accuracy: 0.7961\n",
      "Training loss (for one batch) at step 100: 297.4560, Accuracy: 0.7995\n",
      "Training loss (for one batch) at step 110: 311.3690, Accuracy: 0.8021\n",
      "---- Training ----\n",
      "Training loss: 127.3725\n",
      "Training acc over epoch: 0.8009\n",
      "---- Validation ----\n",
      "Validation loss: 42.1757\n",
      "Validation acc: 0.7523\n",
      "Time taken: 10.62s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 337.9099, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 339.2533, Accuracy: 0.7053\n",
      "Training loss (for one batch) at step 20: 302.9515, Accuracy: 0.7444\n",
      "Training loss (for one batch) at step 30: 291.6960, Accuracy: 0.7792\n",
      "Training loss (for one batch) at step 40: 312.1941, Accuracy: 0.8039\n",
      "Training loss (for one batch) at step 50: 281.7553, Accuracy: 0.8195\n",
      "Training loss (for one batch) at step 60: 300.4225, Accuracy: 0.8252\n",
      "Training loss (for one batch) at step 70: 330.8082, Accuracy: 0.8155\n",
      "Training loss (for one batch) at step 80: 336.4498, Accuracy: 0.8012\n",
      "Training loss (for one batch) at step 90: 300.7380, Accuracy: 0.7976\n",
      "Training loss (for one batch) at step 100: 296.6740, Accuracy: 0.8017\n",
      "Training loss (for one batch) at step 110: 323.7796, Accuracy: 0.8034\n",
      "---- Training ----\n",
      "Training loss: 110.4249\n",
      "Training acc over epoch: 0.8021\n",
      "---- Validation ----\n",
      "Validation loss: 39.8946\n",
      "Validation acc: 0.7300\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 356.4424, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 334.8564, Accuracy: 0.6911\n",
      "Training loss (for one batch) at step 20: 312.2905, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 30: 280.0600, Accuracy: 0.7739\n",
      "Training loss (for one batch) at step 40: 293.6449, Accuracy: 0.7994\n",
      "Training loss (for one batch) at step 50: 274.9111, Accuracy: 0.8163\n",
      "Training loss (for one batch) at step 60: 302.6812, Accuracy: 0.8243\n",
      "Training loss (for one batch) at step 70: 323.8535, Accuracy: 0.8143\n",
      "Training loss (for one batch) at step 80: 322.8591, Accuracy: 0.7992\n",
      "Training loss (for one batch) at step 90: 304.1702, Accuracy: 0.7964\n",
      "Training loss (for one batch) at step 100: 283.9314, Accuracy: 0.8014\n",
      "Training loss (for one batch) at step 110: 312.6753, Accuracy: 0.8046\n",
      "---- Training ----\n",
      "Training loss: 102.0992\n",
      "Training acc over epoch: 0.8029\n",
      "---- Validation ----\n",
      "Validation loss: 32.1089\n",
      "Validation acc: 0.7456\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 336.9756, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 329.1217, Accuracy: 0.7230\n",
      "Training loss (for one batch) at step 20: 306.0633, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 30: 308.7357, Accuracy: 0.7838\n",
      "Training loss (for one batch) at step 40: 284.7274, Accuracy: 0.8072\n",
      "Training loss (for one batch) at step 50: 277.1116, Accuracy: 0.8199\n",
      "Training loss (for one batch) at step 60: 299.4741, Accuracy: 0.8289\n",
      "Training loss (for one batch) at step 70: 328.0743, Accuracy: 0.8182\n",
      "Training loss (for one batch) at step 80: 326.0316, Accuracy: 0.8046\n",
      "Training loss (for one batch) at step 90: 308.0970, Accuracy: 0.8013\n",
      "Training loss (for one batch) at step 100: 277.7735, Accuracy: 0.8055\n",
      "Training loss (for one batch) at step 110: 311.6864, Accuracy: 0.8071\n",
      "---- Training ----\n",
      "Training loss: 103.3597\n",
      "Training acc over epoch: 0.8062\n",
      "---- Validation ----\n",
      "Validation loss: 67.5244\n",
      "Validation acc: 0.7442\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 353.7546, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 351.0606, Accuracy: 0.7060\n",
      "Training loss (for one batch) at step 20: 299.4061, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 30: 301.4850, Accuracy: 0.7787\n",
      "Training loss (for one batch) at step 40: 308.1283, Accuracy: 0.7982\n",
      "Training loss (for one batch) at step 50: 282.8950, Accuracy: 0.8182\n",
      "Training loss (for one batch) at step 60: 301.9442, Accuracy: 0.8227\n",
      "Training loss (for one batch) at step 70: 340.8667, Accuracy: 0.8113\n",
      "Training loss (for one batch) at step 80: 321.1773, Accuracy: 0.7993\n",
      "Training loss (for one batch) at step 90: 298.8601, Accuracy: 0.7973\n",
      "Training loss (for one batch) at step 100: 310.0314, Accuracy: 0.8004\n",
      "Training loss (for one batch) at step 110: 320.4343, Accuracy: 0.8017\n",
      "---- Training ----\n",
      "Training loss: 97.8990\n",
      "Training acc over epoch: 0.8004\n",
      "---- Validation ----\n",
      "Validation loss: 41.7619\n",
      "Validation acc: 0.7324\n",
      "Time taken: 10.57s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 337.2247, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 332.3371, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 20: 309.2409, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 30: 282.2008, Accuracy: 0.7739\n",
      "Training loss (for one batch) at step 40: 289.0587, Accuracy: 0.7973\n",
      "Training loss (for one batch) at step 50: 273.7821, Accuracy: 0.8145\n",
      "Training loss (for one batch) at step 60: 308.4634, Accuracy: 0.8240\n",
      "Training loss (for one batch) at step 70: 326.5401, Accuracy: 0.8153\n",
      "Training loss (for one batch) at step 80: 317.9042, Accuracy: 0.7997\n",
      "Training loss (for one batch) at step 90: 290.3124, Accuracy: 0.7941\n",
      "Training loss (for one batch) at step 100: 307.7722, Accuracy: 0.8001\n",
      "Training loss (for one batch) at step 110: 321.2099, Accuracy: 0.8014\n",
      "---- Training ----\n",
      "Training loss: 103.4260\n",
      "Training acc over epoch: 0.8000\n",
      "---- Validation ----\n",
      "Validation loss: 43.3972\n",
      "Validation acc: 0.7491\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 365.6445, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 344.1689, Accuracy: 0.7166\n",
      "Training loss (for one batch) at step 20: 293.5352, Accuracy: 0.7586\n",
      "Training loss (for one batch) at step 30: 301.9024, Accuracy: 0.7850\n",
      "Training loss (for one batch) at step 40: 273.1394, Accuracy: 0.8077\n",
      "Training loss (for one batch) at step 50: 276.4256, Accuracy: 0.8234\n",
      "Training loss (for one batch) at step 60: 280.2603, Accuracy: 0.8312\n",
      "Training loss (for one batch) at step 70: 320.9196, Accuracy: 0.8191\n",
      "Training loss (for one batch) at step 80: 331.0616, Accuracy: 0.8040\n",
      "Training loss (for one batch) at step 90: 309.2174, Accuracy: 0.8010\n",
      "Training loss (for one batch) at step 100: 296.6384, Accuracy: 0.8042\n",
      "Training loss (for one batch) at step 110: 309.2016, Accuracy: 0.8046\n",
      "---- Training ----\n",
      "Training loss: 110.2538\n",
      "Training acc over epoch: 0.8042\n",
      "---- Validation ----\n",
      "Validation loss: 64.7454\n",
      "Validation acc: 0.7356\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 342.9791, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 315.1242, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 20: 291.1430, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 30: 279.7386, Accuracy: 0.7810\n",
      "Training loss (for one batch) at step 40: 280.3984, Accuracy: 0.8035\n",
      "Training loss (for one batch) at step 50: 283.1743, Accuracy: 0.8176\n",
      "Training loss (for one batch) at step 60: 306.7733, Accuracy: 0.8254\n",
      "Training loss (for one batch) at step 70: 314.2227, Accuracy: 0.8145\n",
      "Training loss (for one batch) at step 80: 321.4265, Accuracy: 0.8000\n",
      "Training loss (for one batch) at step 90: 290.3139, Accuracy: 0.7948\n",
      "Training loss (for one batch) at step 100: 291.8885, Accuracy: 0.7997\n",
      "Training loss (for one batch) at step 110: 300.3273, Accuracy: 0.8021\n",
      "---- Training ----\n",
      "Training loss: 88.7303\n",
      "Training acc over epoch: 0.8002\n",
      "---- Validation ----\n",
      "Validation loss: 36.0692\n",
      "Validation acc: 0.7356\n",
      "Time taken: 10.62s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 340.0083, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 315.5215, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 20: 305.8389, Accuracy: 0.7411\n",
      "Training loss (for one batch) at step 30: 278.4725, Accuracy: 0.7802\n",
      "Training loss (for one batch) at step 40: 280.3820, Accuracy: 0.8013\n",
      "Training loss (for one batch) at step 50: 278.3860, Accuracy: 0.8212\n",
      "Training loss (for one batch) at step 60: 292.6548, Accuracy: 0.8300\n",
      "Training loss (for one batch) at step 70: 330.2149, Accuracy: 0.8201\n",
      "Training loss (for one batch) at step 80: 327.8600, Accuracy: 0.8050\n",
      "Training loss (for one batch) at step 90: 292.7584, Accuracy: 0.8019\n",
      "Training loss (for one batch) at step 100: 283.3320, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 110: 301.4459, Accuracy: 0.8063\n",
      "---- Training ----\n",
      "Training loss: 102.5807\n",
      "Training acc over epoch: 0.8051\n",
      "---- Validation ----\n",
      "Validation loss: 52.8648\n",
      "Validation acc: 0.7474\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 341.7634, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 344.4770, Accuracy: 0.7045\n",
      "Training loss (for one batch) at step 20: 288.4102, Accuracy: 0.7396\n",
      "Training loss (for one batch) at step 30: 314.3159, Accuracy: 0.7797\n",
      "Training loss (for one batch) at step 40: 281.2603, Accuracy: 0.8014\n",
      "Training loss (for one batch) at step 50: 286.2671, Accuracy: 0.8185\n",
      "Training loss (for one batch) at step 60: 292.1984, Accuracy: 0.8254\n",
      "Training loss (for one batch) at step 70: 321.0661, Accuracy: 0.8143\n",
      "Training loss (for one batch) at step 80: 314.0460, Accuracy: 0.8009\n",
      "Training loss (for one batch) at step 90: 283.5328, Accuracy: 0.7974\n",
      "Training loss (for one batch) at step 100: 266.3904, Accuracy: 0.8011\n",
      "Training loss (for one batch) at step 110: 318.2196, Accuracy: 0.8041\n",
      "---- Training ----\n",
      "Training loss: 109.1132\n",
      "Training acc over epoch: 0.8027\n",
      "---- Validation ----\n",
      "Validation loss: 43.4105\n",
      "Validation acc: 0.7308\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 342.3562, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 329.7647, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 20: 296.7496, Accuracy: 0.7359\n",
      "Training loss (for one batch) at step 30: 283.0052, Accuracy: 0.7722\n",
      "Training loss (for one batch) at step 40: 280.0243, Accuracy: 0.7959\n",
      "Training loss (for one batch) at step 50: 275.4840, Accuracy: 0.8136\n",
      "Training loss (for one batch) at step 60: 287.0222, Accuracy: 0.8230\n",
      "Training loss (for one batch) at step 70: 321.6133, Accuracy: 0.8113\n",
      "Training loss (for one batch) at step 80: 335.4836, Accuracy: 0.8001\n",
      "Training loss (for one batch) at step 90: 293.7909, Accuracy: 0.7947\n",
      "Training loss (for one batch) at step 100: 289.8549, Accuracy: 0.7992\n",
      "Training loss (for one batch) at step 110: 299.6819, Accuracy: 0.7999\n",
      "---- Training ----\n",
      "Training loss: 98.2106\n",
      "Training acc over epoch: 0.7994\n",
      "---- Validation ----\n",
      "Validation loss: 40.3528\n",
      "Validation acc: 0.7520\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 318.8502, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 351.2199, Accuracy: 0.6925\n",
      "Training loss (for one batch) at step 20: 283.6763, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 30: 274.9552, Accuracy: 0.7772\n",
      "Training loss (for one batch) at step 40: 278.1240, Accuracy: 0.8005\n",
      "Training loss (for one batch) at step 50: 276.3978, Accuracy: 0.8174\n",
      "Training loss (for one batch) at step 60: 281.0202, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 70: 311.9194, Accuracy: 0.8150\n",
      "Training loss (for one batch) at step 80: 337.3287, Accuracy: 0.8002\n",
      "Training loss (for one batch) at step 90: 278.4439, Accuracy: 0.7968\n",
      "Training loss (for one batch) at step 100: 285.5132, Accuracy: 0.8018\n",
      "Training loss (for one batch) at step 110: 315.2610, Accuracy: 0.8036\n",
      "---- Training ----\n",
      "Training loss: 114.5545\n",
      "Training acc over epoch: 0.8023\n",
      "---- Validation ----\n",
      "Validation loss: 47.9854\n",
      "Validation acc: 0.7405\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 341.8273, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 320.4761, Accuracy: 0.7095\n",
      "Training loss (for one batch) at step 20: 299.4103, Accuracy: 0.7411\n",
      "Training loss (for one batch) at step 30: 270.8595, Accuracy: 0.7777\n",
      "Training loss (for one batch) at step 40: 274.2520, Accuracy: 0.8028\n",
      "Training loss (for one batch) at step 50: 273.3688, Accuracy: 0.8186\n",
      "Training loss (for one batch) at step 60: 303.2403, Accuracy: 0.8265\n",
      "Training loss (for one batch) at step 70: 329.8068, Accuracy: 0.8148\n",
      "Training loss (for one batch) at step 80: 308.2799, Accuracy: 0.8008\n",
      "Training loss (for one batch) at step 90: 277.3171, Accuracy: 0.7989\n",
      "Training loss (for one batch) at step 100: 295.7114, Accuracy: 0.8025\n",
      "Training loss (for one batch) at step 110: 318.5004, Accuracy: 0.8046\n",
      "---- Training ----\n",
      "Training loss: 107.3068\n",
      "Training acc over epoch: 0.8031\n",
      "---- Validation ----\n",
      "Validation loss: 29.5622\n",
      "Validation acc: 0.7418\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 333.6448, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 327.4128, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 20: 297.3017, Accuracy: 0.7414\n",
      "Training loss (for one batch) at step 30: 284.3133, Accuracy: 0.7790\n",
      "Training loss (for one batch) at step 40: 278.6421, Accuracy: 0.7999\n",
      "Training loss (for one batch) at step 50: 278.8284, Accuracy: 0.8197\n",
      "Training loss (for one batch) at step 60: 316.9095, Accuracy: 0.8285\n",
      "Training loss (for one batch) at step 70: 331.6191, Accuracy: 0.8190\n",
      "Training loss (for one batch) at step 80: 326.9004, Accuracy: 0.8039\n",
      "Training loss (for one batch) at step 90: 287.2891, Accuracy: 0.7997\n",
      "Training loss (for one batch) at step 100: 284.0483, Accuracy: 0.8036\n",
      "Training loss (for one batch) at step 110: 315.5754, Accuracy: 0.8042\n",
      "---- Training ----\n",
      "Training loss: 97.3625\n",
      "Training acc over epoch: 0.8035\n",
      "---- Validation ----\n",
      "Validation loss: 40.5269\n",
      "Validation acc: 0.7383\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 303.6599, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 336.7717, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 20: 289.0825, Accuracy: 0.7507\n",
      "Training loss (for one batch) at step 30: 290.6483, Accuracy: 0.7835\n",
      "Training loss (for one batch) at step 40: 292.3242, Accuracy: 0.8034\n",
      "Training loss (for one batch) at step 50: 274.7614, Accuracy: 0.8205\n",
      "Training loss (for one batch) at step 60: 312.4691, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 70: 321.4974, Accuracy: 0.8179\n",
      "Training loss (for one batch) at step 80: 303.2748, Accuracy: 0.8049\n",
      "Training loss (for one batch) at step 90: 294.2295, Accuracy: 0.8011\n",
      "Training loss (for one batch) at step 100: 281.2130, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 110: 313.9837, Accuracy: 0.8062\n",
      "---- Training ----\n",
      "Training loss: 101.4213\n",
      "Training acc over epoch: 0.8045\n",
      "---- Validation ----\n",
      "Validation loss: 68.2801\n",
      "Validation acc: 0.7461\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 324.3083, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 319.4888, Accuracy: 0.7010\n",
      "Training loss (for one batch) at step 20: 283.2625, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 30: 279.9193, Accuracy: 0.7850\n",
      "Training loss (for one batch) at step 40: 272.8622, Accuracy: 0.8072\n",
      "Training loss (for one batch) at step 50: 281.1266, Accuracy: 0.8228\n",
      "Training loss (for one batch) at step 60: 282.1951, Accuracy: 0.8300\n",
      "Training loss (for one batch) at step 70: 317.0296, Accuracy: 0.8206\n",
      "Training loss (for one batch) at step 80: 311.1874, Accuracy: 0.8039\n",
      "Training loss (for one batch) at step 90: 296.6590, Accuracy: 0.8013\n",
      "Training loss (for one batch) at step 100: 277.4453, Accuracy: 0.8053\n",
      "Training loss (for one batch) at step 110: 292.3491, Accuracy: 0.8057\n",
      "---- Training ----\n",
      "Training loss: 95.5047\n",
      "Training acc over epoch: 0.8043\n",
      "---- Validation ----\n",
      "Validation loss: 45.3210\n",
      "Validation acc: 0.7442\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 335.8032, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 320.4199, Accuracy: 0.6896\n",
      "Training loss (for one batch) at step 20: 286.2625, Accuracy: 0.7459\n",
      "Training loss (for one batch) at step 30: 267.2868, Accuracy: 0.7828\n",
      "Training loss (for one batch) at step 40: 299.9158, Accuracy: 0.8051\n",
      "Training loss (for one batch) at step 50: 274.3109, Accuracy: 0.8215\n",
      "Training loss (for one batch) at step 60: 305.6703, Accuracy: 0.8285\n",
      "Training loss (for one batch) at step 70: 311.8660, Accuracy: 0.8165\n",
      "Training loss (for one batch) at step 80: 313.7434, Accuracy: 0.8010\n",
      "Training loss (for one batch) at step 90: 283.1220, Accuracy: 0.7976\n",
      "Training loss (for one batch) at step 100: 277.9100, Accuracy: 0.8014\n",
      "Training loss (for one batch) at step 110: 302.2091, Accuracy: 0.8041\n",
      "---- Training ----\n",
      "Training loss: 93.1158\n",
      "Training acc over epoch: 0.8031\n",
      "---- Validation ----\n",
      "Validation loss: 75.7447\n",
      "Validation acc: 0.7456\n",
      "Time taken: 10.60s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 336.1517, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 317.1141, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 20: 280.8378, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 30: 266.6136, Accuracy: 0.7772\n",
      "Training loss (for one batch) at step 40: 298.3951, Accuracy: 0.8037\n",
      "Training loss (for one batch) at step 50: 265.7727, Accuracy: 0.8199\n",
      "Training loss (for one batch) at step 60: 311.1656, Accuracy: 0.8280\n",
      "Training loss (for one batch) at step 70: 332.2114, Accuracy: 0.8162\n",
      "Training loss (for one batch) at step 80: 315.7427, Accuracy: 0.7998\n",
      "Training loss (for one batch) at step 90: 284.4869, Accuracy: 0.7958\n",
      "Training loss (for one batch) at step 100: 275.7693, Accuracy: 0.8028\n",
      "Training loss (for one batch) at step 110: 311.6603, Accuracy: 0.8038\n",
      "---- Training ----\n",
      "Training loss: 89.0633\n",
      "Training acc over epoch: 0.8035\n",
      "---- Validation ----\n",
      "Validation loss: 37.5068\n",
      "Validation acc: 0.7480\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 328.3517, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 317.0201, Accuracy: 0.6946\n",
      "Training loss (for one batch) at step 20: 293.0748, Accuracy: 0.7396\n",
      "Training loss (for one batch) at step 30: 281.3470, Accuracy: 0.7795\n",
      "Training loss (for one batch) at step 40: 257.2774, Accuracy: 0.7997\n",
      "Training loss (for one batch) at step 50: 275.7279, Accuracy: 0.8165\n",
      "Training loss (for one batch) at step 60: 293.5458, Accuracy: 0.8258\n",
      "Training loss (for one batch) at step 70: 318.0434, Accuracy: 0.8129\n",
      "Training loss (for one batch) at step 80: 313.3861, Accuracy: 0.7994\n",
      "Training loss (for one batch) at step 90: 302.9471, Accuracy: 0.7973\n",
      "Training loss (for one batch) at step 100: 274.0581, Accuracy: 0.8014\n",
      "Training loss (for one batch) at step 110: 296.7616, Accuracy: 0.8043\n",
      "---- Training ----\n",
      "Training loss: 107.7169\n",
      "Training acc over epoch: 0.8023\n",
      "---- Validation ----\n",
      "Validation loss: 34.5323\n",
      "Validation acc: 0.7458\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 338.2964, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 325.7885, Accuracy: 0.7010\n",
      "Training loss (for one batch) at step 20: 289.6576, Accuracy: 0.7381\n",
      "Training loss (for one batch) at step 30: 281.1081, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 40: 272.4599, Accuracy: 0.7995\n",
      "Training loss (for one batch) at step 50: 267.2539, Accuracy: 0.8194\n",
      "Training loss (for one batch) at step 60: 290.6364, Accuracy: 0.8256\n",
      "Training loss (for one batch) at step 70: 307.9156, Accuracy: 0.8140\n",
      "Training loss (for one batch) at step 80: 309.1342, Accuracy: 0.7983\n",
      "Training loss (for one batch) at step 90: 296.7628, Accuracy: 0.7964\n",
      "Training loss (for one batch) at step 100: 277.2852, Accuracy: 0.7994\n",
      "Training loss (for one batch) at step 110: 313.5172, Accuracy: 0.8021\n",
      "---- Training ----\n",
      "Training loss: 85.7579\n",
      "Training acc over epoch: 0.8017\n",
      "---- Validation ----\n",
      "Validation loss: 38.0863\n",
      "Validation acc: 0.7474\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 342.2605, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 315.6487, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 20: 289.5224, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 30: 263.3683, Accuracy: 0.7805\n",
      "Training loss (for one batch) at step 40: 253.7037, Accuracy: 0.8062\n",
      "Training loss (for one batch) at step 50: 284.3892, Accuracy: 0.8225\n",
      "Training loss (for one batch) at step 60: 277.5130, Accuracy: 0.8293\n",
      "Training loss (for one batch) at step 70: 317.4676, Accuracy: 0.8146\n",
      "Training loss (for one batch) at step 80: 313.7016, Accuracy: 0.8012\n",
      "Training loss (for one batch) at step 90: 286.5760, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 100: 277.0219, Accuracy: 0.8014\n",
      "Training loss (for one batch) at step 110: 290.0892, Accuracy: 0.8020\n",
      "---- Training ----\n",
      "Training loss: 95.9163\n",
      "Training acc over epoch: 0.8004\n",
      "---- Validation ----\n",
      "Validation loss: 41.7850\n",
      "Validation acc: 0.7415\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 322.7098, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 310.1710, Accuracy: 0.7067\n",
      "Training loss (for one batch) at step 20: 276.9113, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 30: 273.7822, Accuracy: 0.7800\n",
      "Training loss (for one batch) at step 40: 277.7033, Accuracy: 0.8011\n",
      "Training loss (for one batch) at step 50: 263.5946, Accuracy: 0.8183\n",
      "Training loss (for one batch) at step 60: 292.7178, Accuracy: 0.8268\n",
      "Training loss (for one batch) at step 70: 340.0602, Accuracy: 0.8158\n",
      "Training loss (for one batch) at step 80: 332.7238, Accuracy: 0.8017\n",
      "Training loss (for one batch) at step 90: 275.1629, Accuracy: 0.7963\n",
      "Training loss (for one batch) at step 100: 290.1815, Accuracy: 0.8010\n",
      "Training loss (for one batch) at step 110: 278.5799, Accuracy: 0.8029\n",
      "---- Training ----\n",
      "Training loss: 106.6613\n",
      "Training acc over epoch: 0.8030\n",
      "---- Validation ----\n",
      "Validation loss: 53.9643\n",
      "Validation acc: 0.7415\n",
      "Time taken: 10.58s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 331.8000, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 303.5180, Accuracy: 0.6996\n",
      "Training loss (for one batch) at step 20: 262.4161, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 30: 279.6540, Accuracy: 0.7802\n",
      "Training loss (for one batch) at step 40: 279.0447, Accuracy: 0.8053\n",
      "Training loss (for one batch) at step 50: 264.3275, Accuracy: 0.8200\n",
      "Training loss (for one batch) at step 60: 290.3361, Accuracy: 0.8256\n",
      "Training loss (for one batch) at step 70: 299.3080, Accuracy: 0.8157\n",
      "Training loss (for one batch) at step 80: 317.3680, Accuracy: 0.8007\n",
      "Training loss (for one batch) at step 90: 296.9316, Accuracy: 0.7973\n",
      "Training loss (for one batch) at step 100: 262.1047, Accuracy: 0.8027\n",
      "Training loss (for one batch) at step 110: 303.8808, Accuracy: 0.8028\n",
      "---- Training ----\n",
      "Training loss: 106.5868\n",
      "Training acc over epoch: 0.8014\n",
      "---- Validation ----\n",
      "Validation loss: 46.5649\n",
      "Validation acc: 0.7413\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 332.2287, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 310.9881, Accuracy: 0.7045\n",
      "Training loss (for one batch) at step 20: 276.5242, Accuracy: 0.7385\n",
      "Training loss (for one batch) at step 30: 282.4054, Accuracy: 0.7747\n",
      "Training loss (for one batch) at step 40: 290.6066, Accuracy: 0.7967\n",
      "Training loss (for one batch) at step 50: 259.7306, Accuracy: 0.8143\n",
      "Training loss (for one batch) at step 60: 304.0452, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 70: 294.6927, Accuracy: 0.8068\n",
      "Training loss (for one batch) at step 80: 304.2653, Accuracy: 0.7954\n",
      "Training loss (for one batch) at step 90: 256.3739, Accuracy: 0.7936\n",
      "Training loss (for one batch) at step 100: 265.2051, Accuracy: 0.7983\n",
      "Training loss (for one batch) at step 110: 305.8372, Accuracy: 0.7992\n",
      "---- Training ----\n",
      "Training loss: 98.3578\n",
      "Training acc over epoch: 0.7985\n",
      "---- Validation ----\n",
      "Validation loss: 46.7187\n",
      "Validation acc: 0.7415\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 336.2506, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 308.3629, Accuracy: 0.6903\n",
      "Training loss (for one batch) at step 20: 277.5585, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 30: 280.3659, Accuracy: 0.7742\n",
      "Training loss (for one batch) at step 40: 280.1965, Accuracy: 0.7973\n",
      "Training loss (for one batch) at step 50: 271.5714, Accuracy: 0.8143\n",
      "Training loss (for one batch) at step 60: 292.0975, Accuracy: 0.8218\n",
      "Training loss (for one batch) at step 70: 287.7038, Accuracy: 0.8119\n",
      "Training loss (for one batch) at step 80: 321.3442, Accuracy: 0.7992\n",
      "Training loss (for one batch) at step 90: 296.7021, Accuracy: 0.7957\n",
      "Training loss (for one batch) at step 100: 285.0655, Accuracy: 0.7999\n",
      "Training loss (for one batch) at step 110: 304.2191, Accuracy: 0.8020\n",
      "---- Training ----\n",
      "Training loss: 88.4779\n",
      "Training acc over epoch: 0.8010\n",
      "---- Validation ----\n",
      "Validation loss: 45.8286\n",
      "Validation acc: 0.7380\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 328.3120, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 306.5084, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 20: 286.3039, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 30: 258.7278, Accuracy: 0.7780\n",
      "Training loss (for one batch) at step 40: 269.2410, Accuracy: 0.8009\n",
      "Training loss (for one batch) at step 50: 274.1464, Accuracy: 0.8174\n",
      "Training loss (for one batch) at step 60: 270.9241, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 70: 311.7172, Accuracy: 0.8165\n",
      "Training loss (for one batch) at step 80: 322.1934, Accuracy: 0.8031\n",
      "Training loss (for one batch) at step 90: 294.1607, Accuracy: 0.7981\n",
      "Training loss (for one batch) at step 100: 283.7810, Accuracy: 0.8016\n",
      "Training loss (for one batch) at step 110: 288.6135, Accuracy: 0.8035\n",
      "---- Training ----\n",
      "Training loss: 81.1429\n",
      "Training acc over epoch: 0.8025\n",
      "---- Validation ----\n",
      "Validation loss: 52.9024\n",
      "Validation acc: 0.7544\n",
      "Time taken: 10.89s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 329.5367, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 305.0557, Accuracy: 0.6825\n",
      "Training loss (for one batch) at step 20: 310.3626, Accuracy: 0.7396\n",
      "Training loss (for one batch) at step 30: 262.4395, Accuracy: 0.7787\n",
      "Training loss (for one batch) at step 40: 281.5668, Accuracy: 0.8058\n",
      "Training loss (for one batch) at step 50: 257.4645, Accuracy: 0.8214\n",
      "Training loss (for one batch) at step 60: 300.0913, Accuracy: 0.8300\n",
      "Training loss (for one batch) at step 70: 335.3110, Accuracy: 0.8192\n",
      "Training loss (for one batch) at step 80: 326.9029, Accuracy: 0.8023\n",
      "Training loss (for one batch) at step 90: 280.5335, Accuracy: 0.8012\n",
      "Training loss (for one batch) at step 100: 282.6062, Accuracy: 0.8037\n",
      "Training loss (for one batch) at step 110: 274.6823, Accuracy: 0.8051\n",
      "---- Training ----\n",
      "Training loss: 104.4939\n",
      "Training acc over epoch: 0.8037\n",
      "---- Validation ----\n",
      "Validation loss: 48.7318\n",
      "Validation acc: 0.7354\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 328.1311, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 319.8647, Accuracy: 0.6939\n",
      "Training loss (for one batch) at step 20: 271.1712, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 30: 271.9761, Accuracy: 0.7797\n",
      "Training loss (for one batch) at step 40: 284.3185, Accuracy: 0.8001\n",
      "Training loss (for one batch) at step 50: 283.4602, Accuracy: 0.8166\n",
      "Training loss (for one batch) at step 60: 286.0602, Accuracy: 0.8249\n",
      "Training loss (for one batch) at step 70: 307.3364, Accuracy: 0.8116\n",
      "Training loss (for one batch) at step 80: 297.9275, Accuracy: 0.7987\n",
      "Training loss (for one batch) at step 90: 300.0885, Accuracy: 0.7958\n",
      "Training loss (for one batch) at step 100: 272.9542, Accuracy: 0.8004\n",
      "Training loss (for one batch) at step 110: 299.6570, Accuracy: 0.8029\n",
      "---- Training ----\n",
      "Training loss: 97.7185\n",
      "Training acc over epoch: 0.8028\n",
      "---- Validation ----\n",
      "Validation loss: 50.6165\n",
      "Validation acc: 0.7359\n",
      "Time taken: 10.51s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 315.2917, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 298.5685, Accuracy: 0.6960\n",
      "Training loss (for one batch) at step 20: 295.1233, Accuracy: 0.7381\n",
      "Training loss (for one batch) at step 30: 267.2996, Accuracy: 0.7792\n",
      "Training loss (for one batch) at step 40: 280.8458, Accuracy: 0.8020\n",
      "Training loss (for one batch) at step 50: 279.7691, Accuracy: 0.8180\n",
      "Training loss (for one batch) at step 60: 286.6537, Accuracy: 0.8252\n",
      "Training loss (for one batch) at step 70: 299.7859, Accuracy: 0.8134\n",
      "Training loss (for one batch) at step 80: 315.3620, Accuracy: 0.7991\n",
      "Training loss (for one batch) at step 90: 288.4766, Accuracy: 0.7964\n",
      "Training loss (for one batch) at step 100: 273.1201, Accuracy: 0.8011\n",
      "Training loss (for one batch) at step 110: 275.4181, Accuracy: 0.8013\n",
      "---- Training ----\n",
      "Training loss: 118.7263\n",
      "Training acc over epoch: 0.7986\n",
      "---- Validation ----\n",
      "Validation loss: 51.6036\n",
      "Validation acc: 0.7423\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 340.7784, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 311.2529, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 20: 265.3020, Accuracy: 0.7366\n",
      "Training loss (for one batch) at step 30: 263.8486, Accuracy: 0.7800\n",
      "Training loss (for one batch) at step 40: 267.0086, Accuracy: 0.8018\n",
      "Training loss (for one batch) at step 50: 257.1904, Accuracy: 0.8195\n",
      "Training loss (for one batch) at step 60: 269.6887, Accuracy: 0.8285\n",
      "Training loss (for one batch) at step 70: 310.5528, Accuracy: 0.8162\n",
      "Training loss (for one batch) at step 80: 301.9521, Accuracy: 0.8029\n",
      "Training loss (for one batch) at step 90: 266.6595, Accuracy: 0.7990\n",
      "Training loss (for one batch) at step 100: 290.8502, Accuracy: 0.8031\n",
      "Training loss (for one batch) at step 110: 300.3568, Accuracy: 0.8037\n",
      "---- Training ----\n",
      "Training loss: 98.9900\n",
      "Training acc over epoch: 0.8033\n",
      "---- Validation ----\n",
      "Validation loss: 63.8028\n",
      "Validation acc: 0.7362\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 320.8690, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 319.8458, Accuracy: 0.6847\n",
      "Training loss (for one batch) at step 20: 299.9080, Accuracy: 0.7336\n",
      "Training loss (for one batch) at step 30: 254.7611, Accuracy: 0.7752\n",
      "Training loss (for one batch) at step 40: 266.6012, Accuracy: 0.7994\n",
      "Training loss (for one batch) at step 50: 275.5185, Accuracy: 0.8162\n",
      "Training loss (for one batch) at step 60: 280.5567, Accuracy: 0.8266\n",
      "Training loss (for one batch) at step 70: 306.7359, Accuracy: 0.8148\n",
      "Training loss (for one batch) at step 80: 320.0780, Accuracy: 0.7993\n",
      "Training loss (for one batch) at step 90: 272.1501, Accuracy: 0.7950\n",
      "Training loss (for one batch) at step 100: 296.5125, Accuracy: 0.8000\n",
      "Training loss (for one batch) at step 110: 286.3170, Accuracy: 0.8005\n",
      "---- Training ----\n",
      "Training loss: 94.9830\n",
      "Training acc over epoch: 0.8008\n",
      "---- Validation ----\n",
      "Validation loss: 42.5898\n",
      "Validation acc: 0.7397\n",
      "Time taken: 10.40s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 315.9525, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 340.5719, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 20: 294.3743, Accuracy: 0.7407\n",
      "Training loss (for one batch) at step 30: 279.7581, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 40: 267.6224, Accuracy: 0.8037\n",
      "Training loss (for one batch) at step 50: 257.5767, Accuracy: 0.8217\n",
      "Training loss (for one batch) at step 60: 264.9042, Accuracy: 0.8293\n",
      "Training loss (for one batch) at step 70: 316.8685, Accuracy: 0.8176\n",
      "Training loss (for one batch) at step 80: 305.9614, Accuracy: 0.8041\n",
      "Training loss (for one batch) at step 90: 278.5554, Accuracy: 0.8010\n",
      "Training loss (for one batch) at step 100: 281.9685, Accuracy: 0.8056\n",
      "Training loss (for one batch) at step 110: 301.6464, Accuracy: 0.8063\n",
      "---- Training ----\n",
      "Training loss: 88.4860\n",
      "Training acc over epoch: 0.8051\n",
      "---- Validation ----\n",
      "Validation loss: 59.5562\n",
      "Validation acc: 0.7413\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 316.0011, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 310.1355, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 20: 284.6736, Accuracy: 0.7455\n",
      "Training loss (for one batch) at step 30: 260.0188, Accuracy: 0.7795\n",
      "Training loss (for one batch) at step 40: 274.4429, Accuracy: 0.8030\n",
      "Training loss (for one batch) at step 50: 255.6140, Accuracy: 0.8186\n",
      "Training loss (for one batch) at step 60: 277.9576, Accuracy: 0.8263\n",
      "Training loss (for one batch) at step 70: 310.2355, Accuracy: 0.8136\n",
      "Training loss (for one batch) at step 80: 299.9178, Accuracy: 0.8003\n",
      "Training loss (for one batch) at step 90: 281.5887, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 100: 263.8326, Accuracy: 0.8019\n",
      "Training loss (for one batch) at step 110: 302.6449, Accuracy: 0.8036\n",
      "---- Training ----\n",
      "Training loss: 108.0782\n",
      "Training acc over epoch: 0.8025\n",
      "---- Validation ----\n",
      "Validation loss: 58.9343\n",
      "Validation acc: 0.7329\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 323.5285, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 287.5500, Accuracy: 0.6946\n",
      "Training loss (for one batch) at step 20: 271.4108, Accuracy: 0.7440\n",
      "Training loss (for one batch) at step 30: 268.9681, Accuracy: 0.7795\n",
      "Training loss (for one batch) at step 40: 264.8957, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 50: 260.5919, Accuracy: 0.8197\n",
      "Training loss (for one batch) at step 60: 288.5392, Accuracy: 0.8262\n",
      "Training loss (for one batch) at step 70: 290.1668, Accuracy: 0.8147\n",
      "Training loss (for one batch) at step 80: 306.4018, Accuracy: 0.7984\n",
      "Training loss (for one batch) at step 90: 289.1289, Accuracy: 0.7979\n",
      "Training loss (for one batch) at step 100: 305.6364, Accuracy: 0.8011\n",
      "Training loss (for one batch) at step 110: 277.2857, Accuracy: 0.8025\n",
      "---- Training ----\n",
      "Training loss: 97.8012\n",
      "Training acc over epoch: 0.8015\n",
      "---- Validation ----\n",
      "Validation loss: 66.5939\n",
      "Validation acc: 0.7356\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 322.7524, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 335.8496, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 20: 263.0221, Accuracy: 0.7455\n",
      "Training loss (for one batch) at step 30: 260.4128, Accuracy: 0.7818\n",
      "Training loss (for one batch) at step 40: 266.9872, Accuracy: 0.8066\n",
      "Training loss (for one batch) at step 50: 256.8811, Accuracy: 0.8199\n",
      "Training loss (for one batch) at step 60: 290.1439, Accuracy: 0.8275\n",
      "Training loss (for one batch) at step 70: 295.7121, Accuracy: 0.8180\n",
      "Training loss (for one batch) at step 80: 309.3846, Accuracy: 0.8025\n",
      "Training loss (for one batch) at step 90: 281.7596, Accuracy: 0.7982\n",
      "Training loss (for one batch) at step 100: 260.6600, Accuracy: 0.8020\n",
      "Training loss (for one batch) at step 110: 281.5193, Accuracy: 0.8039\n",
      "---- Training ----\n",
      "Training loss: 93.5494\n",
      "Training acc over epoch: 0.8018\n",
      "---- Validation ----\n",
      "Validation loss: 50.1231\n",
      "Validation acc: 0.7367\n",
      "Time taken: 10.58s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 325.3382, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 329.9810, Accuracy: 0.6903\n",
      "Training loss (for one batch) at step 20: 264.5768, Accuracy: 0.7459\n",
      "Training loss (for one batch) at step 30: 279.2447, Accuracy: 0.7828\n",
      "Training loss (for one batch) at step 40: 262.7960, Accuracy: 0.8026\n",
      "Training loss (for one batch) at step 50: 256.9947, Accuracy: 0.8191\n",
      "Training loss (for one batch) at step 60: 281.2942, Accuracy: 0.8289\n",
      "Training loss (for one batch) at step 70: 300.1661, Accuracy: 0.8167\n",
      "Training loss (for one batch) at step 80: 332.5603, Accuracy: 0.8024\n",
      "Training loss (for one batch) at step 90: 270.4160, Accuracy: 0.7979\n",
      "Training loss (for one batch) at step 100: 279.1708, Accuracy: 0.8024\n",
      "Training loss (for one batch) at step 110: 283.4911, Accuracy: 0.8040\n",
      "---- Training ----\n",
      "Training loss: 91.3934\n",
      "Training acc over epoch: 0.8025\n",
      "---- Validation ----\n",
      "Validation loss: 60.4185\n",
      "Validation acc: 0.7389\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 312.9143, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 317.2151, Accuracy: 0.6754\n",
      "Training loss (for one batch) at step 20: 274.5921, Accuracy: 0.7355\n",
      "Training loss (for one batch) at step 30: 281.8996, Accuracy: 0.7732\n",
      "Training loss (for one batch) at step 40: 258.9020, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 50: 274.0288, Accuracy: 0.8148\n",
      "Training loss (for one batch) at step 60: 281.6002, Accuracy: 0.8236\n",
      "Training loss (for one batch) at step 70: 302.0233, Accuracy: 0.8135\n",
      "Training loss (for one batch) at step 80: 308.6129, Accuracy: 0.7975\n",
      "Training loss (for one batch) at step 90: 276.6855, Accuracy: 0.7921\n",
      "Training loss (for one batch) at step 100: 283.2159, Accuracy: 0.7973\n",
      "Training loss (for one batch) at step 110: 286.2082, Accuracy: 0.7993\n",
      "---- Training ----\n",
      "Training loss: 96.3362\n",
      "Training acc over epoch: 0.7984\n",
      "---- Validation ----\n",
      "Validation loss: 72.8982\n",
      "Validation acc: 0.7375\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 336.6929, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 322.2551, Accuracy: 0.7116\n",
      "Training loss (for one batch) at step 20: 304.7129, Accuracy: 0.7455\n",
      "Training loss (for one batch) at step 30: 271.7618, Accuracy: 0.7830\n",
      "Training loss (for one batch) at step 40: 281.7377, Accuracy: 0.8054\n",
      "Training loss (for one batch) at step 50: 265.4731, Accuracy: 0.8226\n",
      "Training loss (for one batch) at step 60: 296.1927, Accuracy: 0.8303\n",
      "Training loss (for one batch) at step 70: 322.3080, Accuracy: 0.8177\n",
      "Training loss (for one batch) at step 80: 328.6183, Accuracy: 0.8003\n",
      "Training loss (for one batch) at step 90: 290.4259, Accuracy: 0.7970\n",
      "Training loss (for one batch) at step 100: 280.8853, Accuracy: 0.8023\n",
      "Training loss (for one batch) at step 110: 288.2774, Accuracy: 0.8024\n",
      "---- Training ----\n",
      "Training loss: 90.8380\n",
      "Training acc over epoch: 0.8020\n",
      "---- Validation ----\n",
      "Validation loss: 54.5057\n",
      "Validation acc: 0.7380\n",
      "Time taken: 10.45s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.7369\n",
      "Validation AUC: 0.7374\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 629.2925, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 557.9579, Accuracy: 0.5057\n",
      "Training loss (for one batch) at step 20: 524.8480, Accuracy: 0.5160\n",
      "Training loss (for one batch) at step 30: 524.8962, Accuracy: 0.5219\n",
      "Training loss (for one batch) at step 40: 499.3623, Accuracy: 0.5183\n",
      "Training loss (for one batch) at step 50: 493.0009, Accuracy: 0.5153\n",
      "Training loss (for one batch) at step 60: 483.7955, Accuracy: 0.5193\n",
      "Training loss (for one batch) at step 70: 473.2819, Accuracy: 0.5235\n",
      "Training loss (for one batch) at step 80: 472.7347, Accuracy: 0.5235\n",
      "Training loss (for one batch) at step 90: 478.7380, Accuracy: 0.5223\n",
      "Training loss (for one batch) at step 100: 461.1266, Accuracy: 0.5208\n",
      "Training loss (for one batch) at step 110: 468.1353, Accuracy: 0.5189\n",
      "---- Training ----\n",
      "Training loss: 140.2579\n",
      "Training acc over epoch: 0.5189\n",
      "---- Validation ----\n",
      "Validation loss: 34.8064\n",
      "Validation acc: 0.5126\n",
      "Time taken: 12.06s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 458.4024, Accuracy: 0.3906\n",
      "Training loss (for one batch) at step 10: 464.3074, Accuracy: 0.5043\n",
      "Training loss (for one batch) at step 20: 458.8056, Accuracy: 0.5186\n",
      "Training loss (for one batch) at step 30: 455.3539, Accuracy: 0.5181\n",
      "Training loss (for one batch) at step 40: 458.6855, Accuracy: 0.5166\n",
      "Training loss (for one batch) at step 50: 456.2657, Accuracy: 0.5205\n",
      "Training loss (for one batch) at step 60: 450.6127, Accuracy: 0.5211\n",
      "Training loss (for one batch) at step 70: 451.0721, Accuracy: 0.5217\n",
      "Training loss (for one batch) at step 80: 444.0799, Accuracy: 0.5256\n",
      "Training loss (for one batch) at step 90: 451.5800, Accuracy: 0.5257\n",
      "Training loss (for one batch) at step 100: 454.4475, Accuracy: 0.5243\n",
      "Training loss (for one batch) at step 110: 447.0590, Accuracy: 0.5234\n",
      "---- Training ----\n",
      "Training loss: 137.8573\n",
      "Training acc over epoch: 0.5234\n",
      "---- Validation ----\n",
      "Validation loss: 34.6912\n",
      "Validation acc: 0.5126\n",
      "Time taken: 10.57s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 449.3524, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 446.4058, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 20: 444.9581, Accuracy: 0.5253\n",
      "Training loss (for one batch) at step 30: 445.8473, Accuracy: 0.5260\n",
      "Training loss (for one batch) at step 40: 446.8369, Accuracy: 0.5215\n",
      "Training loss (for one batch) at step 50: 448.6711, Accuracy: 0.5213\n",
      "Training loss (for one batch) at step 60: 445.4833, Accuracy: 0.5219\n",
      "Training loss (for one batch) at step 70: 444.9652, Accuracy: 0.5277\n",
      "Training loss (for one batch) at step 80: 445.5915, Accuracy: 0.5313\n",
      "Training loss (for one batch) at step 90: 448.7524, Accuracy: 0.5335\n",
      "Training loss (for one batch) at step 100: 443.9382, Accuracy: 0.5333\n",
      "Training loss (for one batch) at step 110: 444.8300, Accuracy: 0.5348\n",
      "---- Training ----\n",
      "Training loss: 138.3990\n",
      "Training acc over epoch: 0.5368\n",
      "---- Validation ----\n",
      "Validation loss: 34.6191\n",
      "Validation acc: 0.5043\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 444.8668, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 443.6456, Accuracy: 0.5170\n",
      "Training loss (for one batch) at step 20: 443.1465, Accuracy: 0.5301\n",
      "Training loss (for one batch) at step 30: 444.9884, Accuracy: 0.5396\n",
      "Training loss (for one batch) at step 40: 444.6555, Accuracy: 0.5446\n",
      "Training loss (for one batch) at step 50: 444.9899, Accuracy: 0.5460\n",
      "Training loss (for one batch) at step 60: 444.1717, Accuracy: 0.5457\n",
      "Training loss (for one batch) at step 70: 447.1819, Accuracy: 0.5476\n",
      "Training loss (for one batch) at step 80: 447.9799, Accuracy: 0.5489\n",
      "Training loss (for one batch) at step 90: 442.2536, Accuracy: 0.5505\n",
      "Training loss (for one batch) at step 100: 444.6983, Accuracy: 0.5511\n",
      "Training loss (for one batch) at step 110: 442.4474, Accuracy: 0.5551\n",
      "---- Training ----\n",
      "Training loss: 139.5090\n",
      "Training acc over epoch: 0.5558\n",
      "---- Validation ----\n",
      "Validation loss: 34.5435\n",
      "Validation acc: 0.5484\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 444.2450, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 442.2637, Accuracy: 0.5611\n",
      "Training loss (for one batch) at step 20: 443.2705, Accuracy: 0.5737\n",
      "Training loss (for one batch) at step 30: 440.3846, Accuracy: 0.5834\n",
      "Training loss (for one batch) at step 40: 440.6920, Accuracy: 0.5817\n",
      "Training loss (for one batch) at step 50: 443.1863, Accuracy: 0.5775\n",
      "Training loss (for one batch) at step 60: 445.0903, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 70: 444.9257, Accuracy: 0.5827\n",
      "Training loss (for one batch) at step 80: 443.1120, Accuracy: 0.5824\n",
      "Training loss (for one batch) at step 90: 443.4894, Accuracy: 0.5796\n",
      "Training loss (for one batch) at step 100: 444.3403, Accuracy: 0.5774\n",
      "Training loss (for one batch) at step 110: 442.8053, Accuracy: 0.5762\n",
      "---- Training ----\n",
      "Training loss: 138.9796\n",
      "Training acc over epoch: 0.5762\n",
      "---- Validation ----\n",
      "Validation loss: 34.8418\n",
      "Validation acc: 0.6136\n",
      "Time taken: 10.56s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 442.3901, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 445.1166, Accuracy: 0.5888\n",
      "Training loss (for one batch) at step 20: 440.2382, Accuracy: 0.5911\n",
      "Training loss (for one batch) at step 30: 443.3181, Accuracy: 0.5932\n",
      "Training loss (for one batch) at step 40: 439.2634, Accuracy: 0.5964\n",
      "Training loss (for one batch) at step 50: 440.5889, Accuracy: 0.5982\n",
      "Training loss (for one batch) at step 60: 443.4805, Accuracy: 0.5982\n",
      "Training loss (for one batch) at step 70: 442.3737, Accuracy: 0.6020\n",
      "Training loss (for one batch) at step 80: 443.2337, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 90: 442.7560, Accuracy: 0.6010\n",
      "Training loss (for one batch) at step 100: 441.6671, Accuracy: 0.5982\n",
      "Training loss (for one batch) at step 110: 444.0129, Accuracy: 0.5953\n",
      "---- Training ----\n",
      "Training loss: 139.9339\n",
      "Training acc over epoch: 0.5956\n",
      "---- Validation ----\n",
      "Validation loss: 34.7799\n",
      "Validation acc: 0.6247\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 440.8351, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 440.9833, Accuracy: 0.6108\n",
      "Training loss (for one batch) at step 20: 445.3183, Accuracy: 0.6053\n",
      "Training loss (for one batch) at step 30: 437.9144, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 40: 443.4435, Accuracy: 0.6214\n",
      "Training loss (for one batch) at step 50: 436.8772, Accuracy: 0.6212\n",
      "Training loss (for one batch) at step 60: 436.5637, Accuracy: 0.6249\n",
      "Training loss (for one batch) at step 70: 442.6001, Accuracy: 0.6264\n",
      "Training loss (for one batch) at step 80: 439.2817, Accuracy: 0.6264\n",
      "Training loss (for one batch) at step 90: 442.8784, Accuracy: 0.6274\n",
      "Training loss (for one batch) at step 100: 441.1084, Accuracy: 0.6251\n",
      "Training loss (for one batch) at step 110: 444.3923, Accuracy: 0.6234\n",
      "---- Training ----\n",
      "Training loss: 137.4552\n",
      "Training acc over epoch: 0.6213\n",
      "---- Validation ----\n",
      "Validation loss: 34.6221\n",
      "Validation acc: 0.5940\n",
      "Time taken: 11.03s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 443.1875, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 441.2407, Accuracy: 0.6520\n",
      "Training loss (for one batch) at step 20: 442.5842, Accuracy: 0.6469\n",
      "Training loss (for one batch) at step 30: 433.3692, Accuracy: 0.6472\n",
      "Training loss (for one batch) at step 40: 437.4371, Accuracy: 0.6467\n",
      "Training loss (for one batch) at step 50: 431.7495, Accuracy: 0.6472\n",
      "Training loss (for one batch) at step 60: 440.7765, Accuracy: 0.6516\n",
      "Training loss (for one batch) at step 70: 441.3454, Accuracy: 0.6531\n",
      "Training loss (for one batch) at step 80: 444.9252, Accuracy: 0.6490\n",
      "Training loss (for one batch) at step 90: 439.7266, Accuracy: 0.6479\n",
      "Training loss (for one batch) at step 100: 442.8875, Accuracy: 0.6448\n",
      "Training loss (for one batch) at step 110: 443.1675, Accuracy: 0.6417\n",
      "---- Training ----\n",
      "Training loss: 137.4407\n",
      "Training acc over epoch: 0.6419\n",
      "---- Validation ----\n",
      "Validation loss: 34.9122\n",
      "Validation acc: 0.6658\n",
      "Time taken: 10.61s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 442.7562, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 442.3542, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 20: 438.9561, Accuracy: 0.6395\n",
      "Training loss (for one batch) at step 30: 438.4417, Accuracy: 0.6411\n",
      "Training loss (for one batch) at step 40: 438.2562, Accuracy: 0.6427\n",
      "Training loss (for one batch) at step 50: 432.8044, Accuracy: 0.6464\n",
      "Training loss (for one batch) at step 60: 431.5298, Accuracy: 0.6527\n",
      "Training loss (for one batch) at step 70: 443.9577, Accuracy: 0.6561\n",
      "Training loss (for one batch) at step 80: 442.6439, Accuracy: 0.6553\n",
      "Training loss (for one batch) at step 90: 438.4363, Accuracy: 0.6527\n",
      "Training loss (for one batch) at step 100: 437.5496, Accuracy: 0.6517\n",
      "Training loss (for one batch) at step 110: 445.6214, Accuracy: 0.6515\n",
      "---- Training ----\n",
      "Training loss: 140.9131\n",
      "Training acc over epoch: 0.6514\n",
      "---- Validation ----\n",
      "Validation loss: 34.2241\n",
      "Validation acc: 0.6771\n",
      "Time taken: 39.35s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 439.2530, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 440.7382, Accuracy: 0.6527\n",
      "Training loss (for one batch) at step 20: 438.4907, Accuracy: 0.6529\n",
      "Training loss (for one batch) at step 30: 438.6001, Accuracy: 0.6578\n",
      "Training loss (for one batch) at step 40: 436.3520, Accuracy: 0.6614\n",
      "Training loss (for one batch) at step 50: 436.5583, Accuracy: 0.6705\n",
      "Training loss (for one batch) at step 60: 439.4531, Accuracy: 0.6733\n",
      "Training loss (for one batch) at step 70: 443.2489, Accuracy: 0.6776\n",
      "Training loss (for one batch) at step 80: 440.8116, Accuracy: 0.6751\n",
      "Training loss (for one batch) at step 90: 436.4785, Accuracy: 0.6736\n",
      "Training loss (for one batch) at step 100: 437.9000, Accuracy: 0.6717\n",
      "Training loss (for one batch) at step 110: 444.0310, Accuracy: 0.6714\n",
      "---- Training ----\n",
      "Training loss: 137.6603\n",
      "Training acc over epoch: 0.6719\n",
      "---- Validation ----\n",
      "Validation loss: 34.7428\n",
      "Validation acc: 0.6671\n",
      "Time taken: 10.97s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 442.3928, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 442.7094, Accuracy: 0.6548\n",
      "Training loss (for one batch) at step 20: 439.0540, Accuracy: 0.6589\n",
      "Training loss (for one batch) at step 30: 433.2823, Accuracy: 0.6724\n",
      "Training loss (for one batch) at step 40: 444.5986, Accuracy: 0.6751\n",
      "Training loss (for one batch) at step 50: 428.4842, Accuracy: 0.6832\n",
      "Training loss (for one batch) at step 60: 435.7633, Accuracy: 0.6908\n",
      "Training loss (for one batch) at step 70: 437.5875, Accuracy: 0.6952\n",
      "Training loss (for one batch) at step 80: 444.7821, Accuracy: 0.6943\n",
      "Training loss (for one batch) at step 90: 441.5278, Accuracy: 0.6876\n",
      "Training loss (for one batch) at step 100: 440.2452, Accuracy: 0.6846\n",
      "Training loss (for one batch) at step 110: 439.5056, Accuracy: 0.6862\n",
      "---- Training ----\n",
      "Training loss: 141.2625\n",
      "Training acc over epoch: 0.6867\n",
      "---- Validation ----\n",
      "Validation loss: 33.8945\n",
      "Validation acc: 0.6983\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 447.1912, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 438.6019, Accuracy: 0.6939\n",
      "Training loss (for one batch) at step 20: 434.1904, Accuracy: 0.6905\n",
      "Training loss (for one batch) at step 30: 429.6224, Accuracy: 0.6981\n",
      "Training loss (for one batch) at step 40: 438.9432, Accuracy: 0.7026\n",
      "Training loss (for one batch) at step 50: 412.3134, Accuracy: 0.7083\n",
      "Training loss (for one batch) at step 60: 430.9623, Accuracy: 0.7131\n",
      "Training loss (for one batch) at step 70: 444.8557, Accuracy: 0.7099\n",
      "Training loss (for one batch) at step 80: 441.2028, Accuracy: 0.7070\n",
      "Training loss (for one batch) at step 90: 438.9887, Accuracy: 0.7041\n",
      "Training loss (for one batch) at step 100: 441.1670, Accuracy: 0.7039\n",
      "Training loss (for one batch) at step 110: 430.0174, Accuracy: 0.7062\n",
      "---- Training ----\n",
      "Training loss: 138.0041\n",
      "Training acc over epoch: 0.7065\n",
      "---- Validation ----\n",
      "Validation loss: 35.0756\n",
      "Validation acc: 0.6784\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 437.7598, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 439.2892, Accuracy: 0.7074\n",
      "Training loss (for one batch) at step 20: 434.6942, Accuracy: 0.7139\n",
      "Training loss (for one batch) at step 30: 429.7881, Accuracy: 0.7185\n",
      "Training loss (for one batch) at step 40: 420.1307, Accuracy: 0.7224\n",
      "Training loss (for one batch) at step 50: 426.0554, Accuracy: 0.7310\n",
      "Training loss (for one batch) at step 60: 429.7415, Accuracy: 0.7354\n",
      "Training loss (for one batch) at step 70: 437.7068, Accuracy: 0.7346\n",
      "Training loss (for one batch) at step 80: 436.2921, Accuracy: 0.7316\n",
      "Training loss (for one batch) at step 90: 433.7338, Accuracy: 0.7282\n",
      "Training loss (for one batch) at step 100: 421.7459, Accuracy: 0.7304\n",
      "Training loss (for one batch) at step 110: 435.1974, Accuracy: 0.7319\n",
      "---- Training ----\n",
      "Training loss: 132.2011\n",
      "Training acc over epoch: 0.7335\n",
      "---- Validation ----\n",
      "Validation loss: 34.3288\n",
      "Validation acc: 0.7469\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 446.8946, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 436.3884, Accuracy: 0.7408\n",
      "Training loss (for one batch) at step 20: 434.9744, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 30: 429.6814, Accuracy: 0.7384\n",
      "Training loss (for one batch) at step 40: 428.0299, Accuracy: 0.7441\n",
      "Training loss (for one batch) at step 50: 423.4934, Accuracy: 0.7472\n",
      "Training loss (for one batch) at step 60: 428.6582, Accuracy: 0.7490\n",
      "Training loss (for one batch) at step 70: 434.5653, Accuracy: 0.7502\n",
      "Training loss (for one batch) at step 80: 435.1038, Accuracy: 0.7486\n",
      "Training loss (for one batch) at step 90: 438.5643, Accuracy: 0.7426\n",
      "Training loss (for one batch) at step 100: 433.0827, Accuracy: 0.7426\n",
      "Training loss (for one batch) at step 110: 426.7206, Accuracy: 0.7420\n",
      "---- Training ----\n",
      "Training loss: 138.5632\n",
      "Training acc over epoch: 0.7432\n",
      "---- Validation ----\n",
      "Validation loss: 34.7628\n",
      "Validation acc: 0.7370\n",
      "Time taken: 10.66s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 441.3495, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 435.6806, Accuracy: 0.7372\n",
      "Training loss (for one batch) at step 20: 436.7424, Accuracy: 0.7307\n",
      "Training loss (for one batch) at step 30: 424.3746, Accuracy: 0.7354\n",
      "Training loss (for one batch) at step 40: 414.0922, Accuracy: 0.7416\n",
      "Training loss (for one batch) at step 50: 407.0049, Accuracy: 0.7549\n",
      "Training loss (for one batch) at step 60: 421.7409, Accuracy: 0.7602\n",
      "Training loss (for one batch) at step 70: 435.2013, Accuracy: 0.7622\n",
      "Training loss (for one batch) at step 80: 437.0473, Accuracy: 0.7583\n",
      "Training loss (for one batch) at step 90: 431.0161, Accuracy: 0.7544\n",
      "Training loss (for one batch) at step 100: 420.4170, Accuracy: 0.7539\n",
      "Training loss (for one batch) at step 110: 422.7280, Accuracy: 0.7533\n",
      "---- Training ----\n",
      "Training loss: 134.5757\n",
      "Training acc over epoch: 0.7540\n",
      "---- Validation ----\n",
      "Validation loss: 33.8042\n",
      "Validation acc: 0.7512\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 439.9071, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 437.3188, Accuracy: 0.7827\n",
      "Training loss (for one batch) at step 20: 431.1909, Accuracy: 0.7589\n",
      "Training loss (for one batch) at step 30: 427.1705, Accuracy: 0.7611\n",
      "Training loss (for one batch) at step 40: 423.2364, Accuracy: 0.7662\n",
      "Training loss (for one batch) at step 50: 445.1031, Accuracy: 0.7714\n",
      "Training loss (for one batch) at step 60: 415.1118, Accuracy: 0.7713\n",
      "Training loss (for one batch) at step 70: 427.4876, Accuracy: 0.7698\n",
      "Training loss (for one batch) at step 80: 432.3941, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 90: 436.4782, Accuracy: 0.7606\n",
      "Training loss (for one batch) at step 100: 422.8771, Accuracy: 0.7616\n",
      "Training loss (for one batch) at step 110: 424.6700, Accuracy: 0.7641\n",
      "---- Training ----\n",
      "Training loss: 135.5308\n",
      "Training acc over epoch: 0.7641\n",
      "---- Validation ----\n",
      "Validation loss: 34.5290\n",
      "Validation acc: 0.7389\n",
      "Time taken: 10.58s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 440.8532, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 427.5327, Accuracy: 0.7642\n",
      "Training loss (for one batch) at step 20: 430.8913, Accuracy: 0.7567\n",
      "Training loss (for one batch) at step 30: 416.8997, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 40: 416.0076, Accuracy: 0.7702\n",
      "Training loss (for one batch) at step 50: 403.0379, Accuracy: 0.7794\n",
      "Training loss (for one batch) at step 60: 429.3719, Accuracy: 0.7847\n",
      "Training loss (for one batch) at step 70: 431.4718, Accuracy: 0.7801\n",
      "Training loss (for one batch) at step 80: 433.3354, Accuracy: 0.7780\n",
      "Training loss (for one batch) at step 90: 431.2338, Accuracy: 0.7746\n",
      "Training loss (for one batch) at step 100: 428.4990, Accuracy: 0.7738\n",
      "Training loss (for one batch) at step 110: 430.5741, Accuracy: 0.7744\n",
      "---- Training ----\n",
      "Training loss: 132.6170\n",
      "Training acc over epoch: 0.7762\n",
      "---- Validation ----\n",
      "Validation loss: 32.7400\n",
      "Validation acc: 0.7566\n",
      "Time taken: 10.57s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 441.7448, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 424.3575, Accuracy: 0.7571\n",
      "Training loss (for one batch) at step 20: 430.4147, Accuracy: 0.7615\n",
      "Training loss (for one batch) at step 30: 416.8310, Accuracy: 0.7702\n",
      "Training loss (for one batch) at step 40: 397.5802, Accuracy: 0.7776\n",
      "Training loss (for one batch) at step 50: 406.9271, Accuracy: 0.7857\n",
      "Training loss (for one batch) at step 60: 411.8661, Accuracy: 0.7900\n",
      "Training loss (for one batch) at step 70: 437.3621, Accuracy: 0.7899\n",
      "Training loss (for one batch) at step 80: 434.2493, Accuracy: 0.7839\n",
      "Training loss (for one batch) at step 90: 430.2056, Accuracy: 0.7824\n",
      "Training loss (for one batch) at step 100: 416.6875, Accuracy: 0.7824\n",
      "Training loss (for one batch) at step 110: 410.2111, Accuracy: 0.7826\n",
      "---- Training ----\n",
      "Training loss: 135.6792\n",
      "Training acc over epoch: 0.7814\n",
      "---- Validation ----\n",
      "Validation loss: 33.0806\n",
      "Validation acc: 0.7544\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 438.6094, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 425.3355, Accuracy: 0.7869\n",
      "Training loss (for one batch) at step 20: 413.7794, Accuracy: 0.7798\n",
      "Training loss (for one batch) at step 30: 416.7026, Accuracy: 0.7896\n",
      "Training loss (for one batch) at step 40: 407.1843, Accuracy: 0.7997\n",
      "Training loss (for one batch) at step 50: 396.9211, Accuracy: 0.8044\n",
      "Training loss (for one batch) at step 60: 410.9786, Accuracy: 0.8067\n",
      "Training loss (for one batch) at step 70: 424.4668, Accuracy: 0.8093\n",
      "Training loss (for one batch) at step 80: 433.6514, Accuracy: 0.8050\n",
      "Training loss (for one batch) at step 90: 424.4563, Accuracy: 0.8026\n",
      "Training loss (for one batch) at step 100: 422.4733, Accuracy: 0.8002\n",
      "Training loss (for one batch) at step 110: 421.4606, Accuracy: 0.7972\n",
      "---- Training ----\n",
      "Training loss: 131.6442\n",
      "Training acc over epoch: 0.7965\n",
      "---- Validation ----\n",
      "Validation loss: 35.9924\n",
      "Validation acc: 0.7875\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 445.8287, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 428.8378, Accuracy: 0.8082\n",
      "Training loss (for one batch) at step 20: 419.3220, Accuracy: 0.7850\n",
      "Training loss (for one batch) at step 30: 409.2374, Accuracy: 0.7886\n",
      "Training loss (for one batch) at step 40: 403.9572, Accuracy: 0.7984\n",
      "Training loss (for one batch) at step 50: 394.1818, Accuracy: 0.8045\n",
      "Training loss (for one batch) at step 60: 411.4112, Accuracy: 0.8106\n",
      "Training loss (for one batch) at step 70: 440.0049, Accuracy: 0.8072\n",
      "Training loss (for one batch) at step 80: 428.3734, Accuracy: 0.8013\n",
      "Training loss (for one batch) at step 90: 412.5226, Accuracy: 0.7970\n",
      "Training loss (for one batch) at step 100: 414.4855, Accuracy: 0.7935\n",
      "Training loss (for one batch) at step 110: 421.7122, Accuracy: 0.7933\n",
      "---- Training ----\n",
      "Training loss: 130.1930\n",
      "Training acc over epoch: 0.7939\n",
      "---- Validation ----\n",
      "Validation loss: 36.4863\n",
      "Validation acc: 0.7399\n",
      "Time taken: 10.61s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 433.0700, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 417.4540, Accuracy: 0.7820\n",
      "Training loss (for one batch) at step 20: 425.0782, Accuracy: 0.7775\n",
      "Training loss (for one batch) at step 30: 407.3717, Accuracy: 0.7838\n",
      "Training loss (for one batch) at step 40: 410.3981, Accuracy: 0.7921\n",
      "Training loss (for one batch) at step 50: 396.9716, Accuracy: 0.8030\n",
      "Training loss (for one batch) at step 60: 400.1906, Accuracy: 0.8093\n",
      "Training loss (for one batch) at step 70: 416.8309, Accuracy: 0.8117\n",
      "Training loss (for one batch) at step 80: 426.0059, Accuracy: 0.8077\n",
      "Training loss (for one batch) at step 90: 419.9871, Accuracy: 0.8039\n",
      "Training loss (for one batch) at step 100: 401.1799, Accuracy: 0.8023\n",
      "Training loss (for one batch) at step 110: 422.5130, Accuracy: 0.8020\n",
      "---- Training ----\n",
      "Training loss: 129.7071\n",
      "Training acc over epoch: 0.8006\n",
      "---- Validation ----\n",
      "Validation loss: 34.7400\n",
      "Validation acc: 0.7547\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 434.6199, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 421.0279, Accuracy: 0.7912\n",
      "Training loss (for one batch) at step 20: 413.8446, Accuracy: 0.7928\n",
      "Training loss (for one batch) at step 30: 409.7251, Accuracy: 0.7996\n",
      "Training loss (for one batch) at step 40: 404.4630, Accuracy: 0.8037\n",
      "Training loss (for one batch) at step 50: 383.8113, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 60: 410.6731, Accuracy: 0.8167\n",
      "Training loss (for one batch) at step 70: 409.5114, Accuracy: 0.8156\n",
      "Training loss (for one batch) at step 80: 436.5925, Accuracy: 0.8114\n",
      "Training loss (for one batch) at step 90: 421.0262, Accuracy: 0.8054\n",
      "Training loss (for one batch) at step 100: 407.5494, Accuracy: 0.8042\n",
      "Training loss (for one batch) at step 110: 405.5770, Accuracy: 0.8027\n",
      "---- Training ----\n",
      "Training loss: 129.3311\n",
      "Training acc over epoch: 0.8029\n",
      "---- Validation ----\n",
      "Validation loss: 30.8066\n",
      "Validation acc: 0.7577\n",
      "Time taken: 10.56s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 429.1476, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 406.4127, Accuracy: 0.7869\n",
      "Training loss (for one batch) at step 20: 415.9801, Accuracy: 0.7876\n",
      "Training loss (for one batch) at step 30: 410.5153, Accuracy: 0.7979\n",
      "Training loss (for one batch) at step 40: 406.3148, Accuracy: 0.8020\n",
      "Training loss (for one batch) at step 50: 372.0985, Accuracy: 0.8108\n",
      "Training loss (for one batch) at step 60: 396.5464, Accuracy: 0.8174\n",
      "Training loss (for one batch) at step 70: 404.8211, Accuracy: 0.8179\n",
      "Training loss (for one batch) at step 80: 427.4193, Accuracy: 0.8149\n",
      "Training loss (for one batch) at step 90: 413.4784, Accuracy: 0.8117\n",
      "Training loss (for one batch) at step 100: 407.0218, Accuracy: 0.8096\n",
      "Training loss (for one batch) at step 110: 411.6515, Accuracy: 0.8075\n",
      "---- Training ----\n",
      "Training loss: 134.4237\n",
      "Training acc over epoch: 0.8063\n",
      "---- Validation ----\n",
      "Validation loss: 37.8903\n",
      "Validation acc: 0.7622\n",
      "Time taken: 10.81s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 433.0249, Accuracy: 0.8594\n",
      "Training loss (for one batch) at step 10: 407.1171, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 20: 403.4696, Accuracy: 0.7984\n",
      "Training loss (for one batch) at step 30: 401.8755, Accuracy: 0.8007\n",
      "Training loss (for one batch) at step 40: 390.8413, Accuracy: 0.8108\n",
      "Training loss (for one batch) at step 50: 382.1375, Accuracy: 0.8199\n",
      "Training loss (for one batch) at step 60: 387.7150, Accuracy: 0.8247\n",
      "Training loss (for one batch) at step 70: 399.2159, Accuracy: 0.8249\n",
      "Training loss (for one batch) at step 80: 435.4668, Accuracy: 0.8215\n",
      "Training loss (for one batch) at step 90: 396.7335, Accuracy: 0.8196\n",
      "Training loss (for one batch) at step 100: 402.8903, Accuracy: 0.8185\n",
      "Training loss (for one batch) at step 110: 398.8898, Accuracy: 0.8186\n",
      "---- Training ----\n",
      "Training loss: 132.5710\n",
      "Training acc over epoch: 0.8176\n",
      "---- Validation ----\n",
      "Validation loss: 33.4532\n",
      "Validation acc: 0.7671\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 406.6084, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 416.9176, Accuracy: 0.7990\n",
      "Training loss (for one batch) at step 20: 402.2711, Accuracy: 0.7958\n",
      "Training loss (for one batch) at step 30: 392.7813, Accuracy: 0.8017\n",
      "Training loss (for one batch) at step 40: 388.4109, Accuracy: 0.8085\n",
      "Training loss (for one batch) at step 50: 385.3086, Accuracy: 0.8191\n",
      "Training loss (for one batch) at step 60: 389.4204, Accuracy: 0.8286\n",
      "Training loss (for one batch) at step 70: 400.8265, Accuracy: 0.8286\n",
      "Training loss (for one batch) at step 80: 394.7330, Accuracy: 0.8248\n",
      "Training loss (for one batch) at step 90: 411.3975, Accuracy: 0.8218\n",
      "Training loss (for one batch) at step 100: 409.9025, Accuracy: 0.8213\n",
      "Training loss (for one batch) at step 110: 402.7754, Accuracy: 0.8204\n",
      "---- Training ----\n",
      "Training loss: 124.1650\n",
      "Training acc over epoch: 0.8188\n",
      "---- Validation ----\n",
      "Validation loss: 36.4922\n",
      "Validation acc: 0.7802\n",
      "Time taken: 10.56s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 414.5906, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 413.9244, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 20: 395.0827, Accuracy: 0.8058\n",
      "Training loss (for one batch) at step 30: 374.6462, Accuracy: 0.8100\n",
      "Training loss (for one batch) at step 40: 372.6926, Accuracy: 0.8207\n",
      "Training loss (for one batch) at step 50: 361.0565, Accuracy: 0.8286\n",
      "Training loss (for one batch) at step 60: 383.1741, Accuracy: 0.8352\n",
      "Training loss (for one batch) at step 70: 422.4869, Accuracy: 0.8337\n",
      "Training loss (for one batch) at step 80: 412.0662, Accuracy: 0.8327\n",
      "Training loss (for one batch) at step 90: 399.8748, Accuracy: 0.8279\n",
      "Training loss (for one batch) at step 100: 405.2222, Accuracy: 0.8269\n",
      "Training loss (for one batch) at step 110: 386.6572, Accuracy: 0.8255\n",
      "---- Training ----\n",
      "Training loss: 132.6418\n",
      "Training acc over epoch: 0.8242\n",
      "---- Validation ----\n",
      "Validation loss: 39.5820\n",
      "Validation acc: 0.7754\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 419.8274, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 402.4167, Accuracy: 0.8161\n",
      "Training loss (for one batch) at step 20: 393.5651, Accuracy: 0.8062\n",
      "Training loss (for one batch) at step 30: 379.5734, Accuracy: 0.8130\n",
      "Training loss (for one batch) at step 40: 373.8301, Accuracy: 0.8226\n",
      "Training loss (for one batch) at step 50: 370.3286, Accuracy: 0.8300\n",
      "Training loss (for one batch) at step 60: 382.3438, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 70: 397.0652, Accuracy: 0.8324\n",
      "Training loss (for one batch) at step 80: 421.5532, Accuracy: 0.8291\n",
      "Training loss (for one batch) at step 90: 405.8672, Accuracy: 0.8256\n",
      "Training loss (for one batch) at step 100: 384.4963, Accuracy: 0.8257\n",
      "Training loss (for one batch) at step 110: 391.1115, Accuracy: 0.8263\n",
      "---- Training ----\n",
      "Training loss: 131.0986\n",
      "Training acc over epoch: 0.8260\n",
      "---- Validation ----\n",
      "Validation loss: 43.3947\n",
      "Validation acc: 0.7630\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 421.7972, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 394.8847, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 20: 390.2021, Accuracy: 0.8010\n",
      "Training loss (for one batch) at step 30: 387.1533, Accuracy: 0.8138\n",
      "Training loss (for one batch) at step 40: 375.9214, Accuracy: 0.8247\n",
      "Training loss (for one batch) at step 50: 365.4292, Accuracy: 0.8318\n",
      "Training loss (for one batch) at step 60: 385.9710, Accuracy: 0.8385\n",
      "Training loss (for one batch) at step 70: 396.2591, Accuracy: 0.8362\n",
      "Training loss (for one batch) at step 80: 408.7896, Accuracy: 0.8340\n",
      "Training loss (for one batch) at step 90: 397.9618, Accuracy: 0.8312\n",
      "Training loss (for one batch) at step 100: 387.9574, Accuracy: 0.8308\n",
      "Training loss (for one batch) at step 110: 397.6336, Accuracy: 0.8304\n",
      "---- Training ----\n",
      "Training loss: 123.8481\n",
      "Training acc over epoch: 0.8296\n",
      "---- Validation ----\n",
      "Validation loss: 37.3344\n",
      "Validation acc: 0.7778\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 413.9171, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 408.9719, Accuracy: 0.7997\n",
      "Training loss (for one batch) at step 20: 384.5200, Accuracy: 0.8106\n",
      "Training loss (for one batch) at step 30: 381.0639, Accuracy: 0.8155\n",
      "Training loss (for one batch) at step 40: 373.3268, Accuracy: 0.8258\n",
      "Training loss (for one batch) at step 50: 363.6980, Accuracy: 0.8344\n",
      "Training loss (for one batch) at step 60: 369.5522, Accuracy: 0.8421\n",
      "Training loss (for one batch) at step 70: 407.6980, Accuracy: 0.8386\n",
      "Training loss (for one batch) at step 80: 392.2922, Accuracy: 0.8346\n",
      "Training loss (for one batch) at step 90: 376.0623, Accuracy: 0.8319\n",
      "Training loss (for one batch) at step 100: 391.3013, Accuracy: 0.8284\n",
      "Training loss (for one batch) at step 110: 377.6813, Accuracy: 0.8285\n",
      "---- Training ----\n",
      "Training loss: 122.2641\n",
      "Training acc over epoch: 0.8283\n",
      "---- Validation ----\n",
      "Validation loss: 32.9636\n",
      "Validation acc: 0.7749\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 400.4700, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 376.7213, Accuracy: 0.8104\n",
      "Training loss (for one batch) at step 20: 379.7325, Accuracy: 0.8177\n",
      "Training loss (for one batch) at step 30: 375.9232, Accuracy: 0.8324\n",
      "Training loss (for one batch) at step 40: 352.5320, Accuracy: 0.8399\n",
      "Training loss (for one batch) at step 50: 352.4223, Accuracy: 0.8479\n",
      "Training loss (for one batch) at step 60: 359.2342, Accuracy: 0.8514\n",
      "Training loss (for one batch) at step 70: 407.9965, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 80: 398.2556, Accuracy: 0.8396\n",
      "Training loss (for one batch) at step 90: 380.7947, Accuracy: 0.8351\n",
      "Training loss (for one batch) at step 100: 405.2334, Accuracy: 0.8328\n",
      "Training loss (for one batch) at step 110: 377.9118, Accuracy: 0.8331\n",
      "---- Training ----\n",
      "Training loss: 126.1483\n",
      "Training acc over epoch: 0.8325\n",
      "---- Validation ----\n",
      "Validation loss: 34.6837\n",
      "Validation acc: 0.7684\n",
      "Time taken: 12.46s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 396.8321, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 388.9924, Accuracy: 0.7933\n",
      "Training loss (for one batch) at step 20: 366.1043, Accuracy: 0.8121\n",
      "Training loss (for one batch) at step 30: 371.7968, Accuracy: 0.8226\n",
      "Training loss (for one batch) at step 40: 368.6367, Accuracy: 0.8333\n",
      "Training loss (for one batch) at step 50: 356.6920, Accuracy: 0.8424\n",
      "Training loss (for one batch) at step 60: 374.9832, Accuracy: 0.8470\n",
      "Training loss (for one batch) at step 70: 404.8513, Accuracy: 0.8421\n",
      "Training loss (for one batch) at step 80: 412.1120, Accuracy: 0.8401\n",
      "Training loss (for one batch) at step 90: 394.6428, Accuracy: 0.8355\n",
      "Training loss (for one batch) at step 100: 375.8079, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 110: 391.0172, Accuracy: 0.8339\n",
      "---- Training ----\n",
      "Training loss: 122.7438\n",
      "Training acc over epoch: 0.8332\n",
      "---- Validation ----\n",
      "Validation loss: 33.9778\n",
      "Validation acc: 0.7724\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 397.4201, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 388.9189, Accuracy: 0.8011\n",
      "Training loss (for one batch) at step 20: 379.6269, Accuracy: 0.8136\n",
      "Training loss (for one batch) at step 30: 366.9065, Accuracy: 0.8306\n",
      "Training loss (for one batch) at step 40: 345.1762, Accuracy: 0.8407\n",
      "Training loss (for one batch) at step 50: 359.5880, Accuracy: 0.8470\n",
      "Training loss (for one batch) at step 60: 363.0264, Accuracy: 0.8499\n",
      "Training loss (for one batch) at step 70: 373.8430, Accuracy: 0.8479\n",
      "Training loss (for one batch) at step 80: 376.6717, Accuracy: 0.8412\n",
      "Training loss (for one batch) at step 90: 371.6917, Accuracy: 0.8369\n",
      "Training loss (for one batch) at step 100: 386.3067, Accuracy: 0.8366\n",
      "Training loss (for one batch) at step 110: 387.9044, Accuracy: 0.8351\n",
      "---- Training ----\n",
      "Training loss: 115.0437\n",
      "Training acc over epoch: 0.8348\n",
      "---- Validation ----\n",
      "Validation loss: 33.1269\n",
      "Validation acc: 0.7633\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 384.6773, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 388.2187, Accuracy: 0.8089\n",
      "Training loss (for one batch) at step 20: 371.8177, Accuracy: 0.8077\n",
      "Training loss (for one batch) at step 30: 362.5237, Accuracy: 0.8206\n",
      "Training loss (for one batch) at step 40: 350.9999, Accuracy: 0.8327\n",
      "Training loss (for one batch) at step 50: 353.4039, Accuracy: 0.8411\n",
      "Training loss (for one batch) at step 60: 373.0553, Accuracy: 0.8461\n",
      "Training loss (for one batch) at step 70: 413.7136, Accuracy: 0.8422\n",
      "Training loss (for one batch) at step 80: 408.1798, Accuracy: 0.8362\n",
      "Training loss (for one batch) at step 90: 366.6184, Accuracy: 0.8334\n",
      "Training loss (for one batch) at step 100: 356.7742, Accuracy: 0.8337\n",
      "Training loss (for one batch) at step 110: 379.5103, Accuracy: 0.8327\n",
      "---- Training ----\n",
      "Training loss: 122.9872\n",
      "Training acc over epoch: 0.8328\n",
      "---- Validation ----\n",
      "Validation loss: 37.1329\n",
      "Validation acc: 0.7665\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 381.6994, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 370.6458, Accuracy: 0.7990\n",
      "Training loss (for one batch) at step 20: 359.0797, Accuracy: 0.8147\n",
      "Training loss (for one batch) at step 30: 361.0197, Accuracy: 0.8294\n",
      "Training loss (for one batch) at step 40: 342.0625, Accuracy: 0.8405\n",
      "Training loss (for one batch) at step 50: 343.0002, Accuracy: 0.8482\n",
      "Training loss (for one batch) at step 60: 359.1562, Accuracy: 0.8534\n",
      "Training loss (for one batch) at step 70: 399.4112, Accuracy: 0.8489\n",
      "Training loss (for one batch) at step 80: 387.8218, Accuracy: 0.8441\n",
      "Training loss (for one batch) at step 90: 382.3190, Accuracy: 0.8405\n",
      "Training loss (for one batch) at step 100: 351.7748, Accuracy: 0.8396\n",
      "Training loss (for one batch) at step 110: 390.5741, Accuracy: 0.8388\n",
      "---- Training ----\n",
      "Training loss: 126.7959\n",
      "Training acc over epoch: 0.8373\n",
      "---- Validation ----\n",
      "Validation loss: 29.6015\n",
      "Validation acc: 0.7751\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 410.4472, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 378.4056, Accuracy: 0.8004\n",
      "Training loss (for one batch) at step 20: 353.2634, Accuracy: 0.8229\n",
      "Training loss (for one batch) at step 30: 361.2847, Accuracy: 0.8296\n",
      "Training loss (for one batch) at step 40: 343.3408, Accuracy: 0.8422\n",
      "Training loss (for one batch) at step 50: 337.8549, Accuracy: 0.8503\n",
      "Training loss (for one batch) at step 60: 363.3099, Accuracy: 0.8573\n",
      "Training loss (for one batch) at step 70: 387.3641, Accuracy: 0.8501\n",
      "Training loss (for one batch) at step 80: 362.4796, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 90: 355.5049, Accuracy: 0.8407\n",
      "Training loss (for one batch) at step 100: 354.2084, Accuracy: 0.8401\n",
      "Training loss (for one batch) at step 110: 373.1231, Accuracy: 0.8400\n",
      "---- Training ----\n",
      "Training loss: 120.6720\n",
      "Training acc over epoch: 0.8388\n",
      "---- Validation ----\n",
      "Validation loss: 37.4837\n",
      "Validation acc: 0.7714\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 414.3159, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 10: 380.4312, Accuracy: 0.8239\n",
      "Training loss (for one batch) at step 20: 354.3925, Accuracy: 0.8289\n",
      "Training loss (for one batch) at step 30: 359.6750, Accuracy: 0.8339\n",
      "Training loss (for one batch) at step 40: 343.3706, Accuracy: 0.8451\n",
      "Training loss (for one batch) at step 50: 333.7478, Accuracy: 0.8519\n",
      "Training loss (for one batch) at step 60: 338.3134, Accuracy: 0.8572\n",
      "Training loss (for one batch) at step 70: 378.4352, Accuracy: 0.8520\n",
      "Training loss (for one batch) at step 80: 358.4930, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 90: 371.9604, Accuracy: 0.8427\n",
      "Training loss (for one batch) at step 100: 351.8213, Accuracy: 0.8425\n",
      "Training loss (for one batch) at step 110: 357.9137, Accuracy: 0.8425\n",
      "---- Training ----\n",
      "Training loss: 115.6664\n",
      "Training acc over epoch: 0.8412\n",
      "---- Validation ----\n",
      "Validation loss: 48.2404\n",
      "Validation acc: 0.7719\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 397.7673, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 413.3360, Accuracy: 0.8111\n",
      "Training loss (for one batch) at step 20: 364.7263, Accuracy: 0.8222\n",
      "Training loss (for one batch) at step 30: 353.9558, Accuracy: 0.8289\n",
      "Training loss (for one batch) at step 40: 335.1501, Accuracy: 0.8399\n",
      "Training loss (for one batch) at step 50: 327.9038, Accuracy: 0.8502\n",
      "Training loss (for one batch) at step 60: 346.8223, Accuracy: 0.8554\n",
      "Training loss (for one batch) at step 70: 378.8347, Accuracy: 0.8516\n",
      "Training loss (for one batch) at step 80: 378.2103, Accuracy: 0.8435\n",
      "Training loss (for one batch) at step 90: 364.5234, Accuracy: 0.8396\n",
      "Training loss (for one batch) at step 100: 352.7124, Accuracy: 0.8393\n",
      "Training loss (for one batch) at step 110: 351.8397, Accuracy: 0.8393\n",
      "---- Training ----\n",
      "Training loss: 121.1527\n",
      "Training acc over epoch: 0.8392\n",
      "---- Validation ----\n",
      "Validation loss: 41.8408\n",
      "Validation acc: 0.7781\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 395.9374, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 376.4117, Accuracy: 0.8146\n",
      "Training loss (for one batch) at step 20: 368.6673, Accuracy: 0.8199\n",
      "Training loss (for one batch) at step 30: 346.1127, Accuracy: 0.8362\n",
      "Training loss (for one batch) at step 40: 355.9260, Accuracy: 0.8464\n",
      "Training loss (for one batch) at step 50: 336.9596, Accuracy: 0.8551\n",
      "Training loss (for one batch) at step 60: 356.9832, Accuracy: 0.8596\n",
      "Training loss (for one batch) at step 70: 372.4881, Accuracy: 0.8517\n",
      "Training loss (for one batch) at step 80: 391.2482, Accuracy: 0.8443\n",
      "Training loss (for one batch) at step 90: 352.2783, Accuracy: 0.8421\n",
      "Training loss (for one batch) at step 100: 333.8315, Accuracy: 0.8394\n",
      "Training loss (for one batch) at step 110: 353.5289, Accuracy: 0.8400\n",
      "---- Training ----\n",
      "Training loss: 114.5370\n",
      "Training acc over epoch: 0.8381\n",
      "---- Validation ----\n",
      "Validation loss: 32.0966\n",
      "Validation acc: 0.7692\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 388.7276, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 385.2378, Accuracy: 0.8111\n",
      "Training loss (for one batch) at step 20: 362.3148, Accuracy: 0.8114\n",
      "Training loss (for one batch) at step 30: 354.6067, Accuracy: 0.8311\n",
      "Training loss (for one batch) at step 40: 342.5201, Accuracy: 0.8426\n",
      "Training loss (for one batch) at step 50: 319.6900, Accuracy: 0.8522\n",
      "Training loss (for one batch) at step 60: 353.7440, Accuracy: 0.8534\n",
      "Training loss (for one batch) at step 70: 388.9344, Accuracy: 0.8493\n",
      "Training loss (for one batch) at step 80: 384.0307, Accuracy: 0.8418\n",
      "Training loss (for one batch) at step 90: 347.3014, Accuracy: 0.8414\n",
      "Training loss (for one batch) at step 100: 334.5866, Accuracy: 0.8425\n",
      "Training loss (for one batch) at step 110: 358.5533, Accuracy: 0.8416\n",
      "---- Training ----\n",
      "Training loss: 120.6896\n",
      "Training acc over epoch: 0.8401\n",
      "---- Validation ----\n",
      "Validation loss: 43.5065\n",
      "Validation acc: 0.7622\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 381.3909, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 387.5246, Accuracy: 0.8068\n",
      "Training loss (for one batch) at step 20: 347.9451, Accuracy: 0.8162\n",
      "Training loss (for one batch) at step 30: 342.2116, Accuracy: 0.8352\n",
      "Training loss (for one batch) at step 40: 336.3318, Accuracy: 0.8445\n",
      "Training loss (for one batch) at step 50: 336.8867, Accuracy: 0.8511\n",
      "Training loss (for one batch) at step 60: 329.7043, Accuracy: 0.8564\n",
      "Training loss (for one batch) at step 70: 388.1477, Accuracy: 0.8526\n",
      "Training loss (for one batch) at step 80: 363.2877, Accuracy: 0.8440\n",
      "Training loss (for one batch) at step 90: 356.0547, Accuracy: 0.8423\n",
      "Training loss (for one batch) at step 100: 318.4084, Accuracy: 0.8432\n",
      "Training loss (for one batch) at step 110: 352.0520, Accuracy: 0.8426\n",
      "---- Training ----\n",
      "Training loss: 123.2048\n",
      "Training acc over epoch: 0.8408\n",
      "---- Validation ----\n",
      "Validation loss: 49.1147\n",
      "Validation acc: 0.7749\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 380.6248, Accuracy: 0.8594\n",
      "Training loss (for one batch) at step 10: 369.1799, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 20: 344.5644, Accuracy: 0.8300\n",
      "Training loss (for one batch) at step 30: 344.5702, Accuracy: 0.8425\n",
      "Training loss (for one batch) at step 40: 341.9237, Accuracy: 0.8510\n",
      "Training loss (for one batch) at step 50: 331.6960, Accuracy: 0.8572\n",
      "Training loss (for one batch) at step 60: 355.3604, Accuracy: 0.8616\n",
      "Training loss (for one batch) at step 70: 359.4831, Accuracy: 0.8559\n",
      "Training loss (for one batch) at step 80: 363.7994, Accuracy: 0.8474\n",
      "Training loss (for one batch) at step 90: 336.6952, Accuracy: 0.8446\n",
      "Training loss (for one batch) at step 100: 328.6196, Accuracy: 0.8437\n",
      "Training loss (for one batch) at step 110: 357.5415, Accuracy: 0.8435\n",
      "---- Training ----\n",
      "Training loss: 102.8088\n",
      "Training acc over epoch: 0.8432\n",
      "---- Validation ----\n",
      "Validation loss: 40.1547\n",
      "Validation acc: 0.7797\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 395.0052, Accuracy: 0.8984\n",
      "Training loss (for one batch) at step 10: 343.7671, Accuracy: 0.8089\n",
      "Training loss (for one batch) at step 20: 350.0769, Accuracy: 0.8196\n",
      "Training loss (for one batch) at step 30: 346.6245, Accuracy: 0.8395\n",
      "Training loss (for one batch) at step 40: 347.5272, Accuracy: 0.8476\n",
      "Training loss (for one batch) at step 50: 321.8886, Accuracy: 0.8594\n",
      "Training loss (for one batch) at step 60: 343.1095, Accuracy: 0.8646\n",
      "Training loss (for one batch) at step 70: 368.0903, Accuracy: 0.8546\n",
      "Training loss (for one batch) at step 80: 367.6183, Accuracy: 0.8475\n",
      "Training loss (for one batch) at step 90: 336.6093, Accuracy: 0.8454\n",
      "Training loss (for one batch) at step 100: 347.5060, Accuracy: 0.8450\n",
      "Training loss (for one batch) at step 110: 350.3192, Accuracy: 0.8443\n",
      "---- Training ----\n",
      "Training loss: 121.4411\n",
      "Training acc over epoch: 0.8430\n",
      "---- Validation ----\n",
      "Validation loss: 39.4444\n",
      "Validation acc: 0.7706\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 397.6351, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 386.9395, Accuracy: 0.8033\n",
      "Training loss (for one batch) at step 20: 348.4077, Accuracy: 0.8158\n",
      "Training loss (for one batch) at step 30: 342.1786, Accuracy: 0.8319\n",
      "Training loss (for one batch) at step 40: 349.8098, Accuracy: 0.8451\n",
      "Training loss (for one batch) at step 50: 334.5524, Accuracy: 0.8523\n",
      "Training loss (for one batch) at step 60: 332.7847, Accuracy: 0.8564\n",
      "Training loss (for one batch) at step 70: 371.5689, Accuracy: 0.8526\n",
      "Training loss (for one batch) at step 80: 338.7982, Accuracy: 0.8447\n",
      "Training loss (for one batch) at step 90: 351.5140, Accuracy: 0.8413\n",
      "Training loss (for one batch) at step 100: 336.3513, Accuracy: 0.8426\n",
      "Training loss (for one batch) at step 110: 361.1636, Accuracy: 0.8447\n",
      "---- Training ----\n",
      "Training loss: 111.0261\n",
      "Training acc over epoch: 0.8426\n",
      "---- Validation ----\n",
      "Validation loss: 36.9482\n",
      "Validation acc: 0.7773\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 379.0788, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 382.1936, Accuracy: 0.8182\n",
      "Training loss (for one batch) at step 20: 340.6553, Accuracy: 0.8248\n",
      "Training loss (for one batch) at step 30: 314.2271, Accuracy: 0.8432\n",
      "Training loss (for one batch) at step 40: 312.8764, Accuracy: 0.8563\n",
      "Training loss (for one batch) at step 50: 329.4943, Accuracy: 0.8615\n",
      "Training loss (for one batch) at step 60: 364.5034, Accuracy: 0.8658\n",
      "Training loss (for one batch) at step 70: 349.7990, Accuracy: 0.8576\n",
      "Training loss (for one batch) at step 80: 373.5576, Accuracy: 0.8492\n",
      "Training loss (for one batch) at step 90: 328.6306, Accuracy: 0.8466\n",
      "Training loss (for one batch) at step 100: 335.8339, Accuracy: 0.8475\n",
      "Training loss (for one batch) at step 110: 348.1791, Accuracy: 0.8457\n",
      "---- Training ----\n",
      "Training loss: 114.6101\n",
      "Training acc over epoch: 0.8450\n",
      "---- Validation ----\n",
      "Validation loss: 60.0419\n",
      "Validation acc: 0.7904\n",
      "Time taken: 10.61s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 375.6793, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 363.0900, Accuracy: 0.7990\n",
      "Training loss (for one batch) at step 20: 344.0828, Accuracy: 0.8196\n",
      "Training loss (for one batch) at step 30: 337.5628, Accuracy: 0.8344\n",
      "Training loss (for one batch) at step 40: 324.5182, Accuracy: 0.8466\n",
      "Training loss (for one batch) at step 50: 333.3605, Accuracy: 0.8565\n",
      "Training loss (for one batch) at step 60: 333.7973, Accuracy: 0.8586\n",
      "Training loss (for one batch) at step 70: 348.2301, Accuracy: 0.8538\n",
      "Training loss (for one batch) at step 80: 368.2955, Accuracy: 0.8484\n",
      "Training loss (for one batch) at step 90: 346.5242, Accuracy: 0.8462\n",
      "Training loss (for one batch) at step 100: 329.5482, Accuracy: 0.8461\n",
      "Training loss (for one batch) at step 110: 340.3275, Accuracy: 0.8465\n",
      "---- Training ----\n",
      "Training loss: 110.8772\n",
      "Training acc over epoch: 0.8454\n",
      "---- Validation ----\n",
      "Validation loss: 33.8664\n",
      "Validation acc: 0.7789\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 355.5138, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 350.8963, Accuracy: 0.7976\n",
      "Training loss (for one batch) at step 20: 340.3011, Accuracy: 0.8173\n",
      "Training loss (for one batch) at step 30: 340.0513, Accuracy: 0.8344\n",
      "Training loss (for one batch) at step 40: 319.4149, Accuracy: 0.8458\n",
      "Training loss (for one batch) at step 50: 329.6898, Accuracy: 0.8539\n",
      "Training loss (for one batch) at step 60: 329.9941, Accuracy: 0.8589\n",
      "Training loss (for one batch) at step 70: 354.4106, Accuracy: 0.8560\n",
      "Training loss (for one batch) at step 80: 349.4402, Accuracy: 0.8481\n",
      "Training loss (for one batch) at step 90: 336.3451, Accuracy: 0.8458\n",
      "Training loss (for one batch) at step 100: 326.4639, Accuracy: 0.8462\n",
      "Training loss (for one batch) at step 110: 344.9386, Accuracy: 0.8464\n",
      "---- Training ----\n",
      "Training loss: 107.0172\n",
      "Training acc over epoch: 0.8453\n",
      "---- Validation ----\n",
      "Validation loss: 47.0874\n",
      "Validation acc: 0.7706\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 377.7652, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 10: 367.9280, Accuracy: 0.7955\n",
      "Training loss (for one batch) at step 20: 325.2541, Accuracy: 0.8118\n",
      "Training loss (for one batch) at step 30: 351.4788, Accuracy: 0.8352\n",
      "Training loss (for one batch) at step 40: 333.4619, Accuracy: 0.8476\n",
      "Training loss (for one batch) at step 50: 315.2042, Accuracy: 0.8578\n",
      "Training loss (for one batch) at step 60: 336.3767, Accuracy: 0.8617\n",
      "Training loss (for one batch) at step 70: 367.5278, Accuracy: 0.8574\n",
      "Training loss (for one batch) at step 80: 369.0773, Accuracy: 0.8491\n",
      "Training loss (for one batch) at step 90: 323.0676, Accuracy: 0.8444\n",
      "Training loss (for one batch) at step 100: 330.8317, Accuracy: 0.8456\n",
      "Training loss (for one batch) at step 110: 361.5958, Accuracy: 0.8454\n",
      "---- Training ----\n",
      "Training loss: 120.4115\n",
      "Training acc over epoch: 0.8436\n",
      "---- Validation ----\n",
      "Validation loss: 38.0331\n",
      "Validation acc: 0.7778\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 356.1037, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 370.1030, Accuracy: 0.8018\n",
      "Training loss (for one batch) at step 20: 345.2928, Accuracy: 0.8166\n",
      "Training loss (for one batch) at step 30: 324.3863, Accuracy: 0.8377\n",
      "Training loss (for one batch) at step 40: 316.9511, Accuracy: 0.8510\n",
      "Training loss (for one batch) at step 50: 316.7771, Accuracy: 0.8640\n",
      "Training loss (for one batch) at step 60: 339.2340, Accuracy: 0.8689\n",
      "Training loss (for one batch) at step 70: 361.7225, Accuracy: 0.8637\n",
      "Training loss (for one batch) at step 80: 365.3836, Accuracy: 0.8542\n",
      "Training loss (for one batch) at step 90: 332.8036, Accuracy: 0.8502\n",
      "Training loss (for one batch) at step 100: 327.7731, Accuracy: 0.8510\n",
      "Training loss (for one batch) at step 110: 333.5520, Accuracy: 0.8512\n",
      "---- Training ----\n",
      "Training loss: 119.3950\n",
      "Training acc over epoch: 0.8499\n",
      "---- Validation ----\n",
      "Validation loss: 41.9422\n",
      "Validation acc: 0.7759\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 361.9843, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 372.7298, Accuracy: 0.8153\n",
      "Training loss (for one batch) at step 20: 330.2184, Accuracy: 0.8318\n",
      "Training loss (for one batch) at step 30: 333.3202, Accuracy: 0.8407\n",
      "Training loss (for one batch) at step 40: 320.5811, Accuracy: 0.8550\n",
      "Training loss (for one batch) at step 50: 334.8440, Accuracy: 0.8623\n",
      "Training loss (for one batch) at step 60: 317.1738, Accuracy: 0.8680\n",
      "Training loss (for one batch) at step 70: 353.1488, Accuracy: 0.8600\n",
      "Training loss (for one batch) at step 80: 341.0904, Accuracy: 0.8531\n",
      "Training loss (for one batch) at step 90: 328.5776, Accuracy: 0.8495\n",
      "Training loss (for one batch) at step 100: 318.8946, Accuracy: 0.8506\n",
      "Training loss (for one batch) at step 110: 350.4149, Accuracy: 0.8507\n",
      "---- Training ----\n",
      "Training loss: 104.8333\n",
      "Training acc over epoch: 0.8498\n",
      "---- Validation ----\n",
      "Validation loss: 42.4276\n",
      "Validation acc: 0.7735\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 357.8082, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 363.1320, Accuracy: 0.7997\n",
      "Training loss (for one batch) at step 20: 321.3884, Accuracy: 0.8296\n",
      "Training loss (for one batch) at step 30: 324.1001, Accuracy: 0.8432\n",
      "Training loss (for one batch) at step 40: 320.5509, Accuracy: 0.8544\n",
      "Training loss (for one batch) at step 50: 316.5407, Accuracy: 0.8629\n",
      "Training loss (for one batch) at step 60: 307.8429, Accuracy: 0.8651\n",
      "Training loss (for one batch) at step 70: 347.2003, Accuracy: 0.8590\n",
      "Training loss (for one batch) at step 80: 351.4871, Accuracy: 0.8510\n",
      "Training loss (for one batch) at step 90: 319.7107, Accuracy: 0.8485\n",
      "Training loss (for one batch) at step 100: 333.1974, Accuracy: 0.8479\n",
      "Training loss (for one batch) at step 110: 347.4553, Accuracy: 0.8480\n",
      "---- Training ----\n",
      "Training loss: 104.5737\n",
      "Training acc over epoch: 0.8464\n",
      "---- Validation ----\n",
      "Validation loss: 54.3317\n",
      "Validation acc: 0.7797\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 354.0345, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 358.2661, Accuracy: 0.8104\n",
      "Training loss (for one batch) at step 20: 344.5027, Accuracy: 0.8292\n",
      "Training loss (for one batch) at step 30: 319.2724, Accuracy: 0.8420\n",
      "Training loss (for one batch) at step 40: 317.3838, Accuracy: 0.8519\n",
      "Training loss (for one batch) at step 50: 317.9788, Accuracy: 0.8620\n",
      "Training loss (for one batch) at step 60: 320.8814, Accuracy: 0.8650\n",
      "Training loss (for one batch) at step 70: 344.6672, Accuracy: 0.8578\n",
      "Training loss (for one batch) at step 80: 349.2682, Accuracy: 0.8510\n",
      "Training loss (for one batch) at step 90: 323.9710, Accuracy: 0.8498\n",
      "Training loss (for one batch) at step 100: 316.5821, Accuracy: 0.8514\n",
      "Training loss (for one batch) at step 110: 349.3825, Accuracy: 0.8504\n",
      "---- Training ----\n",
      "Training loss: 107.0061\n",
      "Training acc over epoch: 0.8491\n",
      "---- Validation ----\n",
      "Validation loss: 41.7908\n",
      "Validation acc: 0.7845\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 351.3859, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 362.8555, Accuracy: 0.8111\n",
      "Training loss (for one batch) at step 20: 316.3328, Accuracy: 0.8278\n",
      "Training loss (for one batch) at step 30: 326.4626, Accuracy: 0.8415\n",
      "Training loss (for one batch) at step 40: 297.2974, Accuracy: 0.8527\n",
      "Training loss (for one batch) at step 50: 308.3170, Accuracy: 0.8618\n",
      "Training loss (for one batch) at step 60: 308.7961, Accuracy: 0.8654\n",
      "Training loss (for one batch) at step 70: 309.3364, Accuracy: 0.8605\n",
      "Training loss (for one batch) at step 80: 365.1717, Accuracy: 0.8532\n",
      "Training loss (for one batch) at step 90: 332.1047, Accuracy: 0.8499\n",
      "Training loss (for one batch) at step 100: 324.0095, Accuracy: 0.8513\n",
      "Training loss (for one batch) at step 110: 349.8461, Accuracy: 0.8498\n",
      "---- Training ----\n",
      "Training loss: 109.1857\n",
      "Training acc over epoch: 0.8491\n",
      "---- Validation ----\n",
      "Validation loss: 39.2159\n",
      "Validation acc: 0.7687\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 366.6408, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 377.0486, Accuracy: 0.7905\n",
      "Training loss (for one batch) at step 20: 328.6895, Accuracy: 0.8177\n",
      "Training loss (for one batch) at step 30: 325.9589, Accuracy: 0.8372\n",
      "Training loss (for one batch) at step 40: 308.5416, Accuracy: 0.8470\n",
      "Training loss (for one batch) at step 50: 301.6581, Accuracy: 0.8552\n",
      "Training loss (for one batch) at step 60: 346.5650, Accuracy: 0.8608\n",
      "Training loss (for one batch) at step 70: 343.0355, Accuracy: 0.8560\n",
      "Training loss (for one batch) at step 80: 359.1452, Accuracy: 0.8508\n",
      "Training loss (for one batch) at step 90: 329.9694, Accuracy: 0.8468\n",
      "Training loss (for one batch) at step 100: 328.6726, Accuracy: 0.8482\n",
      "Training loss (for one batch) at step 110: 322.9477, Accuracy: 0.8481\n",
      "---- Training ----\n",
      "Training loss: 114.9710\n",
      "Training acc over epoch: 0.8472\n",
      "---- Validation ----\n",
      "Validation loss: 47.8668\n",
      "Validation acc: 0.7832\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 374.8905, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 353.6430, Accuracy: 0.7983\n",
      "Training loss (for one batch) at step 20: 336.4375, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 30: 317.5970, Accuracy: 0.8422\n",
      "Training loss (for one batch) at step 40: 306.2366, Accuracy: 0.8546\n",
      "Training loss (for one batch) at step 50: 309.1057, Accuracy: 0.8634\n",
      "Training loss (for one batch) at step 60: 320.6757, Accuracy: 0.8673\n",
      "Training loss (for one batch) at step 70: 355.1595, Accuracy: 0.8603\n",
      "Training loss (for one batch) at step 80: 358.3377, Accuracy: 0.8542\n",
      "Training loss (for one batch) at step 90: 327.0093, Accuracy: 0.8508\n",
      "Training loss (for one batch) at step 100: 323.4262, Accuracy: 0.8517\n",
      "Training loss (for one batch) at step 110: 331.8981, Accuracy: 0.8505\n",
      "---- Training ----\n",
      "Training loss: 101.7001\n",
      "Training acc over epoch: 0.8497\n",
      "---- Validation ----\n",
      "Validation loss: 57.1533\n",
      "Validation acc: 0.7783\n",
      "Time taken: 10.14s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 346.4987, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 360.3049, Accuracy: 0.8040\n",
      "Training loss (for one batch) at step 20: 320.2227, Accuracy: 0.8192\n",
      "Training loss (for one batch) at step 30: 314.0928, Accuracy: 0.8349\n",
      "Training loss (for one batch) at step 40: 307.7037, Accuracy: 0.8502\n",
      "Training loss (for one batch) at step 50: 333.1336, Accuracy: 0.8595\n",
      "Training loss (for one batch) at step 60: 324.6020, Accuracy: 0.8621\n",
      "Training loss (for one batch) at step 70: 352.0872, Accuracy: 0.8556\n",
      "Training loss (for one batch) at step 80: 337.4379, Accuracy: 0.8478\n",
      "Training loss (for one batch) at step 90: 344.0370, Accuracy: 0.8462\n",
      "Training loss (for one batch) at step 100: 320.3493, Accuracy: 0.8478\n",
      "Training loss (for one batch) at step 110: 330.5583, Accuracy: 0.8467\n",
      "---- Training ----\n",
      "Training loss: 104.3822\n",
      "Training acc over epoch: 0.8460\n",
      "---- Validation ----\n",
      "Validation loss: 41.3087\n",
      "Validation acc: 0.7692\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 357.5520, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 343.9019, Accuracy: 0.8075\n",
      "Training loss (for one batch) at step 20: 335.8054, Accuracy: 0.8229\n",
      "Training loss (for one batch) at step 30: 311.9365, Accuracy: 0.8395\n",
      "Training loss (for one batch) at step 40: 317.7571, Accuracy: 0.8506\n",
      "Training loss (for one batch) at step 50: 295.9827, Accuracy: 0.8604\n",
      "Training loss (for one batch) at step 60: 319.6060, Accuracy: 0.8627\n",
      "Training loss (for one batch) at step 70: 351.1515, Accuracy: 0.8540\n",
      "Training loss (for one batch) at step 80: 355.1766, Accuracy: 0.8469\n",
      "Training loss (for one batch) at step 90: 317.7509, Accuracy: 0.8432\n",
      "Training loss (for one batch) at step 100: 315.3167, Accuracy: 0.8439\n",
      "Training loss (for one batch) at step 110: 333.8214, Accuracy: 0.8432\n",
      "---- Training ----\n",
      "Training loss: 107.3188\n",
      "Training acc over epoch: 0.8423\n",
      "---- Validation ----\n",
      "Validation loss: 51.4306\n",
      "Validation acc: 0.7724\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 345.6231, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 352.0290, Accuracy: 0.8068\n",
      "Training loss (for one batch) at step 20: 333.5044, Accuracy: 0.8225\n",
      "Training loss (for one batch) at step 30: 318.1328, Accuracy: 0.8372\n",
      "Training loss (for one batch) at step 40: 316.7569, Accuracy: 0.8510\n",
      "Training loss (for one batch) at step 50: 300.8203, Accuracy: 0.8609\n",
      "Training loss (for one batch) at step 60: 315.7525, Accuracy: 0.8657\n",
      "Training loss (for one batch) at step 70: 333.0574, Accuracy: 0.8599\n",
      "Training loss (for one batch) at step 80: 355.5148, Accuracy: 0.8519\n",
      "Training loss (for one batch) at step 90: 302.4116, Accuracy: 0.8496\n",
      "Training loss (for one batch) at step 100: 318.7762, Accuracy: 0.8500\n",
      "Training loss (for one batch) at step 110: 321.4149, Accuracy: 0.8502\n",
      "---- Training ----\n",
      "Training loss: 100.9842\n",
      "Training acc over epoch: 0.8493\n",
      "---- Validation ----\n",
      "Validation loss: 54.5828\n",
      "Validation acc: 0.7735\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 358.9266, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 342.0164, Accuracy: 0.8111\n",
      "Training loss (for one batch) at step 20: 309.3552, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 30: 320.4340, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 40: 310.1320, Accuracy: 0.8546\n",
      "Training loss (for one batch) at step 50: 310.9955, Accuracy: 0.8666\n",
      "Training loss (for one batch) at step 60: 318.9679, Accuracy: 0.8701\n",
      "Training loss (for one batch) at step 70: 327.5056, Accuracy: 0.8626\n",
      "Training loss (for one batch) at step 80: 350.4923, Accuracy: 0.8564\n",
      "Training loss (for one batch) at step 90: 330.1562, Accuracy: 0.8535\n",
      "Training loss (for one batch) at step 100: 291.5241, Accuracy: 0.8537\n",
      "Training loss (for one batch) at step 110: 329.7571, Accuracy: 0.8531\n",
      "---- Training ----\n",
      "Training loss: 119.3666\n",
      "Training acc over epoch: 0.8520\n",
      "---- Validation ----\n",
      "Validation loss: 45.3367\n",
      "Validation acc: 0.7770\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 350.7901, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 339.2550, Accuracy: 0.7848\n",
      "Training loss (for one batch) at step 20: 310.0770, Accuracy: 0.8114\n",
      "Training loss (for one batch) at step 30: 316.7700, Accuracy: 0.8364\n",
      "Training loss (for one batch) at step 40: 318.5386, Accuracy: 0.8483\n",
      "Training loss (for one batch) at step 50: 290.0952, Accuracy: 0.8604\n",
      "Training loss (for one batch) at step 60: 312.6000, Accuracy: 0.8650\n",
      "Training loss (for one batch) at step 70: 329.2420, Accuracy: 0.8605\n",
      "Training loss (for one batch) at step 80: 326.5925, Accuracy: 0.8528\n",
      "Training loss (for one batch) at step 90: 312.7789, Accuracy: 0.8489\n",
      "Training loss (for one batch) at step 100: 299.6859, Accuracy: 0.8505\n",
      "Training loss (for one batch) at step 110: 310.2046, Accuracy: 0.8492\n",
      "---- Training ----\n",
      "Training loss: 111.8476\n",
      "Training acc over epoch: 0.8479\n",
      "---- Validation ----\n",
      "Validation loss: 48.4675\n",
      "Validation acc: 0.7851\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 345.2522, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 342.4347, Accuracy: 0.7997\n",
      "Training loss (for one batch) at step 20: 346.5293, Accuracy: 0.8251\n",
      "Training loss (for one batch) at step 30: 317.2538, Accuracy: 0.8455\n",
      "Training loss (for one batch) at step 40: 307.7621, Accuracy: 0.8592\n",
      "Training loss (for one batch) at step 50: 307.4879, Accuracy: 0.8652\n",
      "Training loss (for one batch) at step 60: 315.2270, Accuracy: 0.8677\n",
      "Training loss (for one batch) at step 70: 360.8905, Accuracy: 0.8608\n",
      "Training loss (for one batch) at step 80: 343.3528, Accuracy: 0.8521\n",
      "Training loss (for one batch) at step 90: 317.2970, Accuracy: 0.8496\n",
      "Training loss (for one batch) at step 100: 325.2384, Accuracy: 0.8509\n",
      "Training loss (for one batch) at step 110: 322.6255, Accuracy: 0.8508\n",
      "---- Training ----\n",
      "Training loss: 111.9699\n",
      "Training acc over epoch: 0.8505\n",
      "---- Validation ----\n",
      "Validation loss: 42.5957\n",
      "Validation acc: 0.7781\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 357.9281, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 352.5290, Accuracy: 0.7955\n",
      "Training loss (for one batch) at step 20: 317.7698, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 30: 317.1723, Accuracy: 0.8342\n",
      "Training loss (for one batch) at step 40: 319.7126, Accuracy: 0.8491\n",
      "Training loss (for one batch) at step 50: 299.4946, Accuracy: 0.8585\n",
      "Training loss (for one batch) at step 60: 325.8374, Accuracy: 0.8618\n",
      "Training loss (for one batch) at step 70: 339.3912, Accuracy: 0.8557\n",
      "Training loss (for one batch) at step 80: 340.4398, Accuracy: 0.8490\n",
      "Training loss (for one batch) at step 90: 323.2721, Accuracy: 0.8468\n",
      "Training loss (for one batch) at step 100: 305.0912, Accuracy: 0.8485\n",
      "Training loss (for one batch) at step 110: 330.0140, Accuracy: 0.8476\n",
      "---- Training ----\n",
      "Training loss: 99.0731\n",
      "Training acc over epoch: 0.8467\n",
      "---- Validation ----\n",
      "Validation loss: 38.0226\n",
      "Validation acc: 0.7899\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 355.8987, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 348.4044, Accuracy: 0.8075\n",
      "Training loss (for one batch) at step 20: 310.9015, Accuracy: 0.8240\n",
      "Training loss (for one batch) at step 30: 290.8182, Accuracy: 0.8430\n",
      "Training loss (for one batch) at step 40: 308.6482, Accuracy: 0.8552\n",
      "Training loss (for one batch) at step 50: 295.6092, Accuracy: 0.8640\n",
      "Training loss (for one batch) at step 60: 318.7881, Accuracy: 0.8668\n",
      "Training loss (for one batch) at step 70: 318.8055, Accuracy: 0.8597\n",
      "Training loss (for one batch) at step 80: 335.4245, Accuracy: 0.8527\n",
      "Training loss (for one batch) at step 90: 315.2354, Accuracy: 0.8486\n",
      "Training loss (for one batch) at step 100: 309.8287, Accuracy: 0.8496\n",
      "Training loss (for one batch) at step 110: 318.8693, Accuracy: 0.8487\n",
      "---- Training ----\n",
      "Training loss: 105.0173\n",
      "Training acc over epoch: 0.8474\n",
      "---- Validation ----\n",
      "Validation loss: 43.4104\n",
      "Validation acc: 0.7765\n",
      "Time taken: 10.68s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 340.9346, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 10: 344.0597, Accuracy: 0.8111\n",
      "Training loss (for one batch) at step 20: 308.2370, Accuracy: 0.8304\n",
      "Training loss (for one batch) at step 30: 300.5406, Accuracy: 0.8483\n",
      "Training loss (for one batch) at step 40: 302.3967, Accuracy: 0.8594\n",
      "Training loss (for one batch) at step 50: 303.0923, Accuracy: 0.8675\n",
      "Training loss (for one batch) at step 60: 302.7855, Accuracy: 0.8715\n",
      "Training loss (for one batch) at step 70: 324.3511, Accuracy: 0.8659\n",
      "Training loss (for one batch) at step 80: 321.3702, Accuracy: 0.8574\n",
      "Training loss (for one batch) at step 90: 304.5992, Accuracy: 0.8540\n",
      "Training loss (for one batch) at step 100: 313.7654, Accuracy: 0.8549\n",
      "Training loss (for one batch) at step 110: 328.7438, Accuracy: 0.8549\n",
      "---- Training ----\n",
      "Training loss: 118.8831\n",
      "Training acc over epoch: 0.8532\n",
      "---- Validation ----\n",
      "Validation loss: 45.5862\n",
      "Validation acc: 0.7813\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 342.6018, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 336.5133, Accuracy: 0.8182\n",
      "Training loss (for one batch) at step 20: 308.5610, Accuracy: 0.8263\n",
      "Training loss (for one batch) at step 30: 334.9753, Accuracy: 0.8430\n",
      "Training loss (for one batch) at step 40: 294.2673, Accuracy: 0.8542\n",
      "Training loss (for one batch) at step 50: 322.3139, Accuracy: 0.8646\n",
      "Training loss (for one batch) at step 60: 309.4443, Accuracy: 0.8687\n",
      "Training loss (for one batch) at step 70: 324.3305, Accuracy: 0.8622\n",
      "Training loss (for one batch) at step 80: 338.6480, Accuracy: 0.8555\n",
      "Training loss (for one batch) at step 90: 334.0029, Accuracy: 0.8529\n",
      "Training loss (for one batch) at step 100: 301.7569, Accuracy: 0.8530\n",
      "Training loss (for one batch) at step 110: 322.8424, Accuracy: 0.8528\n",
      "---- Training ----\n",
      "Training loss: 100.6873\n",
      "Training acc over epoch: 0.8514\n",
      "---- Validation ----\n",
      "Validation loss: 51.5866\n",
      "Validation acc: 0.7778\n",
      "Time taken: 10.64s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 337.9620, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 10: 354.0137, Accuracy: 0.8196\n",
      "Training loss (for one batch) at step 20: 308.1669, Accuracy: 0.8330\n",
      "Training loss (for one batch) at step 30: 303.6859, Accuracy: 0.8488\n",
      "Training loss (for one batch) at step 40: 314.4648, Accuracy: 0.8584\n",
      "Training loss (for one batch) at step 50: 299.2017, Accuracy: 0.8649\n",
      "Training loss (for one batch) at step 60: 332.0954, Accuracy: 0.8676\n",
      "Training loss (for one batch) at step 70: 329.3394, Accuracy: 0.8614\n",
      "Training loss (for one batch) at step 80: 327.5972, Accuracy: 0.8506\n",
      "Training loss (for one batch) at step 90: 308.2736, Accuracy: 0.8496\n",
      "Training loss (for one batch) at step 100: 300.0695, Accuracy: 0.8506\n",
      "Training loss (for one batch) at step 110: 309.1364, Accuracy: 0.8505\n",
      "---- Training ----\n",
      "Training loss: 113.9913\n",
      "Training acc over epoch: 0.8512\n",
      "---- Validation ----\n",
      "Validation loss: 40.6287\n",
      "Validation acc: 0.7770\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 341.2458, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 323.3869, Accuracy: 0.8132\n",
      "Training loss (for one batch) at step 20: 301.1143, Accuracy: 0.8248\n",
      "Training loss (for one batch) at step 30: 302.6302, Accuracy: 0.8420\n",
      "Training loss (for one batch) at step 40: 285.7784, Accuracy: 0.8546\n",
      "Training loss (for one batch) at step 50: 296.4229, Accuracy: 0.8640\n",
      "Training loss (for one batch) at step 60: 312.5768, Accuracy: 0.8673\n",
      "Training loss (for one batch) at step 70: 362.1017, Accuracy: 0.8614\n",
      "Training loss (for one batch) at step 80: 325.4249, Accuracy: 0.8519\n",
      "Training loss (for one batch) at step 90: 293.0655, Accuracy: 0.8492\n",
      "Training loss (for one batch) at step 100: 306.5139, Accuracy: 0.8504\n",
      "Training loss (for one batch) at step 110: 317.4750, Accuracy: 0.8506\n",
      "---- Training ----\n",
      "Training loss: 111.8706\n",
      "Training acc over epoch: 0.8498\n",
      "---- Validation ----\n",
      "Validation loss: 47.0003\n",
      "Validation acc: 0.7783\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 335.4498, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 341.4366, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 20: 317.0085, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 30: 325.8504, Accuracy: 0.8453\n",
      "Training loss (for one batch) at step 40: 282.7622, Accuracy: 0.8613\n",
      "Training loss (for one batch) at step 50: 296.9359, Accuracy: 0.8701\n",
      "Training loss (for one batch) at step 60: 300.7300, Accuracy: 0.8697\n",
      "Training loss (for one batch) at step 70: 336.2407, Accuracy: 0.8647\n",
      "Training loss (for one batch) at step 80: 339.3907, Accuracy: 0.8569\n",
      "Training loss (for one batch) at step 90: 324.8142, Accuracy: 0.8519\n",
      "Training loss (for one batch) at step 100: 312.8634, Accuracy: 0.8532\n",
      "Training loss (for one batch) at step 110: 301.8145, Accuracy: 0.8530\n",
      "---- Training ----\n",
      "Training loss: 99.1459\n",
      "Training acc over epoch: 0.8511\n",
      "---- Validation ----\n",
      "Validation loss: 35.0936\n",
      "Validation acc: 0.7789\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 349.2871, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 10: 342.9942, Accuracy: 0.8210\n",
      "Training loss (for one batch) at step 20: 320.4971, Accuracy: 0.8326\n",
      "Training loss (for one batch) at step 30: 309.3210, Accuracy: 0.8488\n",
      "Training loss (for one batch) at step 40: 298.8403, Accuracy: 0.8620\n",
      "Training loss (for one batch) at step 50: 293.5581, Accuracy: 0.8707\n",
      "Training loss (for one batch) at step 60: 300.4600, Accuracy: 0.8726\n",
      "Training loss (for one batch) at step 70: 333.1534, Accuracy: 0.8667\n",
      "Training loss (for one batch) at step 80: 351.8787, Accuracy: 0.8580\n",
      "Training loss (for one batch) at step 90: 305.4100, Accuracy: 0.8551\n",
      "Training loss (for one batch) at step 100: 300.7458, Accuracy: 0.8547\n",
      "Training loss (for one batch) at step 110: 313.8514, Accuracy: 0.8541\n",
      "---- Training ----\n",
      "Training loss: 99.8756\n",
      "Training acc over epoch: 0.8524\n",
      "---- Validation ----\n",
      "Validation loss: 49.3172\n",
      "Validation acc: 0.7703\n",
      "Time taken: 10.51s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 341.1447, Accuracy: 0.8672\n",
      "Training loss (for one batch) at step 10: 312.2708, Accuracy: 0.8210\n",
      "Training loss (for one batch) at step 20: 304.8426, Accuracy: 0.8367\n",
      "Training loss (for one batch) at step 30: 313.1982, Accuracy: 0.8485\n",
      "Training loss (for one batch) at step 40: 304.4595, Accuracy: 0.8605\n",
      "Training loss (for one batch) at step 50: 278.0036, Accuracy: 0.8687\n",
      "Training loss (for one batch) at step 60: 316.8109, Accuracy: 0.8712\n",
      "Training loss (for one batch) at step 70: 318.5004, Accuracy: 0.8645\n",
      "Training loss (for one batch) at step 80: 320.9827, Accuracy: 0.8569\n",
      "Training loss (for one batch) at step 90: 302.9991, Accuracy: 0.8551\n",
      "Training loss (for one batch) at step 100: 312.8120, Accuracy: 0.8554\n",
      "Training loss (for one batch) at step 110: 305.5697, Accuracy: 0.8548\n",
      "---- Training ----\n",
      "Training loss: 116.7151\n",
      "Training acc over epoch: 0.8543\n",
      "---- Validation ----\n",
      "Validation loss: 49.3940\n",
      "Validation acc: 0.7770\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 356.2808, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 317.2490, Accuracy: 0.8068\n",
      "Training loss (for one batch) at step 20: 311.5705, Accuracy: 0.8248\n",
      "Training loss (for one batch) at step 30: 332.5235, Accuracy: 0.8432\n",
      "Training loss (for one batch) at step 40: 301.9635, Accuracy: 0.8523\n",
      "Training loss (for one batch) at step 50: 294.3314, Accuracy: 0.8649\n",
      "Training loss (for one batch) at step 60: 307.1135, Accuracy: 0.8680\n",
      "Training loss (for one batch) at step 70: 312.3076, Accuracy: 0.8582\n",
      "Training loss (for one batch) at step 80: 338.9555, Accuracy: 0.8506\n",
      "Training loss (for one batch) at step 90: 299.1455, Accuracy: 0.8490\n",
      "Training loss (for one batch) at step 100: 295.8113, Accuracy: 0.8510\n",
      "Training loss (for one batch) at step 110: 306.1922, Accuracy: 0.8505\n",
      "---- Training ----\n",
      "Training loss: 122.9770\n",
      "Training acc over epoch: 0.8487\n",
      "---- Validation ----\n",
      "Validation loss: 36.4441\n",
      "Validation acc: 0.7821\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 331.7802, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 336.0629, Accuracy: 0.8246\n",
      "Training loss (for one batch) at step 20: 307.8397, Accuracy: 0.8393\n",
      "Training loss (for one batch) at step 30: 306.9461, Accuracy: 0.8521\n",
      "Training loss (for one batch) at step 40: 290.6589, Accuracy: 0.8628\n",
      "Training loss (for one batch) at step 50: 296.9598, Accuracy: 0.8712\n",
      "Training loss (for one batch) at step 60: 313.0569, Accuracy: 0.8731\n",
      "Training loss (for one batch) at step 70: 327.0765, Accuracy: 0.8658\n",
      "Training loss (for one batch) at step 80: 334.3966, Accuracy: 0.8554\n",
      "Training loss (for one batch) at step 90: 299.2860, Accuracy: 0.8553\n",
      "Training loss (for one batch) at step 100: 295.3666, Accuracy: 0.8551\n",
      "Training loss (for one batch) at step 110: 313.3092, Accuracy: 0.8549\n",
      "---- Training ----\n",
      "Training loss: 100.2640\n",
      "Training acc over epoch: 0.8534\n",
      "---- Validation ----\n",
      "Validation loss: 56.6864\n",
      "Validation acc: 0.7813\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 331.9643, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 334.0941, Accuracy: 0.8153\n",
      "Training loss (for one batch) at step 20: 310.7274, Accuracy: 0.8233\n",
      "Training loss (for one batch) at step 30: 299.0497, Accuracy: 0.8417\n",
      "Training loss (for one batch) at step 40: 283.1078, Accuracy: 0.8548\n",
      "Training loss (for one batch) at step 50: 296.1901, Accuracy: 0.8660\n",
      "Training loss (for one batch) at step 60: 305.7195, Accuracy: 0.8704\n",
      "Training loss (for one batch) at step 70: 343.5539, Accuracy: 0.8616\n",
      "Training loss (for one batch) at step 80: 336.7675, Accuracy: 0.8537\n",
      "Training loss (for one batch) at step 90: 334.6056, Accuracy: 0.8520\n",
      "Training loss (for one batch) at step 100: 290.4329, Accuracy: 0.8535\n",
      "Training loss (for one batch) at step 110: 300.1149, Accuracy: 0.8534\n",
      "---- Training ----\n",
      "Training loss: 98.3822\n",
      "Training acc over epoch: 0.8515\n",
      "---- Validation ----\n",
      "Validation loss: 44.5983\n",
      "Validation acc: 0.7757\n",
      "Time taken: 10.16s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 331.5422, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 323.8020, Accuracy: 0.8054\n",
      "Training loss (for one batch) at step 20: 305.3063, Accuracy: 0.8259\n",
      "Training loss (for one batch) at step 30: 302.0976, Accuracy: 0.8448\n",
      "Training loss (for one batch) at step 40: 296.3880, Accuracy: 0.8552\n",
      "Training loss (for one batch) at step 50: 268.4047, Accuracy: 0.8641\n",
      "Training loss (for one batch) at step 60: 299.6779, Accuracy: 0.8691\n",
      "Training loss (for one batch) at step 70: 317.3955, Accuracy: 0.8636\n",
      "Training loss (for one batch) at step 80: 322.0480, Accuracy: 0.8530\n",
      "Training loss (for one batch) at step 90: 290.2291, Accuracy: 0.8515\n",
      "Training loss (for one batch) at step 100: 286.7666, Accuracy: 0.8522\n",
      "Training loss (for one batch) at step 110: 322.0488, Accuracy: 0.8518\n",
      "---- Training ----\n",
      "Training loss: 101.0899\n",
      "Training acc over epoch: 0.8502\n",
      "---- Validation ----\n",
      "Validation loss: 56.9478\n",
      "Validation acc: 0.7773\n",
      "Time taken: 10.83s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 339.1421, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 325.3434, Accuracy: 0.8082\n",
      "Training loss (for one batch) at step 20: 322.1345, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 30: 291.2373, Accuracy: 0.8443\n",
      "Training loss (for one batch) at step 40: 276.1010, Accuracy: 0.8563\n",
      "Training loss (for one batch) at step 50: 276.6943, Accuracy: 0.8658\n",
      "Training loss (for one batch) at step 60: 317.1787, Accuracy: 0.8676\n",
      "Training loss (for one batch) at step 70: 322.4630, Accuracy: 0.8605\n",
      "Training loss (for one batch) at step 80: 329.2636, Accuracy: 0.8521\n",
      "Training loss (for one batch) at step 90: 295.5656, Accuracy: 0.8513\n",
      "Training loss (for one batch) at step 100: 295.0285, Accuracy: 0.8540\n",
      "Training loss (for one batch) at step 110: 310.7564, Accuracy: 0.8536\n",
      "---- Training ----\n",
      "Training loss: 100.0488\n",
      "Training acc over epoch: 0.8522\n",
      "---- Validation ----\n",
      "Validation loss: 49.1895\n",
      "Validation acc: 0.7714\n",
      "Time taken: 10.60s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 331.5085, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 331.2960, Accuracy: 0.7976\n",
      "Training loss (for one batch) at step 20: 296.1290, Accuracy: 0.8207\n",
      "Training loss (for one batch) at step 30: 298.3101, Accuracy: 0.8410\n",
      "Training loss (for one batch) at step 40: 300.9331, Accuracy: 0.8556\n",
      "Training loss (for one batch) at step 50: 291.8001, Accuracy: 0.8631\n",
      "Training loss (for one batch) at step 60: 287.7238, Accuracy: 0.8664\n",
      "Training loss (for one batch) at step 70: 323.3808, Accuracy: 0.8618\n",
      "Training loss (for one batch) at step 80: 318.7189, Accuracy: 0.8536\n",
      "Training loss (for one batch) at step 90: 312.9380, Accuracy: 0.8516\n",
      "Training loss (for one batch) at step 100: 290.9020, Accuracy: 0.8510\n",
      "Training loss (for one batch) at step 110: 298.9181, Accuracy: 0.8519\n",
      "---- Training ----\n",
      "Training loss: 100.3340\n",
      "Training acc over epoch: 0.8502\n",
      "---- Validation ----\n",
      "Validation loss: 42.3099\n",
      "Validation acc: 0.7794\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 323.6545, Accuracy: 0.8750\n",
      "Training loss (for one batch) at step 10: 318.9515, Accuracy: 0.8196\n",
      "Training loss (for one batch) at step 20: 293.9243, Accuracy: 0.8348\n",
      "Training loss (for one batch) at step 30: 290.0582, Accuracy: 0.8533\n",
      "Training loss (for one batch) at step 40: 292.7413, Accuracy: 0.8598\n",
      "Training loss (for one batch) at step 50: 293.2332, Accuracy: 0.8669\n",
      "Training loss (for one batch) at step 60: 311.8459, Accuracy: 0.8713\n",
      "Training loss (for one batch) at step 70: 342.0506, Accuracy: 0.8652\n",
      "Training loss (for one batch) at step 80: 330.9999, Accuracy: 0.8554\n",
      "Training loss (for one batch) at step 90: 298.8470, Accuracy: 0.8540\n",
      "Training loss (for one batch) at step 100: 278.3560, Accuracy: 0.8544\n",
      "Training loss (for one batch) at step 110: 293.4942, Accuracy: 0.8535\n",
      "---- Training ----\n",
      "Training loss: 86.2285\n",
      "Training acc over epoch: 0.8527\n",
      "---- Validation ----\n",
      "Validation loss: 41.7194\n",
      "Validation acc: 0.7778\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 327.4858, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 333.7853, Accuracy: 0.8111\n",
      "Training loss (for one batch) at step 20: 316.8229, Accuracy: 0.8300\n",
      "Training loss (for one batch) at step 30: 311.4938, Accuracy: 0.8432\n",
      "Training loss (for one batch) at step 40: 283.7997, Accuracy: 0.8573\n",
      "Training loss (for one batch) at step 50: 278.8465, Accuracy: 0.8675\n",
      "Training loss (for one batch) at step 60: 301.1387, Accuracy: 0.8706\n",
      "Training loss (for one batch) at step 70: 305.6281, Accuracy: 0.8645\n",
      "Training loss (for one batch) at step 80: 325.4212, Accuracy: 0.8573\n",
      "Training loss (for one batch) at step 90: 328.0113, Accuracy: 0.8526\n",
      "Training loss (for one batch) at step 100: 300.2490, Accuracy: 0.8528\n",
      "Training loss (for one batch) at step 110: 320.1993, Accuracy: 0.8523\n",
      "---- Training ----\n",
      "Training loss: 94.8384\n",
      "Training acc over epoch: 0.8523\n",
      "---- Validation ----\n",
      "Validation loss: 48.0230\n",
      "Validation acc: 0.7749\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 335.9468, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 320.2751, Accuracy: 0.8111\n",
      "Training loss (for one batch) at step 20: 309.9698, Accuracy: 0.8259\n",
      "Training loss (for one batch) at step 30: 294.2063, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 40: 289.6423, Accuracy: 0.8558\n",
      "Training loss (for one batch) at step 50: 279.3731, Accuracy: 0.8641\n",
      "Training loss (for one batch) at step 60: 285.6870, Accuracy: 0.8674\n",
      "Training loss (for one batch) at step 70: 324.8698, Accuracy: 0.8622\n",
      "Training loss (for one batch) at step 80: 314.6604, Accuracy: 0.8540\n",
      "Training loss (for one batch) at step 90: 309.1109, Accuracy: 0.8519\n",
      "Training loss (for one batch) at step 100: 302.4384, Accuracy: 0.8524\n",
      "Training loss (for one batch) at step 110: 307.6070, Accuracy: 0.8525\n",
      "---- Training ----\n",
      "Training loss: 87.7817\n",
      "Training acc over epoch: 0.8518\n",
      "---- Validation ----\n",
      "Validation loss: 36.9408\n",
      "Validation acc: 0.7808\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 371.4578, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 329.5298, Accuracy: 0.8097\n",
      "Training loss (for one batch) at step 20: 326.5987, Accuracy: 0.8285\n",
      "Training loss (for one batch) at step 30: 290.2309, Accuracy: 0.8425\n",
      "Training loss (for one batch) at step 40: 274.7769, Accuracy: 0.8571\n",
      "Training loss (for one batch) at step 50: 270.7337, Accuracy: 0.8667\n",
      "Training loss (for one batch) at step 60: 286.4539, Accuracy: 0.8731\n",
      "Training loss (for one batch) at step 70: 332.9794, Accuracy: 0.8639\n",
      "Training loss (for one batch) at step 80: 317.1302, Accuracy: 0.8567\n",
      "Training loss (for one batch) at step 90: 310.7661, Accuracy: 0.8536\n",
      "Training loss (for one batch) at step 100: 286.5619, Accuracy: 0.8543\n",
      "Training loss (for one batch) at step 110: 312.6874, Accuracy: 0.8525\n",
      "---- Training ----\n",
      "Training loss: 97.2535\n",
      "Training acc over epoch: 0.8526\n",
      "---- Validation ----\n",
      "Validation loss: 46.1136\n",
      "Validation acc: 0.7585\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 343.7708, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 304.1812, Accuracy: 0.8004\n",
      "Training loss (for one batch) at step 20: 304.0413, Accuracy: 0.8207\n",
      "Training loss (for one batch) at step 30: 305.3846, Accuracy: 0.8432\n",
      "Training loss (for one batch) at step 40: 293.7274, Accuracy: 0.8554\n",
      "Training loss (for one batch) at step 50: 286.8300, Accuracy: 0.8661\n",
      "Training loss (for one batch) at step 60: 285.7184, Accuracy: 0.8686\n",
      "Training loss (for one batch) at step 70: 318.9014, Accuracy: 0.8631\n",
      "Training loss (for one batch) at step 80: 330.7462, Accuracy: 0.8538\n",
      "Training loss (for one batch) at step 90: 299.8530, Accuracy: 0.8496\n",
      "Training loss (for one batch) at step 100: 290.1182, Accuracy: 0.8503\n",
      "Training loss (for one batch) at step 110: 337.9417, Accuracy: 0.8488\n",
      "---- Training ----\n",
      "Training loss: 108.5020\n",
      "Training acc over epoch: 0.8481\n",
      "---- Validation ----\n",
      "Validation loss: 38.3973\n",
      "Validation acc: 0.7751\n",
      "Time taken: 10.72s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 333.4078, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 10: 350.0455, Accuracy: 0.8189\n",
      "Training loss (for one batch) at step 20: 305.9378, Accuracy: 0.8322\n",
      "Training loss (for one batch) at step 30: 288.7870, Accuracy: 0.8480\n",
      "Training loss (for one batch) at step 40: 289.8337, Accuracy: 0.8577\n",
      "Training loss (for one batch) at step 50: 276.0709, Accuracy: 0.8683\n",
      "Training loss (for one batch) at step 60: 289.7774, Accuracy: 0.8723\n",
      "Training loss (for one batch) at step 70: 315.2715, Accuracy: 0.8672\n",
      "Training loss (for one batch) at step 80: 315.2670, Accuracy: 0.8568\n",
      "Training loss (for one batch) at step 90: 300.3884, Accuracy: 0.8541\n",
      "Training loss (for one batch) at step 100: 285.3877, Accuracy: 0.8549\n",
      "Training loss (for one batch) at step 110: 286.5606, Accuracy: 0.8554\n",
      "---- Training ----\n",
      "Training loss: 103.7230\n",
      "Training acc over epoch: 0.8538\n",
      "---- Validation ----\n",
      "Validation loss: 46.1082\n",
      "Validation acc: 0.7738\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 330.2819, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 314.1960, Accuracy: 0.8040\n",
      "Training loss (for one batch) at step 20: 295.2096, Accuracy: 0.8263\n",
      "Training loss (for one batch) at step 30: 282.6000, Accuracy: 0.8435\n",
      "Training loss (for one batch) at step 40: 298.7589, Accuracy: 0.8573\n",
      "Training loss (for one batch) at step 50: 278.7155, Accuracy: 0.8658\n",
      "Training loss (for one batch) at step 60: 318.7688, Accuracy: 0.8708\n",
      "Training loss (for one batch) at step 70: 300.2125, Accuracy: 0.8631\n",
      "Training loss (for one batch) at step 80: 330.0200, Accuracy: 0.8582\n",
      "Training loss (for one batch) at step 90: 293.8439, Accuracy: 0.8541\n",
      "Training loss (for one batch) at step 100: 297.3961, Accuracy: 0.8554\n",
      "Training loss (for one batch) at step 110: 299.6708, Accuracy: 0.8533\n",
      "---- Training ----\n",
      "Training loss: 113.8255\n",
      "Training acc over epoch: 0.8528\n",
      "---- Validation ----\n",
      "Validation loss: 75.7095\n",
      "Validation acc: 0.7735\n",
      "Time taken: 10.58s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 323.1658, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 319.5797, Accuracy: 0.8075\n",
      "Training loss (for one batch) at step 20: 313.2896, Accuracy: 0.8251\n",
      "Training loss (for one batch) at step 30: 285.9716, Accuracy: 0.8465\n",
      "Training loss (for one batch) at step 40: 302.1533, Accuracy: 0.8531\n",
      "Training loss (for one batch) at step 50: 277.2601, Accuracy: 0.8650\n",
      "Training loss (for one batch) at step 60: 297.7140, Accuracy: 0.8695\n",
      "Training loss (for one batch) at step 70: 325.1025, Accuracy: 0.8621\n",
      "Training loss (for one batch) at step 80: 331.8430, Accuracy: 0.8540\n",
      "Training loss (for one batch) at step 90: 287.4159, Accuracy: 0.8527\n",
      "Training loss (for one batch) at step 100: 284.4644, Accuracy: 0.8530\n",
      "Training loss (for one batch) at step 110: 308.4132, Accuracy: 0.8537\n",
      "---- Training ----\n",
      "Training loss: 94.1203\n",
      "Training acc over epoch: 0.8524\n",
      "---- Validation ----\n",
      "Validation loss: 64.2911\n",
      "Validation acc: 0.7740\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 319.8203, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 312.7161, Accuracy: 0.8075\n",
      "Training loss (for one batch) at step 20: 305.1881, Accuracy: 0.8289\n",
      "Training loss (for one batch) at step 30: 272.4134, Accuracy: 0.8483\n",
      "Training loss (for one batch) at step 40: 289.4218, Accuracy: 0.8590\n",
      "Training loss (for one batch) at step 50: 269.5775, Accuracy: 0.8672\n",
      "Training loss (for one batch) at step 60: 296.0593, Accuracy: 0.8710\n",
      "Training loss (for one batch) at step 70: 338.5193, Accuracy: 0.8640\n",
      "Training loss (for one batch) at step 80: 307.8383, Accuracy: 0.8562\n",
      "Training loss (for one batch) at step 90: 298.4695, Accuracy: 0.8528\n",
      "Training loss (for one batch) at step 100: 283.3015, Accuracy: 0.8527\n",
      "Training loss (for one batch) at step 110: 303.2244, Accuracy: 0.8531\n",
      "---- Training ----\n",
      "Training loss: 121.3730\n",
      "Training acc over epoch: 0.8510\n",
      "---- Validation ----\n",
      "Validation loss: 65.7622\n",
      "Validation acc: 0.7797\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 319.4030, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 321.0882, Accuracy: 0.8118\n",
      "Training loss (for one batch) at step 20: 305.6008, Accuracy: 0.8367\n",
      "Training loss (for one batch) at step 30: 297.4096, Accuracy: 0.8475\n",
      "Training loss (for one batch) at step 40: 274.9739, Accuracy: 0.8599\n",
      "Training loss (for one batch) at step 50: 287.6667, Accuracy: 0.8670\n",
      "Training loss (for one batch) at step 60: 306.3528, Accuracy: 0.8721\n",
      "Training loss (for one batch) at step 70: 321.1229, Accuracy: 0.8664\n",
      "Training loss (for one batch) at step 80: 330.3791, Accuracy: 0.8585\n",
      "Training loss (for one batch) at step 90: 289.1803, Accuracy: 0.8556\n",
      "Training loss (for one batch) at step 100: 298.1476, Accuracy: 0.8561\n",
      "Training loss (for one batch) at step 110: 336.1348, Accuracy: 0.8549\n",
      "---- Training ----\n",
      "Training loss: 105.9025\n",
      "Training acc over epoch: 0.8541\n",
      "---- Validation ----\n",
      "Validation loss: 40.4674\n",
      "Validation acc: 0.7700\n",
      "Time taken: 10.66s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 327.8198, Accuracy: 0.8906\n",
      "Training loss (for one batch) at step 10: 328.8658, Accuracy: 0.8111\n",
      "Training loss (for one batch) at step 20: 287.2968, Accuracy: 0.8214\n",
      "Training loss (for one batch) at step 30: 286.0186, Accuracy: 0.8405\n",
      "Training loss (for one batch) at step 40: 289.4531, Accuracy: 0.8554\n",
      "Training loss (for one batch) at step 50: 283.6547, Accuracy: 0.8653\n",
      "Training loss (for one batch) at step 60: 293.4388, Accuracy: 0.8674\n",
      "Training loss (for one batch) at step 70: 307.8945, Accuracy: 0.8607\n",
      "Training loss (for one batch) at step 80: 328.3438, Accuracy: 0.8524\n",
      "Training loss (for one batch) at step 90: 307.3752, Accuracy: 0.8504\n",
      "Training loss (for one batch) at step 100: 277.5450, Accuracy: 0.8516\n",
      "Training loss (for one batch) at step 110: 302.6129, Accuracy: 0.8516\n",
      "---- Training ----\n",
      "Training loss: 112.9088\n",
      "Training acc over epoch: 0.8502\n",
      "---- Validation ----\n",
      "Validation loss: 50.5122\n",
      "Validation acc: 0.7751\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 333.9644, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 317.5936, Accuracy: 0.8089\n",
      "Training loss (for one batch) at step 20: 296.7735, Accuracy: 0.8315\n",
      "Training loss (for one batch) at step 30: 292.3834, Accuracy: 0.8478\n",
      "Training loss (for one batch) at step 40: 289.3785, Accuracy: 0.8584\n",
      "Training loss (for one batch) at step 50: 273.2387, Accuracy: 0.8661\n",
      "Training loss (for one batch) at step 60: 313.9754, Accuracy: 0.8689\n",
      "Training loss (for one batch) at step 70: 323.1977, Accuracy: 0.8634\n",
      "Training loss (for one batch) at step 80: 316.2142, Accuracy: 0.8559\n",
      "Training loss (for one batch) at step 90: 291.9245, Accuracy: 0.8511\n",
      "Training loss (for one batch) at step 100: 278.5577, Accuracy: 0.8523\n",
      "Training loss (for one batch) at step 110: 288.2606, Accuracy: 0.8514\n",
      "---- Training ----\n",
      "Training loss: 91.1354\n",
      "Training acc over epoch: 0.8510\n",
      "---- Validation ----\n",
      "Validation loss: 54.0096\n",
      "Validation acc: 0.7703\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 334.3940, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 321.3136, Accuracy: 0.8239\n",
      "Training loss (for one batch) at step 20: 289.3748, Accuracy: 0.8337\n",
      "Training loss (for one batch) at step 30: 309.3369, Accuracy: 0.8475\n",
      "Training loss (for one batch) at step 40: 281.9429, Accuracy: 0.8588\n",
      "Training loss (for one batch) at step 50: 274.3384, Accuracy: 0.8684\n",
      "Training loss (for one batch) at step 60: 295.7703, Accuracy: 0.8708\n",
      "Training loss (for one batch) at step 70: 317.6951, Accuracy: 0.8639\n",
      "Training loss (for one batch) at step 80: 323.1432, Accuracy: 0.8547\n",
      "Training loss (for one batch) at step 90: 286.4166, Accuracy: 0.8536\n",
      "Training loss (for one batch) at step 100: 294.8750, Accuracy: 0.8533\n",
      "Training loss (for one batch) at step 110: 291.1308, Accuracy: 0.8521\n",
      "---- Training ----\n",
      "Training loss: 105.7537\n",
      "Training acc over epoch: 0.8520\n",
      "---- Validation ----\n",
      "Validation loss: 69.5851\n",
      "Validation acc: 0.7751\n",
      "Time taken: 10.67s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 332.3890, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 10: 308.0228, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 20: 310.8677, Accuracy: 0.8304\n",
      "Training loss (for one batch) at step 30: 286.0649, Accuracy: 0.8493\n",
      "Training loss (for one batch) at step 40: 282.2936, Accuracy: 0.8603\n",
      "Training loss (for one batch) at step 50: 288.9135, Accuracy: 0.8689\n",
      "Training loss (for one batch) at step 60: 290.5085, Accuracy: 0.8733\n",
      "Training loss (for one batch) at step 70: 300.8927, Accuracy: 0.8665\n",
      "Training loss (for one batch) at step 80: 322.5297, Accuracy: 0.8575\n",
      "Training loss (for one batch) at step 90: 287.2754, Accuracy: 0.8551\n",
      "Training loss (for one batch) at step 100: 275.7214, Accuracy: 0.8569\n",
      "Training loss (for one batch) at step 110: 305.9009, Accuracy: 0.8564\n",
      "---- Training ----\n",
      "Training loss: 90.1646\n",
      "Training acc over epoch: 0.8557\n",
      "---- Validation ----\n",
      "Validation loss: 68.8886\n",
      "Validation acc: 0.7732\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 322.2474, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 316.7074, Accuracy: 0.8132\n",
      "Training loss (for one batch) at step 20: 295.1924, Accuracy: 0.8307\n",
      "Training loss (for one batch) at step 30: 295.4384, Accuracy: 0.8412\n",
      "Training loss (for one batch) at step 40: 291.5653, Accuracy: 0.8556\n",
      "Training loss (for one batch) at step 50: 280.3591, Accuracy: 0.8647\n",
      "Training loss (for one batch) at step 60: 287.9841, Accuracy: 0.8680\n",
      "Training loss (for one batch) at step 70: 320.2932, Accuracy: 0.8614\n",
      "Training loss (for one batch) at step 80: 314.4073, Accuracy: 0.8538\n",
      "Training loss (for one batch) at step 90: 296.8000, Accuracy: 0.8525\n",
      "Training loss (for one batch) at step 100: 279.7962, Accuracy: 0.8519\n",
      "Training loss (for one batch) at step 110: 309.1277, Accuracy: 0.8506\n",
      "---- Training ----\n",
      "Training loss: 94.1394\n",
      "Training acc over epoch: 0.8501\n",
      "---- Validation ----\n",
      "Validation loss: 57.0466\n",
      "Validation acc: 0.7714\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 320.9034, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 10: 320.2674, Accuracy: 0.8196\n",
      "Training loss (for one batch) at step 20: 281.6035, Accuracy: 0.8259\n",
      "Training loss (for one batch) at step 30: 287.6814, Accuracy: 0.8443\n",
      "Training loss (for one batch) at step 40: 289.4915, Accuracy: 0.8558\n",
      "Training loss (for one batch) at step 50: 282.9029, Accuracy: 0.8643\n",
      "Training loss (for one batch) at step 60: 280.1988, Accuracy: 0.8690\n",
      "Training loss (for one batch) at step 70: 324.4111, Accuracy: 0.8615\n",
      "Training loss (for one batch) at step 80: 320.5015, Accuracy: 0.8526\n",
      "Training loss (for one batch) at step 90: 300.4375, Accuracy: 0.8507\n",
      "Training loss (for one batch) at step 100: 282.7777, Accuracy: 0.8521\n",
      "Training loss (for one batch) at step 110: 296.7214, Accuracy: 0.8504\n",
      "---- Training ----\n",
      "Training loss: 97.1667\n",
      "Training acc over epoch: 0.8493\n",
      "---- Validation ----\n",
      "Validation loss: 42.2050\n",
      "Validation acc: 0.7679\n",
      "Time taken: 10.65s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 325.4897, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 312.8497, Accuracy: 0.8061\n",
      "Training loss (for one batch) at step 20: 302.0043, Accuracy: 0.8248\n",
      "Training loss (for one batch) at step 30: 283.1745, Accuracy: 0.8450\n",
      "Training loss (for one batch) at step 40: 293.4971, Accuracy: 0.8596\n",
      "Training loss (for one batch) at step 50: 282.1311, Accuracy: 0.8675\n",
      "Training loss (for one batch) at step 60: 285.2801, Accuracy: 0.8719\n",
      "Training loss (for one batch) at step 70: 311.1798, Accuracy: 0.8650\n",
      "Training loss (for one batch) at step 80: 322.0818, Accuracy: 0.8557\n",
      "Training loss (for one batch) at step 90: 293.7021, Accuracy: 0.8533\n",
      "Training loss (for one batch) at step 100: 287.5663, Accuracy: 0.8545\n",
      "Training loss (for one batch) at step 110: 289.8879, Accuracy: 0.8542\n",
      "---- Training ----\n",
      "Training loss: 96.2748\n",
      "Training acc over epoch: 0.8531\n",
      "---- Validation ----\n",
      "Validation loss: 50.2029\n",
      "Validation acc: 0.7657\n",
      "Time taken: 10.67s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 325.1196, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 322.1560, Accuracy: 0.8161\n",
      "Training loss (for one batch) at step 20: 288.4534, Accuracy: 0.8311\n",
      "Training loss (for one batch) at step 30: 271.1247, Accuracy: 0.8458\n",
      "Training loss (for one batch) at step 40: 307.3218, Accuracy: 0.8579\n",
      "Training loss (for one batch) at step 50: 306.4574, Accuracy: 0.8680\n",
      "Training loss (for one batch) at step 60: 326.5942, Accuracy: 0.8709\n",
      "Training loss (for one batch) at step 70: 314.0224, Accuracy: 0.8623\n",
      "Training loss (for one batch) at step 80: 309.2726, Accuracy: 0.8543\n",
      "Training loss (for one batch) at step 90: 295.3858, Accuracy: 0.8530\n",
      "Training loss (for one batch) at step 100: 285.0115, Accuracy: 0.8533\n",
      "Training loss (for one batch) at step 110: 304.0994, Accuracy: 0.8530\n",
      "---- Training ----\n",
      "Training loss: 100.6587\n",
      "Training acc over epoch: 0.8518\n",
      "---- Validation ----\n",
      "Validation loss: 56.5834\n",
      "Validation acc: 0.7654\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 327.0209, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 307.2221, Accuracy: 0.8054\n",
      "Training loss (for one batch) at step 20: 299.9528, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 30: 295.6077, Accuracy: 0.8483\n",
      "Training loss (for one batch) at step 40: 286.0966, Accuracy: 0.8592\n",
      "Training loss (for one batch) at step 50: 282.9847, Accuracy: 0.8681\n",
      "Training loss (for one batch) at step 60: 292.8125, Accuracy: 0.8717\n",
      "Training loss (for one batch) at step 70: 310.3874, Accuracy: 0.8664\n",
      "Training loss (for one batch) at step 80: 300.9301, Accuracy: 0.8580\n",
      "Training loss (for one batch) at step 90: 292.0914, Accuracy: 0.8546\n",
      "Training loss (for one batch) at step 100: 280.3700, Accuracy: 0.8545\n",
      "Training loss (for one batch) at step 110: 302.0519, Accuracy: 0.8540\n",
      "---- Training ----\n",
      "Training loss: 94.1433\n",
      "Training acc over epoch: 0.8538\n",
      "---- Validation ----\n",
      "Validation loss: 32.2526\n",
      "Validation acc: 0.7719\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 319.2852, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 317.7146, Accuracy: 0.8182\n",
      "Training loss (for one batch) at step 20: 292.1254, Accuracy: 0.8341\n",
      "Training loss (for one batch) at step 30: 281.1402, Accuracy: 0.8460\n",
      "Training loss (for one batch) at step 40: 284.6776, Accuracy: 0.8577\n",
      "Training loss (for one batch) at step 50: 275.0898, Accuracy: 0.8667\n",
      "Training loss (for one batch) at step 60: 297.4407, Accuracy: 0.8722\n",
      "Training loss (for one batch) at step 70: 316.5954, Accuracy: 0.8634\n",
      "Training loss (for one batch) at step 80: 319.9698, Accuracy: 0.8560\n",
      "Training loss (for one batch) at step 90: 294.2742, Accuracy: 0.8519\n",
      "Training loss (for one batch) at step 100: 271.4150, Accuracy: 0.8539\n",
      "Training loss (for one batch) at step 110: 293.1952, Accuracy: 0.8537\n",
      "---- Training ----\n",
      "Training loss: 102.0536\n",
      "Training acc over epoch: 0.8528\n",
      "---- Validation ----\n",
      "Validation loss: 41.1257\n",
      "Validation acc: 0.7765\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 318.3255, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 324.4641, Accuracy: 0.8189\n",
      "Training loss (for one batch) at step 20: 322.0658, Accuracy: 0.8330\n",
      "Training loss (for one batch) at step 30: 293.3744, Accuracy: 0.8458\n",
      "Training loss (for one batch) at step 40: 273.0914, Accuracy: 0.8559\n",
      "Training loss (for one batch) at step 50: 298.2148, Accuracy: 0.8670\n",
      "Training loss (for one batch) at step 60: 279.0495, Accuracy: 0.8735\n",
      "Training loss (for one batch) at step 70: 326.6327, Accuracy: 0.8672\n",
      "Training loss (for one batch) at step 80: 316.9414, Accuracy: 0.8600\n",
      "Training loss (for one batch) at step 90: 287.2810, Accuracy: 0.8551\n",
      "Training loss (for one batch) at step 100: 279.0302, Accuracy: 0.8567\n",
      "Training loss (for one batch) at step 110: 295.7355, Accuracy: 0.8568\n",
      "---- Training ----\n",
      "Training loss: 94.5988\n",
      "Training acc over epoch: 0.8558\n",
      "---- Validation ----\n",
      "Validation loss: 47.3659\n",
      "Validation acc: 0.7797\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 342.7075, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 323.9688, Accuracy: 0.8146\n",
      "Training loss (for one batch) at step 20: 297.3311, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 30: 297.1949, Accuracy: 0.8485\n",
      "Training loss (for one batch) at step 40: 281.5079, Accuracy: 0.8605\n",
      "Training loss (for one batch) at step 50: 269.4256, Accuracy: 0.8693\n",
      "Training loss (for one batch) at step 60: 304.1458, Accuracy: 0.8722\n",
      "Training loss (for one batch) at step 70: 321.5376, Accuracy: 0.8658\n",
      "Training loss (for one batch) at step 80: 307.9960, Accuracy: 0.8585\n",
      "Training loss (for one batch) at step 90: 292.1968, Accuracy: 0.8544\n",
      "Training loss (for one batch) at step 100: 280.8252, Accuracy: 0.8558\n",
      "Training loss (for one batch) at step 110: 277.7244, Accuracy: 0.8558\n",
      "---- Training ----\n",
      "Training loss: 97.3550\n",
      "Training acc over epoch: 0.8542\n",
      "---- Validation ----\n",
      "Validation loss: 36.2128\n",
      "Validation acc: 0.7773\n",
      "Time taken: 10.58s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 311.3922, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 302.1128, Accuracy: 0.8232\n",
      "Training loss (for one batch) at step 20: 293.8291, Accuracy: 0.8419\n",
      "Training loss (for one batch) at step 30: 277.8468, Accuracy: 0.8516\n",
      "Training loss (for one batch) at step 40: 268.7551, Accuracy: 0.8628\n",
      "Training loss (for one batch) at step 50: 278.7632, Accuracy: 0.8696\n",
      "Training loss (for one batch) at step 60: 271.7389, Accuracy: 0.8731\n",
      "Training loss (for one batch) at step 70: 302.2275, Accuracy: 0.8659\n",
      "Training loss (for one batch) at step 80: 303.0952, Accuracy: 0.8573\n",
      "Training loss (for one batch) at step 90: 287.2557, Accuracy: 0.8537\n",
      "Training loss (for one batch) at step 100: 276.7504, Accuracy: 0.8543\n",
      "Training loss (for one batch) at step 110: 293.5527, Accuracy: 0.8536\n",
      "---- Training ----\n",
      "Training loss: 91.1071\n",
      "Training acc over epoch: 0.8520\n",
      "---- Validation ----\n",
      "Validation loss: 52.3596\n",
      "Validation acc: 0.7749\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 320.0546, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 305.1581, Accuracy: 0.8310\n",
      "Training loss (for one batch) at step 20: 291.6830, Accuracy: 0.8411\n",
      "Training loss (for one batch) at step 30: 280.1749, Accuracy: 0.8553\n",
      "Training loss (for one batch) at step 40: 275.0242, Accuracy: 0.8660\n",
      "Training loss (for one batch) at step 50: 268.7396, Accuracy: 0.8747\n",
      "Training loss (for one batch) at step 60: 306.4724, Accuracy: 0.8779\n",
      "Training loss (for one batch) at step 70: 300.3420, Accuracy: 0.8691\n",
      "Training loss (for one batch) at step 80: 306.6316, Accuracy: 0.8597\n",
      "Training loss (for one batch) at step 90: 278.4256, Accuracy: 0.8562\n",
      "Training loss (for one batch) at step 100: 283.4507, Accuracy: 0.8572\n",
      "Training loss (for one batch) at step 110: 288.2632, Accuracy: 0.8557\n",
      "---- Training ----\n",
      "Training loss: 105.1243\n",
      "Training acc over epoch: 0.8542\n",
      "---- Validation ----\n",
      "Validation loss: 64.0787\n",
      "Validation acc: 0.7724\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 326.9426, Accuracy: 0.8516\n",
      "Training loss (for one batch) at step 10: 322.9617, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 20: 282.7677, Accuracy: 0.8348\n",
      "Training loss (for one batch) at step 30: 300.8513, Accuracy: 0.8488\n",
      "Training loss (for one batch) at step 40: 277.5258, Accuracy: 0.8592\n",
      "Training loss (for one batch) at step 50: 284.3698, Accuracy: 0.8684\n",
      "Training loss (for one batch) at step 60: 302.7748, Accuracy: 0.8710\n",
      "Training loss (for one batch) at step 70: 315.2583, Accuracy: 0.8654\n",
      "Training loss (for one batch) at step 80: 320.1569, Accuracy: 0.8560\n",
      "Training loss (for one batch) at step 90: 280.7267, Accuracy: 0.8528\n",
      "Training loss (for one batch) at step 100: 278.1841, Accuracy: 0.8543\n",
      "Training loss (for one batch) at step 110: 298.9940, Accuracy: 0.8528\n",
      "---- Training ----\n",
      "Training loss: 107.7260\n",
      "Training acc over epoch: 0.8518\n",
      "---- Validation ----\n",
      "Validation loss: 43.6378\n",
      "Validation acc: 0.7751\n",
      "Time taken: 10.53s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.7706\n",
      "Validation AUC: 0.7714\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 677.2673, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 581.2394, Accuracy: 0.5071\n",
      "Training loss (for one batch) at step 20: 579.2819, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 30: 555.8775, Accuracy: 0.5073\n",
      "Training loss (for one batch) at step 40: 532.6993, Accuracy: 0.5061\n",
      "Training loss (for one batch) at step 50: 492.2612, Accuracy: 0.5057\n",
      "Training loss (for one batch) at step 60: 490.4420, Accuracy: 0.5092\n",
      "Training loss (for one batch) at step 70: 478.1641, Accuracy: 0.5059\n",
      "Training loss (for one batch) at step 80: 494.5961, Accuracy: 0.5082\n",
      "Training loss (for one batch) at step 90: 470.5297, Accuracy: 0.5076\n",
      "Training loss (for one batch) at step 100: 483.2734, Accuracy: 0.5077\n",
      "Training loss (for one batch) at step 110: 464.3497, Accuracy: 0.5076\n",
      "---- Training ----\n",
      "Training loss: 144.3322\n",
      "Training acc over epoch: 0.5085\n",
      "---- Validation ----\n",
      "Validation loss: 34.5667\n",
      "Validation acc: 0.4987\n",
      "Time taken: 12.59s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 464.8659, Accuracy: 0.4688\n",
      "Training loss (for one batch) at step 10: 462.1730, Accuracy: 0.5206\n",
      "Training loss (for one batch) at step 20: 464.7255, Accuracy: 0.5190\n",
      "Training loss (for one batch) at step 30: 463.6003, Accuracy: 0.5242\n",
      "Training loss (for one batch) at step 40: 458.6390, Accuracy: 0.5196\n",
      "Training loss (for one batch) at step 50: 457.6321, Accuracy: 0.5201\n",
      "Training loss (for one batch) at step 60: 455.8428, Accuracy: 0.5214\n",
      "Training loss (for one batch) at step 70: 458.2590, Accuracy: 0.5217\n",
      "Training loss (for one batch) at step 80: 450.9918, Accuracy: 0.5223\n",
      "Training loss (for one batch) at step 90: 452.5443, Accuracy: 0.5261\n",
      "Training loss (for one batch) at step 100: 448.6747, Accuracy: 0.5253\n",
      "Training loss (for one batch) at step 110: 446.3013, Accuracy: 0.5241\n",
      "---- Training ----\n",
      "Training loss: 140.5462\n",
      "Training acc over epoch: 0.5239\n",
      "---- Validation ----\n",
      "Validation loss: 34.7966\n",
      "Validation acc: 0.4860\n",
      "Time taken: 12.78s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 446.4506, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 447.6201, Accuracy: 0.5320\n",
      "Training loss (for one batch) at step 20: 449.3411, Accuracy: 0.5331\n",
      "Training loss (for one batch) at step 30: 445.9627, Accuracy: 0.5323\n",
      "Training loss (for one batch) at step 40: 445.8136, Accuracy: 0.5373\n",
      "Training loss (for one batch) at step 50: 449.8102, Accuracy: 0.5377\n",
      "Training loss (for one batch) at step 60: 442.7346, Accuracy: 0.5392\n",
      "Training loss (for one batch) at step 70: 446.5030, Accuracy: 0.5454\n",
      "Training loss (for one batch) at step 80: 446.9936, Accuracy: 0.5478\n",
      "Training loss (for one batch) at step 90: 444.7494, Accuracy: 0.5454\n",
      "Training loss (for one batch) at step 100: 450.0935, Accuracy: 0.5427\n",
      "Training loss (for one batch) at step 110: 447.6829, Accuracy: 0.5436\n",
      "---- Training ----\n",
      "Training loss: 139.5912\n",
      "Training acc over epoch: 0.5443\n",
      "---- Validation ----\n",
      "Validation loss: 34.4702\n",
      "Validation acc: 0.5142\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 443.0083, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 447.0536, Accuracy: 0.5440\n",
      "Training loss (for one batch) at step 20: 442.0904, Accuracy: 0.5614\n",
      "Training loss (for one batch) at step 30: 443.8350, Accuracy: 0.5585\n",
      "Training loss (for one batch) at step 40: 443.6723, Accuracy: 0.5602\n",
      "Training loss (for one batch) at step 50: 443.6167, Accuracy: 0.5594\n",
      "Training loss (for one batch) at step 60: 443.1429, Accuracy: 0.5598\n",
      "Training loss (for one batch) at step 70: 443.7202, Accuracy: 0.5637\n",
      "Training loss (for one batch) at step 80: 448.4958, Accuracy: 0.5662\n",
      "Training loss (for one batch) at step 90: 444.5283, Accuracy: 0.5663\n",
      "Training loss (for one batch) at step 100: 441.4042, Accuracy: 0.5649\n",
      "Training loss (for one batch) at step 110: 444.7212, Accuracy: 0.5634\n",
      "---- Training ----\n",
      "Training loss: 139.7853\n",
      "Training acc over epoch: 0.5625\n",
      "---- Validation ----\n",
      "Validation loss: 34.4876\n",
      "Validation acc: 0.5605\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 442.4558, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 443.1179, Accuracy: 0.5852\n",
      "Training loss (for one batch) at step 20: 442.6433, Accuracy: 0.5926\n",
      "Training loss (for one batch) at step 30: 444.9324, Accuracy: 0.5897\n",
      "Training loss (for one batch) at step 40: 440.5774, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 50: 440.5498, Accuracy: 0.5933\n",
      "Training loss (for one batch) at step 60: 443.3193, Accuracy: 0.5941\n",
      "Training loss (for one batch) at step 70: 441.5272, Accuracy: 0.5971\n",
      "Training loss (for one batch) at step 80: 448.4784, Accuracy: 0.5947\n",
      "Training loss (for one batch) at step 90: 445.0501, Accuracy: 0.5934\n",
      "Training loss (for one batch) at step 100: 441.7631, Accuracy: 0.5897\n",
      "Training loss (for one batch) at step 110: 442.1584, Accuracy: 0.5874\n",
      "---- Training ----\n",
      "Training loss: 136.4060\n",
      "Training acc over epoch: 0.5891\n",
      "---- Validation ----\n",
      "Validation loss: 34.5836\n",
      "Validation acc: 0.6333\n",
      "Time taken: 10.89s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 441.2460, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 445.2133, Accuracy: 0.6009\n",
      "Training loss (for one batch) at step 20: 443.9429, Accuracy: 0.5882\n",
      "Training loss (for one batch) at step 30: 439.6685, Accuracy: 0.5930\n",
      "Training loss (for one batch) at step 40: 440.4455, Accuracy: 0.5985\n",
      "Training loss (for one batch) at step 50: 440.6529, Accuracy: 0.6023\n",
      "Training loss (for one batch) at step 60: 439.9369, Accuracy: 0.6086\n",
      "Training loss (for one batch) at step 70: 449.8538, Accuracy: 0.6134\n",
      "Training loss (for one batch) at step 80: 438.7595, Accuracy: 0.6125\n",
      "Training loss (for one batch) at step 90: 441.7398, Accuracy: 0.6155\n",
      "Training loss (for one batch) at step 100: 437.4805, Accuracy: 0.6137\n",
      "Training loss (for one batch) at step 110: 441.3376, Accuracy: 0.6141\n",
      "---- Training ----\n",
      "Training loss: 137.1547\n",
      "Training acc over epoch: 0.6124\n",
      "---- Validation ----\n",
      "Validation loss: 34.9540\n",
      "Validation acc: 0.6077\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 440.6086, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 442.3529, Accuracy: 0.6378\n",
      "Training loss (for one batch) at step 20: 442.2672, Accuracy: 0.6209\n",
      "Training loss (for one batch) at step 30: 438.3810, Accuracy: 0.6205\n",
      "Training loss (for one batch) at step 40: 438.9249, Accuracy: 0.6273\n",
      "Training loss (for one batch) at step 50: 437.1301, Accuracy: 0.6377\n",
      "Training loss (for one batch) at step 60: 439.9996, Accuracy: 0.6413\n",
      "Training loss (for one batch) at step 70: 438.1773, Accuracy: 0.6419\n",
      "Training loss (for one batch) at step 80: 440.5350, Accuracy: 0.6375\n",
      "Training loss (for one batch) at step 90: 436.1188, Accuracy: 0.6362\n",
      "Training loss (for one batch) at step 100: 439.4239, Accuracy: 0.6355\n",
      "Training loss (for one batch) at step 110: 445.4628, Accuracy: 0.6347\n",
      "---- Training ----\n",
      "Training loss: 137.4289\n",
      "Training acc over epoch: 0.6343\n",
      "---- Validation ----\n",
      "Validation loss: 34.8652\n",
      "Validation acc: 0.6274\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 443.2073, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 440.4510, Accuracy: 0.6335\n",
      "Training loss (for one batch) at step 20: 438.0289, Accuracy: 0.6217\n",
      "Training loss (for one batch) at step 30: 440.5944, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 40: 432.8438, Accuracy: 0.6366\n",
      "Training loss (for one batch) at step 50: 435.9658, Accuracy: 0.6429\n",
      "Training loss (for one batch) at step 60: 437.4071, Accuracy: 0.6496\n",
      "Training loss (for one batch) at step 70: 440.0954, Accuracy: 0.6557\n",
      "Training loss (for one batch) at step 80: 437.5114, Accuracy: 0.6552\n",
      "Training loss (for one batch) at step 90: 437.1205, Accuracy: 0.6564\n",
      "Training loss (for one batch) at step 100: 437.1130, Accuracy: 0.6556\n",
      "Training loss (for one batch) at step 110: 440.3513, Accuracy: 0.6566\n",
      "---- Training ----\n",
      "Training loss: 135.0636\n",
      "Training acc over epoch: 0.6566\n",
      "---- Validation ----\n",
      "Validation loss: 34.3416\n",
      "Validation acc: 0.6760\n",
      "Time taken: 10.81s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 441.8034, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 441.3863, Accuracy: 0.6371\n",
      "Training loss (for one batch) at step 20: 436.4974, Accuracy: 0.6324\n",
      "Training loss (for one batch) at step 30: 436.0327, Accuracy: 0.6396\n",
      "Training loss (for one batch) at step 40: 431.3131, Accuracy: 0.6532\n",
      "Training loss (for one batch) at step 50: 432.0320, Accuracy: 0.6621\n",
      "Training loss (for one batch) at step 60: 440.5312, Accuracy: 0.6691\n",
      "Training loss (for one batch) at step 70: 437.6582, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 80: 440.6906, Accuracy: 0.6678\n",
      "Training loss (for one batch) at step 90: 436.9073, Accuracy: 0.6671\n",
      "Training loss (for one batch) at step 100: 433.5949, Accuracy: 0.6669\n",
      "Training loss (for one batch) at step 110: 441.3997, Accuracy: 0.6679\n",
      "---- Training ----\n",
      "Training loss: 137.4535\n",
      "Training acc over epoch: 0.6682\n",
      "---- Validation ----\n",
      "Validation loss: 33.3669\n",
      "Validation acc: 0.6948\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 437.5460, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 442.3649, Accuracy: 0.6605\n",
      "Training loss (for one batch) at step 20: 438.6232, Accuracy: 0.6510\n",
      "Training loss (for one batch) at step 30: 438.4321, Accuracy: 0.6593\n",
      "Training loss (for one batch) at step 40: 428.3584, Accuracy: 0.6732\n",
      "Training loss (for one batch) at step 50: 425.3256, Accuracy: 0.6794\n",
      "Training loss (for one batch) at step 60: 442.1224, Accuracy: 0.6825\n",
      "Training loss (for one batch) at step 70: 438.7300, Accuracy: 0.6846\n",
      "Training loss (for one batch) at step 80: 436.7626, Accuracy: 0.6805\n",
      "Training loss (for one batch) at step 90: 431.3433, Accuracy: 0.6791\n",
      "Training loss (for one batch) at step 100: 438.3932, Accuracy: 0.6791\n",
      "Training loss (for one batch) at step 110: 436.7545, Accuracy: 0.6773\n",
      "---- Training ----\n",
      "Training loss: 134.7939\n",
      "Training acc over epoch: 0.6779\n",
      "---- Validation ----\n",
      "Validation loss: 35.1673\n",
      "Validation acc: 0.6792\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 439.2866, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 436.1134, Accuracy: 0.6996\n",
      "Training loss (for one batch) at step 20: 434.8713, Accuracy: 0.6782\n",
      "Training loss (for one batch) at step 30: 434.6031, Accuracy: 0.6741\n",
      "Training loss (for one batch) at step 40: 428.1767, Accuracy: 0.6810\n",
      "Training loss (for one batch) at step 50: 419.8405, Accuracy: 0.6880\n",
      "Training loss (for one batch) at step 60: 425.8524, Accuracy: 0.6960\n",
      "Training loss (for one batch) at step 70: 436.6333, Accuracy: 0.6993\n",
      "Training loss (for one batch) at step 80: 437.6504, Accuracy: 0.6978\n",
      "Training loss (for one batch) at step 90: 439.8659, Accuracy: 0.6963\n",
      "Training loss (for one batch) at step 100: 425.5933, Accuracy: 0.6965\n",
      "Training loss (for one batch) at step 110: 435.9454, Accuracy: 0.6977\n",
      "---- Training ----\n",
      "Training loss: 137.3038\n",
      "Training acc over epoch: 0.6994\n",
      "---- Validation ----\n",
      "Validation loss: 34.2215\n",
      "Validation acc: 0.6953\n",
      "Time taken: 10.87s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 441.4544, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 434.5100, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 20: 439.9641, Accuracy: 0.6856\n",
      "Training loss (for one batch) at step 30: 431.4008, Accuracy: 0.6850\n",
      "Training loss (for one batch) at step 40: 418.8847, Accuracy: 0.6968\n",
      "Training loss (for one batch) at step 50: 423.7908, Accuracy: 0.7022\n",
      "Training loss (for one batch) at step 60: 428.6934, Accuracy: 0.7063\n",
      "Training loss (for one batch) at step 70: 441.2367, Accuracy: 0.7064\n",
      "Training loss (for one batch) at step 80: 430.3654, Accuracy: 0.7045\n",
      "Training loss (for one batch) at step 90: 434.5731, Accuracy: 0.6998\n",
      "Training loss (for one batch) at step 100: 427.1565, Accuracy: 0.6989\n",
      "Training loss (for one batch) at step 110: 438.8370, Accuracy: 0.7009\n",
      "---- Training ----\n",
      "Training loss: 133.1715\n",
      "Training acc over epoch: 0.7013\n",
      "---- Validation ----\n",
      "Validation loss: 34.3355\n",
      "Validation acc: 0.7039\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 436.7478, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 427.1711, Accuracy: 0.7038\n",
      "Training loss (for one batch) at step 20: 434.7709, Accuracy: 0.6916\n",
      "Training loss (for one batch) at step 30: 429.4750, Accuracy: 0.6951\n",
      "Training loss (for one batch) at step 40: 424.3183, Accuracy: 0.7050\n",
      "Training loss (for one batch) at step 50: 424.7177, Accuracy: 0.7122\n",
      "Training loss (for one batch) at step 60: 427.3314, Accuracy: 0.7225\n",
      "Training loss (for one batch) at step 70: 445.9889, Accuracy: 0.7217\n",
      "Training loss (for one batch) at step 80: 428.5839, Accuracy: 0.7199\n",
      "Training loss (for one batch) at step 90: 435.9554, Accuracy: 0.7177\n",
      "Training loss (for one batch) at step 100: 427.3769, Accuracy: 0.7174\n",
      "Training loss (for one batch) at step 110: 424.6147, Accuracy: 0.7173\n",
      "---- Training ----\n",
      "Training loss: 139.7351\n",
      "Training acc over epoch: 0.7184\n",
      "---- Validation ----\n",
      "Validation loss: 34.3176\n",
      "Validation acc: 0.7278\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 438.9641, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 430.2778, Accuracy: 0.6911\n",
      "Training loss (for one batch) at step 20: 427.5414, Accuracy: 0.7057\n",
      "Training loss (for one batch) at step 30: 423.0846, Accuracy: 0.7185\n",
      "Training loss (for one batch) at step 40: 422.5021, Accuracy: 0.7237\n",
      "Training loss (for one batch) at step 50: 421.2093, Accuracy: 0.7284\n",
      "Training loss (for one batch) at step 60: 417.2529, Accuracy: 0.7345\n",
      "Training loss (for one batch) at step 70: 432.6679, Accuracy: 0.7361\n",
      "Training loss (for one batch) at step 80: 433.0209, Accuracy: 0.7323\n",
      "Training loss (for one batch) at step 90: 431.6317, Accuracy: 0.7292\n",
      "Training loss (for one batch) at step 100: 436.6175, Accuracy: 0.7280\n",
      "Training loss (for one batch) at step 110: 427.0572, Accuracy: 0.7299\n",
      "---- Training ----\n",
      "Training loss: 141.2633\n",
      "Training acc over epoch: 0.7310\n",
      "---- Validation ----\n",
      "Validation loss: 36.9776\n",
      "Validation acc: 0.7217\n",
      "Time taken: 10.76s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 443.7318, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 430.0679, Accuracy: 0.7237\n",
      "Training loss (for one batch) at step 20: 434.7518, Accuracy: 0.7262\n",
      "Training loss (for one batch) at step 30: 427.8959, Accuracy: 0.7336\n",
      "Training loss (for one batch) at step 40: 429.0648, Accuracy: 0.7389\n",
      "Training loss (for one batch) at step 50: 414.3381, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 60: 422.9557, Accuracy: 0.7555\n",
      "Training loss (for one batch) at step 70: 428.8489, Accuracy: 0.7552\n",
      "Training loss (for one batch) at step 80: 426.6693, Accuracy: 0.7554\n",
      "Training loss (for one batch) at step 90: 442.3469, Accuracy: 0.7509\n",
      "Training loss (for one batch) at step 100: 418.7415, Accuracy: 0.7492\n",
      "Training loss (for one batch) at step 110: 420.5645, Accuracy: 0.7490\n",
      "---- Training ----\n",
      "Training loss: 131.6894\n",
      "Training acc over epoch: 0.7509\n",
      "---- Validation ----\n",
      "Validation loss: 33.1495\n",
      "Validation acc: 0.7598\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 432.0602, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 429.6133, Accuracy: 0.7607\n",
      "Training loss (for one batch) at step 20: 435.0574, Accuracy: 0.7452\n",
      "Training loss (for one batch) at step 30: 416.5325, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 40: 417.7135, Accuracy: 0.7443\n",
      "Training loss (for one batch) at step 50: 410.8358, Accuracy: 0.7541\n",
      "Training loss (for one batch) at step 60: 417.6462, Accuracy: 0.7592\n",
      "Training loss (for one batch) at step 70: 432.8798, Accuracy: 0.7589\n",
      "Training loss (for one batch) at step 80: 423.5776, Accuracy: 0.7577\n",
      "Training loss (for one batch) at step 90: 426.4222, Accuracy: 0.7547\n",
      "Training loss (for one batch) at step 100: 427.1886, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 110: 419.4133, Accuracy: 0.7526\n",
      "---- Training ----\n",
      "Training loss: 131.5611\n",
      "Training acc over epoch: 0.7556\n",
      "---- Validation ----\n",
      "Validation loss: 35.0059\n",
      "Validation acc: 0.7563\n",
      "Time taken: 10.65s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 439.5753, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 428.1664, Accuracy: 0.7706\n",
      "Training loss (for one batch) at step 20: 427.5363, Accuracy: 0.7690\n",
      "Training loss (for one batch) at step 30: 418.4284, Accuracy: 0.7724\n",
      "Training loss (for one batch) at step 40: 411.4120, Accuracy: 0.7736\n",
      "Training loss (for one batch) at step 50: 399.9776, Accuracy: 0.7826\n",
      "Training loss (for one batch) at step 60: 424.4024, Accuracy: 0.7868\n",
      "Training loss (for one batch) at step 70: 423.3490, Accuracy: 0.7853\n",
      "Training loss (for one batch) at step 80: 433.8413, Accuracy: 0.7815\n",
      "Training loss (for one batch) at step 90: 413.6685, Accuracy: 0.7778\n",
      "Training loss (for one batch) at step 100: 409.4390, Accuracy: 0.7758\n",
      "Training loss (for one batch) at step 110: 425.7174, Accuracy: 0.7767\n",
      "---- Training ----\n",
      "Training loss: 135.0017\n",
      "Training acc over epoch: 0.7781\n",
      "---- Validation ----\n",
      "Validation loss: 33.5822\n",
      "Validation acc: 0.7708\n",
      "Time taken: 10.97s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 429.2609, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 421.1105, Accuracy: 0.7848\n",
      "Training loss (for one batch) at step 20: 425.6301, Accuracy: 0.7731\n",
      "Training loss (for one batch) at step 30: 421.0854, Accuracy: 0.7719\n",
      "Training loss (for one batch) at step 40: 405.9682, Accuracy: 0.7807\n",
      "Training loss (for one batch) at step 50: 413.5468, Accuracy: 0.7880\n",
      "Training loss (for one batch) at step 60: 415.0499, Accuracy: 0.7901\n",
      "Training loss (for one batch) at step 70: 428.8753, Accuracy: 0.7916\n",
      "Training loss (for one batch) at step 80: 426.2506, Accuracy: 0.7896\n",
      "Training loss (for one batch) at step 90: 426.0892, Accuracy: 0.7856\n",
      "Training loss (for one batch) at step 100: 408.7941, Accuracy: 0.7843\n",
      "Training loss (for one batch) at step 110: 414.7542, Accuracy: 0.7846\n",
      "---- Training ----\n",
      "Training loss: 128.1788\n",
      "Training acc over epoch: 0.7861\n",
      "---- Validation ----\n",
      "Validation loss: 31.4967\n",
      "Validation acc: 0.7657\n",
      "Time taken: 10.56s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 430.2687, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 417.8111, Accuracy: 0.7834\n",
      "Training loss (for one batch) at step 20: 416.7890, Accuracy: 0.7719\n",
      "Training loss (for one batch) at step 30: 407.6727, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 40: 405.0453, Accuracy: 0.7792\n",
      "Training loss (for one batch) at step 50: 391.0949, Accuracy: 0.7895\n",
      "Training loss (for one batch) at step 60: 403.5813, Accuracy: 0.7898\n",
      "Training loss (for one batch) at step 70: 422.1541, Accuracy: 0.7902\n",
      "Training loss (for one batch) at step 80: 414.8164, Accuracy: 0.7899\n",
      "Training loss (for one batch) at step 90: 429.5353, Accuracy: 0.7879\n",
      "Training loss (for one batch) at step 100: 410.3341, Accuracy: 0.7874\n",
      "Training loss (for one batch) at step 110: 413.9721, Accuracy: 0.7882\n",
      "---- Training ----\n",
      "Training loss: 128.4570\n",
      "Training acc over epoch: 0.7870\n",
      "---- Validation ----\n",
      "Validation loss: 33.7593\n",
      "Validation acc: 0.7628\n",
      "Time taken: 10.69s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 425.3927, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 416.2864, Accuracy: 0.7827\n",
      "Training loss (for one batch) at step 20: 410.2612, Accuracy: 0.7790\n",
      "Training loss (for one batch) at step 30: 412.8137, Accuracy: 0.7888\n",
      "Training loss (for one batch) at step 40: 394.9622, Accuracy: 0.7995\n",
      "Training loss (for one batch) at step 50: 403.9757, Accuracy: 0.8036\n",
      "Training loss (for one batch) at step 60: 403.5212, Accuracy: 0.8072\n",
      "Training loss (for one batch) at step 70: 421.6627, Accuracy: 0.8051\n",
      "Training loss (for one batch) at step 80: 419.7746, Accuracy: 0.8023\n",
      "Training loss (for one batch) at step 90: 420.0815, Accuracy: 0.8000\n",
      "Training loss (for one batch) at step 100: 404.1140, Accuracy: 0.7982\n",
      "Training loss (for one batch) at step 110: 414.9644, Accuracy: 0.7984\n",
      "---- Training ----\n",
      "Training loss: 132.6571\n",
      "Training acc over epoch: 0.7973\n",
      "---- Validation ----\n",
      "Validation loss: 36.0196\n",
      "Validation acc: 0.7714\n",
      "Time taken: 10.86s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 419.9167, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 407.0996, Accuracy: 0.7997\n",
      "Training loss (for one batch) at step 20: 404.2572, Accuracy: 0.8036\n",
      "Training loss (for one batch) at step 30: 406.5097, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 40: 385.9386, Accuracy: 0.8087\n",
      "Training loss (for one batch) at step 50: 388.6976, Accuracy: 0.8183\n",
      "Training loss (for one batch) at step 60: 403.8168, Accuracy: 0.8208\n",
      "Training loss (for one batch) at step 70: 423.0021, Accuracy: 0.8182\n",
      "Training loss (for one batch) at step 80: 415.4297, Accuracy: 0.8160\n",
      "Training loss (for one batch) at step 90: 406.8246, Accuracy: 0.8141\n",
      "Training loss (for one batch) at step 100: 392.5364, Accuracy: 0.8126\n",
      "Training loss (for one batch) at step 110: 407.6012, Accuracy: 0.8104\n",
      "---- Training ----\n",
      "Training loss: 130.4114\n",
      "Training acc over epoch: 0.8084\n",
      "---- Validation ----\n",
      "Validation loss: 32.9873\n",
      "Validation acc: 0.7622\n",
      "Time taken: 10.73s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 423.3012, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 10: 408.4063, Accuracy: 0.7841\n",
      "Training loss (for one batch) at step 20: 407.7357, Accuracy: 0.7846\n",
      "Training loss (for one batch) at step 30: 393.4982, Accuracy: 0.7906\n",
      "Training loss (for one batch) at step 40: 396.1083, Accuracy: 0.7984\n",
      "Training loss (for one batch) at step 50: 376.4335, Accuracy: 0.8087\n",
      "Training loss (for one batch) at step 60: 403.3264, Accuracy: 0.8140\n",
      "Training loss (for one batch) at step 70: 395.6392, Accuracy: 0.8133\n",
      "Training loss (for one batch) at step 80: 415.8322, Accuracy: 0.8110\n",
      "Training loss (for one batch) at step 90: 413.8331, Accuracy: 0.8085\n",
      "Training loss (for one batch) at step 100: 400.8291, Accuracy: 0.8084\n",
      "Training loss (for one batch) at step 110: 394.7391, Accuracy: 0.8076\n",
      "---- Training ----\n",
      "Training loss: 125.4064\n",
      "Training acc over epoch: 0.8076\n",
      "---- Validation ----\n",
      "Validation loss: 34.3483\n",
      "Validation acc: 0.7708\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 417.0527, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 415.8273, Accuracy: 0.7876\n",
      "Training loss (for one batch) at step 20: 405.4127, Accuracy: 0.7946\n",
      "Training loss (for one batch) at step 30: 400.1285, Accuracy: 0.8024\n",
      "Training loss (for one batch) at step 40: 384.7379, Accuracy: 0.8117\n",
      "Training loss (for one batch) at step 50: 391.7111, Accuracy: 0.8172\n",
      "Training loss (for one batch) at step 60: 388.3569, Accuracy: 0.8201\n",
      "Training loss (for one batch) at step 70: 410.8501, Accuracy: 0.8194\n",
      "Training loss (for one batch) at step 80: 408.2659, Accuracy: 0.8170\n",
      "Training loss (for one batch) at step 90: 404.2373, Accuracy: 0.8154\n",
      "Training loss (for one batch) at step 100: 379.6933, Accuracy: 0.8130\n",
      "Training loss (for one batch) at step 110: 397.8228, Accuracy: 0.8125\n",
      "---- Training ----\n",
      "Training loss: 128.9834\n",
      "Training acc over epoch: 0.8124\n",
      "---- Validation ----\n",
      "Validation loss: 38.0520\n",
      "Validation acc: 0.7544\n",
      "Time taken: 10.81s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 399.8945, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 401.9043, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 20: 401.8482, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 30: 405.5928, Accuracy: 0.8044\n",
      "Training loss (for one batch) at step 40: 371.5076, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 50: 380.4864, Accuracy: 0.8192\n",
      "Training loss (for one batch) at step 60: 398.4462, Accuracy: 0.8259\n",
      "Training loss (for one batch) at step 70: 415.9331, Accuracy: 0.8236\n",
      "Training loss (for one batch) at step 80: 400.7784, Accuracy: 0.8214\n",
      "Training loss (for one batch) at step 90: 383.4714, Accuracy: 0.8189\n",
      "Training loss (for one batch) at step 100: 405.2007, Accuracy: 0.8164\n",
      "Training loss (for one batch) at step 110: 405.2706, Accuracy: 0.8159\n",
      "---- Training ----\n",
      "Training loss: 128.7243\n",
      "Training acc over epoch: 0.8145\n",
      "---- Validation ----\n",
      "Validation loss: 38.5386\n",
      "Validation acc: 0.7536\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 399.7682, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 399.9964, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 20: 396.1204, Accuracy: 0.8151\n",
      "Training loss (for one batch) at step 30: 392.5513, Accuracy: 0.8185\n",
      "Training loss (for one batch) at step 40: 375.3886, Accuracy: 0.8230\n",
      "Training loss (for one batch) at step 50: 365.2898, Accuracy: 0.8300\n",
      "Training loss (for one batch) at step 60: 394.9301, Accuracy: 0.8340\n",
      "Training loss (for one batch) at step 70: 399.6221, Accuracy: 0.8314\n",
      "Training loss (for one batch) at step 80: 401.2676, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 90: 383.0656, Accuracy: 0.8268\n",
      "Training loss (for one batch) at step 100: 405.9620, Accuracy: 0.8254\n",
      "Training loss (for one batch) at step 110: 399.8782, Accuracy: 0.8244\n",
      "---- Training ----\n",
      "Training loss: 125.0442\n",
      "Training acc over epoch: 0.8227\n",
      "---- Validation ----\n",
      "Validation loss: 35.1754\n",
      "Validation acc: 0.7579\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 410.0665, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 10: 393.8785, Accuracy: 0.8026\n",
      "Training loss (for one batch) at step 20: 394.9547, Accuracy: 0.7987\n",
      "Training loss (for one batch) at step 30: 387.5130, Accuracy: 0.8054\n",
      "Training loss (for one batch) at step 40: 366.3856, Accuracy: 0.8119\n",
      "Training loss (for one batch) at step 50: 373.5953, Accuracy: 0.8214\n",
      "Training loss (for one batch) at step 60: 392.4486, Accuracy: 0.8225\n",
      "Training loss (for one batch) at step 70: 392.6331, Accuracy: 0.8233\n",
      "Training loss (for one batch) at step 80: 394.1107, Accuracy: 0.8218\n",
      "Training loss (for one batch) at step 90: 386.9005, Accuracy: 0.8166\n",
      "Training loss (for one batch) at step 100: 373.1960, Accuracy: 0.8164\n",
      "Training loss (for one batch) at step 110: 388.5936, Accuracy: 0.8163\n",
      "---- Training ----\n",
      "Training loss: 123.1404\n",
      "Training acc over epoch: 0.8157\n",
      "---- Validation ----\n",
      "Validation loss: 41.4463\n",
      "Validation acc: 0.7749\n",
      "Time taken: 10.83s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 400.8409, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 392.7908, Accuracy: 0.8089\n",
      "Training loss (for one batch) at step 20: 380.0320, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 30: 387.0012, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 40: 371.0172, Accuracy: 0.8157\n",
      "Training loss (for one batch) at step 50: 368.0935, Accuracy: 0.8255\n",
      "Training loss (for one batch) at step 60: 387.2258, Accuracy: 0.8298\n",
      "Training loss (for one batch) at step 70: 385.7882, Accuracy: 0.8289\n",
      "Training loss (for one batch) at step 80: 395.3142, Accuracy: 0.8256\n",
      "Training loss (for one batch) at step 90: 381.1264, Accuracy: 0.8236\n",
      "Training loss (for one batch) at step 100: 375.1392, Accuracy: 0.8230\n",
      "Training loss (for one batch) at step 110: 380.3836, Accuracy: 0.8240\n",
      "---- Training ----\n",
      "Training loss: 120.9890\n",
      "Training acc over epoch: 0.8225\n",
      "---- Validation ----\n",
      "Validation loss: 37.4060\n",
      "Validation acc: 0.7633\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 394.4951, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 388.0198, Accuracy: 0.7990\n",
      "Training loss (for one batch) at step 20: 379.3996, Accuracy: 0.8069\n",
      "Training loss (for one batch) at step 30: 369.3843, Accuracy: 0.8062\n",
      "Training loss (for one batch) at step 40: 358.1204, Accuracy: 0.8184\n",
      "Training loss (for one batch) at step 50: 355.4960, Accuracy: 0.8255\n",
      "Training loss (for one batch) at step 60: 384.6211, Accuracy: 0.8272\n",
      "Training loss (for one batch) at step 70: 390.1536, Accuracy: 0.8259\n",
      "Training loss (for one batch) at step 80: 367.7871, Accuracy: 0.8236\n",
      "Training loss (for one batch) at step 90: 380.4765, Accuracy: 0.8232\n",
      "Training loss (for one batch) at step 100: 384.8299, Accuracy: 0.8218\n",
      "Training loss (for one batch) at step 110: 378.2631, Accuracy: 0.8231\n",
      "---- Training ----\n",
      "Training loss: 125.6189\n",
      "Training acc over epoch: 0.8217\n",
      "---- Validation ----\n",
      "Validation loss: 33.9511\n",
      "Validation acc: 0.7415\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 386.6613, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 10: 390.7158, Accuracy: 0.8061\n",
      "Training loss (for one batch) at step 20: 366.3828, Accuracy: 0.8136\n",
      "Training loss (for one batch) at step 30: 378.5334, Accuracy: 0.8140\n",
      "Training loss (for one batch) at step 40: 359.0334, Accuracy: 0.8209\n",
      "Training loss (for one batch) at step 50: 359.9614, Accuracy: 0.8289\n",
      "Training loss (for one batch) at step 60: 375.6619, Accuracy: 0.8338\n",
      "Training loss (for one batch) at step 70: 370.0365, Accuracy: 0.8301\n",
      "Training loss (for one batch) at step 80: 376.2784, Accuracy: 0.8272\n",
      "Training loss (for one batch) at step 90: 381.1819, Accuracy: 0.8249\n",
      "Training loss (for one batch) at step 100: 373.0674, Accuracy: 0.8245\n",
      "Training loss (for one batch) at step 110: 374.2797, Accuracy: 0.8238\n",
      "---- Training ----\n",
      "Training loss: 120.2400\n",
      "Training acc over epoch: 0.8227\n",
      "---- Validation ----\n",
      "Validation loss: 43.1141\n",
      "Validation acc: 0.7472\n",
      "Time taken: 10.86s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 419.9386, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 385.6642, Accuracy: 0.8004\n",
      "Training loss (for one batch) at step 20: 374.5140, Accuracy: 0.8129\n",
      "Training loss (for one batch) at step 30: 369.5493, Accuracy: 0.8198\n",
      "Training loss (for one batch) at step 40: 368.4387, Accuracy: 0.8256\n",
      "Training loss (for one batch) at step 50: 353.6860, Accuracy: 0.8378\n",
      "Training loss (for one batch) at step 60: 366.3031, Accuracy: 0.8407\n",
      "Training loss (for one batch) at step 70: 374.9124, Accuracy: 0.8369\n",
      "Training loss (for one batch) at step 80: 384.6907, Accuracy: 0.8347\n",
      "Training loss (for one batch) at step 90: 363.4086, Accuracy: 0.8334\n",
      "Training loss (for one batch) at step 100: 390.5773, Accuracy: 0.8308\n",
      "Training loss (for one batch) at step 110: 367.5106, Accuracy: 0.8299\n",
      "---- Training ----\n",
      "Training loss: 124.8327\n",
      "Training acc over epoch: 0.8289\n",
      "---- Validation ----\n",
      "Validation loss: 39.4714\n",
      "Validation acc: 0.7456\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 414.6310, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 380.1789, Accuracy: 0.8061\n",
      "Training loss (for one batch) at step 20: 364.0997, Accuracy: 0.8207\n",
      "Training loss (for one batch) at step 30: 362.4333, Accuracy: 0.8254\n",
      "Training loss (for one batch) at step 40: 342.9465, Accuracy: 0.8317\n",
      "Training loss (for one batch) at step 50: 341.1954, Accuracy: 0.8410\n",
      "Training loss (for one batch) at step 60: 368.6921, Accuracy: 0.8421\n",
      "Training loss (for one batch) at step 70: 384.7936, Accuracy: 0.8349\n",
      "Training loss (for one batch) at step 80: 384.7249, Accuracy: 0.8307\n",
      "Training loss (for one batch) at step 90: 367.8988, Accuracy: 0.8273\n",
      "Training loss (for one batch) at step 100: 372.4095, Accuracy: 0.8282\n",
      "Training loss (for one batch) at step 110: 370.8755, Accuracy: 0.8284\n",
      "---- Training ----\n",
      "Training loss: 119.6460\n",
      "Training acc over epoch: 0.8276\n",
      "---- Validation ----\n",
      "Validation loss: 37.1635\n",
      "Validation acc: 0.7703\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 390.4384, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 10: 378.9771, Accuracy: 0.7947\n",
      "Training loss (for one batch) at step 20: 373.0873, Accuracy: 0.8051\n",
      "Training loss (for one batch) at step 30: 350.2036, Accuracy: 0.8145\n",
      "Training loss (for one batch) at step 40: 360.9561, Accuracy: 0.8239\n",
      "Training loss (for one batch) at step 50: 362.3550, Accuracy: 0.8327\n",
      "Training loss (for one batch) at step 60: 355.3258, Accuracy: 0.8367\n",
      "Training loss (for one batch) at step 70: 376.1572, Accuracy: 0.8313\n",
      "Training loss (for one batch) at step 80: 379.8579, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 90: 373.7169, Accuracy: 0.8260\n",
      "Training loss (for one batch) at step 100: 377.6058, Accuracy: 0.8257\n",
      "Training loss (for one batch) at step 110: 352.5640, Accuracy: 0.8274\n",
      "---- Training ----\n",
      "Training loss: 112.8408\n",
      "Training acc over epoch: 0.8258\n",
      "---- Validation ----\n",
      "Validation loss: 41.0636\n",
      "Validation acc: 0.7098\n",
      "Time taken: 10.65s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 387.9246, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 374.1193, Accuracy: 0.8189\n",
      "Training loss (for one batch) at step 20: 367.5858, Accuracy: 0.8222\n",
      "Training loss (for one batch) at step 30: 338.3187, Accuracy: 0.8291\n",
      "Training loss (for one batch) at step 40: 351.7524, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 50: 336.0775, Accuracy: 0.8424\n",
      "Training loss (for one batch) at step 60: 346.1010, Accuracy: 0.8435\n",
      "Training loss (for one batch) at step 70: 370.8035, Accuracy: 0.8381\n",
      "Training loss (for one batch) at step 80: 375.7392, Accuracy: 0.8324\n",
      "Training loss (for one batch) at step 90: 346.6074, Accuracy: 0.8312\n",
      "Training loss (for one batch) at step 100: 366.0533, Accuracy: 0.8297\n",
      "Training loss (for one batch) at step 110: 355.8560, Accuracy: 0.8281\n",
      "---- Training ----\n",
      "Training loss: 120.7325\n",
      "Training acc over epoch: 0.8288\n",
      "---- Validation ----\n",
      "Validation loss: 35.3169\n",
      "Validation acc: 0.7372\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 381.7867, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 407.1985, Accuracy: 0.8026\n",
      "Training loss (for one batch) at step 20: 350.7893, Accuracy: 0.8199\n",
      "Training loss (for one batch) at step 30: 359.8756, Accuracy: 0.8291\n",
      "Training loss (for one batch) at step 40: 352.5099, Accuracy: 0.8323\n",
      "Training loss (for one batch) at step 50: 350.8496, Accuracy: 0.8415\n",
      "Training loss (for one batch) at step 60: 340.0128, Accuracy: 0.8435\n",
      "Training loss (for one batch) at step 70: 370.4892, Accuracy: 0.8402\n",
      "Training loss (for one batch) at step 80: 380.7016, Accuracy: 0.8348\n",
      "Training loss (for one batch) at step 90: 354.7298, Accuracy: 0.8328\n",
      "Training loss (for one batch) at step 100: 350.7638, Accuracy: 0.8318\n",
      "Training loss (for one batch) at step 110: 358.0717, Accuracy: 0.8313\n",
      "---- Training ----\n",
      "Training loss: 108.4092\n",
      "Training acc over epoch: 0.8306\n",
      "---- Validation ----\n",
      "Validation loss: 44.4971\n",
      "Validation acc: 0.7426\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 379.0904, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 378.7473, Accuracy: 0.8182\n",
      "Training loss (for one batch) at step 20: 354.8942, Accuracy: 0.8225\n",
      "Training loss (for one batch) at step 30: 348.5494, Accuracy: 0.8319\n",
      "Training loss (for one batch) at step 40: 337.6037, Accuracy: 0.8388\n",
      "Training loss (for one batch) at step 50: 338.0192, Accuracy: 0.8416\n",
      "Training loss (for one batch) at step 60: 348.0163, Accuracy: 0.8423\n",
      "Training loss (for one batch) at step 70: 352.6171, Accuracy: 0.8384\n",
      "Training loss (for one batch) at step 80: 371.7386, Accuracy: 0.8313\n",
      "Training loss (for one batch) at step 90: 343.8834, Accuracy: 0.8276\n",
      "Training loss (for one batch) at step 100: 351.4565, Accuracy: 0.8302\n",
      "Training loss (for one batch) at step 110: 338.9189, Accuracy: 0.8304\n",
      "---- Training ----\n",
      "Training loss: 108.2276\n",
      "Training acc over epoch: 0.8299\n",
      "---- Validation ----\n",
      "Validation loss: 47.7393\n",
      "Validation acc: 0.7316\n",
      "Time taken: 10.72s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 372.1556, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 368.0620, Accuracy: 0.8153\n",
      "Training loss (for one batch) at step 20: 363.2142, Accuracy: 0.8263\n",
      "Training loss (for one batch) at step 30: 355.1920, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 40: 332.7200, Accuracy: 0.8380\n",
      "Training loss (for one batch) at step 50: 342.4356, Accuracy: 0.8447\n",
      "Training loss (for one batch) at step 60: 365.0797, Accuracy: 0.8457\n",
      "Training loss (for one batch) at step 70: 381.7963, Accuracy: 0.8411\n",
      "Training loss (for one batch) at step 80: 382.4759, Accuracy: 0.8346\n",
      "Training loss (for one batch) at step 90: 358.6606, Accuracy: 0.8323\n",
      "Training loss (for one batch) at step 100: 347.7515, Accuracy: 0.8328\n",
      "Training loss (for one batch) at step 110: 372.5328, Accuracy: 0.8329\n",
      "---- Training ----\n",
      "Training loss: 117.3428\n",
      "Training acc over epoch: 0.8328\n",
      "---- Validation ----\n",
      "Validation loss: 60.6699\n",
      "Validation acc: 0.7297\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 370.2850, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 369.4814, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 20: 347.5038, Accuracy: 0.8229\n",
      "Training loss (for one batch) at step 30: 356.4919, Accuracy: 0.8284\n",
      "Training loss (for one batch) at step 40: 346.7072, Accuracy: 0.8352\n",
      "Training loss (for one batch) at step 50: 336.6038, Accuracy: 0.8421\n",
      "Training loss (for one batch) at step 60: 327.6166, Accuracy: 0.8436\n",
      "Training loss (for one batch) at step 70: 367.2706, Accuracy: 0.8393\n",
      "Training loss (for one batch) at step 80: 363.5889, Accuracy: 0.8343\n",
      "Training loss (for one batch) at step 90: 344.8210, Accuracy: 0.8338\n",
      "Training loss (for one batch) at step 100: 343.1276, Accuracy: 0.8345\n",
      "Training loss (for one batch) at step 110: 347.0275, Accuracy: 0.8342\n",
      "---- Training ----\n",
      "Training loss: 106.3082\n",
      "Training acc over epoch: 0.8331\n",
      "---- Validation ----\n",
      "Validation loss: 35.3667\n",
      "Validation acc: 0.7458\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 357.3643, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 10: 362.1574, Accuracy: 0.8189\n",
      "Training loss (for one batch) at step 20: 359.2006, Accuracy: 0.8192\n",
      "Training loss (for one batch) at step 30: 352.8504, Accuracy: 0.8289\n",
      "Training loss (for one batch) at step 40: 345.9195, Accuracy: 0.8371\n",
      "Training loss (for one batch) at step 50: 322.5231, Accuracy: 0.8427\n",
      "Training loss (for one batch) at step 60: 357.2232, Accuracy: 0.8455\n",
      "Training loss (for one batch) at step 70: 378.2239, Accuracy: 0.8366\n",
      "Training loss (for one batch) at step 80: 352.8869, Accuracy: 0.8321\n",
      "Training loss (for one batch) at step 90: 357.8944, Accuracy: 0.8298\n",
      "Training loss (for one batch) at step 100: 364.3795, Accuracy: 0.8305\n",
      "Training loss (for one batch) at step 110: 357.1899, Accuracy: 0.8288\n",
      "---- Training ----\n",
      "Training loss: 112.1259\n",
      "Training acc over epoch: 0.8288\n",
      "---- Validation ----\n",
      "Validation loss: 39.8842\n",
      "Validation acc: 0.7630\n",
      "Time taken: 10.74s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 371.2601, Accuracy: 0.8672\n",
      "Training loss (for one batch) at step 10: 346.4732, Accuracy: 0.8182\n",
      "Training loss (for one batch) at step 20: 337.0356, Accuracy: 0.8181\n",
      "Training loss (for one batch) at step 30: 342.7325, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 40: 334.1151, Accuracy: 0.8396\n",
      "Training loss (for one batch) at step 50: 328.5401, Accuracy: 0.8448\n",
      "Training loss (for one batch) at step 60: 334.7983, Accuracy: 0.8487\n",
      "Training loss (for one batch) at step 70: 370.1428, Accuracy: 0.8409\n",
      "Training loss (for one batch) at step 80: 340.0923, Accuracy: 0.8346\n",
      "Training loss (for one batch) at step 90: 348.7240, Accuracy: 0.8324\n",
      "Training loss (for one batch) at step 100: 344.8047, Accuracy: 0.8321\n",
      "Training loss (for one batch) at step 110: 345.3186, Accuracy: 0.8322\n",
      "---- Training ----\n",
      "Training loss: 107.9498\n",
      "Training acc over epoch: 0.8310\n",
      "---- Validation ----\n",
      "Validation loss: 41.3234\n",
      "Validation acc: 0.7590\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 358.5656, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 10: 350.9325, Accuracy: 0.8153\n",
      "Training loss (for one batch) at step 20: 343.1652, Accuracy: 0.8188\n",
      "Training loss (for one batch) at step 30: 337.5601, Accuracy: 0.8362\n",
      "Training loss (for one batch) at step 40: 342.1459, Accuracy: 0.8407\n",
      "Training loss (for one batch) at step 50: 321.2509, Accuracy: 0.8474\n",
      "Training loss (for one batch) at step 60: 333.0501, Accuracy: 0.8495\n",
      "Training loss (for one batch) at step 70: 347.4436, Accuracy: 0.8430\n",
      "Training loss (for one batch) at step 80: 354.9506, Accuracy: 0.8354\n",
      "Training loss (for one batch) at step 90: 337.7305, Accuracy: 0.8354\n",
      "Training loss (for one batch) at step 100: 334.1659, Accuracy: 0.8366\n",
      "Training loss (for one batch) at step 110: 338.6839, Accuracy: 0.8352\n",
      "---- Training ----\n",
      "Training loss: 109.4362\n",
      "Training acc over epoch: 0.8350\n",
      "---- Validation ----\n",
      "Validation loss: 44.1402\n",
      "Validation acc: 0.7657\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 356.0041, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 348.4496, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 20: 339.8063, Accuracy: 0.8326\n",
      "Training loss (for one batch) at step 30: 328.6171, Accuracy: 0.8407\n",
      "Training loss (for one batch) at step 40: 318.8518, Accuracy: 0.8455\n",
      "Training loss (for one batch) at step 50: 313.1960, Accuracy: 0.8494\n",
      "Training loss (for one batch) at step 60: 326.4072, Accuracy: 0.8505\n",
      "Training loss (for one batch) at step 70: 349.0359, Accuracy: 0.8426\n",
      "Training loss (for one batch) at step 80: 357.8921, Accuracy: 0.8374\n",
      "Training loss (for one batch) at step 90: 332.1858, Accuracy: 0.8353\n",
      "Training loss (for one batch) at step 100: 324.1991, Accuracy: 0.8349\n",
      "Training loss (for one batch) at step 110: 338.1265, Accuracy: 0.8354\n",
      "---- Training ----\n",
      "Training loss: 117.6285\n",
      "Training acc over epoch: 0.8348\n",
      "---- Validation ----\n",
      "Validation loss: 43.4026\n",
      "Validation acc: 0.7571\n",
      "Time taken: 10.71s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 358.0113, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 10: 345.0473, Accuracy: 0.8175\n",
      "Training loss (for one batch) at step 20: 339.0035, Accuracy: 0.8255\n",
      "Training loss (for one batch) at step 30: 336.4261, Accuracy: 0.8342\n",
      "Training loss (for one batch) at step 40: 352.9470, Accuracy: 0.8394\n",
      "Training loss (for one batch) at step 50: 323.0765, Accuracy: 0.8451\n",
      "Training loss (for one batch) at step 60: 327.3376, Accuracy: 0.8476\n",
      "Training loss (for one batch) at step 70: 342.8575, Accuracy: 0.8451\n",
      "Training loss (for one batch) at step 80: 354.5333, Accuracy: 0.8371\n",
      "Training loss (for one batch) at step 90: 341.7596, Accuracy: 0.8348\n",
      "Training loss (for one batch) at step 100: 337.4032, Accuracy: 0.8376\n",
      "Training loss (for one batch) at step 110: 352.2674, Accuracy: 0.8366\n",
      "---- Training ----\n",
      "Training loss: 109.9030\n",
      "Training acc over epoch: 0.8356\n",
      "---- Validation ----\n",
      "Validation loss: 44.2648\n",
      "Validation acc: 0.7246\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 376.5818, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 350.1094, Accuracy: 0.8182\n",
      "Training loss (for one batch) at step 20: 345.7233, Accuracy: 0.8255\n",
      "Training loss (for one batch) at step 30: 345.7340, Accuracy: 0.8327\n",
      "Training loss (for one batch) at step 40: 320.5317, Accuracy: 0.8361\n",
      "Training loss (for one batch) at step 50: 335.6164, Accuracy: 0.8441\n",
      "Training loss (for one batch) at step 60: 330.7849, Accuracy: 0.8461\n",
      "Training loss (for one batch) at step 70: 350.7098, Accuracy: 0.8407\n",
      "Training loss (for one batch) at step 80: 329.9475, Accuracy: 0.8356\n",
      "Training loss (for one batch) at step 90: 339.2383, Accuracy: 0.8341\n",
      "Training loss (for one batch) at step 100: 335.2661, Accuracy: 0.8355\n",
      "Training loss (for one batch) at step 110: 337.1411, Accuracy: 0.8350\n",
      "---- Training ----\n",
      "Training loss: 102.3274\n",
      "Training acc over epoch: 0.8336\n",
      "---- Validation ----\n",
      "Validation loss: 38.8023\n",
      "Validation acc: 0.7268\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 364.2447, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 350.0736, Accuracy: 0.8004\n",
      "Training loss (for one batch) at step 20: 342.4365, Accuracy: 0.8263\n",
      "Training loss (for one batch) at step 30: 328.7602, Accuracy: 0.8382\n",
      "Training loss (for one batch) at step 40: 319.6667, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 50: 323.2008, Accuracy: 0.8487\n",
      "Training loss (for one batch) at step 60: 332.5305, Accuracy: 0.8502\n",
      "Training loss (for one batch) at step 70: 359.3831, Accuracy: 0.8423\n",
      "Training loss (for one batch) at step 80: 349.5553, Accuracy: 0.8354\n",
      "Training loss (for one batch) at step 90: 358.1023, Accuracy: 0.8347\n",
      "Training loss (for one batch) at step 100: 333.8991, Accuracy: 0.8357\n",
      "Training loss (for one batch) at step 110: 325.4631, Accuracy: 0.8350\n",
      "---- Training ----\n",
      "Training loss: 111.3036\n",
      "Training acc over epoch: 0.8334\n",
      "---- Validation ----\n",
      "Validation loss: 44.4912\n",
      "Validation acc: 0.7453\n",
      "Time taken: 10.80s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 347.0643, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 339.3142, Accuracy: 0.8338\n",
      "Training loss (for one batch) at step 20: 331.1411, Accuracy: 0.8311\n",
      "Training loss (for one batch) at step 30: 316.9718, Accuracy: 0.8392\n",
      "Training loss (for one batch) at step 40: 326.3898, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 50: 318.4972, Accuracy: 0.8511\n",
      "Training loss (for one batch) at step 60: 324.2135, Accuracy: 0.8518\n",
      "Training loss (for one batch) at step 70: 364.3018, Accuracy: 0.8447\n",
      "Training loss (for one batch) at step 80: 335.1498, Accuracy: 0.8392\n",
      "Training loss (for one batch) at step 90: 337.2907, Accuracy: 0.8378\n",
      "Training loss (for one batch) at step 100: 327.4544, Accuracy: 0.8372\n",
      "Training loss (for one batch) at step 110: 343.8282, Accuracy: 0.8363\n",
      "---- Training ----\n",
      "Training loss: 107.8153\n",
      "Training acc over epoch: 0.8359\n",
      "---- Validation ----\n",
      "Validation loss: 63.8831\n",
      "Validation acc: 0.7577\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 345.7307, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 333.2645, Accuracy: 0.8303\n",
      "Training loss (for one batch) at step 20: 345.9015, Accuracy: 0.8311\n",
      "Training loss (for one batch) at step 30: 314.6141, Accuracy: 0.8380\n",
      "Training loss (for one batch) at step 40: 320.3911, Accuracy: 0.8447\n",
      "Training loss (for one batch) at step 50: 328.0533, Accuracy: 0.8539\n",
      "Training loss (for one batch) at step 60: 353.9788, Accuracy: 0.8532\n",
      "Training loss (for one batch) at step 70: 331.7473, Accuracy: 0.8439\n",
      "Training loss (for one batch) at step 80: 346.0386, Accuracy: 0.8378\n",
      "Training loss (for one batch) at step 90: 320.0264, Accuracy: 0.8368\n",
      "Training loss (for one batch) at step 100: 319.9510, Accuracy: 0.8373\n",
      "Training loss (for one batch) at step 110: 321.9978, Accuracy: 0.8373\n",
      "---- Training ----\n",
      "Training loss: 102.7851\n",
      "Training acc over epoch: 0.8373\n",
      "---- Validation ----\n",
      "Validation loss: 42.4566\n",
      "Validation acc: 0.7708\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 377.0306, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 10: 369.0138, Accuracy: 0.8189\n",
      "Training loss (for one batch) at step 20: 327.1331, Accuracy: 0.8296\n",
      "Training loss (for one batch) at step 30: 321.2393, Accuracy: 0.8387\n",
      "Training loss (for one batch) at step 40: 338.3189, Accuracy: 0.8430\n",
      "Training loss (for one batch) at step 50: 297.1056, Accuracy: 0.8513\n",
      "Training loss (for one batch) at step 60: 326.0262, Accuracy: 0.8531\n",
      "Training loss (for one batch) at step 70: 337.7243, Accuracy: 0.8455\n",
      "Training loss (for one batch) at step 80: 343.2959, Accuracy: 0.8398\n",
      "Training loss (for one batch) at step 90: 315.5656, Accuracy: 0.8369\n",
      "Training loss (for one batch) at step 100: 320.3932, Accuracy: 0.8375\n",
      "Training loss (for one batch) at step 110: 337.6430, Accuracy: 0.8371\n",
      "---- Training ----\n",
      "Training loss: 106.2621\n",
      "Training acc over epoch: 0.8362\n",
      "---- Validation ----\n",
      "Validation loss: 41.5064\n",
      "Validation acc: 0.7630\n",
      "Time taken: 10.79s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 360.9742, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 347.6127, Accuracy: 0.8175\n",
      "Training loss (for one batch) at step 20: 311.0925, Accuracy: 0.8344\n",
      "Training loss (for one batch) at step 30: 319.9902, Accuracy: 0.8397\n",
      "Training loss (for one batch) at step 40: 315.5725, Accuracy: 0.8447\n",
      "Training loss (for one batch) at step 50: 319.9169, Accuracy: 0.8536\n",
      "Training loss (for one batch) at step 60: 319.4745, Accuracy: 0.8527\n",
      "Training loss (for one batch) at step 70: 348.9293, Accuracy: 0.8456\n",
      "Training loss (for one batch) at step 80: 345.4490, Accuracy: 0.8394\n",
      "Training loss (for one batch) at step 90: 324.4885, Accuracy: 0.8377\n",
      "Training loss (for one batch) at step 100: 321.2798, Accuracy: 0.8369\n",
      "Training loss (for one batch) at step 110: 316.5952, Accuracy: 0.8371\n",
      "---- Training ----\n",
      "Training loss: 93.8695\n",
      "Training acc over epoch: 0.8364\n",
      "---- Validation ----\n",
      "Validation loss: 41.3027\n",
      "Validation acc: 0.7555\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 346.6847, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 10: 342.7770, Accuracy: 0.8068\n",
      "Training loss (for one batch) at step 20: 313.3621, Accuracy: 0.8255\n",
      "Training loss (for one batch) at step 30: 317.0532, Accuracy: 0.8377\n",
      "Training loss (for one batch) at step 40: 319.8239, Accuracy: 0.8485\n",
      "Training loss (for one batch) at step 50: 329.9959, Accuracy: 0.8552\n",
      "Training loss (for one batch) at step 60: 320.8734, Accuracy: 0.8535\n",
      "Training loss (for one batch) at step 70: 363.3604, Accuracy: 0.8460\n",
      "Training loss (for one batch) at step 80: 339.2744, Accuracy: 0.8396\n",
      "Training loss (for one batch) at step 90: 329.1046, Accuracy: 0.8367\n",
      "Training loss (for one batch) at step 100: 322.9244, Accuracy: 0.8368\n",
      "Training loss (for one batch) at step 110: 333.8345, Accuracy: 0.8369\n",
      "---- Training ----\n",
      "Training loss: 97.2276\n",
      "Training acc over epoch: 0.8360\n",
      "---- Validation ----\n",
      "Validation loss: 42.7006\n",
      "Validation acc: 0.7509\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 351.0512, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 10: 343.9245, Accuracy: 0.8317\n",
      "Training loss (for one batch) at step 20: 327.3965, Accuracy: 0.8318\n",
      "Training loss (for one batch) at step 30: 313.0450, Accuracy: 0.8407\n",
      "Training loss (for one batch) at step 40: 322.0420, Accuracy: 0.8493\n",
      "Training loss (for one batch) at step 50: 313.2689, Accuracy: 0.8537\n",
      "Training loss (for one batch) at step 60: 323.6618, Accuracy: 0.8566\n",
      "Training loss (for one batch) at step 70: 341.5871, Accuracy: 0.8478\n",
      "Training loss (for one batch) at step 80: 347.4480, Accuracy: 0.8397\n",
      "Training loss (for one batch) at step 90: 336.2260, Accuracy: 0.8384\n",
      "Training loss (for one batch) at step 100: 332.3508, Accuracy: 0.8389\n",
      "Training loss (for one batch) at step 110: 325.6441, Accuracy: 0.8388\n",
      "---- Training ----\n",
      "Training loss: 114.9050\n",
      "Training acc over epoch: 0.8371\n",
      "---- Validation ----\n",
      "Validation loss: 44.0813\n",
      "Validation acc: 0.7601\n",
      "Time taken: 10.76s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 330.1692, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 347.7265, Accuracy: 0.8132\n",
      "Training loss (for one batch) at step 20: 317.2620, Accuracy: 0.8229\n",
      "Training loss (for one batch) at step 30: 320.9773, Accuracy: 0.8334\n",
      "Training loss (for one batch) at step 40: 307.0342, Accuracy: 0.8424\n",
      "Training loss (for one batch) at step 50: 317.1603, Accuracy: 0.8490\n",
      "Training loss (for one batch) at step 60: 313.6942, Accuracy: 0.8522\n",
      "Training loss (for one batch) at step 70: 355.5453, Accuracy: 0.8425\n",
      "Training loss (for one batch) at step 80: 334.7508, Accuracy: 0.8365\n",
      "Training loss (for one batch) at step 90: 315.6801, Accuracy: 0.8363\n",
      "Training loss (for one batch) at step 100: 307.9882, Accuracy: 0.8379\n",
      "Training loss (for one batch) at step 110: 317.2647, Accuracy: 0.8363\n",
      "---- Training ----\n",
      "Training loss: 106.2982\n",
      "Training acc over epoch: 0.8354\n",
      "---- Validation ----\n",
      "Validation loss: 39.6034\n",
      "Validation acc: 0.7574\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 329.7183, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 340.8414, Accuracy: 0.8061\n",
      "Training loss (for one batch) at step 20: 335.3445, Accuracy: 0.8237\n",
      "Training loss (for one batch) at step 30: 330.1978, Accuracy: 0.8349\n",
      "Training loss (for one batch) at step 40: 313.3985, Accuracy: 0.8422\n",
      "Training loss (for one batch) at step 50: 302.1586, Accuracy: 0.8497\n",
      "Training loss (for one batch) at step 60: 312.6133, Accuracy: 0.8505\n",
      "Training loss (for one batch) at step 70: 325.1696, Accuracy: 0.8461\n",
      "Training loss (for one batch) at step 80: 340.0424, Accuracy: 0.8408\n",
      "Training loss (for one batch) at step 90: 316.9077, Accuracy: 0.8395\n",
      "Training loss (for one batch) at step 100: 322.0306, Accuracy: 0.8409\n",
      "Training loss (for one batch) at step 110: 316.5048, Accuracy: 0.8381\n",
      "---- Training ----\n",
      "Training loss: 104.3600\n",
      "Training acc over epoch: 0.8381\n",
      "---- Validation ----\n",
      "Validation loss: 42.7629\n",
      "Validation acc: 0.7469\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 365.4028, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 339.7097, Accuracy: 0.8196\n",
      "Training loss (for one batch) at step 20: 309.4395, Accuracy: 0.8382\n",
      "Training loss (for one batch) at step 30: 308.7679, Accuracy: 0.8417\n",
      "Training loss (for one batch) at step 40: 299.7430, Accuracy: 0.8458\n",
      "Training loss (for one batch) at step 50: 297.7479, Accuracy: 0.8537\n",
      "Training loss (for one batch) at step 60: 301.3828, Accuracy: 0.8526\n",
      "Training loss (for one batch) at step 70: 333.2069, Accuracy: 0.8452\n",
      "Training loss (for one batch) at step 80: 336.6010, Accuracy: 0.8383\n",
      "Training loss (for one batch) at step 90: 330.9465, Accuracy: 0.8386\n",
      "Training loss (for one batch) at step 100: 312.9161, Accuracy: 0.8393\n",
      "Training loss (for one batch) at step 110: 333.4901, Accuracy: 0.8363\n",
      "---- Training ----\n",
      "Training loss: 110.3304\n",
      "Training acc over epoch: 0.8372\n",
      "---- Validation ----\n",
      "Validation loss: 45.1412\n",
      "Validation acc: 0.7550\n",
      "Time taken: 10.69s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 323.6373, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 353.5455, Accuracy: 0.8153\n",
      "Training loss (for one batch) at step 20: 310.9631, Accuracy: 0.8292\n",
      "Training loss (for one batch) at step 30: 298.1146, Accuracy: 0.8402\n",
      "Training loss (for one batch) at step 40: 320.8795, Accuracy: 0.8455\n",
      "Training loss (for one batch) at step 50: 299.1216, Accuracy: 0.8528\n",
      "Training loss (for one batch) at step 60: 303.0935, Accuracy: 0.8536\n",
      "Training loss (for one batch) at step 70: 332.9294, Accuracy: 0.8445\n",
      "Training loss (for one batch) at step 80: 350.7409, Accuracy: 0.8405\n",
      "Training loss (for one batch) at step 90: 312.7914, Accuracy: 0.8392\n",
      "Training loss (for one batch) at step 100: 322.6998, Accuracy: 0.8386\n",
      "Training loss (for one batch) at step 110: 333.5908, Accuracy: 0.8400\n",
      "---- Training ----\n",
      "Training loss: 105.9980\n",
      "Training acc over epoch: 0.8381\n",
      "---- Validation ----\n",
      "Validation loss: 38.7368\n",
      "Validation acc: 0.7617\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 354.5078, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 330.0187, Accuracy: 0.8317\n",
      "Training loss (for one batch) at step 20: 317.6615, Accuracy: 0.8337\n",
      "Training loss (for one batch) at step 30: 317.6534, Accuracy: 0.8427\n",
      "Training loss (for one batch) at step 40: 310.4967, Accuracy: 0.8495\n",
      "Training loss (for one batch) at step 50: 321.6949, Accuracy: 0.8536\n",
      "Training loss (for one batch) at step 60: 308.8151, Accuracy: 0.8535\n",
      "Training loss (for one batch) at step 70: 325.0822, Accuracy: 0.8467\n",
      "Training loss (for one batch) at step 80: 347.9938, Accuracy: 0.8410\n",
      "Training loss (for one batch) at step 90: 324.5893, Accuracy: 0.8386\n",
      "Training loss (for one batch) at step 100: 315.3628, Accuracy: 0.8392\n",
      "Training loss (for one batch) at step 110: 311.3773, Accuracy: 0.8396\n",
      "---- Training ----\n",
      "Training loss: 103.1189\n",
      "Training acc over epoch: 0.8389\n",
      "---- Validation ----\n",
      "Validation loss: 60.8290\n",
      "Validation acc: 0.7652\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 330.7328, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 10: 340.7063, Accuracy: 0.8295\n",
      "Training loss (for one batch) at step 20: 305.7169, Accuracy: 0.8326\n",
      "Training loss (for one batch) at step 30: 325.4631, Accuracy: 0.8380\n",
      "Training loss (for one batch) at step 40: 314.3402, Accuracy: 0.8451\n",
      "Training loss (for one batch) at step 50: 303.4472, Accuracy: 0.8529\n",
      "Training loss (for one batch) at step 60: 316.0510, Accuracy: 0.8535\n",
      "Training loss (for one batch) at step 70: 325.6965, Accuracy: 0.8468\n",
      "Training loss (for one batch) at step 80: 335.2074, Accuracy: 0.8432\n",
      "Training loss (for one batch) at step 90: 318.6696, Accuracy: 0.8417\n",
      "Training loss (for one batch) at step 100: 301.8447, Accuracy: 0.8417\n",
      "Training loss (for one batch) at step 110: 320.7021, Accuracy: 0.8408\n",
      "---- Training ----\n",
      "Training loss: 96.7895\n",
      "Training acc over epoch: 0.8401\n",
      "---- Validation ----\n",
      "Validation loss: 61.9642\n",
      "Validation acc: 0.7614\n",
      "Time taken: 10.92s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 329.0896, Accuracy: 0.8594\n",
      "Training loss (for one batch) at step 10: 322.1368, Accuracy: 0.8288\n",
      "Training loss (for one batch) at step 20: 323.0358, Accuracy: 0.8408\n",
      "Training loss (for one batch) at step 30: 305.7363, Accuracy: 0.8448\n",
      "Training loss (for one batch) at step 40: 320.7745, Accuracy: 0.8533\n",
      "Training loss (for one batch) at step 50: 314.6752, Accuracy: 0.8586\n",
      "Training loss (for one batch) at step 60: 317.1959, Accuracy: 0.8585\n",
      "Training loss (for one batch) at step 70: 341.6734, Accuracy: 0.8512\n",
      "Training loss (for one batch) at step 80: 339.5188, Accuracy: 0.8444\n",
      "Training loss (for one batch) at step 90: 298.7718, Accuracy: 0.8420\n",
      "Training loss (for one batch) at step 100: 315.6879, Accuracy: 0.8414\n",
      "Training loss (for one batch) at step 110: 327.5150, Accuracy: 0.8395\n",
      "---- Training ----\n",
      "Training loss: 94.3170\n",
      "Training acc over epoch: 0.8395\n",
      "---- Validation ----\n",
      "Validation loss: 60.9633\n",
      "Validation acc: 0.7628\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 340.6397, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 333.6695, Accuracy: 0.8153\n",
      "Training loss (for one batch) at step 20: 319.1328, Accuracy: 0.8389\n",
      "Training loss (for one batch) at step 30: 296.9874, Accuracy: 0.8448\n",
      "Training loss (for one batch) at step 40: 300.9264, Accuracy: 0.8508\n",
      "Training loss (for one batch) at step 50: 299.9051, Accuracy: 0.8603\n",
      "Training loss (for one batch) at step 60: 298.6554, Accuracy: 0.8601\n",
      "Training loss (for one batch) at step 70: 332.8376, Accuracy: 0.8527\n",
      "Training loss (for one batch) at step 80: 327.1884, Accuracy: 0.8456\n",
      "Training loss (for one batch) at step 90: 305.9292, Accuracy: 0.8425\n",
      "Training loss (for one batch) at step 100: 294.8202, Accuracy: 0.8431\n",
      "Training loss (for one batch) at step 110: 328.5383, Accuracy: 0.8433\n",
      "---- Training ----\n",
      "Training loss: 102.1668\n",
      "Training acc over epoch: 0.8419\n",
      "---- Validation ----\n",
      "Validation loss: 41.4523\n",
      "Validation acc: 0.7628\n",
      "Time taken: 10.51s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 329.9897, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 319.7136, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 20: 304.9239, Accuracy: 0.8348\n",
      "Training loss (for one batch) at step 30: 290.3973, Accuracy: 0.8385\n",
      "Training loss (for one batch) at step 40: 330.5527, Accuracy: 0.8491\n",
      "Training loss (for one batch) at step 50: 292.3787, Accuracy: 0.8549\n",
      "Training loss (for one batch) at step 60: 320.3127, Accuracy: 0.8545\n",
      "Training loss (for one batch) at step 70: 341.2560, Accuracy: 0.8484\n",
      "Training loss (for one batch) at step 80: 332.5679, Accuracy: 0.8421\n",
      "Training loss (for one batch) at step 90: 303.2251, Accuracy: 0.8418\n",
      "Training loss (for one batch) at step 100: 307.0418, Accuracy: 0.8413\n",
      "Training loss (for one batch) at step 110: 307.1208, Accuracy: 0.8394\n",
      "---- Training ----\n",
      "Training loss: 90.9644\n",
      "Training acc over epoch: 0.8395\n",
      "---- Validation ----\n",
      "Validation loss: 51.5498\n",
      "Validation acc: 0.7501\n",
      "Time taken: 10.73s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 322.7911, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 326.8643, Accuracy: 0.8111\n",
      "Training loss (for one batch) at step 20: 327.7578, Accuracy: 0.8333\n",
      "Training loss (for one batch) at step 30: 295.5681, Accuracy: 0.8364\n",
      "Training loss (for one batch) at step 40: 298.2180, Accuracy: 0.8451\n",
      "Training loss (for one batch) at step 50: 287.2758, Accuracy: 0.8529\n",
      "Training loss (for one batch) at step 60: 292.4103, Accuracy: 0.8557\n",
      "Training loss (for one batch) at step 70: 347.0017, Accuracy: 0.8499\n",
      "Training loss (for one batch) at step 80: 318.1913, Accuracy: 0.8433\n",
      "Training loss (for one batch) at step 90: 312.9697, Accuracy: 0.8413\n",
      "Training loss (for one batch) at step 100: 308.1862, Accuracy: 0.8410\n",
      "Training loss (for one batch) at step 110: 327.2711, Accuracy: 0.8416\n",
      "---- Training ----\n",
      "Training loss: 97.9767\n",
      "Training acc over epoch: 0.8403\n",
      "---- Validation ----\n",
      "Validation loss: 51.3800\n",
      "Validation acc: 0.7625\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 326.6252, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 336.0428, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 20: 288.4379, Accuracy: 0.8367\n",
      "Training loss (for one batch) at step 30: 326.5837, Accuracy: 0.8430\n",
      "Training loss (for one batch) at step 40: 302.6908, Accuracy: 0.8510\n",
      "Training loss (for one batch) at step 50: 299.7662, Accuracy: 0.8551\n",
      "Training loss (for one batch) at step 60: 309.5654, Accuracy: 0.8580\n",
      "Training loss (for one batch) at step 70: 310.6267, Accuracy: 0.8496\n",
      "Training loss (for one batch) at step 80: 331.4083, Accuracy: 0.8420\n",
      "Training loss (for one batch) at step 90: 314.8676, Accuracy: 0.8385\n",
      "Training loss (for one batch) at step 100: 300.9146, Accuracy: 0.8403\n",
      "Training loss (for one batch) at step 110: 313.1682, Accuracy: 0.8399\n",
      "---- Training ----\n",
      "Training loss: 111.8265\n",
      "Training acc over epoch: 0.8382\n",
      "---- Validation ----\n",
      "Validation loss: 53.1766\n",
      "Validation acc: 0.7681\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 339.9744, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 326.8631, Accuracy: 0.8210\n",
      "Training loss (for one batch) at step 20: 318.8077, Accuracy: 0.8356\n",
      "Training loss (for one batch) at step 30: 295.0150, Accuracy: 0.8440\n",
      "Training loss (for one batch) at step 40: 307.8795, Accuracy: 0.8491\n",
      "Training loss (for one batch) at step 50: 321.5035, Accuracy: 0.8589\n",
      "Training loss (for one batch) at step 60: 317.1275, Accuracy: 0.8591\n",
      "Training loss (for one batch) at step 70: 326.8065, Accuracy: 0.8519\n",
      "Training loss (for one batch) at step 80: 338.8054, Accuracy: 0.8440\n",
      "Training loss (for one batch) at step 90: 319.0305, Accuracy: 0.8400\n",
      "Training loss (for one batch) at step 100: 295.6331, Accuracy: 0.8415\n",
      "Training loss (for one batch) at step 110: 305.9216, Accuracy: 0.8407\n",
      "---- Training ----\n",
      "Training loss: 103.4730\n",
      "Training acc over epoch: 0.8399\n",
      "---- Validation ----\n",
      "Validation loss: 53.9308\n",
      "Validation acc: 0.7692\n",
      "Time taken: 10.74s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 326.9287, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 316.5306, Accuracy: 0.8168\n",
      "Training loss (for one batch) at step 20: 291.8832, Accuracy: 0.8337\n",
      "Training loss (for one batch) at step 30: 316.3194, Accuracy: 0.8458\n",
      "Training loss (for one batch) at step 40: 305.0101, Accuracy: 0.8506\n",
      "Training loss (for one batch) at step 50: 290.6607, Accuracy: 0.8563\n",
      "Training loss (for one batch) at step 60: 315.5816, Accuracy: 0.8571\n",
      "Training loss (for one batch) at step 70: 332.0219, Accuracy: 0.8508\n",
      "Training loss (for one batch) at step 80: 336.9965, Accuracy: 0.8436\n",
      "Training loss (for one batch) at step 90: 321.4654, Accuracy: 0.8436\n",
      "Training loss (for one batch) at step 100: 286.8776, Accuracy: 0.8444\n",
      "Training loss (for one batch) at step 110: 297.4596, Accuracy: 0.8434\n",
      "---- Training ----\n",
      "Training loss: 95.3608\n",
      "Training acc over epoch: 0.8426\n",
      "---- Validation ----\n",
      "Validation loss: 46.3019\n",
      "Validation acc: 0.7515\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 355.8203, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 326.7701, Accuracy: 0.8260\n",
      "Training loss (for one batch) at step 20: 302.9138, Accuracy: 0.8389\n",
      "Training loss (for one batch) at step 30: 296.6051, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 40: 297.1787, Accuracy: 0.8510\n",
      "Training loss (for one batch) at step 50: 287.9824, Accuracy: 0.8583\n",
      "Training loss (for one batch) at step 60: 314.1355, Accuracy: 0.8594\n",
      "Training loss (for one batch) at step 70: 321.2213, Accuracy: 0.8523\n",
      "Training loss (for one batch) at step 80: 310.0763, Accuracy: 0.8464\n",
      "Training loss (for one batch) at step 90: 309.6938, Accuracy: 0.8439\n",
      "Training loss (for one batch) at step 100: 303.5458, Accuracy: 0.8451\n",
      "Training loss (for one batch) at step 110: 312.9618, Accuracy: 0.8445\n",
      "---- Training ----\n",
      "Training loss: 108.8082\n",
      "Training acc over epoch: 0.8422\n",
      "---- Validation ----\n",
      "Validation loss: 33.4237\n",
      "Validation acc: 0.7536\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 321.8923, Accuracy: 0.8750\n",
      "Training loss (for one batch) at step 10: 324.3859, Accuracy: 0.8175\n",
      "Training loss (for one batch) at step 20: 306.3841, Accuracy: 0.8300\n",
      "Training loss (for one batch) at step 30: 302.8076, Accuracy: 0.8443\n",
      "Training loss (for one batch) at step 40: 309.1250, Accuracy: 0.8508\n",
      "Training loss (for one batch) at step 50: 282.9518, Accuracy: 0.8585\n",
      "Training loss (for one batch) at step 60: 301.6522, Accuracy: 0.8582\n",
      "Training loss (for one batch) at step 70: 325.6588, Accuracy: 0.8505\n",
      "Training loss (for one batch) at step 80: 310.9960, Accuracy: 0.8436\n",
      "Training loss (for one batch) at step 90: 300.4754, Accuracy: 0.8427\n",
      "Training loss (for one batch) at step 100: 297.4760, Accuracy: 0.8444\n",
      "Training loss (for one batch) at step 110: 328.2083, Accuracy: 0.8414\n",
      "---- Training ----\n",
      "Training loss: 107.1976\n",
      "Training acc over epoch: 0.8404\n",
      "---- Validation ----\n",
      "Validation loss: 39.3614\n",
      "Validation acc: 0.7517\n",
      "Time taken: 10.65s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 325.5030, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 312.7557, Accuracy: 0.8168\n",
      "Training loss (for one batch) at step 20: 304.0770, Accuracy: 0.8326\n",
      "Training loss (for one batch) at step 30: 289.6385, Accuracy: 0.8392\n",
      "Training loss (for one batch) at step 40: 304.8221, Accuracy: 0.8483\n",
      "Training loss (for one batch) at step 50: 285.9377, Accuracy: 0.8565\n",
      "Training loss (for one batch) at step 60: 322.6993, Accuracy: 0.8548\n",
      "Training loss (for one batch) at step 70: 323.0789, Accuracy: 0.8475\n",
      "Training loss (for one batch) at step 80: 329.5064, Accuracy: 0.8412\n",
      "Training loss (for one batch) at step 90: 298.2455, Accuracy: 0.8385\n",
      "Training loss (for one batch) at step 100: 285.7933, Accuracy: 0.8411\n",
      "Training loss (for one batch) at step 110: 327.0089, Accuracy: 0.8407\n",
      "---- Training ----\n",
      "Training loss: 103.8167\n",
      "Training acc over epoch: 0.8395\n",
      "---- Validation ----\n",
      "Validation loss: 43.0288\n",
      "Validation acc: 0.7638\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 334.0820, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 326.6618, Accuracy: 0.8253\n",
      "Training loss (for one batch) at step 20: 304.9003, Accuracy: 0.8330\n",
      "Training loss (for one batch) at step 30: 286.5568, Accuracy: 0.8480\n",
      "Training loss (for one batch) at step 40: 290.4250, Accuracy: 0.8521\n",
      "Training loss (for one batch) at step 50: 292.8947, Accuracy: 0.8572\n",
      "Training loss (for one batch) at step 60: 322.9070, Accuracy: 0.8603\n",
      "Training loss (for one batch) at step 70: 318.2322, Accuracy: 0.8523\n",
      "Training loss (for one batch) at step 80: 356.5163, Accuracy: 0.8431\n",
      "Training loss (for one batch) at step 90: 304.2734, Accuracy: 0.8420\n",
      "Training loss (for one batch) at step 100: 281.8691, Accuracy: 0.8431\n",
      "Training loss (for one batch) at step 110: 297.2499, Accuracy: 0.8428\n",
      "---- Training ----\n",
      "Training loss: 105.3870\n",
      "Training acc over epoch: 0.8413\n",
      "---- Validation ----\n",
      "Validation loss: 31.3768\n",
      "Validation acc: 0.7577\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 329.6988, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 326.7822, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 20: 307.2930, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 30: 300.5448, Accuracy: 0.8415\n",
      "Training loss (for one batch) at step 40: 304.5595, Accuracy: 0.8495\n",
      "Training loss (for one batch) at step 50: 275.8941, Accuracy: 0.8545\n",
      "Training loss (for one batch) at step 60: 299.4840, Accuracy: 0.8571\n",
      "Training loss (for one batch) at step 70: 320.4530, Accuracy: 0.8520\n",
      "Training loss (for one batch) at step 80: 325.3127, Accuracy: 0.8434\n",
      "Training loss (for one batch) at step 90: 287.9719, Accuracy: 0.8409\n",
      "Training loss (for one batch) at step 100: 297.7733, Accuracy: 0.8415\n",
      "Training loss (for one batch) at step 110: 312.8176, Accuracy: 0.8411\n",
      "---- Training ----\n",
      "Training loss: 105.6058\n",
      "Training acc over epoch: 0.8395\n",
      "---- Validation ----\n",
      "Validation loss: 42.2535\n",
      "Validation acc: 0.7579\n",
      "Time taken: 10.76s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 312.4397, Accuracy: 0.8750\n",
      "Training loss (for one batch) at step 10: 337.6021, Accuracy: 0.8388\n",
      "Training loss (for one batch) at step 20: 313.7220, Accuracy: 0.8318\n",
      "Training loss (for one batch) at step 30: 303.6225, Accuracy: 0.8453\n",
      "Training loss (for one batch) at step 40: 290.7435, Accuracy: 0.8527\n",
      "Training loss (for one batch) at step 50: 287.4912, Accuracy: 0.8580\n",
      "Training loss (for one batch) at step 60: 290.3348, Accuracy: 0.8582\n",
      "Training loss (for one batch) at step 70: 312.6579, Accuracy: 0.8527\n",
      "Training loss (for one batch) at step 80: 310.6085, Accuracy: 0.8463\n",
      "Training loss (for one batch) at step 90: 295.9417, Accuracy: 0.8435\n",
      "Training loss (for one batch) at step 100: 286.7637, Accuracy: 0.8448\n",
      "Training loss (for one batch) at step 110: 302.6651, Accuracy: 0.8421\n",
      "---- Training ----\n",
      "Training loss: 116.0194\n",
      "Training acc over epoch: 0.8418\n",
      "---- Validation ----\n",
      "Validation loss: 56.3927\n",
      "Validation acc: 0.7630\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 326.6721, Accuracy: 0.8672\n",
      "Training loss (for one batch) at step 10: 308.5227, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 20: 299.7219, Accuracy: 0.8352\n",
      "Training loss (for one batch) at step 30: 291.6248, Accuracy: 0.8463\n",
      "Training loss (for one batch) at step 40: 298.5303, Accuracy: 0.8531\n",
      "Training loss (for one batch) at step 50: 282.4230, Accuracy: 0.8598\n",
      "Training loss (for one batch) at step 60: 327.4669, Accuracy: 0.8573\n",
      "Training loss (for one batch) at step 70: 327.6273, Accuracy: 0.8513\n",
      "Training loss (for one batch) at step 80: 323.5374, Accuracy: 0.8426\n",
      "Training loss (for one batch) at step 90: 305.5007, Accuracy: 0.8410\n",
      "Training loss (for one batch) at step 100: 294.0406, Accuracy: 0.8411\n",
      "Training loss (for one batch) at step 110: 297.9438, Accuracy: 0.8400\n",
      "---- Training ----\n",
      "Training loss: 97.4733\n",
      "Training acc over epoch: 0.8399\n",
      "---- Validation ----\n",
      "Validation loss: 45.1641\n",
      "Validation acc: 0.7520\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 320.9525, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 327.3874, Accuracy: 0.8232\n",
      "Training loss (for one batch) at step 20: 292.8238, Accuracy: 0.8300\n",
      "Training loss (for one batch) at step 30: 289.5968, Accuracy: 0.8390\n",
      "Training loss (for one batch) at step 40: 293.8457, Accuracy: 0.8479\n",
      "Training loss (for one batch) at step 50: 295.0635, Accuracy: 0.8562\n",
      "Training loss (for one batch) at step 60: 312.6435, Accuracy: 0.8590\n",
      "Training loss (for one batch) at step 70: 309.3492, Accuracy: 0.8502\n",
      "Training loss (for one batch) at step 80: 317.0025, Accuracy: 0.8442\n",
      "Training loss (for one batch) at step 90: 298.1835, Accuracy: 0.8410\n",
      "Training loss (for one batch) at step 100: 298.2820, Accuracy: 0.8412\n",
      "Training loss (for one batch) at step 110: 304.7957, Accuracy: 0.8416\n",
      "---- Training ----\n",
      "Training loss: 93.6850\n",
      "Training acc over epoch: 0.8402\n",
      "---- Validation ----\n",
      "Validation loss: 54.1547\n",
      "Validation acc: 0.7611\n",
      "Time taken: 10.71s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 336.0293, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 347.3317, Accuracy: 0.8033\n",
      "Training loss (for one batch) at step 20: 291.2567, Accuracy: 0.8270\n",
      "Training loss (for one batch) at step 30: 300.4584, Accuracy: 0.8362\n",
      "Training loss (for one batch) at step 40: 282.8620, Accuracy: 0.8506\n",
      "Training loss (for one batch) at step 50: 289.4673, Accuracy: 0.8571\n",
      "Training loss (for one batch) at step 60: 294.1470, Accuracy: 0.8586\n",
      "Training loss (for one batch) at step 70: 326.9971, Accuracy: 0.8517\n",
      "Training loss (for one batch) at step 80: 327.4506, Accuracy: 0.8431\n",
      "Training loss (for one batch) at step 90: 286.4088, Accuracy: 0.8431\n",
      "Training loss (for one batch) at step 100: 325.1592, Accuracy: 0.8422\n",
      "Training loss (for one batch) at step 110: 309.9609, Accuracy: 0.8413\n",
      "---- Training ----\n",
      "Training loss: 105.7750\n",
      "Training acc over epoch: 0.8405\n",
      "---- Validation ----\n",
      "Validation loss: 38.7247\n",
      "Validation acc: 0.7536\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 307.5075, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 309.6460, Accuracy: 0.8182\n",
      "Training loss (for one batch) at step 20: 292.4195, Accuracy: 0.8356\n",
      "Training loss (for one batch) at step 30: 280.4532, Accuracy: 0.8485\n",
      "Training loss (for one batch) at step 40: 278.7054, Accuracy: 0.8544\n",
      "Training loss (for one batch) at step 50: 293.4215, Accuracy: 0.8612\n",
      "Training loss (for one batch) at step 60: 296.2535, Accuracy: 0.8603\n",
      "Training loss (for one batch) at step 70: 299.1312, Accuracy: 0.8529\n",
      "Training loss (for one batch) at step 80: 314.9607, Accuracy: 0.8471\n",
      "Training loss (for one batch) at step 90: 304.8271, Accuracy: 0.8432\n",
      "Training loss (for one batch) at step 100: 283.7210, Accuracy: 0.8427\n",
      "Training loss (for one batch) at step 110: 306.9109, Accuracy: 0.8425\n",
      "---- Training ----\n",
      "Training loss: 97.7727\n",
      "Training acc over epoch: 0.8413\n",
      "---- Validation ----\n",
      "Validation loss: 52.9223\n",
      "Validation acc: 0.7638\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 322.3100, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 306.7167, Accuracy: 0.8303\n",
      "Training loss (for one batch) at step 20: 289.0774, Accuracy: 0.8352\n",
      "Training loss (for one batch) at step 30: 294.8635, Accuracy: 0.8475\n",
      "Training loss (for one batch) at step 40: 293.2144, Accuracy: 0.8537\n",
      "Training loss (for one batch) at step 50: 280.2904, Accuracy: 0.8571\n",
      "Training loss (for one batch) at step 60: 304.8902, Accuracy: 0.8599\n",
      "Training loss (for one batch) at step 70: 324.5078, Accuracy: 0.8532\n",
      "Training loss (for one batch) at step 80: 314.4008, Accuracy: 0.8454\n",
      "Training loss (for one batch) at step 90: 299.2353, Accuracy: 0.8412\n",
      "Training loss (for one batch) at step 100: 302.7172, Accuracy: 0.8419\n",
      "Training loss (for one batch) at step 110: 306.5046, Accuracy: 0.8411\n",
      "---- Training ----\n",
      "Training loss: 97.3409\n",
      "Training acc over epoch: 0.8408\n",
      "---- Validation ----\n",
      "Validation loss: 55.9417\n",
      "Validation acc: 0.7550\n",
      "Time taken: 10.88s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 341.3189, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 10: 308.8887, Accuracy: 0.8260\n",
      "Training loss (for one batch) at step 20: 293.1809, Accuracy: 0.8296\n",
      "Training loss (for one batch) at step 30: 278.9434, Accuracy: 0.8422\n",
      "Training loss (for one batch) at step 40: 289.9168, Accuracy: 0.8508\n",
      "Training loss (for one batch) at step 50: 276.6910, Accuracy: 0.8572\n",
      "Training loss (for one batch) at step 60: 300.3650, Accuracy: 0.8587\n",
      "Training loss (for one batch) at step 70: 320.0325, Accuracy: 0.8524\n",
      "Training loss (for one batch) at step 80: 357.9568, Accuracy: 0.8439\n",
      "Training loss (for one batch) at step 90: 291.9694, Accuracy: 0.8416\n",
      "Training loss (for one batch) at step 100: 294.7199, Accuracy: 0.8435\n",
      "Training loss (for one batch) at step 110: 314.5137, Accuracy: 0.8433\n",
      "---- Training ----\n",
      "Training loss: 95.2576\n",
      "Training acc over epoch: 0.8419\n",
      "---- Validation ----\n",
      "Validation loss: 48.1612\n",
      "Validation acc: 0.7515\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 323.1835, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 327.4373, Accuracy: 0.8239\n",
      "Training loss (for one batch) at step 20: 314.8067, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 30: 291.7361, Accuracy: 0.8440\n",
      "Training loss (for one batch) at step 40: 277.7820, Accuracy: 0.8483\n",
      "Training loss (for one batch) at step 50: 292.6662, Accuracy: 0.8552\n",
      "Training loss (for one batch) at step 60: 286.1201, Accuracy: 0.8545\n",
      "Training loss (for one batch) at step 70: 314.1208, Accuracy: 0.8500\n",
      "Training loss (for one batch) at step 80: 312.1012, Accuracy: 0.8414\n",
      "Training loss (for one batch) at step 90: 301.2843, Accuracy: 0.8409\n",
      "Training loss (for one batch) at step 100: 292.0390, Accuracy: 0.8428\n",
      "Training loss (for one batch) at step 110: 307.1426, Accuracy: 0.8423\n",
      "---- Training ----\n",
      "Training loss: 97.1256\n",
      "Training acc over epoch: 0.8415\n",
      "---- Validation ----\n",
      "Validation loss: 41.4945\n",
      "Validation acc: 0.7630\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 310.2021, Accuracy: 0.8516\n",
      "Training loss (for one batch) at step 10: 325.8524, Accuracy: 0.8097\n",
      "Training loss (for one batch) at step 20: 285.8374, Accuracy: 0.8307\n",
      "Training loss (for one batch) at step 30: 290.4539, Accuracy: 0.8440\n",
      "Training loss (for one batch) at step 40: 287.1859, Accuracy: 0.8514\n",
      "Training loss (for one batch) at step 50: 278.6545, Accuracy: 0.8588\n",
      "Training loss (for one batch) at step 60: 306.1625, Accuracy: 0.8594\n",
      "Training loss (for one batch) at step 70: 306.1964, Accuracy: 0.8520\n",
      "Training loss (for one batch) at step 80: 322.9732, Accuracy: 0.8465\n",
      "Training loss (for one batch) at step 90: 285.0336, Accuracy: 0.8452\n",
      "Training loss (for one batch) at step 100: 278.1626, Accuracy: 0.8460\n",
      "Training loss (for one batch) at step 110: 317.0305, Accuracy: 0.8459\n",
      "---- Training ----\n",
      "Training loss: 93.6029\n",
      "Training acc over epoch: 0.8448\n",
      "---- Validation ----\n",
      "Validation loss: 69.1998\n",
      "Validation acc: 0.7587\n",
      "Time taken: 10.86s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 318.0822, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 312.4965, Accuracy: 0.8168\n",
      "Training loss (for one batch) at step 20: 297.6409, Accuracy: 0.8263\n",
      "Training loss (for one batch) at step 30: 289.3163, Accuracy: 0.8427\n",
      "Training loss (for one batch) at step 40: 280.9990, Accuracy: 0.8512\n",
      "Training loss (for one batch) at step 50: 281.3223, Accuracy: 0.8554\n",
      "Training loss (for one batch) at step 60: 307.5771, Accuracy: 0.8557\n",
      "Training loss (for one batch) at step 70: 338.0724, Accuracy: 0.8487\n",
      "Training loss (for one batch) at step 80: 319.1284, Accuracy: 0.8421\n",
      "Training loss (for one batch) at step 90: 283.7560, Accuracy: 0.8392\n",
      "Training loss (for one batch) at step 100: 289.4502, Accuracy: 0.8413\n",
      "Training loss (for one batch) at step 110: 313.3356, Accuracy: 0.8411\n",
      "---- Training ----\n",
      "Training loss: 91.4500\n",
      "Training acc over epoch: 0.8399\n",
      "---- Validation ----\n",
      "Validation loss: 49.8930\n",
      "Validation acc: 0.7646\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 318.6870, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 316.1971, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 20: 284.7569, Accuracy: 0.8333\n",
      "Training loss (for one batch) at step 30: 297.1068, Accuracy: 0.8422\n",
      "Training loss (for one batch) at step 40: 288.4553, Accuracy: 0.8510\n",
      "Training loss (for one batch) at step 50: 271.3143, Accuracy: 0.8565\n",
      "Training loss (for one batch) at step 60: 301.3153, Accuracy: 0.8575\n",
      "Training loss (for one batch) at step 70: 316.9464, Accuracy: 0.8493\n",
      "Training loss (for one batch) at step 80: 321.1541, Accuracy: 0.8449\n",
      "Training loss (for one batch) at step 90: 276.5037, Accuracy: 0.8424\n",
      "Training loss (for one batch) at step 100: 278.9525, Accuracy: 0.8429\n",
      "Training loss (for one batch) at step 110: 286.7615, Accuracy: 0.8426\n",
      "---- Training ----\n",
      "Training loss: 109.5505\n",
      "Training acc over epoch: 0.8411\n",
      "---- Validation ----\n",
      "Validation loss: 49.9614\n",
      "Validation acc: 0.7579\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 306.2394, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 10: 318.4937, Accuracy: 0.8153\n",
      "Training loss (for one batch) at step 20: 302.9692, Accuracy: 0.8311\n",
      "Training loss (for one batch) at step 30: 310.0557, Accuracy: 0.8405\n",
      "Training loss (for one batch) at step 40: 303.4783, Accuracy: 0.8506\n",
      "Training loss (for one batch) at step 50: 272.4257, Accuracy: 0.8588\n",
      "Training loss (for one batch) at step 60: 296.7572, Accuracy: 0.8604\n",
      "Training loss (for one batch) at step 70: 305.6683, Accuracy: 0.8499\n",
      "Training loss (for one batch) at step 80: 307.4582, Accuracy: 0.8435\n",
      "Training loss (for one batch) at step 90: 305.0630, Accuracy: 0.8414\n",
      "Training loss (for one batch) at step 100: 297.4607, Accuracy: 0.8430\n",
      "Training loss (for one batch) at step 110: 286.9902, Accuracy: 0.8418\n",
      "---- Training ----\n",
      "Training loss: 101.1056\n",
      "Training acc over epoch: 0.8403\n",
      "---- Validation ----\n",
      "Validation loss: 62.8985\n",
      "Validation acc: 0.7480\n",
      "Time taken: 10.54s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 305.8817, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 339.2025, Accuracy: 0.8118\n",
      "Training loss (for one batch) at step 20: 297.2713, Accuracy: 0.8311\n",
      "Training loss (for one batch) at step 30: 302.1753, Accuracy: 0.8372\n",
      "Training loss (for one batch) at step 40: 289.5702, Accuracy: 0.8464\n",
      "Training loss (for one batch) at step 50: 281.8142, Accuracy: 0.8540\n",
      "Training loss (for one batch) at step 60: 304.7153, Accuracy: 0.8550\n",
      "Training loss (for one batch) at step 70: 316.3293, Accuracy: 0.8473\n",
      "Training loss (for one batch) at step 80: 312.0690, Accuracy: 0.8422\n",
      "Training loss (for one batch) at step 90: 287.0304, Accuracy: 0.8419\n",
      "Training loss (for one batch) at step 100: 274.4378, Accuracy: 0.8427\n",
      "Training loss (for one batch) at step 110: 299.4013, Accuracy: 0.8414\n",
      "---- Training ----\n",
      "Training loss: 98.9508\n",
      "Training acc over epoch: 0.8398\n",
      "---- Validation ----\n",
      "Validation loss: 55.4320\n",
      "Validation acc: 0.7633\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 324.3362, Accuracy: 0.8828\n",
      "Training loss (for one batch) at step 10: 309.3104, Accuracy: 0.8310\n",
      "Training loss (for one batch) at step 20: 292.9009, Accuracy: 0.8337\n",
      "Training loss (for one batch) at step 30: 278.1239, Accuracy: 0.8407\n",
      "Training loss (for one batch) at step 40: 284.0229, Accuracy: 0.8472\n",
      "Training loss (for one batch) at step 50: 304.5115, Accuracy: 0.8554\n",
      "Training loss (for one batch) at step 60: 287.5166, Accuracy: 0.8548\n",
      "Training loss (for one batch) at step 70: 313.0530, Accuracy: 0.8490\n",
      "Training loss (for one batch) at step 80: 299.1040, Accuracy: 0.8402\n",
      "Training loss (for one batch) at step 90: 276.6039, Accuracy: 0.8409\n",
      "Training loss (for one batch) at step 100: 277.4598, Accuracy: 0.8422\n",
      "Training loss (for one batch) at step 110: 284.8208, Accuracy: 0.8406\n",
      "---- Training ----\n",
      "Training loss: 109.0854\n",
      "Training acc over epoch: 0.8400\n",
      "---- Validation ----\n",
      "Validation loss: 80.9758\n",
      "Validation acc: 0.7628\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 307.4121, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 356.4253, Accuracy: 0.8288\n",
      "Training loss (for one batch) at step 20: 297.0430, Accuracy: 0.8356\n",
      "Training loss (for one batch) at step 30: 284.0220, Accuracy: 0.8465\n",
      "Training loss (for one batch) at step 40: 272.6579, Accuracy: 0.8527\n",
      "Training loss (for one batch) at step 50: 280.0659, Accuracy: 0.8591\n",
      "Training loss (for one batch) at step 60: 297.3591, Accuracy: 0.8581\n",
      "Training loss (for one batch) at step 70: 309.8973, Accuracy: 0.8516\n",
      "Training loss (for one batch) at step 80: 289.2633, Accuracy: 0.8469\n",
      "Training loss (for one batch) at step 90: 292.0327, Accuracy: 0.8447\n",
      "Training loss (for one batch) at step 100: 287.1606, Accuracy: 0.8452\n",
      "Training loss (for one batch) at step 110: 310.1949, Accuracy: 0.8433\n",
      "---- Training ----\n",
      "Training loss: 88.7642\n",
      "Training acc over epoch: 0.8428\n",
      "---- Validation ----\n",
      "Validation loss: 41.8689\n",
      "Validation acc: 0.7665\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 295.9108, Accuracy: 0.8594\n",
      "Training loss (for one batch) at step 10: 300.6105, Accuracy: 0.8253\n",
      "Training loss (for one batch) at step 20: 289.2527, Accuracy: 0.8419\n",
      "Training loss (for one batch) at step 30: 275.7811, Accuracy: 0.8506\n",
      "Training loss (for one batch) at step 40: 283.2299, Accuracy: 0.8561\n",
      "Training loss (for one batch) at step 50: 265.8017, Accuracy: 0.8606\n",
      "Training loss (for one batch) at step 60: 305.0031, Accuracy: 0.8609\n",
      "Training loss (for one batch) at step 70: 303.4639, Accuracy: 0.8542\n",
      "Training loss (for one batch) at step 80: 307.4758, Accuracy: 0.8473\n",
      "Training loss (for one batch) at step 90: 287.1021, Accuracy: 0.8470\n",
      "Training loss (for one batch) at step 100: 287.3297, Accuracy: 0.8466\n",
      "Training loss (for one batch) at step 110: 294.1610, Accuracy: 0.8460\n",
      "---- Training ----\n",
      "Training loss: 95.4038\n",
      "Training acc over epoch: 0.8455\n",
      "---- Validation ----\n",
      "Validation loss: 59.4885\n",
      "Validation acc: 0.7571\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 313.1907, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 319.7221, Accuracy: 0.8146\n",
      "Training loss (for one batch) at step 20: 292.9431, Accuracy: 0.8315\n",
      "Training loss (for one batch) at step 30: 274.6880, Accuracy: 0.8435\n",
      "Training loss (for one batch) at step 40: 288.8882, Accuracy: 0.8510\n",
      "Training loss (for one batch) at step 50: 286.4811, Accuracy: 0.8601\n",
      "Training loss (for one batch) at step 60: 286.3386, Accuracy: 0.8594\n",
      "Training loss (for one batch) at step 70: 324.6365, Accuracy: 0.8502\n",
      "Training loss (for one batch) at step 80: 322.0458, Accuracy: 0.8423\n",
      "Training loss (for one batch) at step 90: 299.3776, Accuracy: 0.8415\n",
      "Training loss (for one batch) at step 100: 294.8145, Accuracy: 0.8432\n",
      "Training loss (for one batch) at step 110: 299.1818, Accuracy: 0.8415\n",
      "---- Training ----\n",
      "Training loss: 108.8159\n",
      "Training acc over epoch: 0.8405\n",
      "---- Validation ----\n",
      "Validation loss: 31.3967\n",
      "Validation acc: 0.7630\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 317.7332, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 321.0126, Accuracy: 0.8246\n",
      "Training loss (for one batch) at step 20: 285.0193, Accuracy: 0.8415\n",
      "Training loss (for one batch) at step 30: 279.2301, Accuracy: 0.8445\n",
      "Training loss (for one batch) at step 40: 295.2352, Accuracy: 0.8518\n",
      "Training loss (for one batch) at step 50: 312.9990, Accuracy: 0.8585\n",
      "Training loss (for one batch) at step 60: 281.3349, Accuracy: 0.8608\n",
      "Training loss (for one batch) at step 70: 299.9198, Accuracy: 0.8535\n",
      "Training loss (for one batch) at step 80: 295.1180, Accuracy: 0.8480\n",
      "Training loss (for one batch) at step 90: 279.8697, Accuracy: 0.8456\n",
      "Training loss (for one batch) at step 100: 296.3269, Accuracy: 0.8455\n",
      "Training loss (for one batch) at step 110: 288.7889, Accuracy: 0.8439\n",
      "---- Training ----\n",
      "Training loss: 97.2205\n",
      "Training acc over epoch: 0.8438\n",
      "---- Validation ----\n",
      "Validation loss: 58.7747\n",
      "Validation acc: 0.7555\n",
      "Time taken: 10.60s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 310.5159, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 10: 313.5803, Accuracy: 0.8295\n",
      "Training loss (for one batch) at step 20: 299.8264, Accuracy: 0.8292\n",
      "Training loss (for one batch) at step 30: 270.6325, Accuracy: 0.8415\n",
      "Training loss (for one batch) at step 40: 292.3245, Accuracy: 0.8516\n",
      "Training loss (for one batch) at step 50: 281.1490, Accuracy: 0.8591\n",
      "Training loss (for one batch) at step 60: 296.2820, Accuracy: 0.8600\n",
      "Training loss (for one batch) at step 70: 304.0028, Accuracy: 0.8522\n",
      "Training loss (for one batch) at step 80: 305.1220, Accuracy: 0.8432\n",
      "Training loss (for one batch) at step 90: 305.8781, Accuracy: 0.8419\n",
      "Training loss (for one batch) at step 100: 286.5481, Accuracy: 0.8422\n",
      "Training loss (for one batch) at step 110: 309.6992, Accuracy: 0.8408\n",
      "---- Training ----\n",
      "Training loss: 88.7405\n",
      "Training acc over epoch: 0.8395\n",
      "---- Validation ----\n",
      "Validation loss: 67.3446\n",
      "Validation acc: 0.7614\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 325.2403, Accuracy: 0.8594\n",
      "Training loss (for one batch) at step 10: 329.0441, Accuracy: 0.8224\n",
      "Training loss (for one batch) at step 20: 287.9735, Accuracy: 0.8352\n",
      "Training loss (for one batch) at step 30: 282.9477, Accuracy: 0.8455\n",
      "Training loss (for one batch) at step 40: 287.1795, Accuracy: 0.8533\n",
      "Training loss (for one batch) at step 50: 295.7212, Accuracy: 0.8608\n",
      "Training loss (for one batch) at step 60: 297.8834, Accuracy: 0.8604\n",
      "Training loss (for one batch) at step 70: 328.9386, Accuracy: 0.8532\n",
      "Training loss (for one batch) at step 80: 291.5836, Accuracy: 0.8490\n",
      "Training loss (for one batch) at step 90: 306.7111, Accuracy: 0.8447\n",
      "Training loss (for one batch) at step 100: 280.0587, Accuracy: 0.8455\n",
      "Training loss (for one batch) at step 110: 294.2463, Accuracy: 0.8445\n",
      "---- Training ----\n",
      "Training loss: 102.1501\n",
      "Training acc over epoch: 0.8434\n",
      "---- Validation ----\n",
      "Validation loss: 48.9034\n",
      "Validation acc: 0.7552\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 314.2198, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 309.6061, Accuracy: 0.8040\n",
      "Training loss (for one batch) at step 20: 287.7946, Accuracy: 0.8289\n",
      "Training loss (for one batch) at step 30: 264.7408, Accuracy: 0.8390\n",
      "Training loss (for one batch) at step 40: 289.1590, Accuracy: 0.8479\n",
      "Training loss (for one batch) at step 50: 298.3746, Accuracy: 0.8559\n",
      "Training loss (for one batch) at step 60: 287.4935, Accuracy: 0.8580\n",
      "Training loss (for one batch) at step 70: 295.7855, Accuracy: 0.8499\n",
      "Training loss (for one batch) at step 80: 307.9081, Accuracy: 0.8445\n",
      "Training loss (for one batch) at step 90: 287.3380, Accuracy: 0.8409\n",
      "Training loss (for one batch) at step 100: 272.5754, Accuracy: 0.8427\n",
      "Training loss (for one batch) at step 110: 307.8180, Accuracy: 0.8430\n",
      "---- Training ----\n",
      "Training loss: 97.0005\n",
      "Training acc over epoch: 0.8418\n",
      "---- Validation ----\n",
      "Validation loss: 73.7499\n",
      "Validation acc: 0.7582\n",
      "Time taken: 10.72s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 300.1353, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 10: 309.0368, Accuracy: 0.8224\n",
      "Training loss (for one batch) at step 20: 316.9965, Accuracy: 0.8378\n",
      "Training loss (for one batch) at step 30: 283.8412, Accuracy: 0.8465\n",
      "Training loss (for one batch) at step 40: 275.3121, Accuracy: 0.8538\n",
      "Training loss (for one batch) at step 50: 278.6051, Accuracy: 0.8595\n",
      "Training loss (for one batch) at step 60: 304.9677, Accuracy: 0.8609\n",
      "Training loss (for one batch) at step 70: 313.3531, Accuracy: 0.8532\n",
      "Training loss (for one batch) at step 80: 318.3035, Accuracy: 0.8458\n",
      "Training loss (for one batch) at step 90: 283.4619, Accuracy: 0.8442\n",
      "Training loss (for one batch) at step 100: 287.7286, Accuracy: 0.8445\n",
      "Training loss (for one batch) at step 110: 299.0931, Accuracy: 0.8428\n",
      "---- Training ----\n",
      "Training loss: 87.9175\n",
      "Training acc over epoch: 0.8409\n",
      "---- Validation ----\n",
      "Validation loss: 53.6064\n",
      "Validation acc: 0.7671\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 302.6692, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 298.1292, Accuracy: 0.8111\n",
      "Training loss (for one batch) at step 20: 275.4164, Accuracy: 0.8244\n",
      "Training loss (for one batch) at step 30: 303.2094, Accuracy: 0.8390\n",
      "Training loss (for one batch) at step 40: 290.0137, Accuracy: 0.8535\n",
      "Training loss (for one batch) at step 50: 266.6694, Accuracy: 0.8585\n",
      "Training loss (for one batch) at step 60: 304.4319, Accuracy: 0.8605\n",
      "Training loss (for one batch) at step 70: 307.8974, Accuracy: 0.8524\n",
      "Training loss (for one batch) at step 80: 291.3774, Accuracy: 0.8458\n",
      "Training loss (for one batch) at step 90: 301.2378, Accuracy: 0.8449\n",
      "Training loss (for one batch) at step 100: 287.6458, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 110: 293.8174, Accuracy: 0.8437\n",
      "---- Training ----\n",
      "Training loss: 95.5584\n",
      "Training acc over epoch: 0.8421\n",
      "---- Validation ----\n",
      "Validation loss: 43.2853\n",
      "Validation acc: 0.7590\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 333.7302, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 289.4889, Accuracy: 0.8210\n",
      "Training loss (for one batch) at step 20: 282.7089, Accuracy: 0.8307\n",
      "Training loss (for one batch) at step 30: 283.5881, Accuracy: 0.8410\n",
      "Training loss (for one batch) at step 40: 295.2282, Accuracy: 0.8498\n",
      "Training loss (for one batch) at step 50: 281.6656, Accuracy: 0.8594\n",
      "Training loss (for one batch) at step 60: 288.1738, Accuracy: 0.8581\n",
      "Training loss (for one batch) at step 70: 315.1794, Accuracy: 0.8515\n",
      "Training loss (for one batch) at step 80: 301.6570, Accuracy: 0.8470\n",
      "Training loss (for one batch) at step 90: 263.5103, Accuracy: 0.8437\n",
      "Training loss (for one batch) at step 100: 293.7474, Accuracy: 0.8426\n",
      "Training loss (for one batch) at step 110: 300.2532, Accuracy: 0.8428\n",
      "---- Training ----\n",
      "Training loss: 95.2003\n",
      "Training acc over epoch: 0.8420\n",
      "---- Validation ----\n",
      "Validation loss: 33.4231\n",
      "Validation acc: 0.7687\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 318.6896, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 305.1446, Accuracy: 0.8104\n",
      "Training loss (for one batch) at step 20: 296.8875, Accuracy: 0.8311\n",
      "Training loss (for one batch) at step 30: 264.1686, Accuracy: 0.8435\n",
      "Training loss (for one batch) at step 40: 286.4037, Accuracy: 0.8495\n",
      "Training loss (for one batch) at step 50: 307.4347, Accuracy: 0.8562\n",
      "Training loss (for one batch) at step 60: 296.5406, Accuracy: 0.8584\n",
      "Training loss (for one batch) at step 70: 306.0753, Accuracy: 0.8526\n",
      "Training loss (for one batch) at step 80: 301.2793, Accuracy: 0.8452\n",
      "Training loss (for one batch) at step 90: 286.3747, Accuracy: 0.8425\n",
      "Training loss (for one batch) at step 100: 294.8486, Accuracy: 0.8434\n",
      "Training loss (for one batch) at step 110: 289.9390, Accuracy: 0.8427\n",
      "---- Training ----\n",
      "Training loss: 94.1270\n",
      "Training acc over epoch: 0.8416\n",
      "---- Validation ----\n",
      "Validation loss: 44.4446\n",
      "Validation acc: 0.7593\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 301.2352, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 10: 314.5191, Accuracy: 0.8260\n",
      "Training loss (for one batch) at step 20: 275.0520, Accuracy: 0.8430\n",
      "Training loss (for one batch) at step 30: 266.7336, Accuracy: 0.8435\n",
      "Training loss (for one batch) at step 40: 266.9845, Accuracy: 0.8525\n",
      "Training loss (for one batch) at step 50: 280.0299, Accuracy: 0.8586\n",
      "Training loss (for one batch) at step 60: 299.3631, Accuracy: 0.8605\n",
      "Training loss (for one batch) at step 70: 303.5242, Accuracy: 0.8521\n",
      "Training loss (for one batch) at step 80: 298.9861, Accuracy: 0.8473\n",
      "Training loss (for one batch) at step 90: 295.2758, Accuracy: 0.8452\n",
      "Training loss (for one batch) at step 100: 286.3814, Accuracy: 0.8441\n",
      "Training loss (for one batch) at step 110: 284.8197, Accuracy: 0.8435\n",
      "---- Training ----\n",
      "Training loss: 103.3517\n",
      "Training acc over epoch: 0.8425\n",
      "---- Validation ----\n",
      "Validation loss: 46.0089\n",
      "Validation acc: 0.7536\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 336.5191, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 10: 294.5060, Accuracy: 0.8182\n",
      "Training loss (for one batch) at step 20: 291.0368, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 30: 278.9315, Accuracy: 0.8417\n",
      "Training loss (for one batch) at step 40: 266.4126, Accuracy: 0.8516\n",
      "Training loss (for one batch) at step 50: 302.2125, Accuracy: 0.8577\n",
      "Training loss (for one batch) at step 60: 292.3198, Accuracy: 0.8560\n",
      "Training loss (for one batch) at step 70: 296.5766, Accuracy: 0.8493\n",
      "Training loss (for one batch) at step 80: 303.2721, Accuracy: 0.8437\n",
      "Training loss (for one batch) at step 90: 284.0549, Accuracy: 0.8404\n",
      "Training loss (for one batch) at step 100: 268.0451, Accuracy: 0.8424\n",
      "Training loss (for one batch) at step 110: 284.0674, Accuracy: 0.8414\n",
      "---- Training ----\n",
      "Training loss: 93.6640\n",
      "Training acc over epoch: 0.8407\n",
      "---- Validation ----\n",
      "Validation loss: 53.6797\n",
      "Validation acc: 0.7593\n",
      "Time taken: 10.66s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 338.0928, Accuracy: 0.8516\n",
      "Training loss (for one batch) at step 10: 293.2510, Accuracy: 0.8239\n",
      "Training loss (for one batch) at step 20: 292.8285, Accuracy: 0.8378\n",
      "Training loss (for one batch) at step 30: 279.5187, Accuracy: 0.8440\n",
      "Training loss (for one batch) at step 40: 279.4224, Accuracy: 0.8531\n",
      "Training loss (for one batch) at step 50: 259.9087, Accuracy: 0.8600\n",
      "Training loss (for one batch) at step 60: 289.2363, Accuracy: 0.8622\n",
      "Training loss (for one batch) at step 70: 309.9219, Accuracy: 0.8552\n",
      "Training loss (for one batch) at step 80: 297.2830, Accuracy: 0.8474\n",
      "Training loss (for one batch) at step 90: 284.5737, Accuracy: 0.8444\n",
      "Training loss (for one batch) at step 100: 279.7870, Accuracy: 0.8437\n",
      "Training loss (for one batch) at step 110: 272.5767, Accuracy: 0.8442\n",
      "---- Training ----\n",
      "Training loss: 84.3580\n",
      "Training acc over epoch: 0.8430\n",
      "---- Validation ----\n",
      "Validation loss: 39.7216\n",
      "Validation acc: 0.7544\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 285.9931, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 301.9458, Accuracy: 0.8161\n",
      "Training loss (for one batch) at step 20: 267.9196, Accuracy: 0.8382\n",
      "Training loss (for one batch) at step 30: 280.1757, Accuracy: 0.8443\n",
      "Training loss (for one batch) at step 40: 281.5511, Accuracy: 0.8500\n",
      "Training loss (for one batch) at step 50: 298.1629, Accuracy: 0.8565\n",
      "Training loss (for one batch) at step 60: 263.9676, Accuracy: 0.8598\n",
      "Training loss (for one batch) at step 70: 285.4733, Accuracy: 0.8505\n",
      "Training loss (for one batch) at step 80: 317.9511, Accuracy: 0.8447\n",
      "Training loss (for one batch) at step 90: 265.0114, Accuracy: 0.8429\n",
      "Training loss (for one batch) at step 100: 296.6962, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 110: 285.4617, Accuracy: 0.8430\n",
      "---- Training ----\n",
      "Training loss: 104.7405\n",
      "Training acc over epoch: 0.8416\n",
      "---- Validation ----\n",
      "Validation loss: 93.1343\n",
      "Validation acc: 0.7582\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 292.3876, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 290.2201, Accuracy: 0.8146\n",
      "Training loss (for one batch) at step 20: 289.3498, Accuracy: 0.8307\n",
      "Training loss (for one batch) at step 30: 274.1408, Accuracy: 0.8425\n",
      "Training loss (for one batch) at step 40: 270.7350, Accuracy: 0.8464\n",
      "Training loss (for one batch) at step 50: 291.1441, Accuracy: 0.8543\n",
      "Training loss (for one batch) at step 60: 270.3830, Accuracy: 0.8584\n",
      "Training loss (for one batch) at step 70: 299.4584, Accuracy: 0.8526\n",
      "Training loss (for one batch) at step 80: 315.4980, Accuracy: 0.8459\n",
      "Training loss (for one batch) at step 90: 303.6138, Accuracy: 0.8436\n",
      "Training loss (for one batch) at step 100: 303.4309, Accuracy: 0.8452\n",
      "Training loss (for one batch) at step 110: 287.2690, Accuracy: 0.8440\n",
      "---- Training ----\n",
      "Training loss: 107.9825\n",
      "Training acc over epoch: 0.8431\n",
      "---- Validation ----\n",
      "Validation loss: 50.5002\n",
      "Validation acc: 0.7480\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 320.7648, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 308.4487, Accuracy: 0.8239\n",
      "Training loss (for one batch) at step 20: 273.1768, Accuracy: 0.8318\n",
      "Training loss (for one batch) at step 30: 293.1702, Accuracy: 0.8400\n",
      "Training loss (for one batch) at step 40: 259.3313, Accuracy: 0.8504\n",
      "Training loss (for one batch) at step 50: 282.2356, Accuracy: 0.8555\n",
      "Training loss (for one batch) at step 60: 295.6033, Accuracy: 0.8568\n",
      "Training loss (for one batch) at step 70: 305.6726, Accuracy: 0.8513\n",
      "Training loss (for one batch) at step 80: 310.9852, Accuracy: 0.8457\n",
      "Training loss (for one batch) at step 90: 285.2519, Accuracy: 0.8446\n",
      "Training loss (for one batch) at step 100: 278.0165, Accuracy: 0.8448\n",
      "Training loss (for one batch) at step 110: 277.6733, Accuracy: 0.8447\n",
      "---- Training ----\n",
      "Training loss: 92.5154\n",
      "Training acc over epoch: 0.8435\n",
      "---- Validation ----\n",
      "Validation loss: 42.4322\n",
      "Validation acc: 0.7539\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 300.0920, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 293.7221, Accuracy: 0.8189\n",
      "Training loss (for one batch) at step 20: 286.5697, Accuracy: 0.8341\n",
      "Training loss (for one batch) at step 30: 292.5477, Accuracy: 0.8440\n",
      "Training loss (for one batch) at step 40: 279.4313, Accuracy: 0.8518\n",
      "Training loss (for one batch) at step 50: 276.5638, Accuracy: 0.8574\n",
      "Training loss (for one batch) at step 60: 279.6918, Accuracy: 0.8604\n",
      "Training loss (for one batch) at step 70: 304.7771, Accuracy: 0.8542\n",
      "Training loss (for one batch) at step 80: 275.9527, Accuracy: 0.8472\n",
      "Training loss (for one batch) at step 90: 276.5495, Accuracy: 0.8455\n",
      "Training loss (for one batch) at step 100: 279.0537, Accuracy: 0.8457\n",
      "Training loss (for one batch) at step 110: 308.1187, Accuracy: 0.8442\n",
      "---- Training ----\n",
      "Training loss: 81.5367\n",
      "Training acc over epoch: 0.8443\n",
      "---- Validation ----\n",
      "Validation loss: 50.9989\n",
      "Validation acc: 0.7560\n",
      "Time taken: 10.24s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.7648\n",
      "Validation AUC: 0.7640\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 670.7859, Accuracy: 0.3984\n",
      "Training loss (for one batch) at step 10: 622.7382, Accuracy: 0.4858\n",
      "Training loss (for one batch) at step 20: 575.2712, Accuracy: 0.5011\n",
      "Training loss (for one batch) at step 30: 542.3860, Accuracy: 0.5023\n",
      "Training loss (for one batch) at step 40: 507.1797, Accuracy: 0.5051\n",
      "Training loss (for one batch) at step 50: 499.2445, Accuracy: 0.5083\n",
      "Training loss (for one batch) at step 60: 508.9822, Accuracy: 0.5120\n",
      "Training loss (for one batch) at step 70: 506.2785, Accuracy: 0.5124\n",
      "Training loss (for one batch) at step 80: 487.1513, Accuracy: 0.5102\n",
      "Training loss (for one batch) at step 90: 482.4382, Accuracy: 0.5110\n",
      "Training loss (for one batch) at step 100: 475.3534, Accuracy: 0.5134\n",
      "Training loss (for one batch) at step 110: 473.2705, Accuracy: 0.5137\n",
      "---- Training ----\n",
      "Training loss: 151.4904\n",
      "Training acc over epoch: 0.5135\n",
      "---- Validation ----\n",
      "Validation loss: 34.0558\n",
      "Validation acc: 0.5134\n",
      "Time taken: 12.52s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 472.6801, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 463.9652, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 20: 471.6434, Accuracy: 0.5257\n",
      "Training loss (for one batch) at step 30: 462.1229, Accuracy: 0.5262\n",
      "Training loss (for one batch) at step 40: 461.5172, Accuracy: 0.5225\n",
      "Training loss (for one batch) at step 50: 459.0759, Accuracy: 0.5251\n",
      "Training loss (for one batch) at step 60: 453.5662, Accuracy: 0.5250\n",
      "Training loss (for one batch) at step 70: 447.0713, Accuracy: 0.5305\n",
      "Training loss (for one batch) at step 80: 456.4576, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 90: 452.1750, Accuracy: 0.5311\n",
      "Training loss (for one batch) at step 100: 454.2793, Accuracy: 0.5306\n",
      "Training loss (for one batch) at step 110: 446.0375, Accuracy: 0.5321\n",
      "---- Training ----\n",
      "Training loss: 138.1153\n",
      "Training acc over epoch: 0.5326\n",
      "---- Validation ----\n",
      "Validation loss: 36.1347\n",
      "Validation acc: 0.5134\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 442.0502, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 443.0574, Accuracy: 0.5213\n",
      "Training loss (for one batch) at step 20: 443.0091, Accuracy: 0.5231\n",
      "Training loss (for one batch) at step 30: 448.6019, Accuracy: 0.5323\n",
      "Training loss (for one batch) at step 40: 445.8020, Accuracy: 0.5389\n",
      "Training loss (for one batch) at step 50: 444.3089, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 60: 443.4305, Accuracy: 0.5416\n",
      "Training loss (for one batch) at step 70: 446.5508, Accuracy: 0.5432\n",
      "Training loss (for one batch) at step 80: 442.9410, Accuracy: 0.5429\n",
      "Training loss (for one batch) at step 90: 443.2758, Accuracy: 0.5410\n",
      "Training loss (for one batch) at step 100: 442.6678, Accuracy: 0.5432\n",
      "Training loss (for one batch) at step 110: 445.0987, Accuracy: 0.5467\n",
      "---- Training ----\n",
      "Training loss: 140.7706\n",
      "Training acc over epoch: 0.5457\n",
      "---- Validation ----\n",
      "Validation loss: 35.1342\n",
      "Validation acc: 0.5140\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 443.1447, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 441.8679, Accuracy: 0.5483\n",
      "Training loss (for one batch) at step 20: 444.9921, Accuracy: 0.5439\n",
      "Training loss (for one batch) at step 30: 442.3839, Accuracy: 0.5461\n",
      "Training loss (for one batch) at step 40: 444.0330, Accuracy: 0.5501\n",
      "Training loss (for one batch) at step 50: 441.8243, Accuracy: 0.5510\n",
      "Training loss (for one batch) at step 60: 444.8272, Accuracy: 0.5506\n",
      "Training loss (for one batch) at step 70: 446.1648, Accuracy: 0.5530\n",
      "Training loss (for one batch) at step 80: 442.5119, Accuracy: 0.5507\n",
      "Training loss (for one batch) at step 90: 443.8994, Accuracy: 0.5512\n",
      "Training loss (for one batch) at step 100: 445.9975, Accuracy: 0.5528\n",
      "Training loss (for one batch) at step 110: 442.5847, Accuracy: 0.5560\n",
      "---- Training ----\n",
      "Training loss: 138.1365\n",
      "Training acc over epoch: 0.5561\n",
      "---- Validation ----\n",
      "Validation loss: 34.6731\n",
      "Validation acc: 0.5425\n",
      "Time taken: 10.70s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 447.2190, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 442.4594, Accuracy: 0.5320\n",
      "Training loss (for one batch) at step 20: 440.0393, Accuracy: 0.5499\n",
      "Training loss (for one batch) at step 30: 448.9312, Accuracy: 0.5496\n",
      "Training loss (for one batch) at step 40: 443.7332, Accuracy: 0.5503\n",
      "Training loss (for one batch) at step 50: 439.5765, Accuracy: 0.5521\n",
      "Training loss (for one batch) at step 60: 440.9758, Accuracy: 0.5575\n",
      "Training loss (for one batch) at step 70: 441.3746, Accuracy: 0.5628\n",
      "Training loss (for one batch) at step 80: 443.5347, Accuracy: 0.5665\n",
      "Training loss (for one batch) at step 90: 442.8996, Accuracy: 0.5642\n",
      "Training loss (for one batch) at step 100: 441.0669, Accuracy: 0.5647\n",
      "Training loss (for one batch) at step 110: 438.3865, Accuracy: 0.5658\n",
      "---- Training ----\n",
      "Training loss: 139.5614\n",
      "Training acc over epoch: 0.5654\n",
      "---- Validation ----\n",
      "Validation loss: 34.5333\n",
      "Validation acc: 0.5865\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 439.8693, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 443.7988, Accuracy: 0.5575\n",
      "Training loss (for one batch) at step 20: 441.3211, Accuracy: 0.5565\n",
      "Training loss (for one batch) at step 30: 443.2940, Accuracy: 0.5532\n",
      "Training loss (for one batch) at step 40: 441.7232, Accuracy: 0.5623\n",
      "Training loss (for one batch) at step 50: 443.7803, Accuracy: 0.5677\n",
      "Training loss (for one batch) at step 60: 440.5759, Accuracy: 0.5763\n",
      "Training loss (for one batch) at step 70: 440.6808, Accuracy: 0.5730\n",
      "Training loss (for one batch) at step 80: 444.8345, Accuracy: 0.5746\n",
      "Training loss (for one batch) at step 90: 442.0807, Accuracy: 0.5722\n",
      "Training loss (for one batch) at step 100: 440.1766, Accuracy: 0.5672\n",
      "Training loss (for one batch) at step 110: 438.2864, Accuracy: 0.5686\n",
      "---- Training ----\n",
      "Training loss: 138.0564\n",
      "Training acc over epoch: 0.5693\n",
      "---- Validation ----\n",
      "Validation loss: 34.8651\n",
      "Validation acc: 0.5634\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 446.2776, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 439.5145, Accuracy: 0.5866\n",
      "Training loss (for one batch) at step 20: 437.1949, Accuracy: 0.5629\n",
      "Training loss (for one batch) at step 30: 445.7238, Accuracy: 0.5678\n",
      "Training loss (for one batch) at step 40: 436.0305, Accuracy: 0.5762\n",
      "Training loss (for one batch) at step 50: 438.4055, Accuracy: 0.5744\n",
      "Training loss (for one batch) at step 60: 438.3554, Accuracy: 0.5844\n",
      "Training loss (for one batch) at step 70: 443.0343, Accuracy: 0.5853\n",
      "Training loss (for one batch) at step 80: 440.3107, Accuracy: 0.5773\n",
      "Training loss (for one batch) at step 90: 442.2224, Accuracy: 0.5747\n",
      "Training loss (for one batch) at step 100: 441.6257, Accuracy: 0.5784\n",
      "Training loss (for one batch) at step 110: 439.0410, Accuracy: 0.5807\n",
      "---- Training ----\n",
      "Training loss: 137.0372\n",
      "Training acc over epoch: 0.5821\n",
      "---- Validation ----\n",
      "Validation loss: 34.5548\n",
      "Validation acc: 0.5618\n",
      "Time taken: 10.68s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 438.0943, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 438.8044, Accuracy: 0.5362\n",
      "Training loss (for one batch) at step 20: 437.8701, Accuracy: 0.5409\n",
      "Training loss (for one batch) at step 30: 436.8097, Accuracy: 0.5514\n",
      "Training loss (for one batch) at step 40: 442.7542, Accuracy: 0.5629\n",
      "Training loss (for one batch) at step 50: 438.6260, Accuracy: 0.5731\n",
      "Training loss (for one batch) at step 60: 442.4359, Accuracy: 0.5776\n",
      "Training loss (for one batch) at step 70: 443.8748, Accuracy: 0.5822\n",
      "Training loss (for one batch) at step 80: 442.0747, Accuracy: 0.5786\n",
      "Training loss (for one batch) at step 90: 434.7018, Accuracy: 0.5690\n",
      "Training loss (for one batch) at step 100: 432.6995, Accuracy: 0.5689\n",
      "Training loss (for one batch) at step 110: 443.8798, Accuracy: 0.5695\n",
      "---- Training ----\n",
      "Training loss: 141.8853\n",
      "Training acc over epoch: 0.5701\n",
      "---- Validation ----\n",
      "Validation loss: 34.2828\n",
      "Validation acc: 0.4750\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 440.7510, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 435.4721, Accuracy: 0.5497\n",
      "Training loss (for one batch) at step 20: 437.0960, Accuracy: 0.5372\n",
      "Training loss (for one batch) at step 30: 435.4514, Accuracy: 0.5496\n",
      "Training loss (for one batch) at step 40: 432.1411, Accuracy: 0.5598\n",
      "Training loss (for one batch) at step 50: 429.9738, Accuracy: 0.5720\n",
      "Training loss (for one batch) at step 60: 439.8604, Accuracy: 0.5788\n",
      "Training loss (for one batch) at step 70: 442.2694, Accuracy: 0.5826\n",
      "Training loss (for one batch) at step 80: 441.4746, Accuracy: 0.5776\n",
      "Training loss (for one batch) at step 90: 435.4145, Accuracy: 0.5710\n",
      "Training loss (for one batch) at step 100: 430.3047, Accuracy: 0.5693\n",
      "Training loss (for one batch) at step 110: 440.5824, Accuracy: 0.5693\n",
      "---- Training ----\n",
      "Training loss: 137.6106\n",
      "Training acc over epoch: 0.5676\n",
      "---- Validation ----\n",
      "Validation loss: 35.6486\n",
      "Validation acc: 0.5648\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 439.4225, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 433.1277, Accuracy: 0.5298\n",
      "Training loss (for one batch) at step 20: 433.6041, Accuracy: 0.5290\n",
      "Training loss (for one batch) at step 30: 434.7258, Accuracy: 0.5403\n",
      "Training loss (for one batch) at step 40: 422.2873, Accuracy: 0.5501\n",
      "Training loss (for one batch) at step 50: 432.5251, Accuracy: 0.5600\n",
      "Training loss (for one batch) at step 60: 426.7803, Accuracy: 0.5642\n",
      "Training loss (for one batch) at step 70: 434.0145, Accuracy: 0.5645\n",
      "Training loss (for one batch) at step 80: 437.3556, Accuracy: 0.5607\n",
      "Training loss (for one batch) at step 90: 438.7678, Accuracy: 0.5543\n",
      "Training loss (for one batch) at step 100: 440.3165, Accuracy: 0.5541\n",
      "Training loss (for one batch) at step 110: 428.9626, Accuracy: 0.5562\n",
      "---- Training ----\n",
      "Training loss: 138.0324\n",
      "Training acc over epoch: 0.5554\n",
      "---- Validation ----\n",
      "Validation loss: 33.8554\n",
      "Validation acc: 0.5258\n",
      "Time taken: 10.78s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 440.7031, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 435.3290, Accuracy: 0.5334\n",
      "Training loss (for one batch) at step 20: 438.1446, Accuracy: 0.5350\n",
      "Training loss (for one batch) at step 30: 429.3354, Accuracy: 0.5353\n",
      "Training loss (for one batch) at step 40: 431.4922, Accuracy: 0.5398\n",
      "Training loss (for one batch) at step 50: 424.0315, Accuracy: 0.5516\n",
      "Training loss (for one batch) at step 60: 436.3974, Accuracy: 0.5589\n",
      "Training loss (for one batch) at step 70: 438.2112, Accuracy: 0.5647\n",
      "Training loss (for one batch) at step 80: 433.0621, Accuracy: 0.5601\n",
      "Training loss (for one batch) at step 90: 431.4310, Accuracy: 0.5532\n",
      "Training loss (for one batch) at step 100: 428.5080, Accuracy: 0.5537\n",
      "Training loss (for one batch) at step 110: 439.6819, Accuracy: 0.5545\n",
      "---- Training ----\n",
      "Training loss: 136.4659\n",
      "Training acc over epoch: 0.5535\n",
      "---- Validation ----\n",
      "Validation loss: 36.3879\n",
      "Validation acc: 0.5363\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 438.4847, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 436.4963, Accuracy: 0.5320\n",
      "Training loss (for one batch) at step 20: 427.3055, Accuracy: 0.5394\n",
      "Training loss (for one batch) at step 30: 429.6615, Accuracy: 0.5398\n",
      "Training loss (for one batch) at step 40: 423.0721, Accuracy: 0.5373\n",
      "Training loss (for one batch) at step 50: 424.8733, Accuracy: 0.5472\n",
      "Training loss (for one batch) at step 60: 425.4597, Accuracy: 0.5597\n",
      "Training loss (for one batch) at step 70: 439.9619, Accuracy: 0.5607\n",
      "Training loss (for one batch) at step 80: 430.6904, Accuracy: 0.5545\n",
      "Training loss (for one batch) at step 90: 436.0385, Accuracy: 0.5507\n",
      "Training loss (for one batch) at step 100: 433.6547, Accuracy: 0.5511\n",
      "Training loss (for one batch) at step 110: 433.9193, Accuracy: 0.5526\n",
      "---- Training ----\n",
      "Training loss: 133.8264\n",
      "Training acc over epoch: 0.5518\n",
      "---- Validation ----\n",
      "Validation loss: 34.5558\n",
      "Validation acc: 0.5161\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 439.5592, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 432.5930, Accuracy: 0.5412\n",
      "Training loss (for one batch) at step 20: 425.6155, Accuracy: 0.5346\n",
      "Training loss (for one batch) at step 30: 422.6180, Accuracy: 0.5449\n",
      "Training loss (for one batch) at step 40: 420.2548, Accuracy: 0.5488\n",
      "Training loss (for one batch) at step 50: 418.3413, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 60: 435.7310, Accuracy: 0.5606\n",
      "Training loss (for one batch) at step 70: 434.5480, Accuracy: 0.5638\n",
      "Training loss (for one batch) at step 80: 427.1829, Accuracy: 0.5614\n",
      "Training loss (for one batch) at step 90: 430.5117, Accuracy: 0.5557\n",
      "Training loss (for one batch) at step 100: 431.4136, Accuracy: 0.5546\n",
      "Training loss (for one batch) at step 110: 434.3052, Accuracy: 0.5555\n",
      "---- Training ----\n",
      "Training loss: 136.0908\n",
      "Training acc over epoch: 0.5549\n",
      "---- Validation ----\n",
      "Validation loss: 34.7044\n",
      "Validation acc: 0.5290\n",
      "Time taken: 10.71s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 433.9077, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 426.8213, Accuracy: 0.5099\n",
      "Training loss (for one batch) at step 20: 432.6712, Accuracy: 0.5175\n",
      "Training loss (for one batch) at step 30: 419.4025, Accuracy: 0.5360\n",
      "Training loss (for one batch) at step 40: 421.1926, Accuracy: 0.5434\n",
      "Training loss (for one batch) at step 50: 411.5543, Accuracy: 0.5555\n",
      "Training loss (for one batch) at step 60: 437.9565, Accuracy: 0.5645\n",
      "Training loss (for one batch) at step 70: 420.4296, Accuracy: 0.5690\n",
      "Training loss (for one batch) at step 80: 434.0545, Accuracy: 0.5651\n",
      "Training loss (for one batch) at step 90: 432.0263, Accuracy: 0.5599\n",
      "Training loss (for one batch) at step 100: 427.1709, Accuracy: 0.5606\n",
      "Training loss (for one batch) at step 110: 434.6919, Accuracy: 0.5621\n",
      "---- Training ----\n",
      "Training loss: 130.9610\n",
      "Training acc over epoch: 0.5609\n",
      "---- Validation ----\n",
      "Validation loss: 35.6512\n",
      "Validation acc: 0.5333\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 436.8336, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 430.2827, Accuracy: 0.5533\n",
      "Training loss (for one batch) at step 20: 424.6982, Accuracy: 0.5458\n",
      "Training loss (for one batch) at step 30: 425.3598, Accuracy: 0.5476\n",
      "Training loss (for one batch) at step 40: 412.0261, Accuracy: 0.5532\n",
      "Training loss (for one batch) at step 50: 409.6358, Accuracy: 0.5623\n",
      "Training loss (for one batch) at step 60: 417.1418, Accuracy: 0.5708\n",
      "Training loss (for one batch) at step 70: 433.7855, Accuracy: 0.5721\n",
      "Training loss (for one batch) at step 80: 426.1299, Accuracy: 0.5638\n",
      "Training loss (for one batch) at step 90: 419.9229, Accuracy: 0.5594\n",
      "Training loss (for one batch) at step 100: 422.2344, Accuracy: 0.5589\n",
      "Training loss (for one batch) at step 110: 438.2249, Accuracy: 0.5606\n",
      "---- Training ----\n",
      "Training loss: 137.6769\n",
      "Training acc over epoch: 0.5608\n",
      "---- Validation ----\n",
      "Validation loss: 33.2045\n",
      "Validation acc: 0.5193\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 430.5105, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 436.7402, Accuracy: 0.5291\n",
      "Training loss (for one batch) at step 20: 417.6742, Accuracy: 0.5406\n",
      "Training loss (for one batch) at step 30: 428.1971, Accuracy: 0.5502\n",
      "Training loss (for one batch) at step 40: 412.2028, Accuracy: 0.5598\n",
      "Training loss (for one batch) at step 50: 415.7847, Accuracy: 0.5669\n",
      "Training loss (for one batch) at step 60: 415.3769, Accuracy: 0.5786\n",
      "Training loss (for one batch) at step 70: 443.5952, Accuracy: 0.5785\n",
      "Training loss (for one batch) at step 80: 432.6556, Accuracy: 0.5694\n",
      "Training loss (for one batch) at step 90: 428.0738, Accuracy: 0.5628\n",
      "Training loss (for one batch) at step 100: 417.5213, Accuracy: 0.5637\n",
      "Training loss (for one batch) at step 110: 413.0749, Accuracy: 0.5636\n",
      "---- Training ----\n",
      "Training loss: 135.8357\n",
      "Training acc over epoch: 0.5642\n",
      "---- Validation ----\n",
      "Validation loss: 36.0753\n",
      "Validation acc: 0.5459\n",
      "Time taken: 10.75s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 437.3112, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 429.1032, Accuracy: 0.5206\n",
      "Training loss (for one batch) at step 20: 429.6310, Accuracy: 0.5346\n",
      "Training loss (for one batch) at step 30: 411.5919, Accuracy: 0.5428\n",
      "Training loss (for one batch) at step 40: 412.7562, Accuracy: 0.5560\n",
      "Training loss (for one batch) at step 50: 400.2892, Accuracy: 0.5666\n",
      "Training loss (for one batch) at step 60: 419.0046, Accuracy: 0.5791\n",
      "Training loss (for one batch) at step 70: 422.3698, Accuracy: 0.5814\n",
      "Training loss (for one batch) at step 80: 441.7710, Accuracy: 0.5747\n",
      "Training loss (for one batch) at step 90: 425.1915, Accuracy: 0.5672\n",
      "Training loss (for one batch) at step 100: 417.4778, Accuracy: 0.5689\n",
      "Training loss (for one batch) at step 110: 422.0094, Accuracy: 0.5710\n",
      "---- Training ----\n",
      "Training loss: 134.1426\n",
      "Training acc over epoch: 0.5709\n",
      "---- Validation ----\n",
      "Validation loss: 34.1622\n",
      "Validation acc: 0.5312\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 433.7812, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 430.7795, Accuracy: 0.5476\n",
      "Training loss (for one batch) at step 20: 430.2576, Accuracy: 0.5484\n",
      "Training loss (for one batch) at step 30: 429.3042, Accuracy: 0.5575\n",
      "Training loss (for one batch) at step 40: 410.5608, Accuracy: 0.5650\n",
      "Training loss (for one batch) at step 50: 391.4304, Accuracy: 0.5760\n",
      "Training loss (for one batch) at step 60: 409.2586, Accuracy: 0.5843\n",
      "Training loss (for one batch) at step 70: 415.9676, Accuracy: 0.5878\n",
      "Training loss (for one batch) at step 80: 427.5691, Accuracy: 0.5825\n",
      "Training loss (for one batch) at step 90: 425.1388, Accuracy: 0.5769\n",
      "Training loss (for one batch) at step 100: 406.9970, Accuracy: 0.5768\n",
      "Training loss (for one batch) at step 110: 419.9797, Accuracy: 0.5779\n",
      "---- Training ----\n",
      "Training loss: 134.1165\n",
      "Training acc over epoch: 0.5764\n",
      "---- Validation ----\n",
      "Validation loss: 34.9915\n",
      "Validation acc: 0.5564\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 429.2491, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 423.1877, Accuracy: 0.5497\n",
      "Training loss (for one batch) at step 20: 426.1936, Accuracy: 0.5543\n",
      "Training loss (for one batch) at step 30: 410.5646, Accuracy: 0.5590\n",
      "Training loss (for one batch) at step 40: 402.2441, Accuracy: 0.5657\n",
      "Training loss (for one batch) at step 50: 396.4652, Accuracy: 0.5775\n",
      "Training loss (for one batch) at step 60: 397.3124, Accuracy: 0.5849\n",
      "Training loss (for one batch) at step 70: 419.6786, Accuracy: 0.5854\n",
      "Training loss (for one batch) at step 80: 418.8424, Accuracy: 0.5803\n",
      "Training loss (for one batch) at step 90: 414.8691, Accuracy: 0.5768\n",
      "Training loss (for one batch) at step 100: 408.2067, Accuracy: 0.5760\n",
      "Training loss (for one batch) at step 110: 433.0459, Accuracy: 0.5767\n",
      "---- Training ----\n",
      "Training loss: 129.0368\n",
      "Training acc over epoch: 0.5768\n",
      "---- Validation ----\n",
      "Validation loss: 34.5415\n",
      "Validation acc: 0.5156\n",
      "Time taken: 10.80s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 433.7669, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 418.1547, Accuracy: 0.5462\n",
      "Training loss (for one batch) at step 20: 419.4860, Accuracy: 0.5476\n",
      "Training loss (for one batch) at step 30: 413.8347, Accuracy: 0.5565\n",
      "Training loss (for one batch) at step 40: 396.6563, Accuracy: 0.5661\n",
      "Training loss (for one batch) at step 50: 381.3403, Accuracy: 0.5807\n",
      "Training loss (for one batch) at step 60: 401.4919, Accuracy: 0.5879\n",
      "Training loss (for one batch) at step 70: 422.3852, Accuracy: 0.5873\n",
      "Training loss (for one batch) at step 80: 423.7993, Accuracy: 0.5800\n",
      "Training loss (for one batch) at step 90: 408.2408, Accuracy: 0.5767\n",
      "Training loss (for one batch) at step 100: 412.6471, Accuracy: 0.5760\n",
      "Training loss (for one batch) at step 110: 426.2561, Accuracy: 0.5768\n",
      "---- Training ----\n",
      "Training loss: 129.4522\n",
      "Training acc over epoch: 0.5770\n",
      "---- Validation ----\n",
      "Validation loss: 32.6809\n",
      "Validation acc: 0.5290\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 426.3518, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 415.4879, Accuracy: 0.5462\n",
      "Training loss (for one batch) at step 20: 409.4283, Accuracy: 0.5487\n",
      "Training loss (for one batch) at step 30: 399.7388, Accuracy: 0.5680\n",
      "Training loss (for one batch) at step 40: 400.9123, Accuracy: 0.5756\n",
      "Training loss (for one batch) at step 50: 385.3958, Accuracy: 0.5867\n",
      "Training loss (for one batch) at step 60: 406.7515, Accuracy: 0.5949\n",
      "Training loss (for one batch) at step 70: 410.1313, Accuracy: 0.5966\n",
      "Training loss (for one batch) at step 80: 413.2202, Accuracy: 0.5886\n",
      "Training loss (for one batch) at step 90: 406.8901, Accuracy: 0.5822\n",
      "Training loss (for one batch) at step 100: 395.7503, Accuracy: 0.5843\n",
      "Training loss (for one batch) at step 110: 418.6418, Accuracy: 0.5845\n",
      "---- Training ----\n",
      "Training loss: 128.0249\n",
      "Training acc over epoch: 0.5846\n",
      "---- Validation ----\n",
      "Validation loss: 41.0621\n",
      "Validation acc: 0.5634\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 426.0696, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 415.8524, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 20: 405.3015, Accuracy: 0.5536\n",
      "Training loss (for one batch) at step 30: 399.6667, Accuracy: 0.5663\n",
      "Training loss (for one batch) at step 40: 402.0189, Accuracy: 0.5804\n",
      "Training loss (for one batch) at step 50: 390.5070, Accuracy: 0.5887\n",
      "Training loss (for one batch) at step 60: 412.6234, Accuracy: 0.5975\n",
      "Training loss (for one batch) at step 70: 403.6220, Accuracy: 0.5947\n",
      "Training loss (for one batch) at step 80: 410.8921, Accuracy: 0.5879\n",
      "Training loss (for one batch) at step 90: 404.4109, Accuracy: 0.5834\n",
      "Training loss (for one batch) at step 100: 390.2508, Accuracy: 0.5847\n",
      "Training loss (for one batch) at step 110: 396.5541, Accuracy: 0.5848\n",
      "---- Training ----\n",
      "Training loss: 124.4908\n",
      "Training acc over epoch: 0.5848\n",
      "---- Validation ----\n",
      "Validation loss: 34.0539\n",
      "Validation acc: 0.5637\n",
      "Time taken: 10.98s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 438.3723, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 418.9469, Accuracy: 0.5412\n",
      "Training loss (for one batch) at step 20: 411.5605, Accuracy: 0.5406\n",
      "Training loss (for one batch) at step 30: 398.5751, Accuracy: 0.5635\n",
      "Training loss (for one batch) at step 40: 395.4905, Accuracy: 0.5793\n",
      "Training loss (for one batch) at step 50: 385.5307, Accuracy: 0.5928\n",
      "Training loss (for one batch) at step 60: 378.0632, Accuracy: 0.5993\n",
      "Training loss (for one batch) at step 70: 424.8570, Accuracy: 0.5975\n",
      "Training loss (for one batch) at step 80: 408.8006, Accuracy: 0.5900\n",
      "Training loss (for one batch) at step 90: 388.8328, Accuracy: 0.5856\n",
      "Training loss (for one batch) at step 100: 383.4687, Accuracy: 0.5861\n",
      "Training loss (for one batch) at step 110: 423.0925, Accuracy: 0.5867\n",
      "---- Training ----\n",
      "Training loss: 124.0146\n",
      "Training acc over epoch: 0.5864\n",
      "---- Validation ----\n",
      "Validation loss: 37.3882\n",
      "Validation acc: 0.5411\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 413.2693, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 411.8509, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 20: 397.1172, Accuracy: 0.5603\n",
      "Training loss (for one batch) at step 30: 394.0056, Accuracy: 0.5708\n",
      "Training loss (for one batch) at step 40: 386.5708, Accuracy: 0.5848\n",
      "Training loss (for one batch) at step 50: 378.3321, Accuracy: 0.5964\n",
      "Training loss (for one batch) at step 60: 405.6996, Accuracy: 0.6023\n",
      "Training loss (for one batch) at step 70: 414.0398, Accuracy: 0.6024\n",
      "Training loss (for one batch) at step 80: 414.4246, Accuracy: 0.5962\n",
      "Training loss (for one batch) at step 90: 401.5634, Accuracy: 0.5933\n",
      "Training loss (for one batch) at step 100: 387.5072, Accuracy: 0.5920\n",
      "Training loss (for one batch) at step 110: 416.0829, Accuracy: 0.5910\n",
      "---- Training ----\n",
      "Training loss: 128.8872\n",
      "Training acc over epoch: 0.5898\n",
      "---- Validation ----\n",
      "Validation loss: 37.2185\n",
      "Validation acc: 0.5570\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 423.4504, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 408.4667, Accuracy: 0.5724\n",
      "Training loss (for one batch) at step 20: 396.5022, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 30: 377.6668, Accuracy: 0.5824\n",
      "Training loss (for one batch) at step 40: 370.9609, Accuracy: 0.5943\n",
      "Training loss (for one batch) at step 50: 383.7864, Accuracy: 0.6032\n",
      "Training loss (for one batch) at step 60: 384.0038, Accuracy: 0.6126\n",
      "Training loss (for one batch) at step 70: 407.1888, Accuracy: 0.6104\n",
      "Training loss (for one batch) at step 80: 400.3339, Accuracy: 0.6002\n",
      "Training loss (for one batch) at step 90: 392.7608, Accuracy: 0.5971\n",
      "Training loss (for one batch) at step 100: 377.9131, Accuracy: 0.5961\n",
      "Training loss (for one batch) at step 110: 414.4582, Accuracy: 0.5954\n",
      "---- Training ----\n",
      "Training loss: 128.1989\n",
      "Training acc over epoch: 0.5938\n",
      "---- Validation ----\n",
      "Validation loss: 37.1152\n",
      "Validation acc: 0.5774\n",
      "Time taken: 10.90s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 421.7411, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 393.4828, Accuracy: 0.5462\n",
      "Training loss (for one batch) at step 20: 402.0439, Accuracy: 0.5569\n",
      "Training loss (for one batch) at step 30: 375.9742, Accuracy: 0.5685\n",
      "Training loss (for one batch) at step 40: 381.0075, Accuracy: 0.5796\n",
      "Training loss (for one batch) at step 50: 372.0593, Accuracy: 0.5939\n",
      "Training loss (for one batch) at step 60: 383.9531, Accuracy: 0.6009\n",
      "Training loss (for one batch) at step 70: 391.5177, Accuracy: 0.5984\n",
      "Training loss (for one batch) at step 80: 405.6447, Accuracy: 0.5904\n",
      "Training loss (for one batch) at step 90: 384.7195, Accuracy: 0.5878\n",
      "Training loss (for one batch) at step 100: 371.5736, Accuracy: 0.5900\n",
      "Training loss (for one batch) at step 110: 397.4103, Accuracy: 0.5906\n",
      "---- Training ----\n",
      "Training loss: 127.6540\n",
      "Training acc over epoch: 0.5898\n",
      "---- Validation ----\n",
      "Validation loss: 38.2823\n",
      "Validation acc: 0.5602\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 410.0790, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 407.2612, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 20: 378.1629, Accuracy: 0.5551\n",
      "Training loss (for one batch) at step 30: 367.0753, Accuracy: 0.5829\n",
      "Training loss (for one batch) at step 40: 357.8376, Accuracy: 0.5915\n",
      "Training loss (for one batch) at step 50: 368.7354, Accuracy: 0.6020\n",
      "Training loss (for one batch) at step 60: 385.8628, Accuracy: 0.6092\n",
      "Training loss (for one batch) at step 70: 392.3450, Accuracy: 0.6056\n",
      "Training loss (for one batch) at step 80: 417.1482, Accuracy: 0.5978\n",
      "Training loss (for one batch) at step 90: 383.1259, Accuracy: 0.5931\n",
      "Training loss (for one batch) at step 100: 382.2133, Accuracy: 0.5927\n",
      "Training loss (for one batch) at step 110: 398.5168, Accuracy: 0.5914\n",
      "---- Training ----\n",
      "Training loss: 119.0651\n",
      "Training acc over epoch: 0.5909\n",
      "---- Validation ----\n",
      "Validation loss: 36.1229\n",
      "Validation acc: 0.5613\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 416.9925, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 409.3673, Accuracy: 0.5533\n",
      "Training loss (for one batch) at step 20: 387.5079, Accuracy: 0.5688\n",
      "Training loss (for one batch) at step 30: 364.9807, Accuracy: 0.5849\n",
      "Training loss (for one batch) at step 40: 380.1958, Accuracy: 0.5976\n",
      "Training loss (for one batch) at step 50: 357.9147, Accuracy: 0.6051\n",
      "Training loss (for one batch) at step 60: 355.2450, Accuracy: 0.6127\n",
      "Training loss (for one batch) at step 70: 399.4274, Accuracy: 0.6052\n",
      "Training loss (for one batch) at step 80: 393.3501, Accuracy: 0.5954\n",
      "Training loss (for one batch) at step 90: 369.3230, Accuracy: 0.5913\n",
      "Training loss (for one batch) at step 100: 379.9963, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 110: 395.9543, Accuracy: 0.5959\n",
      "---- Training ----\n",
      "Training loss: 124.7639\n",
      "Training acc over epoch: 0.5939\n",
      "---- Validation ----\n",
      "Validation loss: 40.5327\n",
      "Validation acc: 0.5484\n",
      "Time taken: 10.70s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 396.0591, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 398.2616, Accuracy: 0.5632\n",
      "Training loss (for one batch) at step 20: 372.6970, Accuracy: 0.5796\n",
      "Training loss (for one batch) at step 30: 374.2121, Accuracy: 0.5895\n",
      "Training loss (for one batch) at step 40: 370.2786, Accuracy: 0.5958\n",
      "Training loss (for one batch) at step 50: 351.7849, Accuracy: 0.6037\n",
      "Training loss (for one batch) at step 60: 360.0144, Accuracy: 0.6109\n",
      "Training loss (for one batch) at step 70: 394.8967, Accuracy: 0.6068\n",
      "Training loss (for one batch) at step 80: 396.6860, Accuracy: 0.6013\n",
      "Training loss (for one batch) at step 90: 371.5243, Accuracy: 0.5988\n",
      "Training loss (for one batch) at step 100: 357.9483, Accuracy: 0.5991\n",
      "Training loss (for one batch) at step 110: 385.5187, Accuracy: 0.5980\n",
      "---- Training ----\n",
      "Training loss: 118.1982\n",
      "Training acc over epoch: 0.5963\n",
      "---- Validation ----\n",
      "Validation loss: 32.3052\n",
      "Validation acc: 0.5596\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 400.4068, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 373.6212, Accuracy: 0.5376\n",
      "Training loss (for one batch) at step 20: 396.3130, Accuracy: 0.5666\n",
      "Training loss (for one batch) at step 30: 364.5123, Accuracy: 0.5814\n",
      "Training loss (for one batch) at step 40: 369.6303, Accuracy: 0.5945\n",
      "Training loss (for one batch) at step 50: 368.2642, Accuracy: 0.6052\n",
      "Training loss (for one batch) at step 60: 366.0561, Accuracy: 0.6103\n",
      "Training loss (for one batch) at step 70: 383.7469, Accuracy: 0.6042\n",
      "Training loss (for one batch) at step 80: 394.2172, Accuracy: 0.5987\n",
      "Training loss (for one batch) at step 90: 374.9277, Accuracy: 0.5968\n",
      "Training loss (for one batch) at step 100: 377.7968, Accuracy: 0.5971\n",
      "Training loss (for one batch) at step 110: 367.6447, Accuracy: 0.5964\n",
      "---- Training ----\n",
      "Training loss: 118.9050\n",
      "Training acc over epoch: 0.5942\n",
      "---- Validation ----\n",
      "Validation loss: 31.9366\n",
      "Validation acc: 0.5516\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 406.8665, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 397.0105, Accuracy: 0.5462\n",
      "Training loss (for one batch) at step 20: 360.7123, Accuracy: 0.5647\n",
      "Training loss (for one batch) at step 30: 370.2065, Accuracy: 0.5824\n",
      "Training loss (for one batch) at step 40: 367.6513, Accuracy: 0.5920\n",
      "Training loss (for one batch) at step 50: 353.2738, Accuracy: 0.6046\n",
      "Training loss (for one batch) at step 60: 361.9788, Accuracy: 0.6109\n",
      "Training loss (for one batch) at step 70: 385.5925, Accuracy: 0.6054\n",
      "Training loss (for one batch) at step 80: 377.3950, Accuracy: 0.5950\n",
      "Training loss (for one batch) at step 90: 364.0715, Accuracy: 0.5916\n",
      "Training loss (for one batch) at step 100: 354.9995, Accuracy: 0.5940\n",
      "Training loss (for one batch) at step 110: 368.4150, Accuracy: 0.5944\n",
      "---- Training ----\n",
      "Training loss: 120.2136\n",
      "Training acc over epoch: 0.5940\n",
      "---- Validation ----\n",
      "Validation loss: 39.4109\n",
      "Validation acc: 0.5140\n",
      "Time taken: 11.03s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 406.6795, Accuracy: 0.4375\n",
      "Training loss (for one batch) at step 10: 391.0351, Accuracy: 0.5369\n",
      "Training loss (for one batch) at step 20: 364.1218, Accuracy: 0.5610\n",
      "Training loss (for one batch) at step 30: 360.4843, Accuracy: 0.5766\n",
      "Training loss (for one batch) at step 40: 360.4355, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 50: 349.2512, Accuracy: 0.6002\n",
      "Training loss (for one batch) at step 60: 368.9004, Accuracy: 0.6048\n",
      "Training loss (for one batch) at step 70: 368.1547, Accuracy: 0.5995\n",
      "Training loss (for one batch) at step 80: 395.4956, Accuracy: 0.5944\n",
      "Training loss (for one batch) at step 90: 346.0795, Accuracy: 0.5923\n",
      "Training loss (for one batch) at step 100: 356.7999, Accuracy: 0.5942\n",
      "Training loss (for one batch) at step 110: 374.1023, Accuracy: 0.5945\n",
      "---- Training ----\n",
      "Training loss: 123.6747\n",
      "Training acc over epoch: 0.5930\n",
      "---- Validation ----\n",
      "Validation loss: 50.2130\n",
      "Validation acc: 0.5521\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 384.8555, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 388.4944, Accuracy: 0.5426\n",
      "Training loss (for one batch) at step 20: 364.7747, Accuracy: 0.5725\n",
      "Training loss (for one batch) at step 30: 361.6920, Accuracy: 0.5842\n",
      "Training loss (for one batch) at step 40: 354.3934, Accuracy: 0.5943\n",
      "Training loss (for one batch) at step 50: 334.0225, Accuracy: 0.6075\n",
      "Training loss (for one batch) at step 60: 347.3058, Accuracy: 0.6133\n",
      "Training loss (for one batch) at step 70: 370.0829, Accuracy: 0.6067\n",
      "Training loss (for one batch) at step 80: 405.4420, Accuracy: 0.5977\n",
      "Training loss (for one batch) at step 90: 355.5098, Accuracy: 0.5946\n",
      "Training loss (for one batch) at step 100: 378.5726, Accuracy: 0.5954\n",
      "Training loss (for one batch) at step 110: 373.7880, Accuracy: 0.5959\n",
      "---- Training ----\n",
      "Training loss: 119.2675\n",
      "Training acc over epoch: 0.5944\n",
      "---- Validation ----\n",
      "Validation loss: 38.9913\n",
      "Validation acc: 0.5556\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 372.9009, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 364.4287, Accuracy: 0.5440\n",
      "Training loss (for one batch) at step 20: 350.1266, Accuracy: 0.5670\n",
      "Training loss (for one batch) at step 30: 347.0800, Accuracy: 0.5839\n",
      "Training loss (for one batch) at step 40: 347.7415, Accuracy: 0.5983\n",
      "Training loss (for one batch) at step 50: 346.3489, Accuracy: 0.6086\n",
      "Training loss (for one batch) at step 60: 365.8066, Accuracy: 0.6148\n",
      "Training loss (for one batch) at step 70: 358.1468, Accuracy: 0.6086\n",
      "Training loss (for one batch) at step 80: 387.3656, Accuracy: 0.5991\n",
      "Training loss (for one batch) at step 90: 362.1214, Accuracy: 0.5980\n",
      "Training loss (for one batch) at step 100: 340.7253, Accuracy: 0.6002\n",
      "Training loss (for one batch) at step 110: 366.8871, Accuracy: 0.5994\n",
      "---- Training ----\n",
      "Training loss: 119.7225\n",
      "Training acc over epoch: 0.5974\n",
      "---- Validation ----\n",
      "Validation loss: 40.1942\n",
      "Validation acc: 0.5459\n",
      "Time taken: 10.84s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 384.4467, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 376.2585, Accuracy: 0.5419\n",
      "Training loss (for one batch) at step 20: 374.6327, Accuracy: 0.5655\n",
      "Training loss (for one batch) at step 30: 353.7868, Accuracy: 0.5822\n",
      "Training loss (for one batch) at step 40: 348.1719, Accuracy: 0.5960\n",
      "Training loss (for one batch) at step 50: 357.0186, Accuracy: 0.6068\n",
      "Training loss (for one batch) at step 60: 348.9521, Accuracy: 0.6128\n",
      "Training loss (for one batch) at step 70: 371.5738, Accuracy: 0.6061\n",
      "Training loss (for one batch) at step 80: 368.8430, Accuracy: 0.5965\n",
      "Training loss (for one batch) at step 90: 358.0779, Accuracy: 0.5954\n",
      "Training loss (for one batch) at step 100: 345.8246, Accuracy: 0.5961\n",
      "Training loss (for one batch) at step 110: 367.7750, Accuracy: 0.5960\n",
      "---- Training ----\n",
      "Training loss: 114.1474\n",
      "Training acc over epoch: 0.5950\n",
      "---- Validation ----\n",
      "Validation loss: 36.8067\n",
      "Validation acc: 0.5680\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 375.2923, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 385.9539, Accuracy: 0.5476\n",
      "Training loss (for one batch) at step 20: 365.4210, Accuracy: 0.5692\n",
      "Training loss (for one batch) at step 30: 351.3933, Accuracy: 0.5900\n",
      "Training loss (for one batch) at step 40: 348.0282, Accuracy: 0.6038\n",
      "Training loss (for one batch) at step 50: 338.5174, Accuracy: 0.6108\n",
      "Training loss (for one batch) at step 60: 331.8614, Accuracy: 0.6162\n",
      "Training loss (for one batch) at step 70: 367.3781, Accuracy: 0.6100\n",
      "Training loss (for one batch) at step 80: 370.4622, Accuracy: 0.6019\n",
      "Training loss (for one batch) at step 90: 334.8746, Accuracy: 0.5969\n",
      "Training loss (for one batch) at step 100: 365.5741, Accuracy: 0.5977\n",
      "Training loss (for one batch) at step 110: 379.0162, Accuracy: 0.5980\n",
      "---- Training ----\n",
      "Training loss: 123.1138\n",
      "Training acc over epoch: 0.5979\n",
      "---- Validation ----\n",
      "Validation loss: 36.4794\n",
      "Validation acc: 0.5556\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 385.3829, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 383.9576, Accuracy: 0.5440\n",
      "Training loss (for one batch) at step 20: 365.7606, Accuracy: 0.5644\n",
      "Training loss (for one batch) at step 30: 344.2249, Accuracy: 0.5842\n",
      "Training loss (for one batch) at step 40: 340.3747, Accuracy: 0.5972\n",
      "Training loss (for one batch) at step 50: 334.6118, Accuracy: 0.6046\n",
      "Training loss (for one batch) at step 60: 343.5939, Accuracy: 0.6126\n",
      "Training loss (for one batch) at step 70: 372.6136, Accuracy: 0.6056\n",
      "Training loss (for one batch) at step 80: 386.5228, Accuracy: 0.5965\n",
      "Training loss (for one batch) at step 90: 358.7965, Accuracy: 0.5943\n",
      "Training loss (for one batch) at step 100: 335.6524, Accuracy: 0.5965\n",
      "Training loss (for one batch) at step 110: 353.6212, Accuracy: 0.5960\n",
      "---- Training ----\n",
      "Training loss: 127.6692\n",
      "Training acc over epoch: 0.5957\n",
      "---- Validation ----\n",
      "Validation loss: 38.3520\n",
      "Validation acc: 0.5545\n",
      "Time taken: 10.96s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 377.6882, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 361.5905, Accuracy: 0.5433\n",
      "Training loss (for one batch) at step 20: 351.4057, Accuracy: 0.5666\n",
      "Training loss (for one batch) at step 30: 362.6752, Accuracy: 0.5897\n",
      "Training loss (for one batch) at step 40: 332.6289, Accuracy: 0.6010\n",
      "Training loss (for one batch) at step 50: 327.9415, Accuracy: 0.6083\n",
      "Training loss (for one batch) at step 60: 334.2107, Accuracy: 0.6130\n",
      "Training loss (for one batch) at step 70: 375.9434, Accuracy: 0.6063\n",
      "Training loss (for one batch) at step 80: 363.0390, Accuracy: 0.5941\n",
      "Training loss (for one batch) at step 90: 352.1121, Accuracy: 0.5937\n",
      "Training loss (for one batch) at step 100: 348.9334, Accuracy: 0.5954\n",
      "Training loss (for one batch) at step 110: 357.4868, Accuracy: 0.5952\n",
      "---- Training ----\n",
      "Training loss: 115.0824\n",
      "Training acc over epoch: 0.5953\n",
      "---- Validation ----\n",
      "Validation loss: 46.1679\n",
      "Validation acc: 0.5478\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 385.5872, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 358.2291, Accuracy: 0.5305\n",
      "Training loss (for one batch) at step 20: 349.1105, Accuracy: 0.5513\n",
      "Training loss (for one batch) at step 30: 318.5117, Accuracy: 0.5784\n",
      "Training loss (for one batch) at step 40: 340.7279, Accuracy: 0.5953\n",
      "Training loss (for one batch) at step 50: 336.3345, Accuracy: 0.6048\n",
      "Training loss (for one batch) at step 60: 341.6586, Accuracy: 0.6119\n",
      "Training loss (for one batch) at step 70: 368.2702, Accuracy: 0.6071\n",
      "Training loss (for one batch) at step 80: 380.7722, Accuracy: 0.5965\n",
      "Training loss (for one batch) at step 90: 335.3890, Accuracy: 0.5952\n",
      "Training loss (for one batch) at step 100: 348.0477, Accuracy: 0.5951\n",
      "Training loss (for one batch) at step 110: 350.0982, Accuracy: 0.5964\n",
      "---- Training ----\n",
      "Training loss: 111.7204\n",
      "Training acc over epoch: 0.5958\n",
      "---- Validation ----\n",
      "Validation loss: 43.6560\n",
      "Validation acc: 0.5621\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 382.2405, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 372.3222, Accuracy: 0.5497\n",
      "Training loss (for one batch) at step 20: 345.7133, Accuracy: 0.5644\n",
      "Training loss (for one batch) at step 30: 340.8480, Accuracy: 0.5900\n",
      "Training loss (for one batch) at step 40: 341.4016, Accuracy: 0.6035\n",
      "Training loss (for one batch) at step 50: 318.4192, Accuracy: 0.6131\n",
      "Training loss (for one batch) at step 60: 355.2807, Accuracy: 0.6183\n",
      "Training loss (for one batch) at step 70: 356.9417, Accuracy: 0.6106\n",
      "Training loss (for one batch) at step 80: 366.0279, Accuracy: 0.6017\n",
      "Training loss (for one batch) at step 90: 348.3597, Accuracy: 0.5993\n",
      "Training loss (for one batch) at step 100: 316.4190, Accuracy: 0.6002\n",
      "Training loss (for one batch) at step 110: 346.4296, Accuracy: 0.5997\n",
      "---- Training ----\n",
      "Training loss: 126.0662\n",
      "Training acc over epoch: 0.5986\n",
      "---- Validation ----\n",
      "Validation loss: 38.4685\n",
      "Validation acc: 0.5330\n",
      "Time taken: 10.80s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 382.8712, Accuracy: 0.4609\n",
      "Training loss (for one batch) at step 10: 366.4614, Accuracy: 0.5249\n",
      "Training loss (for one batch) at step 20: 323.5263, Accuracy: 0.5584\n",
      "Training loss (for one batch) at step 30: 353.6863, Accuracy: 0.5832\n",
      "Training loss (for one batch) at step 40: 322.9203, Accuracy: 0.5960\n",
      "Training loss (for one batch) at step 50: 316.5049, Accuracy: 0.6066\n",
      "Training loss (for one batch) at step 60: 322.3309, Accuracy: 0.6157\n",
      "Training loss (for one batch) at step 70: 361.8179, Accuracy: 0.6105\n",
      "Training loss (for one batch) at step 80: 356.4585, Accuracy: 0.6012\n",
      "Training loss (for one batch) at step 90: 340.9946, Accuracy: 0.5975\n",
      "Training loss (for one batch) at step 100: 342.1812, Accuracy: 0.5989\n",
      "Training loss (for one batch) at step 110: 332.9679, Accuracy: 0.5998\n",
      "---- Training ----\n",
      "Training loss: 102.5437\n",
      "Training acc over epoch: 0.5985\n",
      "---- Validation ----\n",
      "Validation loss: 50.5667\n",
      "Validation acc: 0.5814\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 355.1258, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 369.6404, Accuracy: 0.5447\n",
      "Training loss (for one batch) at step 20: 339.1596, Accuracy: 0.5796\n",
      "Training loss (for one batch) at step 30: 339.7435, Accuracy: 0.5975\n",
      "Training loss (for one batch) at step 40: 338.0216, Accuracy: 0.6082\n",
      "Training loss (for one batch) at step 50: 329.9815, Accuracy: 0.6141\n",
      "Training loss (for one batch) at step 60: 331.9284, Accuracy: 0.6157\n",
      "Training loss (for one batch) at step 70: 346.9982, Accuracy: 0.6114\n",
      "Training loss (for one batch) at step 80: 372.7596, Accuracy: 0.5978\n",
      "Training loss (for one batch) at step 90: 334.7644, Accuracy: 0.5964\n",
      "Training loss (for one batch) at step 100: 371.4851, Accuracy: 0.5974\n",
      "Training loss (for one batch) at step 110: 346.8937, Accuracy: 0.5976\n",
      "---- Training ----\n",
      "Training loss: 108.2655\n",
      "Training acc over epoch: 0.5963\n",
      "---- Validation ----\n",
      "Validation loss: 33.1644\n",
      "Validation acc: 0.5430\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 367.5578, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 383.9286, Accuracy: 0.5334\n",
      "Training loss (for one batch) at step 20: 335.8346, Accuracy: 0.5651\n",
      "Training loss (for one batch) at step 30: 335.5327, Accuracy: 0.5844\n",
      "Training loss (for one batch) at step 40: 331.6255, Accuracy: 0.5932\n",
      "Training loss (for one batch) at step 50: 329.2911, Accuracy: 0.6060\n",
      "Training loss (for one batch) at step 60: 331.6765, Accuracy: 0.6122\n",
      "Training loss (for one batch) at step 70: 357.4825, Accuracy: 0.6077\n",
      "Training loss (for one batch) at step 80: 353.2289, Accuracy: 0.6020\n",
      "Training loss (for one batch) at step 90: 335.1322, Accuracy: 0.6008\n",
      "Training loss (for one batch) at step 100: 348.0678, Accuracy: 0.6021\n",
      "Training loss (for one batch) at step 110: 346.4366, Accuracy: 0.5990\n",
      "---- Training ----\n",
      "Training loss: 119.0194\n",
      "Training acc over epoch: 0.5989\n",
      "---- Validation ----\n",
      "Validation loss: 43.3867\n",
      "Validation acc: 0.5406\n",
      "Time taken: 10.78s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 366.8242, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 352.7867, Accuracy: 0.5384\n",
      "Training loss (for one batch) at step 20: 329.3301, Accuracy: 0.5725\n",
      "Training loss (for one batch) at step 30: 326.4649, Accuracy: 0.5958\n",
      "Training loss (for one batch) at step 40: 339.1269, Accuracy: 0.6086\n",
      "Training loss (for one batch) at step 50: 328.5458, Accuracy: 0.6173\n",
      "Training loss (for one batch) at step 60: 344.8367, Accuracy: 0.6194\n",
      "Training loss (for one batch) at step 70: 362.2660, Accuracy: 0.6115\n",
      "Training loss (for one batch) at step 80: 373.6114, Accuracy: 0.6029\n",
      "Training loss (for one batch) at step 90: 335.7676, Accuracy: 0.5992\n",
      "Training loss (for one batch) at step 100: 342.0412, Accuracy: 0.6003\n",
      "Training loss (for one batch) at step 110: 338.5791, Accuracy: 0.5996\n",
      "---- Training ----\n",
      "Training loss: 125.6851\n",
      "Training acc over epoch: 0.5978\n",
      "---- Validation ----\n",
      "Validation loss: 34.3471\n",
      "Validation acc: 0.5650\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 356.5300, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 350.2376, Accuracy: 0.5348\n",
      "Training loss (for one batch) at step 20: 329.6117, Accuracy: 0.5766\n",
      "Training loss (for one batch) at step 30: 325.7991, Accuracy: 0.5912\n",
      "Training loss (for one batch) at step 40: 321.2730, Accuracy: 0.6044\n",
      "Training loss (for one batch) at step 50: 325.7448, Accuracy: 0.6126\n",
      "Training loss (for one batch) at step 60: 337.6317, Accuracy: 0.6133\n",
      "Training loss (for one batch) at step 70: 353.0693, Accuracy: 0.6090\n",
      "Training loss (for one batch) at step 80: 369.9939, Accuracy: 0.5985\n",
      "Training loss (for one batch) at step 90: 330.0424, Accuracy: 0.5976\n",
      "Training loss (for one batch) at step 100: 370.3501, Accuracy: 0.5996\n",
      "Training loss (for one batch) at step 110: 329.3253, Accuracy: 0.5998\n",
      "---- Training ----\n",
      "Training loss: 105.2868\n",
      "Training acc over epoch: 0.5991\n",
      "---- Validation ----\n",
      "Validation loss: 48.8126\n",
      "Validation acc: 0.5500\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 334.6259, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 347.7836, Accuracy: 0.5320\n",
      "Training loss (for one batch) at step 20: 343.5161, Accuracy: 0.5606\n",
      "Training loss (for one batch) at step 30: 342.6429, Accuracy: 0.5864\n",
      "Training loss (for one batch) at step 40: 318.2162, Accuracy: 0.5983\n",
      "Training loss (for one batch) at step 50: 316.8380, Accuracy: 0.6074\n",
      "Training loss (for one batch) at step 60: 335.9330, Accuracy: 0.6151\n",
      "Training loss (for one batch) at step 70: 351.6800, Accuracy: 0.6084\n",
      "Training loss (for one batch) at step 80: 354.8190, Accuracy: 0.5979\n",
      "Training loss (for one batch) at step 90: 332.2340, Accuracy: 0.5980\n",
      "Training loss (for one batch) at step 100: 328.9761, Accuracy: 0.5992\n",
      "Training loss (for one batch) at step 110: 339.1015, Accuracy: 0.5995\n",
      "---- Training ----\n",
      "Training loss: 106.6625\n",
      "Training acc over epoch: 0.5987\n",
      "---- Validation ----\n",
      "Validation loss: 40.9801\n",
      "Validation acc: 0.5376\n",
      "Time taken: 11.00s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 360.0378, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 350.3035, Accuracy: 0.5540\n",
      "Training loss (for one batch) at step 20: 327.1254, Accuracy: 0.5770\n",
      "Training loss (for one batch) at step 30: 337.0188, Accuracy: 0.5958\n",
      "Training loss (for one batch) at step 40: 335.8232, Accuracy: 0.6065\n",
      "Training loss (for one batch) at step 50: 313.5634, Accuracy: 0.6114\n",
      "Training loss (for one batch) at step 60: 341.2801, Accuracy: 0.6185\n",
      "Training loss (for one batch) at step 70: 345.1598, Accuracy: 0.6123\n",
      "Training loss (for one batch) at step 80: 343.5514, Accuracy: 0.6036\n",
      "Training loss (for one batch) at step 90: 324.9577, Accuracy: 0.6000\n",
      "Training loss (for one batch) at step 100: 318.3942, Accuracy: 0.6009\n",
      "Training loss (for one batch) at step 110: 333.7969, Accuracy: 0.6012\n",
      "---- Training ----\n",
      "Training loss: 103.3382\n",
      "Training acc over epoch: 0.5999\n",
      "---- Validation ----\n",
      "Validation loss: 44.8290\n",
      "Validation acc: 0.5521\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 364.9061, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 339.8984, Accuracy: 0.5497\n",
      "Training loss (for one batch) at step 20: 324.1718, Accuracy: 0.5748\n",
      "Training loss (for one batch) at step 30: 322.3456, Accuracy: 0.6003\n",
      "Training loss (for one batch) at step 40: 321.8842, Accuracy: 0.6058\n",
      "Training loss (for one batch) at step 50: 304.2467, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 60: 346.2791, Accuracy: 0.6214\n",
      "Training loss (for one batch) at step 70: 335.7681, Accuracy: 0.6155\n",
      "Training loss (for one batch) at step 80: 359.5835, Accuracy: 0.6058\n",
      "Training loss (for one batch) at step 90: 333.7167, Accuracy: 0.6025\n",
      "Training loss (for one batch) at step 100: 314.5725, Accuracy: 0.6037\n",
      "Training loss (for one batch) at step 110: 326.2978, Accuracy: 0.6029\n",
      "---- Training ----\n",
      "Training loss: 116.6524\n",
      "Training acc over epoch: 0.6015\n",
      "---- Validation ----\n",
      "Validation loss: 31.6721\n",
      "Validation acc: 0.5629\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 356.0909, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 347.2982, Accuracy: 0.5540\n",
      "Training loss (for one batch) at step 20: 333.7098, Accuracy: 0.5882\n",
      "Training loss (for one batch) at step 30: 327.2098, Accuracy: 0.5993\n",
      "Training loss (for one batch) at step 40: 319.4987, Accuracy: 0.6090\n",
      "Training loss (for one batch) at step 50: 316.8608, Accuracy: 0.6170\n",
      "Training loss (for one batch) at step 60: 328.2293, Accuracy: 0.6222\n",
      "Training loss (for one batch) at step 70: 352.9082, Accuracy: 0.6140\n",
      "Training loss (for one batch) at step 80: 336.6690, Accuracy: 0.6035\n",
      "Training loss (for one batch) at step 90: 334.1019, Accuracy: 0.6010\n",
      "Training loss (for one batch) at step 100: 318.5924, Accuracy: 0.6026\n",
      "Training loss (for one batch) at step 110: 325.7437, Accuracy: 0.6030\n",
      "---- Training ----\n",
      "Training loss: 119.4701\n",
      "Training acc over epoch: 0.6011\n",
      "---- Validation ----\n",
      "Validation loss: 47.6960\n",
      "Validation acc: 0.5556\n",
      "Time taken: 10.88s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 348.3699, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 338.7000, Accuracy: 0.5483\n",
      "Training loss (for one batch) at step 20: 325.4975, Accuracy: 0.5863\n",
      "Training loss (for one batch) at step 30: 315.7620, Accuracy: 0.6048\n",
      "Training loss (for one batch) at step 40: 347.8320, Accuracy: 0.6080\n",
      "Training loss (for one batch) at step 50: 316.2169, Accuracy: 0.6158\n",
      "Training loss (for one batch) at step 60: 322.5217, Accuracy: 0.6214\n",
      "Training loss (for one batch) at step 70: 338.1254, Accuracy: 0.6129\n",
      "Training loss (for one batch) at step 80: 365.8117, Accuracy: 0.6012\n",
      "Training loss (for one batch) at step 90: 313.7341, Accuracy: 0.6010\n",
      "Training loss (for one batch) at step 100: 313.3990, Accuracy: 0.6023\n",
      "Training loss (for one batch) at step 110: 317.2570, Accuracy: 0.6004\n",
      "---- Training ----\n",
      "Training loss: 125.6833\n",
      "Training acc over epoch: 0.5999\n",
      "---- Validation ----\n",
      "Validation loss: 39.7387\n",
      "Validation acc: 0.5623\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 334.4642, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 340.5439, Accuracy: 0.5518\n",
      "Training loss (for one batch) at step 20: 327.0206, Accuracy: 0.5822\n",
      "Training loss (for one batch) at step 30: 328.7638, Accuracy: 0.5950\n",
      "Training loss (for one batch) at step 40: 322.5592, Accuracy: 0.6040\n",
      "Training loss (for one batch) at step 50: 313.7874, Accuracy: 0.6140\n",
      "Training loss (for one batch) at step 60: 331.1724, Accuracy: 0.6190\n",
      "Training loss (for one batch) at step 70: 346.0457, Accuracy: 0.6143\n",
      "Training loss (for one batch) at step 80: 347.6099, Accuracy: 0.6035\n",
      "Training loss (for one batch) at step 90: 338.5756, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 100: 322.6185, Accuracy: 0.6022\n",
      "Training loss (for one batch) at step 110: 330.1220, Accuracy: 0.6019\n",
      "---- Training ----\n",
      "Training loss: 105.4009\n",
      "Training acc over epoch: 0.6004\n",
      "---- Validation ----\n",
      "Validation loss: 31.8266\n",
      "Validation acc: 0.5696\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 353.1020, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 342.9436, Accuracy: 0.5497\n",
      "Training loss (for one batch) at step 20: 322.8574, Accuracy: 0.5759\n",
      "Training loss (for one batch) at step 30: 311.7600, Accuracy: 0.5943\n",
      "Training loss (for one batch) at step 40: 311.8002, Accuracy: 0.6056\n",
      "Training loss (for one batch) at step 50: 316.7754, Accuracy: 0.6137\n",
      "Training loss (for one batch) at step 60: 342.5847, Accuracy: 0.6208\n",
      "Training loss (for one batch) at step 70: 331.6476, Accuracy: 0.6159\n",
      "Training loss (for one batch) at step 80: 342.1590, Accuracy: 0.6059\n",
      "Training loss (for one batch) at step 90: 312.8558, Accuracy: 0.6028\n",
      "Training loss (for one batch) at step 100: 305.4832, Accuracy: 0.6043\n",
      "Training loss (for one batch) at step 110: 322.4023, Accuracy: 0.6031\n",
      "---- Training ----\n",
      "Training loss: 102.9744\n",
      "Training acc over epoch: 0.6024\n",
      "---- Validation ----\n",
      "Validation loss: 38.9838\n",
      "Validation acc: 0.5578\n",
      "Time taken: 10.82s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 328.4628, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 342.6351, Accuracy: 0.5298\n",
      "Training loss (for one batch) at step 20: 317.4444, Accuracy: 0.5707\n",
      "Training loss (for one batch) at step 30: 315.5659, Accuracy: 0.5907\n",
      "Training loss (for one batch) at step 40: 332.7544, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 50: 311.8343, Accuracy: 0.6104\n",
      "Training loss (for one batch) at step 60: 326.2840, Accuracy: 0.6190\n",
      "Training loss (for one batch) at step 70: 331.3423, Accuracy: 0.6143\n",
      "Training loss (for one batch) at step 80: 339.1362, Accuracy: 0.6047\n",
      "Training loss (for one batch) at step 90: 324.3754, Accuracy: 0.6013\n",
      "Training loss (for one batch) at step 100: 316.7801, Accuracy: 0.6021\n",
      "Training loss (for one batch) at step 110: 334.4973, Accuracy: 0.6018\n",
      "---- Training ----\n",
      "Training loss: 108.2246\n",
      "Training acc over epoch: 0.6001\n",
      "---- Validation ----\n",
      "Validation loss: 51.9360\n",
      "Validation acc: 0.5553\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 345.1923, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 340.8080, Accuracy: 0.5462\n",
      "Training loss (for one batch) at step 20: 334.7150, Accuracy: 0.5848\n",
      "Training loss (for one batch) at step 30: 317.1232, Accuracy: 0.5973\n",
      "Training loss (for one batch) at step 40: 319.0822, Accuracy: 0.6088\n",
      "Training loss (for one batch) at step 50: 306.3864, Accuracy: 0.6229\n",
      "Training loss (for one batch) at step 60: 334.8857, Accuracy: 0.6241\n",
      "Training loss (for one batch) at step 70: 354.7899, Accuracy: 0.6127\n",
      "Training loss (for one batch) at step 80: 361.6792, Accuracy: 0.6018\n",
      "Training loss (for one batch) at step 90: 334.8878, Accuracy: 0.5998\n",
      "Training loss (for one batch) at step 100: 348.2787, Accuracy: 0.6011\n",
      "Training loss (for one batch) at step 110: 311.6469, Accuracy: 0.6018\n",
      "---- Training ----\n",
      "Training loss: 121.6165\n",
      "Training acc over epoch: 0.6020\n",
      "---- Validation ----\n",
      "Validation loss: 48.7409\n",
      "Validation acc: 0.5631\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 327.5306, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 328.9437, Accuracy: 0.5504\n",
      "Training loss (for one batch) at step 20: 318.2643, Accuracy: 0.5766\n",
      "Training loss (for one batch) at step 30: 319.3121, Accuracy: 0.5980\n",
      "Training loss (for one batch) at step 40: 325.2646, Accuracy: 0.6077\n",
      "Training loss (for one batch) at step 50: 296.8114, Accuracy: 0.6160\n",
      "Training loss (for one batch) at step 60: 319.7753, Accuracy: 0.6204\n",
      "Training loss (for one batch) at step 70: 342.8922, Accuracy: 0.6141\n",
      "Training loss (for one batch) at step 80: 338.7435, Accuracy: 0.6050\n",
      "Training loss (for one batch) at step 90: 347.3041, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 100: 345.6359, Accuracy: 0.6023\n",
      "Training loss (for one batch) at step 110: 317.3612, Accuracy: 0.6005\n",
      "---- Training ----\n",
      "Training loss: 97.2077\n",
      "Training acc over epoch: 0.5999\n",
      "---- Validation ----\n",
      "Validation loss: 37.0721\n",
      "Validation acc: 0.5551\n",
      "Time taken: 10.80s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 331.8228, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 325.3713, Accuracy: 0.5419\n",
      "Training loss (for one batch) at step 20: 313.3956, Accuracy: 0.5811\n",
      "Training loss (for one batch) at step 30: 314.3494, Accuracy: 0.5973\n",
      "Training loss (for one batch) at step 40: 323.6212, Accuracy: 0.6126\n",
      "Training loss (for one batch) at step 50: 313.2712, Accuracy: 0.6167\n",
      "Training loss (for one batch) at step 60: 309.4924, Accuracy: 0.6228\n",
      "Training loss (for one batch) at step 70: 343.8466, Accuracy: 0.6140\n",
      "Training loss (for one batch) at step 80: 336.5667, Accuracy: 0.6017\n",
      "Training loss (for one batch) at step 90: 297.7160, Accuracy: 0.6002\n",
      "Training loss (for one batch) at step 100: 315.7044, Accuracy: 0.6006\n",
      "Training loss (for one batch) at step 110: 325.4453, Accuracy: 0.6003\n",
      "---- Training ----\n",
      "Training loss: 105.9498\n",
      "Training acc over epoch: 0.5995\n",
      "---- Validation ----\n",
      "Validation loss: 51.7602\n",
      "Validation acc: 0.5543\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 330.0674, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 330.8410, Accuracy: 0.5291\n",
      "Training loss (for one batch) at step 20: 320.4085, Accuracy: 0.5722\n",
      "Training loss (for one batch) at step 30: 307.3238, Accuracy: 0.5910\n",
      "Training loss (for one batch) at step 40: 292.4497, Accuracy: 0.6061\n",
      "Training loss (for one batch) at step 50: 306.2319, Accuracy: 0.6138\n",
      "Training loss (for one batch) at step 60: 313.5754, Accuracy: 0.6181\n",
      "Training loss (for one batch) at step 70: 331.9171, Accuracy: 0.6127\n",
      "Training loss (for one batch) at step 80: 346.5615, Accuracy: 0.6052\n",
      "Training loss (for one batch) at step 90: 318.1991, Accuracy: 0.6025\n",
      "Training loss (for one batch) at step 100: 328.3567, Accuracy: 0.6025\n",
      "Training loss (for one batch) at step 110: 314.2763, Accuracy: 0.6016\n",
      "---- Training ----\n",
      "Training loss: 99.2339\n",
      "Training acc over epoch: 0.6018\n",
      "---- Validation ----\n",
      "Validation loss: 43.4057\n",
      "Validation acc: 0.5540\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 339.0536, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 343.0886, Accuracy: 0.5270\n",
      "Training loss (for one batch) at step 20: 323.2444, Accuracy: 0.5688\n",
      "Training loss (for one batch) at step 30: 313.2134, Accuracy: 0.5857\n",
      "Training loss (for one batch) at step 40: 306.9252, Accuracy: 0.5991\n",
      "Training loss (for one batch) at step 50: 317.8669, Accuracy: 0.6123\n",
      "Training loss (for one batch) at step 60: 346.2646, Accuracy: 0.6186\n",
      "Training loss (for one batch) at step 70: 338.1111, Accuracy: 0.6120\n",
      "Training loss (for one batch) at step 80: 326.3393, Accuracy: 0.6009\n",
      "Training loss (for one batch) at step 90: 316.3179, Accuracy: 0.5998\n",
      "Training loss (for one batch) at step 100: 321.2404, Accuracy: 0.5996\n",
      "Training loss (for one batch) at step 110: 318.7029, Accuracy: 0.6001\n",
      "---- Training ----\n",
      "Training loss: 115.7106\n",
      "Training acc over epoch: 0.5989\n",
      "---- Validation ----\n",
      "Validation loss: 36.3360\n",
      "Validation acc: 0.5419\n",
      "Time taken: 10.92s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 340.4364, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 338.9564, Accuracy: 0.5455\n",
      "Training loss (for one batch) at step 20: 300.3493, Accuracy: 0.5807\n",
      "Training loss (for one batch) at step 30: 318.3327, Accuracy: 0.5953\n",
      "Training loss (for one batch) at step 40: 294.7499, Accuracy: 0.6071\n",
      "Training loss (for one batch) at step 50: 310.2724, Accuracy: 0.6164\n",
      "Training loss (for one batch) at step 60: 309.8804, Accuracy: 0.6201\n",
      "Training loss (for one batch) at step 70: 322.5745, Accuracy: 0.6140\n",
      "Training loss (for one batch) at step 80: 339.2338, Accuracy: 0.6026\n",
      "Training loss (for one batch) at step 90: 299.3951, Accuracy: 0.6031\n",
      "Training loss (for one batch) at step 100: 305.2877, Accuracy: 0.6040\n",
      "Training loss (for one batch) at step 110: 314.3307, Accuracy: 0.6019\n",
      "---- Training ----\n",
      "Training loss: 97.0518\n",
      "Training acc over epoch: 0.6023\n",
      "---- Validation ----\n",
      "Validation loss: 35.9627\n",
      "Validation acc: 0.5578\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 339.0717, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 315.6425, Accuracy: 0.5447\n",
      "Training loss (for one batch) at step 20: 310.8199, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 30: 296.7899, Accuracy: 0.5955\n",
      "Training loss (for one batch) at step 40: 310.9400, Accuracy: 0.6033\n",
      "Training loss (for one batch) at step 50: 304.1230, Accuracy: 0.6134\n",
      "Training loss (for one batch) at step 60: 334.7399, Accuracy: 0.6217\n",
      "Training loss (for one batch) at step 70: 339.4008, Accuracy: 0.6125\n",
      "Training loss (for one batch) at step 80: 320.0479, Accuracy: 0.6035\n",
      "Training loss (for one batch) at step 90: 315.8693, Accuracy: 0.6001\n",
      "Training loss (for one batch) at step 100: 322.5947, Accuracy: 0.6028\n",
      "Training loss (for one batch) at step 110: 312.7538, Accuracy: 0.6030\n",
      "---- Training ----\n",
      "Training loss: 109.5798\n",
      "Training acc over epoch: 0.6020\n",
      "---- Validation ----\n",
      "Validation loss: 38.3066\n",
      "Validation acc: 0.5513\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 336.1710, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 343.5034, Accuracy: 0.5582\n",
      "Training loss (for one batch) at step 20: 308.9728, Accuracy: 0.5837\n",
      "Training loss (for one batch) at step 30: 300.6549, Accuracy: 0.6051\n",
      "Training loss (for one batch) at step 40: 298.2340, Accuracy: 0.6128\n",
      "Training loss (for one batch) at step 50: 312.3908, Accuracy: 0.6213\n",
      "Training loss (for one batch) at step 60: 319.3728, Accuracy: 0.6264\n",
      "Training loss (for one batch) at step 70: 339.6680, Accuracy: 0.6177\n",
      "Training loss (for one batch) at step 80: 342.6055, Accuracy: 0.6072\n",
      "Training loss (for one batch) at step 90: 315.0172, Accuracy: 0.6030\n",
      "Training loss (for one batch) at step 100: 302.3165, Accuracy: 0.6060\n",
      "Training loss (for one batch) at step 110: 366.2497, Accuracy: 0.6049\n",
      "---- Training ----\n",
      "Training loss: 110.4663\n",
      "Training acc over epoch: 0.6039\n",
      "---- Validation ----\n",
      "Validation loss: 40.9571\n",
      "Validation acc: 0.5355\n",
      "Time taken: 10.89s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 308.8965, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 333.5045, Accuracy: 0.5348\n",
      "Training loss (for one batch) at step 20: 299.6728, Accuracy: 0.5647\n",
      "Training loss (for one batch) at step 30: 309.9890, Accuracy: 0.5890\n",
      "Training loss (for one batch) at step 40: 302.7100, Accuracy: 0.6010\n",
      "Training loss (for one batch) at step 50: 308.7941, Accuracy: 0.6097\n",
      "Training loss (for one batch) at step 60: 313.1963, Accuracy: 0.6162\n",
      "Training loss (for one batch) at step 70: 320.3608, Accuracy: 0.6100\n",
      "Training loss (for one batch) at step 80: 328.1906, Accuracy: 0.5993\n",
      "Training loss (for one batch) at step 90: 311.7286, Accuracy: 0.5975\n",
      "Training loss (for one batch) at step 100: 318.2677, Accuracy: 0.5974\n",
      "Training loss (for one batch) at step 110: 319.1992, Accuracy: 0.5985\n",
      "---- Training ----\n",
      "Training loss: 110.6504\n",
      "Training acc over epoch: 0.5977\n",
      "---- Validation ----\n",
      "Validation loss: 39.1011\n",
      "Validation acc: 0.5607\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 326.8427, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 331.5268, Accuracy: 0.5455\n",
      "Training loss (for one batch) at step 20: 311.2024, Accuracy: 0.5755\n",
      "Training loss (for one batch) at step 30: 303.5661, Accuracy: 0.5965\n",
      "Training loss (for one batch) at step 40: 296.9824, Accuracy: 0.6037\n",
      "Training loss (for one batch) at step 50: 309.2337, Accuracy: 0.6131\n",
      "Training loss (for one batch) at step 60: 309.9756, Accuracy: 0.6208\n",
      "Training loss (for one batch) at step 70: 330.6053, Accuracy: 0.6137\n",
      "Training loss (for one batch) at step 80: 342.3112, Accuracy: 0.6041\n",
      "Training loss (for one batch) at step 90: 312.6426, Accuracy: 0.6023\n",
      "Training loss (for one batch) at step 100: 307.8724, Accuracy: 0.6053\n",
      "Training loss (for one batch) at step 110: 302.7740, Accuracy: 0.6047\n",
      "---- Training ----\n",
      "Training loss: 103.1392\n",
      "Training acc over epoch: 0.6042\n",
      "---- Validation ----\n",
      "Validation loss: 42.8119\n",
      "Validation acc: 0.5712\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 340.4968, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 319.8461, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 20: 292.3031, Accuracy: 0.5725\n",
      "Training loss (for one batch) at step 30: 307.9872, Accuracy: 0.5897\n",
      "Training loss (for one batch) at step 40: 305.8012, Accuracy: 0.6065\n",
      "Training loss (for one batch) at step 50: 296.1838, Accuracy: 0.6147\n",
      "Training loss (for one batch) at step 60: 302.2815, Accuracy: 0.6196\n",
      "Training loss (for one batch) at step 70: 342.7917, Accuracy: 0.6145\n",
      "Training loss (for one batch) at step 80: 338.1966, Accuracy: 0.6052\n",
      "Training loss (for one batch) at step 90: 335.0919, Accuracy: 0.6015\n",
      "Training loss (for one batch) at step 100: 312.8598, Accuracy: 0.6023\n",
      "Training loss (for one batch) at step 110: 303.4692, Accuracy: 0.6031\n",
      "---- Training ----\n",
      "Training loss: 100.1404\n",
      "Training acc over epoch: 0.6010\n",
      "---- Validation ----\n",
      "Validation loss: 33.5724\n",
      "Validation acc: 0.5527\n",
      "Time taken: 10.68s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 349.0891, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 311.8987, Accuracy: 0.5639\n",
      "Training loss (for one batch) at step 20: 294.5032, Accuracy: 0.5844\n",
      "Training loss (for one batch) at step 30: 308.9692, Accuracy: 0.5963\n",
      "Training loss (for one batch) at step 40: 293.3201, Accuracy: 0.6122\n",
      "Training loss (for one batch) at step 50: 312.8768, Accuracy: 0.6216\n",
      "Training loss (for one batch) at step 60: 318.8973, Accuracy: 0.6232\n",
      "Training loss (for one batch) at step 70: 319.3266, Accuracy: 0.6151\n",
      "Training loss (for one batch) at step 80: 329.3350, Accuracy: 0.6052\n",
      "Training loss (for one batch) at step 90: 310.7570, Accuracy: 0.6031\n",
      "Training loss (for one batch) at step 100: 295.7384, Accuracy: 0.6041\n",
      "Training loss (for one batch) at step 110: 304.1650, Accuracy: 0.6027\n",
      "---- Training ----\n",
      "Training loss: 106.3895\n",
      "Training acc over epoch: 0.6016\n",
      "---- Validation ----\n",
      "Validation loss: 51.7249\n",
      "Validation acc: 0.5621\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 332.8678, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 331.2303, Accuracy: 0.5384\n",
      "Training loss (for one batch) at step 20: 315.1516, Accuracy: 0.5740\n",
      "Training loss (for one batch) at step 30: 303.2671, Accuracy: 0.5943\n",
      "Training loss (for one batch) at step 40: 307.2191, Accuracy: 0.6077\n",
      "Training loss (for one batch) at step 50: 297.9381, Accuracy: 0.6132\n",
      "Training loss (for one batch) at step 60: 311.0887, Accuracy: 0.6235\n",
      "Training loss (for one batch) at step 70: 349.0790, Accuracy: 0.6156\n",
      "Training loss (for one batch) at step 80: 318.1476, Accuracy: 0.6049\n",
      "Training loss (for one batch) at step 90: 295.9418, Accuracy: 0.6023\n",
      "Training loss (for one batch) at step 100: 305.8135, Accuracy: 0.6046\n",
      "Training loss (for one batch) at step 110: 321.5188, Accuracy: 0.6046\n",
      "---- Training ----\n",
      "Training loss: 98.7497\n",
      "Training acc over epoch: 0.6028\n",
      "---- Validation ----\n",
      "Validation loss: 45.7242\n",
      "Validation acc: 0.5605\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 342.8885, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 343.9001, Accuracy: 0.5526\n",
      "Training loss (for one batch) at step 20: 325.1980, Accuracy: 0.5837\n",
      "Training loss (for one batch) at step 30: 310.9106, Accuracy: 0.5973\n",
      "Training loss (for one batch) at step 40: 312.5207, Accuracy: 0.6073\n",
      "Training loss (for one batch) at step 50: 317.0243, Accuracy: 0.6163\n",
      "Training loss (for one batch) at step 60: 326.4807, Accuracy: 0.6227\n",
      "Training loss (for one batch) at step 70: 322.2682, Accuracy: 0.6159\n",
      "Training loss (for one batch) at step 80: 331.7469, Accuracy: 0.6046\n",
      "Training loss (for one batch) at step 90: 315.7081, Accuracy: 0.6010\n",
      "Training loss (for one batch) at step 100: 322.9411, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 110: 319.7590, Accuracy: 0.6007\n",
      "---- Training ----\n",
      "Training loss: 102.0329\n",
      "Training acc over epoch: 0.5991\n",
      "---- Validation ----\n",
      "Validation loss: 38.7538\n",
      "Validation acc: 0.5607\n",
      "Time taken: 10.69s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 334.0168, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 325.8224, Accuracy: 0.5398\n",
      "Training loss (for one batch) at step 20: 299.9282, Accuracy: 0.5737\n",
      "Training loss (for one batch) at step 30: 304.5987, Accuracy: 0.5980\n",
      "Training loss (for one batch) at step 40: 306.7115, Accuracy: 0.6077\n",
      "Training loss (for one batch) at step 50: 296.3154, Accuracy: 0.6164\n",
      "Training loss (for one batch) at step 60: 307.7735, Accuracy: 0.6206\n",
      "Training loss (for one batch) at step 70: 321.4751, Accuracy: 0.6154\n",
      "Training loss (for one batch) at step 80: 328.0456, Accuracy: 0.6050\n",
      "Training loss (for one batch) at step 90: 306.5355, Accuracy: 0.6019\n",
      "Training loss (for one batch) at step 100: 292.8916, Accuracy: 0.6023\n",
      "Training loss (for one batch) at step 110: 307.6307, Accuracy: 0.6031\n",
      "---- Training ----\n",
      "Training loss: 102.7246\n",
      "Training acc over epoch: 0.6022\n",
      "---- Validation ----\n",
      "Validation loss: 44.3708\n",
      "Validation acc: 0.5607\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 314.7938, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 322.7802, Accuracy: 0.5348\n",
      "Training loss (for one batch) at step 20: 307.7933, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 30: 326.2904, Accuracy: 0.5940\n",
      "Training loss (for one batch) at step 40: 297.0998, Accuracy: 0.6080\n",
      "Training loss (for one batch) at step 50: 302.8557, Accuracy: 0.6131\n",
      "Training loss (for one batch) at step 60: 302.3275, Accuracy: 0.6208\n",
      "Training loss (for one batch) at step 70: 332.7805, Accuracy: 0.6140\n",
      "Training loss (for one batch) at step 80: 318.4868, Accuracy: 0.6051\n",
      "Training loss (for one batch) at step 90: 306.8004, Accuracy: 0.5998\n",
      "Training loss (for one batch) at step 100: 309.1409, Accuracy: 0.6017\n",
      "Training loss (for one batch) at step 110: 293.9055, Accuracy: 0.6012\n",
      "---- Training ----\n",
      "Training loss: 93.7913\n",
      "Training acc over epoch: 0.6011\n",
      "---- Validation ----\n",
      "Validation loss: 47.0583\n",
      "Validation acc: 0.5588\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 341.9064, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 326.0282, Accuracy: 0.5405\n",
      "Training loss (for one batch) at step 20: 298.7932, Accuracy: 0.5722\n",
      "Training loss (for one batch) at step 30: 288.5857, Accuracy: 0.5948\n",
      "Training loss (for one batch) at step 40: 293.5927, Accuracy: 0.6046\n",
      "Training loss (for one batch) at step 50: 300.8835, Accuracy: 0.6169\n",
      "Training loss (for one batch) at step 60: 296.9991, Accuracy: 0.6223\n",
      "Training loss (for one batch) at step 70: 317.9110, Accuracy: 0.6147\n",
      "Training loss (for one batch) at step 80: 313.9690, Accuracy: 0.6032\n",
      "Training loss (for one batch) at step 90: 300.9175, Accuracy: 0.6010\n",
      "Training loss (for one batch) at step 100: 299.9288, Accuracy: 0.6021\n",
      "Training loss (for one batch) at step 110: 306.9719, Accuracy: 0.6028\n",
      "---- Training ----\n",
      "Training loss: 101.7204\n",
      "Training acc over epoch: 0.6014\n",
      "---- Validation ----\n",
      "Validation loss: 34.4250\n",
      "Validation acc: 0.5631\n",
      "Time taken: 10.81s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 303.9508, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 328.5858, Accuracy: 0.5426\n",
      "Training loss (for one batch) at step 20: 323.4326, Accuracy: 0.5785\n",
      "Training loss (for one batch) at step 30: 309.5649, Accuracy: 0.5958\n",
      "Training loss (for one batch) at step 40: 301.4370, Accuracy: 0.6092\n",
      "Training loss (for one batch) at step 50: 324.0699, Accuracy: 0.6141\n",
      "Training loss (for one batch) at step 60: 299.9070, Accuracy: 0.6192\n",
      "Training loss (for one batch) at step 70: 315.2266, Accuracy: 0.6131\n",
      "Training loss (for one batch) at step 80: 351.2197, Accuracy: 0.6032\n",
      "Training loss (for one batch) at step 90: 313.0356, Accuracy: 0.6048\n",
      "Training loss (for one batch) at step 100: 292.7458, Accuracy: 0.6060\n",
      "Training loss (for one batch) at step 110: 308.0390, Accuracy: 0.6058\n",
      "---- Training ----\n",
      "Training loss: 94.9966\n",
      "Training acc over epoch: 0.6039\n",
      "---- Validation ----\n",
      "Validation loss: 39.4389\n",
      "Validation acc: 0.5688\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 322.2182, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 327.3523, Accuracy: 0.5526\n",
      "Training loss (for one batch) at step 20: 320.1620, Accuracy: 0.5800\n",
      "Training loss (for one batch) at step 30: 292.7973, Accuracy: 0.6021\n",
      "Training loss (for one batch) at step 40: 286.9931, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 50: 296.5602, Accuracy: 0.6184\n",
      "Training loss (for one batch) at step 60: 292.0309, Accuracy: 0.6199\n",
      "Training loss (for one batch) at step 70: 314.5340, Accuracy: 0.6132\n",
      "Training loss (for one batch) at step 80: 335.4161, Accuracy: 0.6037\n",
      "Training loss (for one batch) at step 90: 321.1669, Accuracy: 0.5985\n",
      "Training loss (for one batch) at step 100: 321.7069, Accuracy: 0.6015\n",
      "Training loss (for one batch) at step 110: 294.1927, Accuracy: 0.6002\n",
      "---- Training ----\n",
      "Training loss: 106.8549\n",
      "Training acc over epoch: 0.6006\n",
      "---- Validation ----\n",
      "Validation loss: 40.0995\n",
      "Validation acc: 0.5615\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 346.1883, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 308.9081, Accuracy: 0.5568\n",
      "Training loss (for one batch) at step 20: 310.4314, Accuracy: 0.5826\n",
      "Training loss (for one batch) at step 30: 311.7841, Accuracy: 0.5960\n",
      "Training loss (for one batch) at step 40: 298.0773, Accuracy: 0.6061\n",
      "Training loss (for one batch) at step 50: 318.2160, Accuracy: 0.6146\n",
      "Training loss (for one batch) at step 60: 324.3086, Accuracy: 0.6195\n",
      "Training loss (for one batch) at step 70: 315.8962, Accuracy: 0.6116\n",
      "Training loss (for one batch) at step 80: 320.3417, Accuracy: 0.6012\n",
      "Training loss (for one batch) at step 90: 299.8548, Accuracy: 0.5988\n",
      "Training loss (for one batch) at step 100: 302.8860, Accuracy: 0.6000\n",
      "Training loss (for one batch) at step 110: 316.3039, Accuracy: 0.6009\n",
      "---- Training ----\n",
      "Training loss: 99.3768\n",
      "Training acc over epoch: 0.6001\n",
      "---- Validation ----\n",
      "Validation loss: 44.6669\n",
      "Validation acc: 0.5465\n",
      "Time taken: 10.81s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 332.0874, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 320.5381, Accuracy: 0.5462\n",
      "Training loss (for one batch) at step 20: 287.0413, Accuracy: 0.5811\n",
      "Training loss (for one batch) at step 30: 281.1594, Accuracy: 0.6046\n",
      "Training loss (for one batch) at step 40: 292.1074, Accuracy: 0.6113\n",
      "Training loss (for one batch) at step 50: 296.2113, Accuracy: 0.6184\n",
      "Training loss (for one batch) at step 60: 308.2607, Accuracy: 0.6227\n",
      "Training loss (for one batch) at step 70: 305.8776, Accuracy: 0.6147\n",
      "Training loss (for one batch) at step 80: 305.4109, Accuracy: 0.6039\n",
      "Training loss (for one batch) at step 90: 303.9998, Accuracy: 0.6004\n",
      "Training loss (for one batch) at step 100: 301.4906, Accuracy: 0.6023\n",
      "Training loss (for one batch) at step 110: 309.9212, Accuracy: 0.6009\n",
      "---- Training ----\n",
      "Training loss: 112.1187\n",
      "Training acc over epoch: 0.5995\n",
      "---- Validation ----\n",
      "Validation loss: 46.0415\n",
      "Validation acc: 0.5685\n",
      "Time taken: 10.63s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 344.3906, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 315.4760, Accuracy: 0.5511\n",
      "Training loss (for one batch) at step 20: 301.3340, Accuracy: 0.5807\n",
      "Training loss (for one batch) at step 30: 300.9977, Accuracy: 0.5973\n",
      "Training loss (for one batch) at step 40: 307.4156, Accuracy: 0.6086\n",
      "Training loss (for one batch) at step 50: 309.1634, Accuracy: 0.6198\n",
      "Training loss (for one batch) at step 60: 310.0169, Accuracy: 0.6230\n",
      "Training loss (for one batch) at step 70: 302.0465, Accuracy: 0.6154\n",
      "Training loss (for one batch) at step 80: 322.3730, Accuracy: 0.6056\n",
      "Training loss (for one batch) at step 90: 292.4830, Accuracy: 0.6034\n",
      "Training loss (for one batch) at step 100: 305.8381, Accuracy: 0.6052\n",
      "Training loss (for one batch) at step 110: 327.2582, Accuracy: 0.6032\n",
      "---- Training ----\n",
      "Training loss: 104.1065\n",
      "Training acc over epoch: 0.6017\n",
      "---- Validation ----\n",
      "Validation loss: 37.8294\n",
      "Validation acc: 0.5669\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 341.2440, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 343.2604, Accuracy: 0.5426\n",
      "Training loss (for one batch) at step 20: 297.1910, Accuracy: 0.5763\n",
      "Training loss (for one batch) at step 30: 295.3204, Accuracy: 0.5958\n",
      "Training loss (for one batch) at step 40: 291.2443, Accuracy: 0.6111\n",
      "Training loss (for one batch) at step 50: 285.4935, Accuracy: 0.6198\n",
      "Training loss (for one batch) at step 60: 300.3690, Accuracy: 0.6222\n",
      "Training loss (for one batch) at step 70: 311.7524, Accuracy: 0.6131\n",
      "Training loss (for one batch) at step 80: 326.7468, Accuracy: 0.6012\n",
      "Training loss (for one batch) at step 90: 293.8605, Accuracy: 0.6001\n",
      "Training loss (for one batch) at step 100: 275.9550, Accuracy: 0.6003\n",
      "Training loss (for one batch) at step 110: 297.9464, Accuracy: 0.5997\n",
      "---- Training ----\n",
      "Training loss: 97.5175\n",
      "Training acc over epoch: 0.5991\n",
      "---- Validation ----\n",
      "Validation loss: 45.7514\n",
      "Validation acc: 0.5790\n",
      "Time taken: 10.86s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 314.4876, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 306.8762, Accuracy: 0.5483\n",
      "Training loss (for one batch) at step 20: 290.0553, Accuracy: 0.5778\n",
      "Training loss (for one batch) at step 30: 291.1721, Accuracy: 0.5955\n",
      "Training loss (for one batch) at step 40: 286.0692, Accuracy: 0.6019\n",
      "Training loss (for one batch) at step 50: 287.6350, Accuracy: 0.6147\n",
      "Training loss (for one batch) at step 60: 300.5689, Accuracy: 0.6205\n",
      "Training loss (for one batch) at step 70: 306.4261, Accuracy: 0.6156\n",
      "Training loss (for one batch) at step 80: 326.7963, Accuracy: 0.6039\n",
      "Training loss (for one batch) at step 90: 312.1128, Accuracy: 0.6010\n",
      "Training loss (for one batch) at step 100: 284.1176, Accuracy: 0.6021\n",
      "Training loss (for one batch) at step 110: 303.0464, Accuracy: 0.6012\n",
      "---- Training ----\n",
      "Training loss: 103.4379\n",
      "Training acc over epoch: 0.6007\n",
      "---- Validation ----\n",
      "Validation loss: 40.8746\n",
      "Validation acc: 0.5648\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 304.7276, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 330.2556, Accuracy: 0.5661\n",
      "Training loss (for one batch) at step 20: 301.2058, Accuracy: 0.5759\n",
      "Training loss (for one batch) at step 30: 318.9343, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 40: 288.6563, Accuracy: 0.6096\n",
      "Training loss (for one batch) at step 50: 301.7182, Accuracy: 0.6187\n",
      "Training loss (for one batch) at step 60: 317.6988, Accuracy: 0.6254\n",
      "Training loss (for one batch) at step 70: 317.8788, Accuracy: 0.6165\n",
      "Training loss (for one batch) at step 80: 304.3912, Accuracy: 0.6066\n",
      "Training loss (for one batch) at step 90: 294.6966, Accuracy: 0.6011\n",
      "Training loss (for one batch) at step 100: 291.7183, Accuracy: 0.6035\n",
      "Training loss (for one batch) at step 110: 286.1587, Accuracy: 0.6034\n",
      "---- Training ----\n",
      "Training loss: 90.4313\n",
      "Training acc over epoch: 0.6016\n",
      "---- Validation ----\n",
      "Validation loss: 44.7280\n",
      "Validation acc: 0.5672\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 300.2368, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 314.2825, Accuracy: 0.5256\n",
      "Training loss (for one batch) at step 20: 297.6549, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 30: 280.6797, Accuracy: 0.5970\n",
      "Training loss (for one batch) at step 40: 295.0927, Accuracy: 0.6065\n",
      "Training loss (for one batch) at step 50: 307.0232, Accuracy: 0.6167\n",
      "Training loss (for one batch) at step 60: 283.2591, Accuracy: 0.6222\n",
      "Training loss (for one batch) at step 70: 298.3250, Accuracy: 0.6140\n",
      "Training loss (for one batch) at step 80: 342.0343, Accuracy: 0.6023\n",
      "Training loss (for one batch) at step 90: 292.3252, Accuracy: 0.5994\n",
      "Training loss (for one batch) at step 100: 287.3486, Accuracy: 0.6004\n",
      "Training loss (for one batch) at step 110: 310.9476, Accuracy: 0.6018\n",
      "---- Training ----\n",
      "Training loss: 103.9238\n",
      "Training acc over epoch: 0.6005\n",
      "---- Validation ----\n",
      "Validation loss: 66.4657\n",
      "Validation acc: 0.5798\n",
      "Time taken: 10.74s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 324.4477, Accuracy: 0.4922\n",
      "Training loss (for one batch) at step 10: 295.1573, Accuracy: 0.5334\n",
      "Training loss (for one batch) at step 20: 280.5279, Accuracy: 0.5725\n",
      "Training loss (for one batch) at step 30: 282.4547, Accuracy: 0.5965\n",
      "Training loss (for one batch) at step 40: 288.1791, Accuracy: 0.6056\n",
      "Training loss (for one batch) at step 50: 302.3557, Accuracy: 0.6152\n",
      "Training loss (for one batch) at step 60: 299.9411, Accuracy: 0.6212\n",
      "Training loss (for one batch) at step 70: 313.2655, Accuracy: 0.6156\n",
      "Training loss (for one batch) at step 80: 310.7226, Accuracy: 0.6039\n",
      "Training loss (for one batch) at step 90: 299.6959, Accuracy: 0.6015\n",
      "Training loss (for one batch) at step 100: 293.1259, Accuracy: 0.6036\n",
      "Training loss (for one batch) at step 110: 307.4671, Accuracy: 0.6027\n",
      "---- Training ----\n",
      "Training loss: 108.6490\n",
      "Training acc over epoch: 0.6024\n",
      "---- Validation ----\n",
      "Validation loss: 46.1101\n",
      "Validation acc: 0.5629\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 315.1411, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 327.0457, Accuracy: 0.5490\n",
      "Training loss (for one batch) at step 20: 286.4124, Accuracy: 0.5688\n",
      "Training loss (for one batch) at step 30: 293.8362, Accuracy: 0.5925\n",
      "Training loss (for one batch) at step 40: 298.4782, Accuracy: 0.6065\n",
      "Training loss (for one batch) at step 50: 287.4879, Accuracy: 0.6173\n",
      "Training loss (for one batch) at step 60: 298.0503, Accuracy: 0.6214\n",
      "Training loss (for one batch) at step 70: 293.6983, Accuracy: 0.6160\n",
      "Training loss (for one batch) at step 80: 327.3632, Accuracy: 0.6050\n",
      "Training loss (for one batch) at step 90: 284.4706, Accuracy: 0.6019\n",
      "Training loss (for one batch) at step 100: 303.8448, Accuracy: 0.6036\n",
      "Training loss (for one batch) at step 110: 304.4328, Accuracy: 0.6034\n",
      "---- Training ----\n",
      "Training loss: 88.7560\n",
      "Training acc over epoch: 0.6026\n",
      "---- Validation ----\n",
      "Validation loss: 44.7593\n",
      "Validation acc: 0.5607\n",
      "Time taken: 10.40s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 318.1385, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 313.8585, Accuracy: 0.5447\n",
      "Training loss (for one batch) at step 20: 289.7745, Accuracy: 0.5718\n",
      "Training loss (for one batch) at step 30: 278.9920, Accuracy: 0.5998\n",
      "Training loss (for one batch) at step 40: 294.4693, Accuracy: 0.6061\n",
      "Training loss (for one batch) at step 50: 295.4466, Accuracy: 0.6160\n",
      "Training loss (for one batch) at step 60: 303.1151, Accuracy: 0.6212\n",
      "Training loss (for one batch) at step 70: 307.2282, Accuracy: 0.6127\n",
      "Training loss (for one batch) at step 80: 307.7548, Accuracy: 0.6034\n",
      "Training loss (for one batch) at step 90: 295.7966, Accuracy: 0.6011\n",
      "Training loss (for one batch) at step 100: 292.4399, Accuracy: 0.6013\n",
      "Training loss (for one batch) at step 110: 302.5904, Accuracy: 0.6023\n",
      "---- Training ----\n",
      "Training loss: 98.7697\n",
      "Training acc over epoch: 0.6022\n",
      "---- Validation ----\n",
      "Validation loss: 32.0394\n",
      "Validation acc: 0.5596\n",
      "Time taken: 10.68s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 318.7504, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 308.8492, Accuracy: 0.5582\n",
      "Training loss (for one batch) at step 20: 289.8643, Accuracy: 0.5737\n",
      "Training loss (for one batch) at step 30: 307.2596, Accuracy: 0.5902\n",
      "Training loss (for one batch) at step 40: 297.0577, Accuracy: 0.6014\n",
      "Training loss (for one batch) at step 50: 290.5638, Accuracy: 0.6141\n",
      "Training loss (for one batch) at step 60: 291.0155, Accuracy: 0.6177\n",
      "Training loss (for one batch) at step 70: 313.4319, Accuracy: 0.6108\n",
      "Training loss (for one batch) at step 80: 313.5987, Accuracy: 0.5994\n",
      "Training loss (for one batch) at step 90: 285.8714, Accuracy: 0.5971\n",
      "Training loss (for one batch) at step 100: 291.6289, Accuracy: 0.5992\n",
      "Training loss (for one batch) at step 110: 302.2096, Accuracy: 0.5993\n",
      "---- Training ----\n",
      "Training loss: 111.1220\n",
      "Training acc over epoch: 0.5992\n",
      "---- Validation ----\n",
      "Validation loss: 62.9289\n",
      "Validation acc: 0.5607\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 310.9903, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 307.6175, Accuracy: 0.5575\n",
      "Training loss (for one batch) at step 20: 289.8599, Accuracy: 0.5852\n",
      "Training loss (for one batch) at step 30: 292.6546, Accuracy: 0.5955\n",
      "Training loss (for one batch) at step 40: 296.7658, Accuracy: 0.6090\n",
      "Training loss (for one batch) at step 50: 285.2604, Accuracy: 0.6199\n",
      "Training loss (for one batch) at step 60: 301.5813, Accuracy: 0.6240\n",
      "Training loss (for one batch) at step 70: 316.9395, Accuracy: 0.6169\n",
      "Training loss (for one batch) at step 80: 310.3525, Accuracy: 0.6031\n",
      "Training loss (for one batch) at step 90: 311.7877, Accuracy: 0.6009\n",
      "Training loss (for one batch) at step 100: 291.3483, Accuracy: 0.6050\n",
      "Training loss (for one batch) at step 110: 291.3892, Accuracy: 0.6029\n",
      "---- Training ----\n",
      "Training loss: 94.3610\n",
      "Training acc over epoch: 0.6029\n",
      "---- Validation ----\n",
      "Validation loss: 34.0834\n",
      "Validation acc: 0.5639\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 323.5881, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 313.0711, Accuracy: 0.5405\n",
      "Training loss (for one batch) at step 20: 298.6726, Accuracy: 0.5778\n",
      "Training loss (for one batch) at step 30: 278.7921, Accuracy: 0.6046\n",
      "Training loss (for one batch) at step 40: 288.0147, Accuracy: 0.6079\n",
      "Training loss (for one batch) at step 50: 284.4641, Accuracy: 0.6178\n",
      "Training loss (for one batch) at step 60: 287.3380, Accuracy: 0.6215\n",
      "Training loss (for one batch) at step 70: 313.0826, Accuracy: 0.6144\n",
      "Training loss (for one batch) at step 80: 308.6465, Accuracy: 0.6036\n",
      "Training loss (for one batch) at step 90: 299.2736, Accuracy: 0.6014\n",
      "Training loss (for one batch) at step 100: 301.4615, Accuracy: 0.6027\n",
      "Training loss (for one batch) at step 110: 315.7937, Accuracy: 0.6011\n",
      "---- Training ----\n",
      "Training loss: 86.1006\n",
      "Training acc over epoch: 0.6010\n",
      "---- Validation ----\n",
      "Validation loss: 68.0102\n",
      "Validation acc: 0.5621\n",
      "Time taken: 10.89s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 314.2808, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 293.6560, Accuracy: 0.5405\n",
      "Training loss (for one batch) at step 20: 274.1960, Accuracy: 0.5796\n",
      "Training loss (for one batch) at step 30: 298.0838, Accuracy: 0.6033\n",
      "Training loss (for one batch) at step 40: 292.0975, Accuracy: 0.6082\n",
      "Training loss (for one batch) at step 50: 272.6786, Accuracy: 0.6161\n",
      "Training loss (for one batch) at step 60: 306.3777, Accuracy: 0.6254\n",
      "Training loss (for one batch) at step 70: 325.7073, Accuracy: 0.6169\n",
      "Training loss (for one batch) at step 80: 314.0185, Accuracy: 0.6061\n",
      "Training loss (for one batch) at step 90: 270.3514, Accuracy: 0.6040\n",
      "Training loss (for one batch) at step 100: 289.3766, Accuracy: 0.6038\n",
      "Training loss (for one batch) at step 110: 310.4051, Accuracy: 0.6039\n",
      "---- Training ----\n",
      "Training loss: 104.8473\n",
      "Training acc over epoch: 0.6043\n",
      "---- Validation ----\n",
      "Validation loss: 55.1915\n",
      "Validation acc: 0.5648\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 306.9701, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 310.7363, Accuracy: 0.5369\n",
      "Training loss (for one batch) at step 20: 290.9517, Accuracy: 0.5699\n",
      "Training loss (for one batch) at step 30: 275.1776, Accuracy: 0.5897\n",
      "Training loss (for one batch) at step 40: 276.8273, Accuracy: 0.6012\n",
      "Training loss (for one batch) at step 50: 293.9711, Accuracy: 0.6092\n",
      "Training loss (for one batch) at step 60: 283.3784, Accuracy: 0.6196\n",
      "Training loss (for one batch) at step 70: 305.7650, Accuracy: 0.6138\n",
      "Training loss (for one batch) at step 80: 320.1551, Accuracy: 0.6009\n",
      "Training loss (for one batch) at step 90: 294.4996, Accuracy: 0.6004\n",
      "Training loss (for one batch) at step 100: 293.6503, Accuracy: 0.6023\n",
      "Training loss (for one batch) at step 110: 285.7752, Accuracy: 0.6017\n",
      "---- Training ----\n",
      "Training loss: 92.1886\n",
      "Training acc over epoch: 0.6016\n",
      "---- Validation ----\n",
      "Validation loss: 48.1809\n",
      "Validation acc: 0.5605\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 312.8784, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 316.4834, Accuracy: 0.5341\n",
      "Training loss (for one batch) at step 20: 290.3002, Accuracy: 0.5778\n",
      "Training loss (for one batch) at step 30: 284.5952, Accuracy: 0.5948\n",
      "Training loss (for one batch) at step 40: 294.1719, Accuracy: 0.6058\n",
      "Training loss (for one batch) at step 50: 302.8487, Accuracy: 0.6180\n",
      "Training loss (for one batch) at step 60: 293.8773, Accuracy: 0.6215\n",
      "Training loss (for one batch) at step 70: 308.0719, Accuracy: 0.6149\n",
      "Training loss (for one batch) at step 80: 302.8184, Accuracy: 0.6046\n",
      "Training loss (for one batch) at step 90: 301.2409, Accuracy: 0.6008\n",
      "Training loss (for one batch) at step 100: 295.9847, Accuracy: 0.6030\n",
      "Training loss (for one batch) at step 110: 287.8678, Accuracy: 0.6038\n",
      "---- Training ----\n",
      "Training loss: 103.0931\n",
      "Training acc over epoch: 0.6018\n",
      "---- Validation ----\n",
      "Validation loss: 49.8037\n",
      "Validation acc: 0.5768\n",
      "Time taken: 10.71s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 304.8563, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 302.5731, Accuracy: 0.5341\n",
      "Training loss (for one batch) at step 20: 293.8562, Accuracy: 0.5729\n",
      "Training loss (for one batch) at step 30: 294.6445, Accuracy: 0.5953\n",
      "Training loss (for one batch) at step 40: 313.1617, Accuracy: 0.6077\n",
      "Training loss (for one batch) at step 50: 277.9759, Accuracy: 0.6144\n",
      "Training loss (for one batch) at step 60: 317.8470, Accuracy: 0.6197\n",
      "Training loss (for one batch) at step 70: 319.3911, Accuracy: 0.6109\n",
      "Training loss (for one batch) at step 80: 328.4592, Accuracy: 0.6009\n",
      "Training loss (for one batch) at step 90: 286.0515, Accuracy: 0.5985\n",
      "Training loss (for one batch) at step 100: 300.5820, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 110: 310.1773, Accuracy: 0.6014\n",
      "---- Training ----\n",
      "Training loss: 89.0464\n",
      "Training acc over epoch: 0.6014\n",
      "---- Validation ----\n",
      "Validation loss: 35.2965\n",
      "Validation acc: 0.5553\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 308.7879, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 295.0347, Accuracy: 0.5362\n",
      "Training loss (for one batch) at step 20: 308.4735, Accuracy: 0.5621\n",
      "Training loss (for one batch) at step 30: 289.9849, Accuracy: 0.5880\n",
      "Training loss (for one batch) at step 40: 283.2492, Accuracy: 0.6073\n",
      "Training loss (for one batch) at step 50: 277.7747, Accuracy: 0.6141\n",
      "Training loss (for one batch) at step 60: 311.3321, Accuracy: 0.6209\n",
      "Training loss (for one batch) at step 70: 311.5679, Accuracy: 0.6126\n",
      "Training loss (for one batch) at step 80: 319.5662, Accuracy: 0.6031\n",
      "Training loss (for one batch) at step 90: 265.9930, Accuracy: 0.6025\n",
      "Training loss (for one batch) at step 100: 297.6794, Accuracy: 0.6014\n",
      "Training loss (for one batch) at step 110: 299.5042, Accuracy: 0.6015\n",
      "---- Training ----\n",
      "Training loss: 98.8384\n",
      "Training acc over epoch: 0.6013\n",
      "---- Validation ----\n",
      "Validation loss: 41.5010\n",
      "Validation acc: 0.5567\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 322.6725, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 311.3627, Accuracy: 0.5362\n",
      "Training loss (for one batch) at step 20: 285.0967, Accuracy: 0.5692\n",
      "Training loss (for one batch) at step 30: 286.7526, Accuracy: 0.5958\n",
      "Training loss (for one batch) at step 40: 281.7244, Accuracy: 0.6058\n",
      "Training loss (for one batch) at step 50: 303.8672, Accuracy: 0.6161\n",
      "Training loss (for one batch) at step 60: 281.9047, Accuracy: 0.6233\n",
      "Training loss (for one batch) at step 70: 307.3002, Accuracy: 0.6156\n",
      "Training loss (for one batch) at step 80: 311.5927, Accuracy: 0.6032\n",
      "Training loss (for one batch) at step 90: 292.6501, Accuracy: 0.5988\n",
      "Training loss (for one batch) at step 100: 290.4703, Accuracy: 0.6045\n",
      "Training loss (for one batch) at step 110: 293.4397, Accuracy: 0.6032\n",
      "---- Training ----\n",
      "Training loss: 93.7534\n",
      "Training acc over epoch: 0.6016\n",
      "---- Validation ----\n",
      "Validation loss: 47.8298\n",
      "Validation acc: 0.5467\n",
      "Time taken: 10.94s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 324.5901, Accuracy: 0.4688\n",
      "Training loss (for one batch) at step 10: 318.4166, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 20: 290.6773, Accuracy: 0.5647\n",
      "Training loss (for one batch) at step 30: 307.0138, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 40: 280.2409, Accuracy: 0.6046\n",
      "Training loss (for one batch) at step 50: 273.6668, Accuracy: 0.6157\n",
      "Training loss (for one batch) at step 60: 286.6857, Accuracy: 0.6191\n",
      "Training loss (for one batch) at step 70: 292.0785, Accuracy: 0.6134\n",
      "Training loss (for one batch) at step 80: 308.9813, Accuracy: 0.6007\n",
      "Training loss (for one batch) at step 90: 304.0336, Accuracy: 0.5995\n",
      "Training loss (for one batch) at step 100: 305.6711, Accuracy: 0.6022\n",
      "Training loss (for one batch) at step 110: 306.0547, Accuracy: 0.6018\n",
      "---- Training ----\n",
      "Training loss: 91.3581\n",
      "Training acc over epoch: 0.6008\n",
      "---- Validation ----\n",
      "Validation loss: 38.2075\n",
      "Validation acc: 0.5551\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 298.7697, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 315.6778, Accuracy: 0.5405\n",
      "Training loss (for one batch) at step 20: 315.3083, Accuracy: 0.5729\n",
      "Training loss (for one batch) at step 30: 294.6638, Accuracy: 0.5993\n",
      "Training loss (for one batch) at step 40: 281.7258, Accuracy: 0.6124\n",
      "Training loss (for one batch) at step 50: 289.2025, Accuracy: 0.6176\n",
      "Training loss (for one batch) at step 60: 289.5569, Accuracy: 0.6231\n",
      "Training loss (for one batch) at step 70: 314.8060, Accuracy: 0.6149\n",
      "Training loss (for one batch) at step 80: 299.0472, Accuracy: 0.6041\n",
      "Training loss (for one batch) at step 90: 303.6046, Accuracy: 0.6029\n",
      "Training loss (for one batch) at step 100: 279.4997, Accuracy: 0.6065\n",
      "Training loss (for one batch) at step 110: 299.6450, Accuracy: 0.6030\n",
      "---- Training ----\n",
      "Training loss: 85.6581\n",
      "Training acc over epoch: 0.6036\n",
      "---- Validation ----\n",
      "Validation loss: 41.8780\n",
      "Validation acc: 0.5583\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 298.5859, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 325.0718, Accuracy: 0.5341\n",
      "Training loss (for one batch) at step 20: 274.0901, Accuracy: 0.5785\n",
      "Training loss (for one batch) at step 30: 297.1325, Accuracy: 0.5955\n",
      "Training loss (for one batch) at step 40: 298.7725, Accuracy: 0.6090\n",
      "Training loss (for one batch) at step 50: 282.4996, Accuracy: 0.6155\n",
      "Training loss (for one batch) at step 60: 298.2832, Accuracy: 0.6227\n",
      "Training loss (for one batch) at step 70: 305.3369, Accuracy: 0.6169\n",
      "Training loss (for one batch) at step 80: 301.7655, Accuracy: 0.6074\n",
      "Training loss (for one batch) at step 90: 346.1489, Accuracy: 0.6051\n",
      "Training loss (for one batch) at step 100: 294.4991, Accuracy: 0.6054\n",
      "Training loss (for one batch) at step 110: 292.2421, Accuracy: 0.6062\n",
      "---- Training ----\n",
      "Training loss: 90.1556\n",
      "Training acc over epoch: 0.6039\n",
      "---- Validation ----\n",
      "Validation loss: 51.8810\n",
      "Validation acc: 0.5572\n",
      "Time taken: 10.69s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 316.7217, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 305.5182, Accuracy: 0.5298\n",
      "Training loss (for one batch) at step 20: 279.6700, Accuracy: 0.5714\n",
      "Training loss (for one batch) at step 30: 288.9735, Accuracy: 0.5897\n",
      "Training loss (for one batch) at step 40: 282.8437, Accuracy: 0.6023\n",
      "Training loss (for one batch) at step 50: 276.5596, Accuracy: 0.6132\n",
      "Training loss (for one batch) at step 60: 278.5956, Accuracy: 0.6168\n",
      "Training loss (for one batch) at step 70: 303.9030, Accuracy: 0.6110\n",
      "Training loss (for one batch) at step 80: 302.9007, Accuracy: 0.6028\n",
      "Training loss (for one batch) at step 90: 298.8645, Accuracy: 0.5993\n",
      "Training loss (for one batch) at step 100: 291.9188, Accuracy: 0.6007\n",
      "Training loss (for one batch) at step 110: 310.4199, Accuracy: 0.6018\n",
      "---- Training ----\n",
      "Training loss: 88.4802\n",
      "Training acc over epoch: 0.6010\n",
      "---- Validation ----\n",
      "Validation loss: 45.5388\n",
      "Validation acc: 0.5707\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 313.6498, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 298.9264, Accuracy: 0.5504\n",
      "Training loss (for one batch) at step 20: 307.7242, Accuracy: 0.5818\n",
      "Training loss (for one batch) at step 30: 279.7607, Accuracy: 0.5970\n",
      "Training loss (for one batch) at step 40: 279.2866, Accuracy: 0.6086\n",
      "Training loss (for one batch) at step 50: 288.5401, Accuracy: 0.6160\n",
      "Training loss (for one batch) at step 60: 273.4254, Accuracy: 0.6232\n",
      "Training loss (for one batch) at step 70: 299.8099, Accuracy: 0.6173\n",
      "Training loss (for one batch) at step 80: 292.9885, Accuracy: 0.6075\n",
      "Training loss (for one batch) at step 90: 295.6919, Accuracy: 0.6040\n",
      "Training loss (for one batch) at step 100: 281.7677, Accuracy: 0.6057\n",
      "Training loss (for one batch) at step 110: 289.9331, Accuracy: 0.6049\n",
      "---- Training ----\n",
      "Training loss: 112.3565\n",
      "Training acc over epoch: 0.6042\n",
      "---- Validation ----\n",
      "Validation loss: 49.5703\n",
      "Validation acc: 0.5607\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 323.1642, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 297.0795, Accuracy: 0.5568\n",
      "Training loss (for one batch) at step 20: 274.1703, Accuracy: 0.5874\n",
      "Training loss (for one batch) at step 30: 275.1201, Accuracy: 0.5988\n",
      "Training loss (for one batch) at step 40: 278.8049, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 50: 283.2493, Accuracy: 0.6169\n",
      "Training loss (for one batch) at step 60: 292.4533, Accuracy: 0.6240\n",
      "Training loss (for one batch) at step 70: 293.3656, Accuracy: 0.6153\n",
      "Training loss (for one batch) at step 80: 294.9545, Accuracy: 0.6036\n",
      "Training loss (for one batch) at step 90: 282.1998, Accuracy: 0.5997\n",
      "Training loss (for one batch) at step 100: 300.5924, Accuracy: 0.6013\n",
      "Training loss (for one batch) at step 110: 312.0555, Accuracy: 0.6005\n",
      "---- Training ----\n",
      "Training loss: 95.5863\n",
      "Training acc over epoch: 0.5996\n",
      "---- Validation ----\n",
      "Validation loss: 74.6564\n",
      "Validation acc: 0.5656\n",
      "Time taken: 10.60s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 295.4081, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 298.6605, Accuracy: 0.5462\n",
      "Training loss (for one batch) at step 20: 284.9473, Accuracy: 0.5707\n",
      "Training loss (for one batch) at step 30: 280.2525, Accuracy: 0.5995\n",
      "Training loss (for one batch) at step 40: 288.0360, Accuracy: 0.6073\n",
      "Training loss (for one batch) at step 50: 276.8089, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 60: 306.0095, Accuracy: 0.6247\n",
      "Training loss (for one batch) at step 70: 287.8693, Accuracy: 0.6175\n",
      "Training loss (for one batch) at step 80: 313.0818, Accuracy: 0.6053\n",
      "Training loss (for one batch) at step 90: 295.6255, Accuracy: 0.6013\n",
      "Training loss (for one batch) at step 100: 285.0666, Accuracy: 0.6033\n",
      "Training loss (for one batch) at step 110: 284.5464, Accuracy: 0.6033\n",
      "---- Training ----\n",
      "Training loss: 99.9380\n",
      "Training acc over epoch: 0.6020\n",
      "---- Validation ----\n",
      "Validation loss: 44.9988\n",
      "Validation acc: 0.5672\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 314.6465, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 301.0632, Accuracy: 0.5398\n",
      "Training loss (for one batch) at step 20: 283.1462, Accuracy: 0.5711\n",
      "Training loss (for one batch) at step 30: 276.8109, Accuracy: 0.5980\n",
      "Training loss (for one batch) at step 40: 296.8082, Accuracy: 0.6080\n",
      "Training loss (for one batch) at step 50: 290.3383, Accuracy: 0.6184\n",
      "Training loss (for one batch) at step 60: 285.1001, Accuracy: 0.6201\n",
      "Training loss (for one batch) at step 70: 297.9678, Accuracy: 0.6141\n",
      "Training loss (for one batch) at step 80: 310.1076, Accuracy: 0.6046\n",
      "Training loss (for one batch) at step 90: 268.0247, Accuracy: 0.6027\n",
      "Training loss (for one batch) at step 100: 288.2058, Accuracy: 0.6034\n",
      "Training loss (for one batch) at step 110: 283.4944, Accuracy: 0.6036\n",
      "---- Training ----\n",
      "Training loss: 97.8090\n",
      "Training acc over epoch: 0.6026\n",
      "---- Validation ----\n",
      "Validation loss: 58.2356\n",
      "Validation acc: 0.5680\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 304.8584, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 283.4537, Accuracy: 0.5341\n",
      "Training loss (for one batch) at step 20: 286.0801, Accuracy: 0.5640\n",
      "Training loss (for one batch) at step 30: 294.3890, Accuracy: 0.5915\n",
      "Training loss (for one batch) at step 40: 294.1147, Accuracy: 0.6037\n",
      "Training loss (for one batch) at step 50: 290.5558, Accuracy: 0.6141\n",
      "Training loss (for one batch) at step 60: 288.8304, Accuracy: 0.6221\n",
      "Training loss (for one batch) at step 70: 306.8539, Accuracy: 0.6130\n",
      "Training loss (for one batch) at step 80: 304.2975, Accuracy: 0.6030\n",
      "Training loss (for one batch) at step 90: 283.6737, Accuracy: 0.6004\n",
      "Training loss (for one batch) at step 100: 300.1797, Accuracy: 0.6015\n",
      "Training loss (for one batch) at step 110: 290.2581, Accuracy: 0.6016\n",
      "---- Training ----\n",
      "Training loss: 93.3837\n",
      "Training acc over epoch: 0.6013\n",
      "---- Validation ----\n",
      "Validation loss: 89.6462\n",
      "Validation acc: 0.5591\n",
      "Time taken: 10.83s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEjCAYAAADZk82GAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAACSm0lEQVR4nO2dd5hcVf2437NTt/dk03shhVQSSCgJoFQJIC2oELAgUhQFFERF1N9XBRVRinREJPQqPRASenrvySbZlM32Pv38/jj3ztyZnd2d7SXnfZ55ZubWM3fvns/9dCGlRKPRaDQaK0ndPQCNRqPR9Dy0cNBoNBpNI7Rw0Gg0Gk0jtHDQaDQaTSO0cNBoNBpNI7Rw0Gg0Gk0jtHDQaFqBEGKeEKKou8eh0XQ2WjhougwhRKEQ4vTuHodGo2kZLRw0mj6CEMLe3WPQ9B20cNB0O0IIlxDiXiHEQeN1rxDCZazLE0K8KYSoFEKUCyGWCyGSjHU/F0IcEELUCCG2CSFOa+L45wgh1gghqoUQ+4UQd1rWDRdCSCHElUKIfUKIUiHELy3rk4UQTwohKoQQm4HjWvgtfzfOUS2EWCWEOMmyziaEuF0IscsY8yohxBBj3UQhxPvGbywWQtxuLH9SCPF7yzGizFqGNvZzIcR6oE4IYRdC/MJyjs1CiAtixvh9IcQWy/rpQohbhBAvxWx3nxDi7839Xk0fRkqpX/rVJS+gEDg9zvK7gC+AfkA+8BnwO2Pd/wEPAQ7jdRIggHHAfmCgsd1wYFQT550HTEY9DB0LFAPnW/aTwCNAMjAF8ALHGOv/CCwHcoAhwEagqJnf+G0gF7ADPwMOA25j3S3ABmPswjhXLpAOHDK2dxvfZxv7PAn8Pua3FMVc07XG2JKNZRcDA43feylQBwywrDuAEnICGA0MAwYY22UZ29mBI8CM7r5v9Kt7Xt0+AP06el7NCIddwNmW72cAhcbnu4DXgNEx+4w2Jq/TAUcrx3Ev8DfjsykcBlvWfwVcZnzeDZxpWfeD5oRDnHNVAFOMz9uABXG2WQisaWL/RITD1S2MYa15XuBd4MdNbPc28H3j87nA5u6+Z/Sr+17arKTpCQwE9lq+7zWWAdwN7ATeE0LsFkL8AkBKuRP4CXAncEQIsVgIMZA4CCFmCyE+EkKUCCGqgB8CeTGbHbZ8rgfSLGPbHzO2JhFC3GyYbKqEEJVApuVcQ1CCMJamlieKdXwIIa4QQqw1THGVwKQExgDwFErzwXh/uh1j0vRytHDQ9AQOokwbJkONZUgpa6SUP5NSjgTOA35q+haklP+VUp5o7CuBPzVx/P8CrwNDpJSZKDOVSHBsh1ATqnVscTH8C7cClwDZUsosoMpyrv3AqDi77gdGNnHYOiDF8r0gzjbh0spCiGEoE9n1QK4xho0JjAHgVeBYIcQklObwTBPbaY4CtHDQdDUOIYTb8rIDzwJ3CCHyhRB5wK+B/wAIIc4VQowWQgjURBsEQkKIcUKIUw3HtQdoAEJNnDMdKJdSeoQQs4DLWzHe54HbhBDZQojBwA3NbJsOBIASwC6E+DWQYVn/KPA7IcQYoThWCJELvAkMEEL8xHDOpwshZhv7rAXOFkLkCCEKUNpSc6SihEUJgBDiKpTmYB3DzUKIGcYYRhsCBSmlB3gRJUy/klLua+Fcmj6MFg6aruYt1ERuvu4Efg+sBNajHLarjWUAY4APgFrgc+ABKeVHgAvlLC5FmYT6Abc1cc4fAXcJIWpQguf5Voz3tyhT0h7gPZo3tbwLvANsN/bxEG3y+atx7veAauAxlBO5Bvga8A3jt+wA5hv7PA2sQ/kW3gOea26wUsrNwF9Q16oY5Yj/1LL+BeAPKAFQg9IWciyHeMrYR5uUjnKElLrZj0ajUQghhgJbgQIpZXV3j0fTfWjNQaPRAGDkj/wUWKwFg0ZnVGo0GoQQqSgz1F7gzG4ejqYHoM1KGo1Go2mENitpNBqNphFaOGg0Go2mEVo4aDQajaYRWjhoNBqNphFaOGg0Go2mEVo4aDQajaYRWjhoNBqNphFaOGg0Go2mEVo4aDQajaYRWjhoNBqNphFaOGg0Go2mEVo4aDQajaYRWjhoNBqNphFaOGg0Go2mEb26n0NeXp4cPnx4o+V1dXWkpqZ2/YDioMcSn54ylubGsWrVqlIpZX4XDwmIf2/3lGsGeixN0VvGktC9LaXsta8ZM2bIeHz00Udxl3cHeizx6SljaW4cwErZg+7tnnLNpNRjaYreMpZE7m1tVtJoNBpNI7Rw0Gg0Gk0jtHDQaDQaTSN6tUO6J+L3+ykqKsLj8QCQmZnJli1bunlUCj2W+OPYs2cPgwcPxuFwdPdwNJoegxYOHUxRURHp6ekMHz4cIQQ1NTWkp6d397AA9FjiUF1djc/no6ioiBEjRnT3cDSaHoM2K3UwHo+H3NxchBDdPRRNAgghyM3NDWt6Go1GoYVDJ6AFQ+9C/700msb0SeGw8nCAR5fv7u5haDSaXkIgGGLr4epuO78/GOK/X+7DGwi2ar9QSPLh1mJCIdnhY+qTwmFtSZDHPtnT3cPQaDQ9mMNVHi584FO2F9fwxvqDnPX35ew8UtPifsXVHs6//9OEtk2U9zYVc/srG3h59YFmt6us93Hn65s46+/L+Xh7Ce9sOszVT65k+c7SDhuLSZ8UDil2qG7wd/cwuoWysjKmTp3K1KlTKSgoYNCgQeHvPp+v2X1XrlzJjTfe2OI55syZ01HDBeDJJ5/k+uuv79BjajQt8dDHu1i9r5JPdpSy7XAtUsKHW4+E1+8prUMlE0fzwsr9rN1fyatrDrZ7DOv2V1LvC7B8RwkAb2041Oz2936wg6e/2EthaR3/+WIvHxnjXb+/st1jiaVPRiulOAR1vgCBYAi7rU/KvybJzc1l7dq1ANx5552kpaVx8803AypCKBAIYLfH/7PPnDmTmTNntniOzz77rMPGq9F0FG9vOIQEzp48oNntPttVSp03yLNf7QOUECip8QKwdFsJPzh5FC+uKuLmF9bx0LdncMbE/pTV+chLcyGlDD/dL9tRws1njEtobJsPVvPPj3awbn8V88fnc8OpY7AnCS588DMumTmE5TtKEQI+21VGRZ2P7FRno2OEQpJ3Nx3m1PH9GJjpZvGK/aS7Vfj1xoNVAHy6s5TfvrGJ608dQ0ZCI2uaPikcUu3KwVjjCcS9yF3Fb9/YxIb9Fdhstg475oSBGfzmGxNbtc+iRYtwu92sXLmSk08+mcsuu4wf//jHeDwekpOTeeKJJxg3bhxLly7lnnvu4c033+TOO+9k37597N69m3379vGTn/wkrFWkpaVRW1vL0qVLufPOO8nLy2Pjxo3MmDGD//znPwgheOutt/jpT39Kamoqc+fOZffu3bz55pstjrWwsJCrr76a0tJS8vPzeeKJJxg6dCgvvPACv/3tb7HZbGRmZrJs2TI2bdrEVVddhc/nIxQK8dJLLzFmzJg2XdeWEEKcCfwdsAGPSin/GGebS4A7AQmsk1Jebiy/ErjD2Oz3UsqnOmWQRwlSSg5VeRiQ6Q4HE1R7/Nzy4nqSBJw8Np9r/7OKcyYP4LJZQwF4f3MxtiSYOTyHKx77ikBIkiSgIMPNntI6SmuVcFhRWM6qveXc8eoGAD7ZWUKtN8DNL6zjn5dPY2BWMrtL6xiZl8qGA1WU1/nIMeaYHcU1rCsJMM8YZ7XHz9p9lZw4Oo8fL17DkRovM4Zl8/yKIvaW1XP5rKEEQ5LnV+4nGJIsnDWEZ7/az60vrSc7xcF3jh/O5MGZVHv87CmpIxBSv/vmr49jQKabpz7fi7fWS4rTxsYD1byz8RDXPrOaJCG4/eUN3Hl8+/J2+qRwSDGuSbXH363CoSdRVFTEBx98QFZWFtXV1Sxfvhy73c4HH3zA7bffzksvvdRon61bt/LRRx9RU1PDuHHjuPbaaxsliq1Zs4ZNmzYxcOBA5s6dy6effsrMmTO55pprWLZsGSNGjGDhwoUJj/OGG27gyiuv5Morr+Txxx/nxhtv5NVXX+Wuu+7i3XffZdCgQVRWVgLw0EMP8eMf/5hvfetb+Hw+gsHWOfMSRQhhA+4HvgYUASuEEK9LKTdbthkD3AbMlVJWCCH6GctzgN8AM1FCY5Wxb0WnDPYo4Jkv93HHqxuZMiSL356nHpSe+2o/td4AANf+ZxXLd5SyvbiGb84YTCAoufmFdbgdSfz2vIkEQpKffW0s4wdk8L/1B/lqTzkV9X7GF6Sz9XANFz/0OblpLsYVJPPF7nL2lzcA8LPn15GZ7FDHWTCR7zz2Fct3lLBg6iAAbnlxPRuLvCw6V5m0v/3ol2w4UMXXJ/Rnx5Fa/n7ZVBZMHcQf397Ko8t3k5/uwmlPIhAMAfDDU0axorCCJVuKcTtsPL+yiHS3nQZfkEBIkpXiwJ4kOP2Y/qS6bGSlOKis93P5rKE8+ske7nlvOyPzUnnw2zO44P5PeWyDlwvPkCQltS0ar48KB3UxqrrZ7/Cbb0zsMcleF198cViDqaqq4sorr2THjh0IIfD741+nc845B5fLhcvlol+/fhQXFzN48OCobWbNmhVeNnXqVAoLC0lLS2PkyJHhpLKFCxfy8MMPJzTOzz//nJdffhmA73znO9x6660AzJ07l0WLFnHJJZdw4YUXAnDCCSfwhz/8gaKiIi688MJO0xqAWcBOKeVuACHEYmABsNmyzfeB+81JX0ppGq/PAN6XUpYb+74PnAk821mD7ctU1fv5y3vbGF+QzpFqD997aiU/npLE4xv3MGtEDiU1XpbvKCUrxUFxtZe3NhzCGwhR1eCnqgHu/2gXqU4b15wyCqc9ic0Hq3l1rfIdXDRjMI8u38OQnGT+ftk0Xlt7kD+9s5W9ZXVcOH0Qu0vqyE118r2TRjJrRA5ZKQ4e/7SQ8QUZePxB1hp2/7c2HOKNdQfZeriaKYMzeW9zMSPyUjn32IEAnH5MPx76eBevrjnAnFF5FGS62XigimG5qbz0wzmEpMRmEzy/Yj9FFQ2kOG2kOG3c+8EOThqTR6bx9HvJzCFsPVzDqeP78egne9h5pJY7zjmGsf3T+c03JvLF+i20J4apbwoHw6xU3RDo5pH0HKx13X/1q18xf/58XnnlFQoLC5k3b17cfVwuV/izzWYjEGh8PRPZpiN46KGH+PLLL/nf//7HjBkzWLVqFZdffjmzZ8/mf//7H2effTb/+te/OPXUUzvj9IOA/ZbvRcDsmG3GAgghPkWZnu6UUr7TxL6DOmOQRwN/+2A7lQ1+/vO92ThsSZz3z0/41ach7EmCv1wyhU0HqvnDW1u4+6Ip/N9bW/j7BztIShIMz03hQGUDGw5Ucdr4fjjtyhc5Ij/yfzG2fzof3zoPpy0JIQSzR+YA4A9KLpk5hONH5kaN5fazjuGuNzdz1t+XMTQnhVSnjRRbiD++vZWqBj+/P38S500dyM3Pr+Py2UOxGU/w04Zmk5PqpLzOx/Ejc/jhKaMIGKGo5sQP8L2TRkad7+zJA8hMjqy//exjgMhDsMMmuGCaurUuOW4I/ep2hc/ZFvqmcHBE7JCaxlRVVTFokLqJnnzyyQ4//rhx49i9ezeFhYUMHz6c5557LuF958yZw+LFi/nOd77DM888w0knnQTArl27mD17NrNnz+btt99m//79VFVVMXLkSG688Ub27dvH+vXrO0s4JIIdGAPMAwYDy4QQk1tzACHED4AfAPTv35+lS5dGrTf9PD2BrhhLnV+S6hDsrQ6y9kiQvGTBkxt8nDbUTsn2NQB8f5KDz4o8fHNcMr79GxkRkvz8ODf24s2cNzTIYxu91PjgOxOcrBVJbCgNUiAqw2Mvr4qYIg/v3MDnByMBLIGQxGUDexLUFa5n6b7oibYf8Me5Tp7fDsuK6jl9qB0XIf63z8+IzCQGNuxm9Rd7uHwocGgzSw9FFM1jMkN8WgfOqn18srz58NVEGJKexOA0wYaVn4eXtfdv1DeFg/GrjtZw1pa49dZbufLKK/n973/POeec0+HHT05O5oEHHuDMM88kNTWV4447LuF9//GPf3DVVVdx9913hx3SALfccgs7duxASslpp53GlClT+NOf/sTTTz+Nw+GgoKCA22+/vcN/i8EBYIjl+2BjmZUi4EsppR/YI4TYjhIWByDsozT3XRrvJFLKh4GHAWbOnCljNbqlS5c2qeV1NR09luU7SvjnhzvJTnHy87PGs724huv/s4q7zpvIw5t3c6BS/S9PGJDB/d+fg9uhTKTzgBkxYzndeJ8PXPfNEJsPVjN5UCavrTvAzS+s5/vnzGF4ntIYar0B7vz8XRU5dMa8RtGNVzRsJtVl57RTxzY59nO/DjuP1DAkJ4U33/+YvUEH/3fBsUwenNnkPjmjK3lk+R4WfWNKWItpD28d78dhSwpfF+iAv1FL3YB68qupTnBvvf+hHPbzN+W/Pt7ZZCekzmLz5s1R36urq7t8DE3RlWOpqamRUkoZCoXktddeK//6179221iawxxH7N9Nyki3LNRD1G5gBOAE1gETpeVeRPkRnjI+56FMSblADrAHyDZee4Ac2YZ7u7d0GWsOjz8gf/rcWrn1UPTf/9uPfiEn/fodOeaXb8mfPb9WLnr8Szns52/KYT9/U474xZvyldVF8v+9tVnuK6tr81hCoZA8VNnQaPlxv39fzrs7sWO0RG/5G5FAJ7g+qTm4bZAkut8hfTTzyCOP8NRTT+Hz+Zg2bRrXXHNNdw+pzUgpA0KI64F3Uf6Ex6WUm4QQd6H+yV431n1dCLEZCAK3SCnLAIQQvwNWGIe7SxrO6aOJH/x7JcePzGVITgovrS7C4w9y/7emAyrr9/NdZXzvpJFUe/y8tKqIQEhy+eyhbD9cw6nH9OP8aYM4v52uGiEEBZnuRsuPH5lLirPjws37Cn1SOAghyEh2aId0N3LTTTdx0003RS174okn+Pvf/w5AKBQiKSmJuXPncv/993fHEFuFlPIt4K2YZb+2fJbAT41X7L6PA4939hi7m/3l9WQkO6KcpgC7S2p5b3Mxq/dVctKYPADe3XSYI9Ue+mW4eX9zMYGQ5OzJBdiTkvjvlyo57coThjOuoPMj/e5bOK3Tz9Eb6ZPCASAz2aEd0j2Mq666iquuugroOf0cNB2DNxDk/Ps/ZdrQbB69MjrL/v3NxQCU1np5de0BZgzLZtXeCp76vJCfnD6W19YeZFBWMpMHZaoooRE5NPiDXSIYNE3TZ4VDhtuhHdIaTRfxweYjlNX5+GBLMXtK6xiRFwkRfX9zMeML0imv83Gkxsv3ThxBdoqT+z/axb8/20uNN8BPTh8TznZ+9MqZdEKRUU0r6bvCIdlOtUeblTSaruCFVfvJS3NR3eDnyU/38NsFkwClLazaV8GNp45BCHji00JOHpvPvHH9eHF1EZ/vKuXCaYM57Zh+4WOZ9YI03UvfFQ5uB0eqa5vd5vkV+xmYlcyJhh1Uo9G0TI3Hz86KYDg+t7jaw7LtJfxo3mgOVXn49xd72Xiwml+fO4E31x9ESjhrcgFj+qVz1dwRpLrUtPOd44fxneOHddvv0DRPp5UsFUI8LoQ4IoTYGGfdz4QQUgiRZ3wXQoj7hBA7hRDrhRDT23v+DLej2WglKSW/+99m/vnRjvaeSqM5qnjo41384UsP5XWqBPzbGw4RknDB9EH8+hsTuPHUMRyqbODbj37JY5/s4fLZQxlfkIEtSTRyVmt6Lp1Zz/pJVOx3FEKIIcDXgX2WxWehEobGoDJEH2zvyTNTlEP6T+9sDUc/WDlU5aHGE2DTgepO6aLUXcyfP5933303atm9997LtddeG3f7efPmsXLlSgDOPvvscFE7K3feeSf33HNPs+d99dVX2bw5kgH661//mg8++KCVo28a3fOh5/DVnnJVQXCvqh34/pZiRvdLY1R+GpnJDm762lheuHYOmSkO+me4+cVZ47t3wJo20WnCQUq5DIgXz/034FaIqgm1APi3kZ/xBZAlhGi+KHsLZLjtePwh/vXxLu7/aGejph3bilUXpxpvgH3l9e05VY9i4cKFLF68OGrZ4sWLE6qM+tZbb5GVldWm88YKh7vuuovTTz+9mT00vRFvIMi6ItU7YOXecqrq/Xyxu5yvTegftd2grGTe/vFJ/O/Gk8jQPoReSZf6HIQQC4ADUsp1MU3dmypO1qgtUkv1Z0DVFDlcVghASMKBygYefuVDluzzM2+Igwm5Nt7eHemK9tz7nzN7QMdciszMTGpqlOBxffQbkos3EejA/vWhfhPxzv9tk+vPOOMMfvnLX1JWVobT6WTv3r0cOHCAf//73+EeDgsWLOCXv/wlAMFgkLq6Ompqapg0aRIff/wxubm53H333fz3v/8lPz+fQYMGMW3aNGpqanjyySd54okn8Pv9jBw5kocffpgNGzbw2muvsXTpUu666y6efvpp/vznP3PmmWdy/vnns3TpUu644w4CgQDTp0/nb3/7G3a7nWHDhrFw4ULeeecd/H4///73vxk7Nn6ZAo/Hg8/no6amhr1793LddddRVlZGXl4eDzzwAEOGDOGVV17hj3/8IzabjYyMDN555x22bNnCtddei9/vJxQK8fTTTzN69OjwcYPBIDU1NXg8nh5Tt6gns/FANb5ACJuAVYUVfFRwhGBINhIOoB3LvZ0uEw5CiBTgdpRJqc3IFurPgKopMnPQGP6zZS356S7K63w8ukVSUhPkkM/JBwtO4rXiDeSllVLd4CeUNYh5845pz7DCbNmyJRK/73ASEGC3deBldjhxNpMfkJ6ezuzZs/nkk09YsGABb775Jpdeeim33347DoeDlJQUTjvtNPbs2cOxxx6LzWYjNTWV9PR0hBCkpaWxfft2XnnlFdavXx+e0I8//njS09O5/PLLueGGGwC44447eP7557nhhhtYsGAB5557LhdddJEapsNBcnIyDoeDH/3oRyxZsoSxY8dyxRVX8J///Ifvfve7CCEYNGgQa9eu5YEHHuDBBx/k0Ucfjfu73G43TqeT9PR0brvtNq6++upwz4fbb7+dV199lbvvvpv3338/3PMhPT2dp59+mp/+9KdRPR+Sk5PDxzXzLdxuN9Om6WSolli1VxkDZg2wsaqoiqe/2Et+uoupg7O6d2CaDqcrNYdRqNo0ptYwGFgthJhFYoXNWoXp+PrGsQPZXlzDJztLmTAgg82Hqnny00K2Ha5h4sAMyut8bCiqYu3+So4ZkI7L3oFp9Gf9kYZuSPYyTUsLFixg8eLFPPbYYzz//PM89NBDhEIhDh06xObNmzn22GPj7r98+XIuuOACUlJSADjvvPPC6zZu3Mgdd9xBZWUltbW1nHHGGc2OZdu2bYwYMSKsEVx55ZXcf//9fPe73wUI92aYMWNGuI9DS/TQng9HBSsLKxiWm8LM/kE+P+hl1d4K7rl4Spsbymh6Ll3WYFlKuUFK2U9KOVxKORxlOpoupTwMvA5cYUQtHQ9USSmb77TdAiPzU0l32blg2iAuOW4IBRlunrzqOE4d34+/fbCdHUdqGF+QzqRBmXy2q4zz7/+U//e/LY2OEwpJ7nx9E1sOVbdnOF3KggULWLJkCatXr6a+vp6cnBzuueceXn/9ddavX88555yDx+Np07EXLVrEP//5TzZs2MBvfvObNh/HxOwH0RG9IB566CF+//vfs3//fmbMmEFZWRmXX345r7/+OsnJyZx99tl8+OGH7TrH0UwoJFm5t4IZw7IZk23DZU9i0ZzhXDRjcMs7a3odnRnK+izwOTBOCFEkhPhuM5u/hap6uRN4BPhRe88/LDeV9Xd+ncmDMzlvykC+uP00+mW4+fNFx5Kb6sIflIztn865xw5g6pAsThiZyzNf7mNvWV3UcfZX1PPkZ4X8v7caC46eSlpaGvPnz+fqq69m4cKFVFdXk5qaSmZmJsXFxbz99tvN7n/yySfz6quv0tDQQE1NDW+88UZ4XU1NDQMGDMDv9/PMM8+El6enp4d9LVbGjRtHYWEhO3fuBODpp5/mlFNOadfvM3s+AHF7Ptx1113k5+ezf/9+du/eHe75sGDBAtavX9+ucx/NbD5UTXmdj5PG5JHhFHx+22n85hsTuntYmk6iM6OVFkopB0gpHVLKwVLKx2LWD5dSlhqfpZTyOinlKCnlZCnlyo4YQ4zTG4C8NBePXjmTOaNymTs6j7mj83j1urn8/bKpOGxJ/OW97QD4gyGklBRVqP6xy3eUsvlg79EeFi5cyLp161i4cCFTpkxh2rRpzJgxg8svv5y5c+c2u+/06dO59NJLmTJlCmeddVZUP4bf/e53zJ49m7lz5zJ+fCRE8bLLLuPuu+9m2rRp7Nq1K7zc7XbzxBNPcPHFFzN58mSSkpL44Q9/2K7f9o9//IMnnniCY489lqeffjpczO+WW25h8uTJTJo0iTlz5jBlyhSef/55Jk2axNSpU9m4cSNXXHFFu859NLNsRwkAc0erpNGcVGfc/zFNH6Glmt49+dVUP4e21lT/7eub5Jjb35JVDT555r3L5O/e2CSf/XKvHPbzN+Wo2/4nb3x2dYvH0P0cEqOnjCWRfg7d8eqJ/RwWPvy5PONvH/eIsVjRY4lPe/s5dJnPoTdwzrEF+IIh7n5nG1sOVbNibwVFFQ3YkgRXnDCcN9YdZE9pXcsH0mi6koAPSjs307/eF2BlYQUnj83v1PNoeg5aOFiYNiSb/HQXT3+xF4CdxTXsr6hnQKaba+eNwmlP4h8f6nIbnckTTzzB1KlTo17XXXdddw+rZ7PhBXhwDng6z+z5zsbD+IKhcD8GTd+nzxbeawtJSYIzJvbnP1/sI81lp9YbYMWecobmppCf7uI7xw/j0U/2sHpvBT+aN5pLjhsS9zhSSm2LbSPWng9dhZS9vHxKQzkEfeCtBndGxx/eF+Tud7cxaVAGc0dp4XC0oDWHGM6fOghbkuD6U1UW7cEqD4OzVbz/9aeO4eq5I6j1Bnl93cG4+7vdbsrKynr/hHOUIKWkrKwMt7tx+8heQ9DI9vd1fBmYN9Yd5NpnVnGoysOvzpmg8xmOIrTmEMPM4Tms+83X8QdC/PHtrQAMMYRDZrKDX507gcp6P8uNyI1YBg8eTFFRESUlar3H4+kxE48eS/xxZGVlMXhwL47VDxr5If6OFQ6ltV5ueHYNWSkOrp8/mtkjczv0+JqejRYOcUhz2cEFeWlOSmt9DM5Ojlo/tn8aL60uoqreT2ZKdP0Yh8PBiBEjwt+XLl3aY8oy6LH03HG0i5BRmt7f0KGHNcO477loCqfHqZ2k6dtos1IzjO6XBtBIOIzpr5bvONI46Uuj6XJMs1IHaw4HDOEwKOb+1xwdaOHQDGP6qZpIg3NS4i7fXtx8pzmNpksIm5Ua2FFcw6c7SzvksAcqlbAZmKWFw9GIFg7NcNbkAr4+oT8FGdG28UFZyaQ4bWwv1prD0YIQ4kwhxDajW+Ev4qxfJIQoEUKsNV7fs6z7sxBikxBii9HxsGO9uhaz0j8/2sktL6zrkMMerPSQ7rJ3bfe2jS+DV/9f9QS0cGiGOaPyePiKmdhiIjSSkgRj+qWx80gtVfV+HZnUxxFC2ID7UR0LJwALhRDxigo9J6WcarweNfadA8wFjgUmAccB7SsuFYvFrFRe56O0ztch92RRRUPXag1VB+DFq2Dz6113Tk2TaOHQRkb3S+fLPWVM/d17PPTx7qh1gWCIslpvN41M0wnMAnZKKXdLKX3AYlT3wkSQgBtwAi7AARR36OgsZqXKej++QIgGf7Ddhz1Y2dC1/oaAUeE30LGOdU3b0NFKbeSkMXl8uLWYfulu/r5kO+dNHcgg4ynryc8K+b+3t/Krc45hmNYq+gLxOhXOjrPdN4UQJwPbgZuklPullJ8LIT5CdTUUwD+llHFL/LbU5bC2tjZut7rxh4ooAHZv28jh8qEAvLVkGXnJ7Xv221taR4Gjvsluix3dOS+lbh+zgB3btnCgLvFjd8ZY2kqjscggM1fexN5hF1PS76TuHUsr0cKhjZw/bRDnTxtEUUU9p//1Y/709lbuW6hCIlftrSAYktz5xmam97MxYYaH/hlti+k/Uu2hXxv31XQpbwDPSim9QohrgKeAU4UQo4FjUA2sAN4XQpwkpVweewDZQpfDpUuXEq/zISVPQTGMHFKAZ7cNCDD+2BlMGpTZ5h9T6w1Q9867HDdhNPPmjWq0vsmxtIfDG2AFjBkxlDFzEz92p4yljSz/4C1OOvI4nPVnSC+A+nL4eC8Tc4LQxWNs73XRZqV2Mjg7hW9OH8wHW4oJBEMAbDtcw9cn9Oe2s8azoTTITc+tbdOxP9p2hOP/bwlFFR2f+appFS12KpRSlkkpTVvio8AM4/MFwBdSylopZS3wNnBCh47O8DkEffXUeJWJqbzO19weLXKwshvCWIOGYz3YvrF3J+k1u2Dza7DvC7WgXrVV7Y1Odi0cOoDjR+ZS7wuy6WA1Hn+QwrI6xg/I4JpTRnHCQDs7j7Qt5HVlYTkhqaJGNN3KCmCMEGKEEMIJXIbqXhhGCDHA8vU8wDQd7QNOEULYhRAOlDO6YztHhZRA8DdE7rOK+vZNsOEch6w4WquvnpS6fe06flxChp8k0HuFg8NvFD/0VKr3hgr1roXD0cmsETkAfLWnnJ1HaglJGF+gciEyXYLSWi/BUOt9D1sOqRuq0vhHl1Ly788LqWrwd9DINYkgpQwA1wPvoib256WUm4QQdwkhzAbbNxrhquuAG4FFxvIXgV3ABmAdsE5K+QYdifHE7fNEyslXtFFzqPcFeParfTyyXAVZDMpKabzRqieYsepnEOjgoItQ79cc7AFDCHiq1HuDqTm04gGxvlyF9HYz2ufQAfTPcDMsN4WvCsvJSXUCMLa/Eg5ZLkFIQlmtt9W+A7PznCkM9pTW8evXNpHitEf17X1t7QHeWHeIR6+cGfc4VfV+vv3Yl1w6PNTq36ZRSCnfQrWztS77teXzbcBtcfYLAtd06uBMs5I3IhzK61v/AHGwsoFv/OMTyup85KY6OXNiAf3SXY03rD2CLeRTT8P2OOvbStis1Hsffhx+Qzg0VBrvpubQinLq656Fd2+HUfMhObtDx9catHDoIGYNz+GDLcUMz03BaU9ieK564spyqRyJIzVeDld76JfupiCzZSFRXufjcLUyJ5nCwXxv8AWitl26rYSPth1pslT4xoNVbDhQxfE5zrb/QE3PxTArBb0R31RlG8xKn+4spazOx6NXzOS0Y/o1XXbeLNPhrYHUDizh3Qc0h4hZydAc2uJzMAWLt7ZbhYM2K3UQx43IoaLez8urDzA6Pw27TV3azLBw8HDl41/x1/e3JXS8LYciTxrVhlCo9qhJwOOP1gD2l9cTDEl8wfiawd4y9c/sbX/ou6YnYjxpS6Nkt9OW1CaH9M4jtThtScwbl998PxKfRTh0JGa+Rq8WDqZZqVK9t8XnYGoZHVwrq7Vo4dBBfOPYgZw9uYCyOh+TLSGE2YZw2HKohop6P4Wl0X/wqno/f3t/O75A9MRumpSctqSwxlDjMTSHmAQns3pmgy/+7L+3XJkbtHDooxiTqTQmk6G5KW1ySO84UsvI/NTwg02T+Az7eUcLh1AfMis18jm0RjgY2/q6tyWxNit1EMlOGw98awbbi2ui7LSm5vDF7jIA9seEpb6ypoi/L9nBjGHZUf15Nx+qpn+GC7fDRmVYOJiaQ2SW9waCFNco81ODP0hWnLHtC2sOOiGvT2KYlZL8DSQJGJqTwuGq1ke47ThSw9QhCZgxzCdaXwcXngz1Bc3BeOo3TUOmWak110prDn2Tsf3TyUqJ2PbtSYLsFAer9ir18nC1B28gMrl/uUfdPJsORjus9pXXMyo/jcxkRyPNwWpWOljpwUzCrm9Cc9hXrm6yJlZrejvGk3ZS0ENmsoOcVGerNYd6X4CiigbGGGXqm8V8otVmpUY0jlYyzEr++sjva4mw5qCFQ5+nX7o7PHFLS96ClJKvwsKhKmqfkhov/dJdMcLB0BwswmV/eeQGimdWklI2qTl4/MF2J0tpegCGOcYebCArxdkm4bC7pA4paaVwaEUETiL0ZbMSgC9BYerp45qDEOJxIcQRIcRGy7K7hRBbhRDrhRCvCCGyLOtuM8ohbxNCnNFZ4+oO+mVEh/uZE/quklrK6nw4bUlhHwOoCb2kxkt+uouMeMLBYlaymqniFVurqPeHs2ZjfQ4PLN3FBQ982o5fpukRGJOpI+QlK8VBVooDjz/UpA8qHmbjKrORVbOEhUMHm5V6e4Z0MIAjYFwTT6V6EmyoQJXUInFNy9yurwoH4EngzJhl7wOTpJTHooqT3QZglD++DJho7POAUSa5T9AvXYWuThqUAUQm9C92q6eKc6cMYE9ZHbXGJF7nC9LgD5Kf7iIr2UFVvRmtZJqVIv/0pjMa4puV9pZFnFq+GM2hqLy+TbZpTQ/DmFSd0kOW206OYdZsjfawo7gWe5JgWG5qyxv7OylaqTtDWQ9vhKe+0b5Wq2aEUvpA9RsCHqivgIyBanlrhUM3O6Q7TThIKZcB5THL3jOyTQG+IFKMbAGwWErplVLuAXaiyiT3CUzN4fgRuThsgv3l6gZcuq2Efukuzpk8AClV+GowJDli5DfkW8xKUkqL5hDxOUSblRrbNE1/Q7LD1khzqPb48QZCuh9Fb8eYVG2EyE1JCvu8WmMy3HmklmG5KThailSCyKTV0Q7p7vQ5FC6HPcugcn/L2zZFvQo6IWekeq8rUaakLFUpN3HhkKBZacld8J+LWj/OBOnOaKWrgeeMz4NQwsKkyFjWiJbKGkPPK+FbXapuuGDFAXJcsHpbIffVFvHBFi8LRjmo3rsJgCsf/Zxku+B7k5UwObhrG6U1QQIhybtLlrL/sBIah46Uhn/f5r0N5LoFZR7J6vWbcJdG51Es3an+0QpSJPW+YNR12XdYCan3P1yK09axzclaoqf8jXrKONpF0A8iCWSIfFeAfCNabm9ZfcKVWfeV1zMiLwGtATrR59AO4bDjAxhxUtsztmuPqPdE/QLxMCOTcobD3k+golB9zxoK+z5PzAwXDFiiwVoQDofWwYHVbR1ti3SLcBBC/BIIAM+0dt+WyhpDzyrhu3TpUk4aPo5ntq7mrBNnsNO3g6KKBhbvDDK6Xxp/vupEnLYk/r5hKQerPJR5QtjyhwNbOf3EWazZV8Hz2zZw7MzjsW1eCVSTnJbBvHlzAbj5kw84dlgGH20rYdioMcybPSzq/I/v/oqR+fUMyU5hX3FZ1HX5vzXLgBpmnTA3KsKqK+gpf6OeMo52EfQjXekITxW5zhBTBmeSl+bk9XUHOOfYAS3uLqVkX3k9c0YlkO0c8EXMPz0kzyGlrgieuQ4ufBSOvbht564rUe+J+lHqyiA1N3qZ6Xw2NYfyPeo9rDkkIEytwsnfglmpoVKdM+gHW8e3cu3yaCUhxCLgXOBbMmLPaLEkcm/m1PH9+cvFU5g5LJvB2SnsKa2jxhPgLxdPwWW3IYTgrR+fxBOLjgMivgjTrARQWe+3+ByUWSkYkpTVecN24lgHZK03wBe7yjhtfD9SnLZGPgfzeN6ArrnUqwn58dlVLa9xuTbstiQWTB3Eh1uPJFSAr6TWS70vyLDcOEX2YrFOWB3ukG6b5pDccEh9qD3c9nPXlar3ROz8pTvgntFQaARzSAlv/AQ2vaK+m8KhwhQOxgNbIsLUYxEgLWkOZpisKdg6mC4VDkKIM4FbgfOklNZf/jpwmRDCJYQYAYwBvurKsXUmTnsS35wxmKQkwenH9GPWiBxeuW4OU4ZkhbdJcdrDkSIrCstx2ARZyQ4yU5RwqGrwN4pWUr4IGGzU3I8VDsu3l+ALhjjtmP7xfQ4NjR3cml5GKAgyRA3qAWFyP3W/fHP6YPxByZvrD7Z4CLO8ytBEhIN1wmqt5rDlTXjyXGjKx9VGh7TbY3RdNW3+bcGcYBPxoxxYDTKkmhOBGu+qJ2DDC+p79gj1bmoO2a0QDtZtWvI5mA7wWkvX2QOr4J/HKTNbO+nMUNZngc+BcUKIIiHEd4F/AumoblhrhRAPAUgpNwHPA5uBd4DrjGqWfY7TjunP89ecwOh+6Y3W5ae5SHPZqfEEyEtzkZQkwppDPOFgOhzz0104bIL6mEn+gy1HyEx2MHNYNslOW1SeQyAYos4QJrG1mjS9CMMEUxZQtvYsh/qbThiYwfDcFD7ZWdriIUzhMCwnEeFgebJurUO66Cvl+G1qvzZWZXV7DH9Bu4SD6XNI4DeZfr1Ko6eFZRIPCQdkGO5SU3PINOJuOlI4SBnJwq41BNvez5XwLd0O+79s+Vwt0Gk+BynlwjiLH2tm+z8Af+is8fQGhBCMzE9lfVFV2KloCofDVQ3hnhAewwxkhirmpDpxO2yNNIdlO0o4ZWw+dluSYVaKrDMFDWjNoVdjPG0f8LgYB1ETSv8MNxV1LU+0+8rqSBKqq2GLGGYlvz0NR2s1BzNMtL4cXI0fjtquOZjCobz5DZvDNCslYiorMYXDXvVualPOdBrsmaS6jSCAQ+vBnQkZg8GZ1krhIJo3K3lrwHx+ri1WJrk3fwKp+WpdzUFoe5dYQGdI9zhGGhEj+WnRwsHMZ0h12hppDtkpTlKc0cIhEAxRUuNlZL46XrJDCQfTzWNtGBQrHJZtLwmH02p6OMZTdkXQKANvidPPTkksU3pveT0DMpNx2hMPY/U5s1sfrWROdg1NTOJt9DlEzEptFA6+utbVi4oVDua+Z/2RNdP+BHYnOFIACZMvVt9d6YlFQpnXNK1f85qDaVICpfWsfhJKtsIZf1BmrJp2+F8MtHDoYYzMV34HU3NIc9mxJYmwcMhPd9HgDyKlDAuHnFQnKU57lFnJTKjLcCvhkuy0I4mYkExnNEQ0EVDC47tPreDfn+/tpF+o6VAM4VAtjad+y4SSneqgIoGmP3vL6hmel4DWAOEJXgmH2qb9B/EwndlNTeLhUFbLmPd/BdWHmj1su81KZhgrtOyQDvigXHXJa2RWcmcRcBgZ5qb2MPVy9e5KT1BzMIVD/+bHYjqjzfF/9g8YegKMP1cl4bVwzRJBC4cehvmkbwoHIQT5aS42G/0d8tNdSAm+YChKc4g1K5maQYaheaQ4VcK5WWKjuiG+WaneF8QflOEif5oejmGKqcGY3C2miO8cuIvveP7bYpLjvvJ6huYkmONgTPBeV44ya7Qmo9jc1jqxWYlnVlp8OXzyt6aP6amKlKxoq3Cos/hlWprAy3ep310wWdVPaqi0mJUsAjYlF/KPgYHT1feWhMORrfDnkUoYAqQXNK85mP4GUJpMRSGMPQOEUPvWaOHQ5xiZF605AMwcnh3OdDaXe/whKup8JDtsJDttyqzkj0z45uRvmqWSDeFQb2RRR2kOFuFQZynhoekFGE/ZIacqzWKdrAc2bGec2BfWIuNRVuulvM4X7lwY9/iFn6rJCyxmpRzjeyuc0r4WNAfTrBQKQMjQZj3VzU/6ZkZz7mhlrgq1IbjCGgra0u8xTUqjv2acf19kEndYruG598JFj6nJGuILh1AIXv6Bysxe+4z6nVveVAmNKbnN+xxMAevKhL2fqc8Fx6r39AHQUE5SOzPNtXDoYYwvSOe2s8ZzzuRI8tIJoyLJNmadJq8/SHm9L9yzOtlhi6qtZE7+GW57eD1Ewl2rLT4HryVayZxI6uOU4tD0QAzhIJINM0ZdxERiQ+IgQGUzpqXPjT4jx43Iabwy4IO/T4Unz4aXv6eWWc1K0Lpw1rDm0JRZyTLOoE+F6Qa98X0bpuPYtPsPnKbCS71GNdSNL0HpzsTGZV6zlNyWzUol2wABo083zt+EcBhyHPSfGPkeTzgcXAPrn4O3fwGbXlXLfDVqW2dq80lwps8hf6y6RhARDhlq7nD6mtDQEkQLhx5GUpLgmlNGkZsW0RyOHxkRDrGaQ1g4xDikq5swK5kCJNrnYNUcglHbaXo4Zl2l5CwYOQ9WPBp+qrSJEA4CzdZY+nRnKeluO8fGK7PhqYTqIvXZNGMYT9ZelykcWuGUNifRJjWHGOFgChNPdDl79iyHP4+AqqKI3X/gtMixQ0H1RP7F/YmNy9Qcsoe3HK1UvEFt1+8Y9b1yn8Ws1Ixpzp2lTD3W37jzffV+ZBNU7QOnEcHlylSCJhHNIW+cek8fAGlGs7D0AjUcXzuit9DCoVcwMi813F3OFA4N/iDl9X6yDeGgzEqNfQ6xZqUGSwKdSVzNoRN6im49XM2SLcUtb9gJ/PT5tTz9eWG3nLtTMSabtJRk+Pof1ET68Z8BVYjPKQJNRixJKVm+o5QTRubGbw0aMJ5Iba7IxO6vBwR+hyFMWpMlbT6Vx2oOL18Da5+N0Rz8qqopNBYOJVuV8DiyFSr2EkxyQ95Yta6+TEXqhAJQlWCRhbpScGUYmkMNrHkGnrk4fnOeolUwaAYkZ6vw1Mq9Fs0huelzjD9HTehb/xdZtvMDGDBVCRubE074kVpuag5BrxJ08WioVPtkD1ffCyZH1qWrKrAubzvyPtDCoVcghOD4kbkkCcLlmD3+oNIcjAzqJs1KyZH1YDUrBUg3TE6eOFFOdZ1gVvrXx7v5+UvrO/y4ibBseylf7Gn7k5QQ4kyj18hOIcQv4qxfJIQoMZI71wohvmdZN1QI8Z4QYosQYrMQYnibBxJDMKAm/vTUZCiYBGPPCk9ANtG0WSkYkuw8UktRRQMnjmmippJps07OijzF++rAmUbAbphQ1jwNK59IbLDWPAcrW16HvZ9GT4RBX2TS9cRoJ6YDubIQKvbQkNwfUgyzWH0ZVBl+iOoEhUPtEZUf4ExTv2/Xh7DjPRUeaqX6oMofGDxT+RIyhyifRzyzUixjvq7KaHz1sDHOcihaCWPPhAsehvMfhJHz1TpXeuRYTTmlPZVKG0nrp75HCQelObi87dMcdA/pXsL1p45mzqjcsAZgdnHLtpiVPFFmpQBJQuVFgCrPAdFmpZxUJw2+YIxZSQmF2IS6YEjS4A+S5mr7LVNR76O01hfVJrU5giHJoie+4tpTRjFndAJF4ZrB6w+Gf1trMXqL3A98DVUxeIUQ4nUp5eaYTZ+TUl4f5xD/Bv4gpXxfCJEGdFhKenVtPdlARqrx1JqcFZ5kk6QyK8XTHC5/5Itwi9q5TV1bUzi4s1SiVShkCIcUgjbjfOufU5PczKtaHqw50Vk1h4AhBALexmYlU3OJ1RxMM1DlPijfQ0NyAWkphum1viyioVQVtTwm83ipeeBKU5qQWY7iw9/DxpeVkEnJhRnGbxw0Q72n5CptwJeAcEiywXHfg/d/pRzIRzYDEsZ8TQkbZqvjCJuhOViiz+IlDDZUqL91Wn/13fQ3gNJqbC6cvvZpDlo49BLG9k9nbP/0cC/qGk+AWm8grEmkOG3UG/kPQgiqPX4ykh0II1qicSirnwy3g1K7N6p8RqzmEAxJfv3aRl5Zc4BgSPLl7ae1uYKraco6Uu1NaPvyOh/Ld5Qyc1hOu4WDJ9B24YDqLbJTSrkbQAixGNWDJFY4NMJoZGWXUr4PIKXs0Gp1lWHhYNi7RVI4c1YgcRJolOsQCIZYs6+S2SNyOHfKwHDiZSPMyTk5y/juURO5M5WA3dIxzrR/15cr00o884qU8aOVzIk/4GlsVgqbsuqUicdmTFemcKgohIpCPAPOUhO1eWwzc9hTaQizFsJ0vTUWzaFWmaXyj1EmI2+NEgYbX1L5DUn2yETszoyYlZLsKtmtOWZcCSsfh+e+rYTQ0BMioa6gBMLo06D/JHCkRn57PBoqlRAYcRKc9DMlZEyEgIwBWnM42nA7lCXwUJVS0bMt0UrBkMQflDjtgqoGf9jfoPYzzUpmKGuAzGQHboctbihrvVcJmp+/tJ4XVxVx3PBsVhRWsKe0jmlD2yccDieYfV3VoJ5c22viMq9Lbdv9KIMAaxeYImB2nO2+KYQ4GdXl8CYp5X5gLFAphHgZGAF8APwiXu2wlnqVxOs9UbF7EyOA4oP7Wbp0KWMPF5PnaeCzpUuZ4/PgFDY27djDUkekAF9xXQhfMMTE1FqGePbw8cd74v7ojKqtTAfK6oLkAp8u/YBxB/fi9koqfDY2Tvw5OeVrGXDofT7+6EOOW/FjynOmsGv09xodKyno42RUvkWgppRPjN+RXF/EbKCs+CC2oIcsY/sVX3yKLViPOXV+8uFbBBwqXHfqwZ1kAd6dy3EFvVQmZbPrs5WcLOwUbV2DLdgQbgbz1fuvUJ9q9hSLz3FVZdT7k6nzlzLcV0ugsojDBaexc87/U8IWmJRbRF7Zl9SkjWLVp6r1zPjKerIqiynds50C4eKTpUtb7A+SPOZmpq++lYAjm9WDf4R/2bLoDQZdB0Dejs+YBKz4bBl1afsaHWdGyX58zhw2fLYCbCfDZyui1k8NpWCvL2lXrxItHHoZ5iR/oFJNsJFoJfWnbPAFcdqTwpqBSWy0UlWDn/4ZLkM4RDSHsHDwB9lWXMOLq4q45pSRLJgyiLPvW86hKg/T2jh2M4LqcJWHOIpyI0xh0lycfiKYwq8dmkMivAE8K6X0CiGuAZ4CTkX9j50ETAP2oRpcLSJOnbGWepXE6z3xcc0+2AezZ0yl//g5UPsaVK1W232RhDspREp2P+bNi/zVPtxaDMtXcvaJM5g5PE4Iq8keG6yB3MGjoHwlc2dNhUPJkNKftLQ0Jp17O3x+Pxx6l3nHT4NPikl11jAkXn+M+nJYDiTnYG8oZ95JJypNoGglfAW5makQsIGhSBw3fYoyP61R30+cMRlyjGqnG9Xf0WVE44SyhjFv/nxYnc/QvFSoi9jpZ40fCKPijMfKWkHqwKHk958Ie5/HHmxg8PgZDD7p1Mg2Y9LhkVNJHz8v8jdoeBvWrmZw/xyoSmfevHmJ9Qc58VQcjmTmJmc3vc2OAGyC46ZMgCGWppimBrY2AINHN32uknHU7/6yXb1KtEO6l2EKB1NzsOY5ANT7I5pBRnJE9ofX+6LNSi5HUpTPwXy6DoYkByvVOU4d14+BWSq/wlzWWqSU4cm+uAnNobTWy1VPfEVJjTJnmI7U9k7qHSAcWuw3IqUsk1Ka9rJHAcMwTRGwVkq522iR+yownQ6ips7QINMNM4+wRcwqUqpopZhQ1t0lylRhlmppkrBD2pjE/A1hn0MYc11VkYquaSpCyDQpmRVKTVOUGSLbyOfgj86+tvodYvoXeNwFxlhylBCqKoJ+E4xxxRlP8ebo4/k9YHdHm58Mp26YQTPgosfhxJ9ElrkyVCivtzb6mrRExsDIdWuKsM8hxqy06gm4e5Qqj2Ga++Jx1p9ZPf3uxMcUBy0cehluozjagYpo4RD2KcRM/iZJSQJnkpooQyFJpWF2cttteKOilSL/oAcN7SQzxUFmsoNkh43DVW0ryNfgV2U5gCaP8enOUj7aVsKGA5VARHNot3Awake1QwNZAYwRQowQQjiBy1A9SMIIIawt184Dtlj2zRJCGEHonEoCvopEqa1X94HTaeTFJNkiWcIyiCuOQ3pXSR1ZKY7wvdMkVoc0KNu64XMIY05ypTvUe/WB+PWWzIk+LBwMe7iZzGX6HEynrjXPASKTedCvBIsZuipseNzGpc0YoMJcq/bD4OMAET9i6fEzYflfIt8DHnVep0WfNaOArEz6ZqSrGxj1k6SKdnK04NdoLabfJjZaaePLkWvVnIBJ6xep89RGtHDoZZjRStsOq2zLIUaZ5eQ4ZiOrzwHAaVPrD1Y14AuEGJqbgtuRFGNWiggKUzvJNBzbA7LcHGqjcLDmVTTlc9hySP2mBp8aj6k5dJRZyRsIEQi2PlDIeOK/HngXNek/L6XcJIS4SwhxnrHZjUKITUKIdcCNKNMRhm/hZmCJEGIDIIBH2vWDLNQ1GBNokqElGr2kAQgFsccJZd1dUsuoprSGI1sin2Md0r565bB1xBEOZUY2sr8+fu0kf4zmUB8rHIyY/qaEg5lsZ5bSMCOGMgcjzd8+YYGqfeSpUt3Y0vo1jljye1QWtVkGA9R5HG4VrWSSFqM5xMMsrldzqPkch7ZgXuMNL8AXD6rP3hrY9wVMOF+VCxkwtWPPGYMWDr0Mt10JgRpvgIGZ7rBQMM1G720u5rOdpeFoJSsum6DeF2TnERUwM6ZfeiOHtHUiPmRoDlnJ6glzYGYyB6si/7Bmf4nmWLe/kque+IrSmsjTa1Nmpa2H1QRglu6oNARKe7O1ox3ubTuWlPItKeVYKeUoo/cIUspfSylfNz7fJqWcKKWcIqWcL6Xcatn3fSnlsVLKyVLKRVLK9hW9sVDXYFxLm6EFWKKVkCHs0t9Ic9hdWhc/QqloFTxwvGpcD3E0hwY1QVlDK2OFAzR+WpcyMtGbjXDCmoMlWinoj5hTgr5IEpx1O9OkZAoH0w8BMPGCiHDJHKzOFTsWs4RF2S7jPAH1FG5Pbt6sFA+3Uc+q5nDrzEqJYB5v0yuRwoN7lqmxHvdduGEVjDuzY88ZgxYOvYykJIHTyGYdkR+5mU2z0n1LdnDLi+vx+EPhukomLpuaKE3hMLpfGi678jnsLatjf3l9lAnnYFUDTltSOEKqINMdFhj7y+s55tfvsHZ/ZbPj/Xx3mWEqUv/ceWmuJjWHrabmYAm3hY7QHCyhun2sZpTHYwoH40EgyRZJJpNBbDJAvS8YMTd6/Eafjziag1ljyHyqj9Uc/HUqIc2cFKGxWQmi7fxFK+GPQ1V3MoiYZYqM6BqrzyHkV+GkEJ0EB5FEODMBrt8E9XSdOzqyjSsdjjEUucwhkDmosc/B1EAqCtV1ChhCy+GOmJWS7Mp/0RKm5uCvaz7HoS1Yj1dXqsa64311fYYc37HnagItHHohLmOyHmF5+jM1CIADlRFzkBWnTVDvC7DzSC05qc5wBzmvP8TPnl/HbS9voNYbCAuVQ1WeqFyJgZlujtR4CARDbDpYhS+g3pvDNGmYWsH4gnSKq72NykhX1PnCQsPUFCqNJ972+hy8cUJ1+wJSSrxeQzgkGX9rq1lJhkhCvV5fpyZJ88FgZH4czcF8ug/3VYhxSNeXKa3E1YJwqLaYcja/pibkA6vU97wxMOki9TS8+fUYzSHQss/BFA5p/eDbL8FJN0f/hjk3qBpT/ScowVG2Eza82Pg4Ib8yOfmN6+dIiZiV0vpDUgJTo/U6dLRwcGepbPfRX1PXvL4cCj+B4Se1nE/RQWjh0AsxI5ZG5EWe/vLSXAgBC6YODC+LNStlugS7SurYcaSW0caTo9thwxMIUlTRwJ7SOmq9AfplqMikQ1UeslIixxiQlUxIQnGNN9x3uCUHtTnBm1rB2P7p+AIhamMqOmw9HKlYafWbQLQpaOm2I2w80LxAiiVeBnhfoNYbiCSOmZqDGa0kZVhITBuYwv0f7SIQDPGZ0VN62tCsxgc0zTimUIg1K5ndxayag82hnrh9Ncq0lWSPflrf87F6ryhU744UWHC/SvT6+M8W4WBqDqZwMKKVbC51fPOJ3zQrpeTCsBPCFUjDFEyCK15TWsTcn8DQ4+Gl78KjX4Otb0UXCizfHdEcrNFKZtZxS5iaA3S8WSkpCS5fDNO+pb7XFitHe+6ojj1Pc0PosjNpOgzTv2C1G/fPcPPZL07lr5dMDWsM1mglgBn9bewrr2fNvgpG9TOFQxINvhCltV4OV3uoavCHi/z5AqEo7WNApiE0KhvC/SXMiKamMDWHLRbNAaDCE+0YNjULiCTqmT6HOl8grGn85vVNPLA0wVLMBk053Hs7ZbU+HBi/x2pWkqGI9gD86OSh7Cuv5/V1B1my9QhTBmeGS79HYZpxTKFgmpXCjldDOFifmCGiPaTkqeqgpp2/vlz1UQYoL1TvzlRlwhlxspqcTee16XOI1RwcbnV+q88hyR4RWM2RnAXffhlOvQMq9ihtxVqnqXxXRDtxJEdMWon4GyBaOHS05mCSakRNlWxV18j02XQBWjj0QtxxzEoAAzKTsSUJjh+p7KXWPAeAGf3tOG1JhCSMMYSDy26jrM5LICQJhiQ1nkBYOEC0aWpglorIOFTlCQuHw9XN5z1UGlnONZ4AQsDo/uq85R51PlPz2F5cQ7YRXhmrOUgZ0SbqvM33J4hHUw733k5prRe7KRysZiWImIaA+aOymDAgg3ve3cba/ZWcOr6JJ2PTxGLmG1gL70Gk5lAj4WCsT81Vk5epORR+AkZWdNjUZEb1ZI9QtnrTkS2DShhZhUOgQX13Z0SEQ32pEkKJmH1ACZeTb4HhJyonuLWnQvmeiHCwuyPCIVHNoTPNSibmWA6tVe8ZA5vctKPRwqEX4nbYsCcJBmfHD58zC6nlpLqilqc6BPPGqZjw0f0iZqXYsHTTrASQFUdzOFjZEDYrtRTaap3IM9yOcOhtaYPkpVVFzLvnI2q9AcpqffTPcJNsaXdaVe/HlqT8HeEOdd4gVQ1+pJT8Y8kO9pa10JwFokqZf/z+24Ta0i2sB1Ja68MhAkiE0hhAmZUgKqEsSfq55YxxHKzyICWcOj5ODD9ETCzmvgGvEjaOZCV84pmVIFpzyBwUEQR7PlZO46xhEU3GnETNUtPWaCJ/XWOzkiM5RnMoVXWQWouZIGealdIHGGYl0+eQrDK2R85T9YoSwe5UUU7Q8WYlEzPf4uBa9a41B01zuO02huakxK/BD1x63BAeuWJmI80C4NvHDyMvzckko7mLqYVYybc0GrL6LdLdDgZmullRWBF2eh+q9CClZNn2Emb94QNeXRMdHWIVDpnJDvLSnCQ7bJQ0hNh4sAqPP0RZrTccepviVKXHpVSJegWGoKr1BggZlWGrGvyU1vr4y/vbeXN9y71yrWalj995jTFjxnDrrbeydevWZvbq+ZTWepVZyeaItKM0n6itLSKDPuaNy+e44dkMyHQzcWBG44OBRXOw+Bxsxr3gSGlGczCFg6E5VB9U6l7FXsgbHcltSHJEzF/WEFRE5KMZ32+alewxwqG+vPnM4KZIyVU5FWZ01IApKpzValYC5a+Y9M3Ej2ualjpLc3ClK63GDC/O1MJB0wwLZw/hByePbHK9y27jaxPiq8Ynj81n5R1fC2fHuuy2RttkJjvC4bKxEU+njMvno21HCIYk4/qn0+AP8s7Gwyx64iuO1HhZs68CKSUfby8hFJJRMfZmMt3g7GRKGyR7StVTf1WDn+qGABluR7hpUZ0vSDAkGWSYsuq8wahGReZxazwtm4msZqVv33YPa9asYdSoUSxatIjrrruOhx9+mJqaVrS77CGEzUpJlr+RqTlYzEoE/QghePg7M3n+mhNIShLExR+jOQR9kcgYZ4rKBIbGJaTNXgqpeWqyDPoiL7s78vRrfbrOGkpYKKRaKu46Y30OyZEyFaCERJuEQ47SXqr2q4k8e4TSWqxmpbZgalGdJRyEUNfPW63+tomavDqAThMOQojHhRBHhBAbLctyhBDvCyF2GO/ZxnIhhLjPaKSyXgjRYbVn+iIXTBvMZbOGtrxhAlg1h2wjMinNbSfFpSYZa7QSwClj88PJb7MN38bDy3eT6rQzJCeZklov64uquPLxr3h742G8gVBYEJmCZkhOCiX1Mmyaqm4IGJqDPdzu1IxyMms61XoD4eqsNZ4ApUb9JWu706YwQ1mThDJPZWRkcNFFF3HZZZdRVlbGK6+8wvTp0/nHP/7RqmvX3ZTV+kizhxA2q3CIrzmAquA7JKeZSSxsVrI4pM3kOkdyJLmuObOSuX3Qp4SMzRnJNrZmVttdEY3Cmo3saMGs5KmKdgQnipm3UFGohI07U2V7m7WL2jq5d7bmABGndHpBxHzYBXSm5vAkEJvC9wtgiZRyDLDE+A5wFjDGeP0AeLATx6WxYIbFprvsjOmnnghTXXZSjSqvsZrDnNF52I0nT7O39Zp9lcwemcvgrBRKarzsr1CT/rqiSgAmDMiIOtbg7GSO1IcoMrZTmoPf0Bzs1PsDYWf0oGxTcwhENSAqNARLQppDIITbkUSqy87qZR9wwQUXMG/ePPx+Pw8++CBvv/0269at4y9/+UuLx+pJlNZ6SXPIiKkGIpNHbOOcRDDNSiGL5mA1KwEgomsQQUQ4pOaqSd88f9CnxmZqDrElJky/g7WOUZJdCZSwQ9oiHEIhQzhkJfZ7rJj9HioKlXAzBZyZ+Odoo+Zgmtg6y+cAEW2hC53R0InCQUq5DIjtNrEAVcoY4/18y/J/S8UXqEJlMQHMms7A1BzyM1xhB3eayxZOqosVDhluB9OHZeO0JzFlSFZ4+ZxRueSnuzhS4w1HIG0oUk97xwxQk4npvxiSnYInCGb1jYp6HzXeABnJDpINn0OV4aswI6TqfIGoMNRCwxFd3dCy5uDxB3E7bKS57Kz/5F3mLLiCT1es5pZbbiE7W01sKSkpPPZYoyraPZrSWi9pdpmQWSkh4jmkTbOSKRxc6Y0jhaw+B1NQmWYlmzMyucVOoKbfwRo6agqHgOlzcKvjy5AKe/XXtVE4GGOsPqgmdHNSN/0o9vjBHS3SFZpDmllYsGuFQ1f3c+gvpTQ9iIcB04AWr5nKIKCRt7GlhigQvylKd9HTx7LzsJpEHIEGgtXqCXPz+rUEvcpss3vrRmzFW6L2OTk3yHCnja2rv0CgghUdFXvwVvo5XBlgxSaVLbt2nyqSJqpUo5mqkkMsXVpG1eHop/0v1m9DSig5sJfqmhCVNUE+XbkWgLJ96lhrNmzmSFpkUlqxdS8AB46UtXh99+zzIoJBhAiQetyF/HMjbH/iQ751jIuysjIWL15MQUEBNputx/ytEqGs1keqPRTpkAYWs1JbNIcYs1KU5mBMnrHOaIg8lafmR0JFzRLcNkdEOMRWLs02hIPVjm5zRDQHf72adE2fRoXRlKg9ZiWkEnCm36TWSKprq+bQJcLB1By6zhkN3djsR0ophRAtV25rvF+zDVEgflOU7qKnj0VuPQJrVzB2aAFnTS7g7cK1nD1/Lm8dWsOeqjLmz53F6H7RZgTrEQq+XIIvEOJb58ynfvlu3t27FZ8rGyihwZAB551yHCsrN3L+iaOYN7GAvANV/HPtJ+FjiPR84CDTJo3HeaCK9eUHGTxyLKzdwFknz+avq5YxaNgoxhWkw5dfAVBLMlCLcKYwa85c/vHhTm44dXS4V7aVlw+tIdNTSWaKk3ce+DEF376br44I/vG9k1i+9ENuv/12VqxY0Wi/nk5JrZeULBmx80PkqT7UQcIhnuYQy6hT4Zy/wpDZkSqoYbOSs2mzUv549W4tg201K/k9ah9zYi9vh3AwBRhEm5VMzaHNPocuMCulHh2aQ7EQYoCU8pBhNjIMfi03U9F0Di6jP0S/dBdnTCzg01+cSm6aK1zIL7YERyxzRuWRleIgKUmQbyTPbYgpb5GT6uTlH80NfzedoululZS330ioy3A7SHbaafAFw9FIplmp1hsIV2sF2GvsU+3x89nOMh5cuospg7M4c1Lj7NaIWcmGDAURNgdVDX7e2XiYLIcDn6/DiqR2Gd5AkBpPgGRbKL5ZKbZxTiKEy2cY19nqkDYnv1hnNCg/w3HfVZ+jzEp+pXmYZqPYXs5jz4Sr3434Kcz9bc5oh3RHaA6udCV4QgHDrGQco7ZYXTNb8/d5k4Q1hw7u52Clr/kcmuB14Erj85XAa5blVxhRS8cDVRbzk6YTcRkO6X7pLoSITPAprvgO6Vj+cskUfnWu6rpl7ltaGz3ZxkY8ZSY7SLHD8NxUMpMdYcd0RrKdZIcNXzDEkWovKU4bqS47KU4bdd5AVOlun9HAp8YToKxOmcB2Hokfjqoc0jZSnXZsyZnklK5nWG4Kz6/czyeffEJeXl7c/XoyZcY1dts6wKxUURix8Vu3T9SsZMXcPuhTneFsDvXUbibTWUlKUrWPrGGkZi6E1SHdEZqDEJHjuGI0h/b0YjCf6uMJzY5iwLGqyuzAtjbobRudpjkIIZ5FWSDyhBBFwG+APwLPCyG+C+wFLjE2fws4G9gJ1ANXdda4NNGYGkL/jGiba4rDRrLDFjcPoims9XpG5qWyu7QOpy0pXAvKyjG5NmaOzuXL3eXsNvIdzDwHgKKK+nAIbKrLrhzScfo61PuCHK5SwmF7cW3ccSnNIYk0l52cM67j8Ef3seu1+/jMG2DskH689tprcffryYSFQ1IwxqxkOqQTFA6+erj/eDjrj42FQ8BrieM3noxbmgTDoawWs1KSDXJGNf3kG09z8Nerp3y7RXMo322MoQ3CAdRx6o6o32Cax+pKo01OrWXyxSrqKl7nuI4iezjctLHFzTqaThMOUsqFTaw6Lc62Erius8aiaZpx/dO5a8FEvj4xOrnmjEn9yUptnaqdb6nJNGNYNrtLVTtKs+S3lRumuZk37xiuePyr8LJMI1oJYH95A7lGpnaay06tNxguyCdEdCdKs4TG9uL4moPXHyQ71Umqy44jewB3PfU6wu/hl69u5Jen5jB69Oi4+/VkTLObUzRlVrLmOTRjVvLVqif0qgMRs5K1ZHcjzSGOz8FK2KxkOqQNYXH1u03b5aM0B8PEY+Y1mKGsiIhZqS1JcBARAtZoJWT7nMmOZFVEsA+SkFlJCJEqhNJXhRBjhRDnCSHaaKTT9CSSkgRXnDC8kSP31PH9ue2sY1p1rKxkRzgH4rjh6mkv1qQUi9VsFas55IU1B2VWMkNZc42aUea+ewzhsLu0Lm4bUI8/hNuuTFQAFVu/ZO17z1O94lWeePIp7rrrrlb9zp6AWbHWIQJN5DlYQ1mb0RxMbcFTlaBDOlHNwQxlNcaWmtu0+aaRWckZqZ7qSFa/KTkr4jxuq+Zghty6M4zjGvd8WyOV+jiJ+hyWAW4hxCDgPeA7qCQ3jSZMUpIgz3jaH1uQTrrbHm4x2hSZlsqxaW57WDjU+YIRs5LTHnZIux1J5BgazfBcNWGZZTh8gVC4WqyVBsOsNCjLTfUHD/Dlkjd4/qlHsAlY/fky9u7d285f3vVUGZqDXcYIB9HKaCVTW/BUNe7nkKhD2oopTAKWPIeWiGdWsmoOEPEXJNnb/qRvmqdc6Ur9NAVdW0tn9HESFQ5CSlkPXAg8IKW8GJjYecPS9Fb6Zah/9IIMNxMGZDAst/l/ZPPpP91tx5YkSLZoMKZZKdVlDzukU5328D5Dc5UdvLI+0oMint/BjFa6bNZQCjz7+O9//kN2djYnXHwNM6/9C9u3b2/nr+56zIKGtka1leKVz2jGrBTWHCot/RziZUibZqUWntpNYWAeKxHhYLMIB9Mh7amMPm/4qT8zUmSwtVjNShAxkXVmjkIvJmHhIIQ4AfgW8D9jWdcV+dD0GvLTXNiMsNbHFh3H786f1Oz2ZkMi8z3F0u40L01NLOluOzUeJRySnbaIcMiJmCnMUh7xIpZM4eCwJZGWovZJSUlhqMvD/rokDh3qfYFxlQ1+Upw2kkLtNCuZDX3qSiNlteMV3msuz8GKKQzMmkWJhIja7BETj80OeWMjhfbMzGXzqb+tJiWIaB+m9hN2tmvNIR6JCoefALcBr0gpNwkhRgIfddqoNL2W0f3TGJ2fhi1JkOayh2s3NUW4a53xbo1sMs1K+WkujtR4qPMGSHXaw9sOy43Elg/NSWFoTgpr91c2OocnEAr33f7GN75BZWUlt9xyC8/cdjnb/3E137jw4hZ/lxDiTCHENqM45C/irF8khCgRQqw1Xt+LWZ8hhCgSQvyzxZMlQGW9X/XaCHijzTKtNisZmoNpz7duH4hTW6nFaCVDGPgMDc46tuYwTTtJDphqiWWJNSu1RzikGxV5whqEcay2ls7o4yQUrSSl/Bj4GMBwTJdKKW/szIFpeic/+9o4bjh1TMLbR1qaqlvRqjmYZqWCTDcef4jiag8projmMMxSYTQ3zcnXJvTn6c/3UlXvJ9NwhIdCEl9AOaRDoRCnnXYaWVlZfPOb32Tg5Dn86IlPWPSjrzU7RiGEDbgf+BqqtMsKIcTrUsrNMZs+J6W8vonD/A7lu+sQqhp8ZKY4lZ/AajNvbbSSWWzPbORj3T5oqa1kJrAlmufgNYRDoslldpcSKDYHDJwO+cdAyZaIUOoIzWHCAkjvHyn4pzWHZkk0Wum/xpNPKrAR2CyEuKVzh6bpjTjtKp8gUTJiNAdr1FSuoTmYnel2l9aRYjUr5VqFg4sFUwfiC4Z4e2PETOQ1kuXcDhtJSUlcd10kYvqEsQP42xn5UQUEm2AWsFNKuVtK6QMWo4pFJoQQYgaqjth7ie7TElUNpubgiX46b7VZydAcYjUNKaMd0sPmwim/gCGzmh9Y2KxUG/29JcKag035FKZ9S303zVhhzSErsePFPYczOuxU+xyaJVGz0gQpZTWqiurbwAhUxJJG0y4imoNhVorSHNTE0t9wNtd4AqQ47ZwyNp/zpgykX7qbVGP7vFQnkwdlMjIvldfWHgwfw2z0Y1afPe2003jppZeQsb1Rm6epwpCxfNPoR/KiEGIIhDXtvwA3t+aELVFZ71dhwgFvjOYQz6yUgOZgJeQ3ch1kRBNwpsD821o2E4XNSqbPIVHhYBzXdK7P+gFc8m/IH6e+p1gc0h2FjlZqlkQf8RxGXsP5wD+llP62FM3TaGKJ+BzUrRjP51CQGfnnTXHamDY0m2lD1WSR7nZQ5wuSZ5T/OPfYAfzjo53UePykux14AqZwUMf917/+xV//+lfsdjtut5tAIIDdbqe6urq9P+UN4FkppVcIcQ2qJP2pwI+At6SURfGSAa20VHHYWlX3SFU9A50eQr569h88wh5jeXb5RqYAu3ZsY5SxX9G+PexsotrsgIPrGWf5HhJ26qsrWb10CScDu/buZ38rKh8nBb2cDBzet4sCYNO2HZRUxD+3lZneIGnAV6vXUp9qVvrPhCMfA5B/5DATgX0l1exu5rq0hhHFFQwD9h8uZVcHVePt6VWYW0OiwuFfQCGwDlgmhBgGtPu/SaOJjVZy2pOwJwmSnZHSHdayHLHJehnJdg5XR0xQEwZmIiUUltZTkOlmhxHaamoOse1AE6ya22JhSCllmeXro8Cfjc8nACcJIX4EpAFOIUStlLKRU7ulisPmWKWUNLz/DseMHERSaYBhI8cyzNx2N7AeRg0boj4DgwvyGdzUb/xiC1gieZOSM0lzOzj5hFmwHEaNncCo4xvv2+R1CwZgORTkpEIxTJw8FY5p4txWtudA3V5mzZ6j+k7HshvYfDdDx0xi6MnRx2tz5WP7Wtj3IkNGjmVIB1VO7ulVmFtDog7p+4D7LIv2CiHmt/msGo1BRrKda04ZGVVNNdlpC0/25vcMt51qTyDKYQ1Kc0gSkJWith+Zrxynu0tr+edHO1iyRRX+dRuCZtkyi0+4+iCHP1vMsoZyTj7rwuaGuQIYI4QYgRIKlwGXWzcwqw0bX88DtgBIKb9l2WYRMDOeYGgNDf4gvmCIXNPCExWtFK+2UgJ5DiaujEh2M0Qc0olisyvTVqvNSu7I/vHoiGilWEyfgzYrxSUh4SCEyEQVzjO9OR8DdwFVTe6k0SSAEKJRmY4Upy0cqWTSP8NNtac27GMwyXDbyUl1YjPKdgzNSUEIlTW9vqiKgNFuzjQr3X333eF9PeUH+GrVGmZ8fIAPmxEOUsqAEOJ64F1Ufs/jRkj3XcBKKeXrwI1CiPOAAKoD4qJWX4wEMRPgclyGZTeezyHRqqyBGJ+DO0M1wDHzHxKd3K3YnO33OcSSMwL6T4JBHdhePlxuW4eyxiNRs9LjqCgls4rqd4AnUBnTGk2Hku52kB8jHAoy3ew4UhuVQQ0wbWh2lKnJ7bAxKCuZVXsrOFTlwZ4kCIRkOM/hjTfeiOy8+TX2P/JtfrI9q8UxSSnfQlUPti77teXzbahcoOaO8SQdUHbGFA7ZTqNKbdxoJWsoqw+2vqWa8aTGVCCNpzlUHYjsb0swT8FKm4SDGa3UxJTkSodrP239WJpDO6SbJVHhMEpK+U3L998KIdZ2wng0Gv544eRGBftMv0OqK1pzuPG0xjkVI/JS+WyXcgHceuY4XlhZxKj8tMYnCvgYnCHYsnNPB428a6gyiu5lOo2MZuuTb7we0nWlsHghfO0umPvj6IM10hwyI+W2ofVmJVARS2a70NZqDm1tutMWwnkOOpQ1HokKhwYhxIlSyk8AhBBzgYYW9tFo2sTM4TmNlvU3ajbFawMay8i8VJbvKAXgohlD+MHJo8LrbrjhhnAJ8VDxFtZ+Xs/045sv8dHTqGpQE3eWPY7m0MisJCKtOz1xYkgCHpUhbOY7mD6HsFmpLZqDy5LnkGgSnCHgmtIcOoP0Aep6pfdvedujkET/Ej8E/m34HgAqiHR002g6HTOcNdYhHY8RecopPSDTHQ6HNZk5c2b4s32fh4WpnzH33gc7cKSdj2lWynCYwsFa8jqm8J7dDTWGn9w09VjxeyAtHyr3qe/uDOXMbq/m0GCEo/ZkzSF7GNy4NrqHtSZMotFK64ApQogM43u1EOInwPpOHJtGE8Y0KyUkHAwT0oQBjUs9XHTRRbjdbmw2G3zRQPCtxdT7gvQmw4LZyyEtruYQY1ZyuKGhQn02n+atBBpUlzdXhip258pQ+5rmprY6pFu7v7W2UleSPaxrz9eLaFUPaSlltZEpDfDTThiPRhOX6cOymD0ihwkDW+7VO9LQHOJte9ppp9HQYDa18dIQgNO/8c1G2/VkKuv9OGwCN4bpqLloJes6s4w2wIvfhU/uVZqDw618DTZnRNCEHcptdEiHP7eithJEHOqabqc9Br42FlXXaFpPv3Q3z11zQkLbDs5O5o5zjuHsyQMarfN4PKSlGc7pgI80p6C+IU4JiR5MVYOPzGQHwnw6j22zCZE8B+s6c8L31cOmV5TT2PQ5uDOVTyK25HZbzErWfRLVHAYfB6NPb3uvBk2H0yrNIQZdPkPTIxFC8L2TRjIwq3H8empqKqtXr1Zfgl5WHgyRnNybjEqqxlS62xEx3cStymoIB2skkznhH1oHMhjp/mZqDg53ZDIPRxu1U3NItGT3hPPg2y+1/lyaTqNZzUEIUUN8ISAAnTmi6XXce++9XHzxxQwcOBBZUcjhQ/U8916HtFjoMup9QeV7MSOK4uY5+BuvM4XDgVXq3VOpbPyp+UrAOJLj9GNoo88h/Fm3mu+tNCscpJQttH3SaHoXxx13HFu3bmXbtm2w/K+MKn6b5BkzuntYraLOGyDVZW9CczDMMmGzUhzN4cBK9d5QCa40tf/YM2HAlLZXVbViFQht2V/TI2iPWUmj6XXcf//91NXVMWnSJCYNzqA6YOOBBx7o7mG1CtVL25agWSmOQzqsOVQZDulkmHIpzPuFxaxk5im006zU1dFHmg5DCwfNUcUjjzxCVlaW+hL0kZnq5JFHHunWMbWWOl+AFKvm4IjjkI4XreSrVXWTKvdBcrYKY/VURW/TkWalJHsk70LT6+iWv5wQ4iYhxCYhxEYhxLNCCLcQYoQQ4kujR+9zQgitj2o6nGAwGGn0E/ASkHZ8vmYK0/VAVC9tW/ws5thmP+GJXyhTUckW9XX4ierdVxMT7WQIB291zP6twBQO2qTUq+ly4SCEGATciCpdPAlV5fIy4E/A36SUo1EZ2N/t6rFp+j5nnnkml156KUuWLGHJ2r1csbiYs846q7uH1SrqvcGIzyHJHl3mOtasZE7uGYNU1nOtKmFOrqUmlVXzMCf02hJVcyjRaCMrYeGgTUq9me7S+exAshDCDqQAh1Bds1401j+F6jqn0XQof/rTnzj11FN56KGHeOj9bUwYkBJJiusFSCmp8wVIddqVvyD2yT7WrGRO/NnD1btZZylnRGQfq9PanNDrjrS9X7N5DK059Gq6sMqVQkp5QAhxD7APVbzvPWAVUCmlNEtJNtWjt8VWitC3WvV1JHosCpvNhs1m49MdJRzOdjB9vL3HXJeW8AZChCSkuGxQ62n8ZN/IrGRM/NnDYe8nUG00sMu2CIe4msMRyBjYtkGG6yS1QevQ9Bi6XDgIIbKBBcAIoBJ4ATgz0f1baqUIfatVX0dyNI9l+/btPPvsszz77LPk5eVx6aWX8vkHr/HG9ceSddPfu2wc7aXWq56fUp125XOwx6QbNSqfYUzQpuZQuV/tkx7pvBfXIe2phP4T2zZIbVbqE3S5cABOB/ZIKUsAhBAvA3OBLCGE3dAeGvXo1Wjaw/jx4znppJN48803GT1a9Sj+2123EurKEtEdQL1XFdsL+xxiNYdYs1L/iUpLyDXKllcVqUglq8konnAAbVY6yukOn8M+4HghRIpQhfVPAzYDHwEXGdtcCbzWDWPT9FFefvllBgwYwPz58/n+97/PkiVLkFIS6mVx+HU+U3Mw8hxifQ6xPaTHngk/XgspRo+MalM4WHoxW0tsWCf05Oy2DVJHK/UJulw4SCm/RDmeVwMbjDE8DPwc+KkQYieQCzzW1WPT9F3OP/98Fi9ezNatW5k/fz733nsvR2r83Pz8dt57773uHl7C1BvCIaUpzSFsVjLcd6Ym4TSKDTZUqEnf7ox0QLPH8TkAJGe1bZBhzaF3CV5NNN0SrSSl/I2UcryUcpKU8jtSSq+UcreUcpaUcrSU8mIppbc7xqbp26SmpnL55ZfzxhtvUHTneCYNyeZPf/pTdw8rYepMs5KZ59BktJKRu2FqEtZWmOakb5qNopLoLGa2NpuVTIe01hx6Mzp9UXPUku0M8a2TR7NkyZLuHkrC1BkO6RSnPVJR1UqsWcnUJJypkW1ME5MpJDpcc9AO6b6AFg6ao5egt9c5pOt8SnNIC5uVYoWDWXivCbMSRHwJpmZg72ifgyEU2pJAp+kxaOGgOXo4tA7e/SWEy2f4kCKxp1shxJlCiG1GeZdfxFm/SAhRIoRYa7y+ZyyfKoT43CgXs14IcWl7fkLE52AzkuBifQ4ioi2Y3wGcVrOSKRwMp7SjiWildmsO2qzUm9HCQXP0sO1t+Pyf4I+0CU0kWkkIYQPuB84CJgALhRAT4mz6nJRyqvF61FhWD1whpZyIyue5VwiR1dafEPE52OP7HCBiWjLfQW1nCg1TOMQ1K1lDWduoOYST4LRZqTejhYPm6MEiFJASgr5EzUqzgJ1G0IQPWIxK5GwRKeV2KeUO4/NB4AiQ34bRA8rnkCTA7UiKH60EEVOStR+zEOAw/A6NzEqdFa2kNYfeTO8yuGo07cEscR3whZPEEsxzGATst3wvAmbH2e6bQoiTge3ATVJK6z4IIWYBTmBXvJO0VBqmtraWbfsLcdng448/5kRPHYcPl7EzZruTQhIbEJSw3LLuBOy4gLXb9lJ5ZCnDiisZAXzy1SoCjh1qIxlinrH9J6s3EXBE/YSosTRVciS3dDuTgUMlZWzrgrIkuixMfNo7Fi0cNEcPYeHgUdoDJOxzSIA3gGellF4hxDWo4pGnmiuFEAOAp4ErpZSheAdoqTTM0qVLycnPIaPiiCo9stzP4OGjGBxbhuQzJ/i82GyO6BIl63OgvIKpJ8yHgslQ6IBPSjnxtHOitYxlNpDBxstjxtJk+ZMdftgIAwYNZUAXlEg5msvCNEd7x6KFg+bowW8Ih6BPaQ8krDkcAIZYvjcq7yKlLLN8fRT4s/lFCJEB/A/4pZTyi9YPPEK4ImsopH5HPJ+D2WBHxFiNTad0shHKOnyuesVic6pXE4KhRbRDuk+gfQ6ao4eA4XMIeMOaQ4LCYQUwxmhI5UT1H3nduoGhGZicB2wxljuBV4B/SylfpJ3UeQMqUskYf1yfgykUYruwmeGsLYWo2pyQnNn8Ni3tb33X9Eq05qA5ejA7pwW94c+JCAcpZUAIcT3wLqo51eNSyk1CiLuAlVLK14EbhRDnAQGgHFhk7H4JcDKQK4Qwly2SUq5ty0+o8wWNSKU4/aNN4kUrgUqEs7miaynFw+Zoe44DaOHQR9DCQXP04LdqDsqsJEVi/wJSyreAt2KW/dry+Tbgtjj7/Qf4TxtH3Ih6X4B+6e6IoIvNkIaIOSjWrORIUZO+mfvQFDZH20tnmPuDFg69HC0cNEcP5oQaaJ3m0JOo9wZJybVFBF1czcE0K8VoDhMWQP74lk9ic7Q9jBV0+Yw+ghYOmqMH0+cQ9IU1h94mHMIO6UBzPocmNIfJFzXeNh6n/goyh7S8XVPYtVmpL6Ad0preQUMFvLAI6svbfgy/JZQ1YIay9q7nozpv0CidUa8WNBut1MZoo2MvgWEntG1f0D6HPkLv+s/QHL0cWgebXoEpl8PYr7ftGFFJcL1Pc5ChADcHH2PKgQzY+aVaaLb/tGIKhdhopa7CbCYUb2yaXoMWDpqega9OPc2b5aRjMfIS8FS2/RymcAh6e6VwCEr4puNTnGVJkD8aLngwfp9n0USeQ1fhTIVb93Tf+TUdghYOmp7BB7+F/V/ANcvirzeb1zRUtv0cccxKvUk42G120n+TQGv1pCZCWbuStibQaXoMWrRregY1B6H6YNPrgx2oOVjMSh1YPqPn0JRDWqNpBfru0fQM/A2R8Mx4tFdzCIUiWcWtTILrdTQVyqrRtAItHDQ9A3+D8juYjXhiaa/mYGoNEFM+ow9aVtsbraTRoIWDpqfgrwdk9CRuxYzr91S17fixwqF1hfd6F9qspOkA9N2j6RmYJqVY09K+L8BbE+6/0GazklU4BH2dUbK759BU4T2NphXou0fTMzCTunx1kWW+enjibFjzn4i/oKPMSmHNoS+albTmoGk/ffA/Q9MrMcNMTSEBSouQQfBUR4rFtVVz8FuFg7XZTx/8F2iqKqtG0wq65dFCCJElhHhRCLFVCLFFCHGCECJHCPG+EGKH8d6OmsGaXkfYrGQRDtboonY7pC3mqqBPaQ82V8sVSnsj8XpIazStpLv0zr8D70gpxwNTUI1RfgEskVKOAZYY3zVHA1JazEoW4RCniiq+2oj/oTWY+5ufg/74Rev6At2dIa3pE3T53SOEyEQ1P3kMQErpk1JWAgtQfXcx3s/v6rFpuomgX5mPIEZzMLQFczI3aUvEktXRbYay9tXCcEKHsmraT3cYXEcAJcATQogpwCrgx0B/KeUhY5vDQP94OwshfgD8AKB///4sXbq00Ta1tbVxl3cHeizxsY7F7q/lRGP5xrUrKD2gIojSanYzEzi0fw+hJDuDjG2+/PhdGlIGxR6yWXJLVzIZCAk7lSWH8Fb7yA7KHnVNOoywQ7oPmsw0XUZ3CAc7MB24QUr5pRDi78SYkKSUUggRNxtKSvkw8DDAzJkz5bx58xpts3TpUuIt7w70WOITNZbqQ/Cp+jhpzAiYaiwvSoNVMKBfrmocY1TXmH3sOBg8s3Un3FgGGyEpOYucjDTIyAFvBmlpaT3mmnQYOkNa0wF0h1GyCCiSUho1h3kRJSyKzSbtxvuRbhibpjuIilCyhLKGfQ6eaLNSWyKWzGgld2ak8F6f9TnoaCVN++ly4SClPAzsF0KMMxadBmwGXgeuNJZdCbzW1WPTdBNWf4D1sxmtFDCii8wn4rZELJnRSu5MdVx/PTiS2zTcHo/Oc9B0AN0V5H0D8IwQwgnsBq5CCarnhRDfBfYCl3TT2DRdjVUgREUrmQ5pj5roUvKg7ojqCtdaTC3Enamqv3qqwJ3V5iH3aLRZSdMBdMujhZRyrZRyppTyWCnl+VLKCillmZTyNCnlGCnl6VLKdvSD1PQqmjIrhfMcjHIXaf3U97ZoDv4YzcFTpT4niBDiTCHENiHETiFEozBrIcQiIUSJEGKt8fqeZd2VRv7ODiHElbH7djg6lFXTAfTB9FBNr6Mps5JVc0iygSsd7Mlt8zkEPIBQxwh4lYaSnJXQrkIIG3A/8DWUz2yFEOJ1KeXmmE2fk1JeH7NvDvAbYCYggVXGvm1QfxJEm5U0HYC+ezTdj1Vz8MXJkA741MvmUE/7bfI5eMDuVk7oQKs1h1nATinlbimlD1iMystJhDOA96WU5YZAeB84s9Xjbw1CZ0hr2o8WDprux9QWbM5mopV8an1qPtSWtOEcHnC4VckMb7USPIkLh0HAfsv3ImNZLN8UQqw3SsMMaeW+HYc2K2k6AG1W0nQ/puaQkhcTrWSYlYJeCBoTe9YQqChs/TkCHmWSsrsgFFDL3FlQ1+xereEN4FkppVcIcQ0qy//U1hygpQTPRBP2xh8poQAoLiljSycl+PWk5EE9lvi0dyxaOGi6H1MgpORGl+y21laye5VZKa0/7Fmu6jG1JgM44FGCwZrb4M5MVDgcAIZYvg82loWRUpZZvj4K/Nmy77yYfZfGO0lLCZ4JJzFWvgDF0L9/Af07KcGvxyZUdjN9aSxa79R0P2HhkNNMbSWfmtizhoCvpvV+B3+Dymuw1lNK0CENrADGCCFGGOHXl6HycsKYCZwG56GKSQK8C3xdCJFtVBr+urGs8zCFpvY5aNqB1hyOVir3Q2pez0gE89erSduVDnUWf0JUVVbDIZ1pPMBX7ofkFqq6B7zKhORMNbQPt3qZuLOA2haHJ6UMCCGuR03qNuBxKeUmIcRdwEop5evAjUKI84AAUA4sMvYtF0L8DiVgAO7q9DDtcLSSFg6atqM1h6MRKeFfJ8PS/4sse/vn8P5vEj9GfTk8PA9Kd7Z/POZTvTM12qwU1c/BqKKaZQiHqv2NjxPL27fCf76pPoejlSyaQyvyHKSUb0kpx0opR0kp/2As+7UhGJBS3ialnCilnCKlnC+l3GrZ93Ep5Wjj9UTCJ20r4fIZuvCepu1o4XA04m+AhnLY8X5k2Z7lsPujxI9Ruh0OroHCZR0wnnpwpKiXP06GtAypMdtckDlULatMQDiU7oBD64x+EQ2RaCWTvpohrZv9aDoALRy6m2AAtr2jJrBEeec2+N/P2n5Osx/Ckc1QU6w+e6uh5nDix/AZ5piyXW0fh4mpOThS4tdWAsP05FCmMHtyYppDXYnar67EMCslxzikM9o/9p6IDmXVdAD67uludrwLz14KxRsT32ff57D/q7af0+rM3fOxsawaao8oYZUIpvmnfE/bx2Hib1CCwZmijmsKSlNzMLEbbT0zB0P5bnjjx3BofdPHrSuNjDHQEB2tFCso+hK6KqumA9DCobsxn9bNJ/hEaKiIts2bJNo+01p+YvdSNRl7qwGpCtslQlg47E5s++YwK6Q6ktUYAkZ5bavmAJFIo6whsO1tWPUk7Hgv/jGDAWU6A6jYA/VlSlMwzUqJRyr1PpJ04T1N+9HCobupNyawhlYEsMQTDnWl8H9DYPfHLe9vag5ZQ5WvwVeLKvsD1Bxqaq9ozPNX7IFQKLF9miJsVkqNfIfovs+gzEpgRCwZ4/VWxz9mvSXtYM8ydc0Kjo04pFvhjO51aLOSpgPQoazdjTmJ1ScoHEJB5TNwBqOXV+5TppND62DkKZHlvjqlKWRaKjaYPodBM2Dz69E9mRP1O5g+h4BHCZTMdlSE8BtF8JwpkTGn5ETyHEzMp/7s4epd2JQ5zKTwE5UklzcG6ksjy7e8qd4HzYgItb7qjAaLWal3CQe/309RUREej6dV+2VmZrJly5aWN+wCetpY9uzZw+DBg3E4HK3eXwuH7sacxBLVHMyJ3FennthNE4L5BB375P/Jvcr8cvP2yDLTrJQ9AmQQqizJvglrDpaoovLdEeHgb1CvlJzEjmPuYzqkIRKx1JTmMGOR0gLeviVac3jpezBsDlz0eCRfwuYEb5USLP0nRnwUfVlz6KVVWYuKikhPT2f48OGIVoTh1tTUkJ6e3okjS5yeNJbq6mp8Ph9FRUWMGDGi1fv3rrunLxLWHMqa384k3OhGRod9emvUe/WB6O1rDio/gnUSNc1K5hN45V7L9olqDhazltXv8PbP4YmzEjuGiemQjhUOjTQHwySUkgNjTgdXRkRz8NUpwVZr+ExMZ3TBsep9wLFKuBxNZqVe5nPweDzk5ua2SjBomkYIQW5ubqs1MRMtHLqbRMxKH/4BdnygPludydYJ2pwkqw9G7xsWGhaNwFMFzvRI85wKq3BIVHOoVRVSkxwR4RD0w+ZXlYkrHgfXwtb/NV5uOqRNs5LXNFkZ4acmsdFF7oyI0DN/Q6xwGHyceh80Q70fDQ7pXhytpAVDx9Ke66mFQ3fTkkM6FIJP/hrJZra2yPRZSj+Yk2R1zORuTrQ1FqHRUKkmR7P8RGWhenektE5zcGUo7aPcyHUo/EQJHn99Y5OQtxYWX67CT4FJG36v8jUgYlbKHaO0g1VGEnHQF52LYIuxm7oyIsLPrNRqmpPqStTkOHCq+m4KB1PA9GXNIUk7pNtCWVkZU6dOZerUqRQUFDBo0KDwd5/P1+y+K1eu5MYbb2zxHHPmzOmo4XY62ufQ3bRkVqorUfWBDqxUT8dNCQePxefQUAkbX4SZ37VoDgdRBUFRZiV3JiQbfgHzqTtvTOuEgzNVaR+mprD1zcj6hkpI7x/5vvT/IiavgI+M6m2wq1YJv4BhVsocBCf9TG075XIlYFzpUGuE+dpiNYfMyO82hUNDudJg6ktVldeR8+GYb8Do09X6o0E46GY/bSI3N5e1a9cCcOedd5KWlsbNN98cXh8IBLDb40+ZM2fOZObMmdTU1DR7js8++6zDxtvZ6EeL7sRXH7Gv1zfRNdLqQ9j8WoxwsJiVTM1BBuGTv6kM6pJtEQFi1SgaKlW0jqk5hIXDuNaZlZxpKqy0cr/Kldj6VqSwnTXRzlsLXz4UEUaV+3D6q6FsB9QawsicrE+8CdIHwJp/qzwHl1VzsNRFAkNzMIWDJRmvrlS9UvOVgLr0PxEHeVp/OPkWJTD6KjqUtcNYtGgRP/zhD5k9eza33norX331FSeccALTpk1jzpw5bNu2DVDlsc8991xACZarr76aefPmMXLkSO67777w8dLS0sLbz5s3j4suuojx48fzrW99C2kkf7711luMHz+eGTNmcOONN4aP29VozaE7MU1JroymzUqmD8GVAZtehrEWZ6+3VgkImys6HHXzq8bxKyKaQ81BSDPWe6ogZ0REOFQXAQJyR8GG5w1bfwvZw7469WSeNUSNvXy3OseYr6vENKtv5Mhmpf1MWKBMRgdXq+WhAKx7Vn0eMFW9212QNUyZ2wI+pTmYxJqV3IZZKRSKbgBUV6JeqbmNxy0EnHpH87+tt9NLo5Ws/PaNTWw+2EQOSwzBYBCbrWUtacLADH7zjYmtHktRURGfffYZNpuN6upqli9fjt1u54MPPuD222/npZdearTP1q1b+eijj6ipqWHcuHFce+21jcJJ16xZw6ZNmxg4cCBz587l008/ZebMmVxzzTUsW7aMESNGsHDhwlaPt6PovXdPX8A0JeWNURqEta6QiSkcJl+sCt1Zawr5auHBufDp34wnaMP5ZE6UnqomHNKVSnOw2cGVqQrbuTIgY6Ba35z2UFeqzDamWckshGcW7Rs8K3IOk8Mb1PuYr6v3A6sj61Y/bfgGpkWWJWep/YPeaJ9DrMByZQBS9XeoKFRaAajoLFNzOBrRZqUO5eKLLw4Ln6qqKi6++GImTZrETTfdxKZNm+Luc8455+ByucjLy6Nfv34UFzeugDBr1iwGDx5MUlISU6dOpbCwkK1btzJy5Mhw6Gl3CgetOXQnpnDIHQMHVqmn5dhksuoDKiJoxMmw8jEoWgFJdvXU3VCuzClHtirbe/aw6CdoT2V0iKsx9yuzkmHGSc5SeQDuDFWzCFTegxnmasXvgX9MV2YZX50yK5kltHeZwmGmcQ6L+evwBnU+M3LowKrIuoo9MGBKJFIJlOAq3mxoDs2YlUzB0VCpTGNjvqb8Hlaz0tFIHzArteYJv7NzC1JTU8Off/WrXzF//nxeeeUVCgsLm+y05nJFHmRsNhuBQOOaZYls05303runL1Bnag6j1Xs801LNIWWD73eM+l6yNfKEHw7fLFaaQ/aI6Am05rDyQZjHAfXU76+LhHKatnhXRqSRTlMVTw+tVdpI2U7D55CqSnCAKlFhd0P/ScZvqYzsV7wR+k9WZqgkh8riBsgwBKEpNEySs5RwaeRziBOtBKp8eNAbOU5VkRJ4KXnxf0dfRzf76TSqqqoYNEjdt08++WSHH3/cuHHs3r2bwsJCAJ577rkOP0eidJtwEELYhBBrhBBvGt9HCCG+FELsFEI8Z7Rj7B3UHmk6tr85wmalscb3OMKh+qASBjkjlcYAlm5olth+T7V6Ok8foCZgUJMkqCfouhJEyB/xTZjlI0y/gyvdojnECAd/g9Ia9n0ROV84WqlAnc9bDTmjIsczzUqhoNICCiarEMv0ARD0ErC5YchstY1pijJxZylTUSgQ43OIk+cAEWFTMEkJqMJP1PfsYY2v59FAH9Aceiq33nort912G9OmTeuUJ/3k5GQeeOABzjzzTGbMmEF6ejqZmd0TWdedZqUfo/rsmo+GfwL+JqVcLIR4CPgu8GB3Da5VvHWLMo9c00Ljm72fK7OR+bRdXwYINamGv8dQfUA5a20OyB2tNIe0fkpDME1ItcXKHu/OUEIkY6Ay5ZjCIW8c1JXg8pZHnuhNzcGMIHJnqFyD1PzGjXSevlCtMyORqoog5FemoKQk9ZsqCpUGZLOrBDvzPOV7lKZSYGgU6QVQtQ+fMxd7/4nKyW6aokysCWrOFPUELINNaw6HjZIY2SPU+AuXq++m8Dna6KUZ0j2JO++8M+7yE044ge3bI6Vofv/73wMwb9485s2bR01NTaN9N26MlOOvra2N2t7kn//8Z/jz/Pnz2bp1K1JKrrvuOmbOjPn/6CK65dFCCDEYOAd41PgugFOBF41NngLO746xtYmKQtUus7mGPaEQ/PcS+PjPkWX1ZepJO9Uwf3z1iJqIzeNIGdEcAPLHqXd3lnpqDzueK5XW4cqACx6Ci59UWoQpHPKVZuL0lUee6E3NwWpWAqU9WDWHyv2w7zPYtUS9INLDwWmEP5maTK5hHjMdygC7PlTvprkpYwAAXlcuzLxajTV3VPS1shbFs7kiQimuQxqVeS1sSuim5iuNI31ARAgfbfSBaKWjmUceeYSpU6cyceJEqqqquOaaa7plHN2lOdwL3AqYNoNcoFJKaeppRUDcMp9CiB8APwDo378/S5cubbRNbW1t3OWdxQlle3H56/jkgzcIOKK7i5ljSa4/wGxvNeWFG1hvjG3C3q2kkcyKFRs4BdQkDHz11tPUpw7F7q/mxICHnUcaKFq6lOF1boYDe0tq6B9y4PZaei+E/Ow5VM7eVap18cygHVfZHhzAjsokxgCy6gDrvmpgCrB6yy6qDy5leHEVw4ED5bXsWLqUiT43KZXbWWGMcVDRG4wBgklObAEPfns6Dp9ycm/dU8Rhz1LGeRwMALaUBCheupQZQTveop1UPv1TRu96jOr0MazZWorcvpTRVUEGA7W2TNZ9tR7Ihpi/VW5pEZONz9t372WETMIBfPrlSvzOSM9qp7ecOQCVe2lw9+fL5Z8y2ZNELnDEPZLNH7dcvryr75UuoZdWZdUobrrpJm666abuHkbXCwchxLnAESnlKiHEvNbuL6V8GHgYYObMmTJetICZYNIlBAPwsbLjnzhxaKRcQ+xYNr4EX0GO3RsZ2+4/QfIwTjn1a/BFuvpn9lYxK98Ls+bB4Y3wKYyefhKjJ86DvDLY+xzDxk2Bhk1QEt2YZ8T4KYw43jz2INinfBJjjj8bdj5Kpqhj9JipsB6mzzlVaSLurbD3OQaNGM+gefPA+z6sWMu8U05ROQFP/Bn6TcQ2+jT47D4ckxfAmv8AMH7ydMZPmgfyczj8IceceB7HDJ4JhYNJD/rJO/IWDD+JjG+9wCkOo0aSfS0ceJNQav+m/0b73GBo4mOPmQSH06CmhrknnRLxaYBKIvxcfUweeIw6XtULUL6SfjO+QT/zWjRDl94rXYXuIa3pALrj0WIucJ4QohBYjDIn/R3IEkKYwmowcCD+7j2MuhKVJwBxo3wKDi1R5iLTaWrNIaguioSunvUn+M4rkDE4YjM3ndzphlmp3wT1npoHrjQaYc0JsNrts4aCPRmXtyxSHsPMCbA6pEGZlQINyuRVWwJ7P4NjzoV5t8G3XoThJ0eOa5qVhp8I+ePVyzx35V71W0efbnR4M0i3mJWawmpWsrsiEVixDmlHcsRJn22UJDbDV49WfwNoh7SmQ+jyu0dKeZuUcrCUcjhwGfChlPJbwEfARcZmVwKvdfXY2oR1sq/cr14l25WPofATxm37J7x7u2rHCcoW729Q66sPRcI5p30LBs9QE23hJ8rfsO6/yndgOnP7HaNKQUw4X/kcILpGkDXsM2p5OmQMUMKhar9yGJvrTZ+D+d0azrrpZUAa50tReQRmJVeIjGHESXDdlxGB5c6KXBczBNfE8J94Xc2EmVoFm80Z8TnE5jkIEfnNOYZwGDlfJduZpbo7CCHEmUKIbUY03S+a2e6bQggphJhpfHcIIZ4SQmwQQmwRQtzWoQOLOwhTOGjNQdN2elIS3M+BxUKI3wNrgMe6eTyJYS1UV7kPHvuamhjtbkDgc2bh8pUrzcGRojKhaw6pzyF/JHzUZPiJsH4xbHlDdTA78abIJAyRmkDmU3u/CbDPsK1YNQfr07crHdIH4qwsV07qzMFqYoVItJI5yZpJbZX7Yf1zKj+h/4TIseIJh1isk7upTZgMmwPn/YPyygHx940du91l9GAQ8c0kbqP0iKk5jDwluhNeByCEsAH3A19D+cNWCCFel1JujtkuHRWF96Vl8cWAS0o5WQiRAmwWQjwrpSzs0EFa0WYlTQfQrXqnlHKplPJc4/NuKeUsKeVoKeXFUkpvS/u3SKKtN032LIN1i+OvC4VUAlks1sJxO99XE/+078Bx34NJF7Juym8jyVkj56v3msOR7msZMX73kfPUE/Lz31Ghm7ObiFQwhUP6gMYTvDkeUE+RjhTIGIjLW6o0AlMAgJr4pyxUT/8Q0Ry2vKEymadcGn1e0xxlHUMspqnKkRo5nkmSDaZfgUxyNN7PxOGO1hbsbiUk4tWmj9UcOodZwE7jHvWhzKEL4mz3O1RItrW7igRSDZNpMuADEisa1Fa0WalNzJ8/n3fffTdq2b333su1114bd/t58+axcuVKAM4++2wqKysbbXPnnXdyzz33NHveV199lc2bI88Zv/71r/nggw9aOfqOpydpDh3L7o/h6fPha3fBnBua3s5bqwrDDZmlupiVbofhJylfQEMFlO5Q6964UWX6fv+j6Emq5jAgVG0g03R08i3hBKz6pUtVW8uiFTD+HNj2PyVAwgltMZpD1hD4/oew+t8qNDS9IP64TRNOap6asBvK4/scnOlqvBkDVJ5DpT/S2wCU3f6Chyz7ZatyFhueV5PLpIuIIjknknfQlOZgPvnnj4v0Fmgt7iwleO0u5WuINSmFtzOEYLxyHx3HIMDqUCoCopwaQojpwBAp5f+EELdYVr2IEiSHgBTgJill3KeWliLxEo2syivZwiRg85atHClvefu20BlRXpmZmS2WvI5HMBhs036xXHDBBTz99NNRPReeeeYZfve738U9fjAYpK6ujpqamnAmc+xYvF4vDoej2fG98MILnHnmmQwZoh6kbrlF3T7t/U3mWDweT5v+Vn1XOHz1sHIUv/crNdFM+7aaJIMBNbGZMfOvXKPq8VzwsBISAF88AKf8HJ76hkomu36lKpftrVZx+5lDwnZ8ag4pU4s5OWUNbZyZO/VbyrySO1p5UqoPRZ7qYoUDqGzis+9u/veZE3NKnjp/yRZVRM/EnKBNIZI+kCQZUAIv3jlNhIDvfgCbXlH5Ahkx5p+kJCWQaotbNiv1mxB/fSIkZynhYHNFO6VjcWUoJ7Q1k7qLEUIkAX8FFsVZPQsIoipbZQPLhRAfSCl3x27YUiRewpFV2xpgE0yYOIkJkxLYvg10RpTXli1b2lQjqaNqK33729/m97//PS6XC6fTSWFhIcXFxbz22mvccccdNDQ0cNFFF/Hb3/4WUPWQUlNTw32vV65cicvl4r777uOpp56iX79+DBkyJJzp/Mgjj/Dwww/j8/kYPXo0Tz/9NGvXruXtt9/ms88+4y9/+QsvvfQSv/vd7zj33HO56KKLWLJkCTfffDOBQIDjjjuOBx98EJfLxfDhw7nyyit544038Pv9vPDCC4wfH23CNa+L2+1m2rRp8X5ys/RJ4eDylMK2t2H2D5Wt//Xr4YsHVYP53UtVMbpjvqGqoZoNat64ERDKXr3ycbW/mWT29q2Rqqfv3Gb0DhAw98dqok8viJhPrNE8JkKoLGApVdtL01lrT44OzWwNTuOfITU3YupxxzErmZOmmUgHjU09sdidjc1JVtL6KeHgaEFz6Dc+/vpEMI9hdzYvHI67Gsae0fbzJMYBwHrRYqPp0oFJwFKjLWMB8LoQ4jzgcuAdKaUfOCKE+BSYCTQSDh1GX8iQfvsXkWq+LZAcDKjM/JYomAxn/bHJ1Tk5OcyaNYu3336bBQsWsHjxYi655BJuv/12cnJyCAaDnHbaaaxfv55jj40f8LBmzRoWL17M2rVrCQQCTJ8+nRkzlKZ+4YUX8v3vfx+AO+64g8cee4wbbriB8847LywMrHg8HhYtWsSSJUsYO3YsV1xxBQ8++CA/+clPAMjLy2P16tU88MAD3HPPPTz66KMJXK3E6ZNGyYLDHyitYfYPYdH/YMH9arLc87GapKdcqnoOfPQHyD9GmZ0CHhh6Apz5JzWRZgyES55SIZG7PlST09wfQ+k29UQ87kxY9mcV6mnNxjVt9/EwzDvK52CEsba1x6tVc8gdpQSEtbxE2KxkaA6tEQ4tkdpP+QGa+ofMGanqLQ09oe3nMMcf1hya8FGMPh1mXNn28yTGCmCMUf/LiYqye91cKaWsklLmSSmHG1F4XwDnSSlXAvtQ4doIIVKB44GtnTraXtxDurtZuHAhixcrv+PixYtZuHAhzz//PNOnT2fatGls2rQpyj8Qy2effcYFF1xASkoKGRkZnHfeeeF1Gzdu5KSTTmLy5Mk888wzTZb7Ntm2bRsjRoxg7FhV4eDKK69k2bJIiZ4LL7wQgBkzZoQL9XUkfVJzCCU5Vf8D00k57dvqZeXse1SUT84oNfGveQamf0c97d5gKSldfQj2fwnD5ipfQnqBcuDanCrktL5MLRs5X5mPxp1Fs6Qbpqigr7EzujWEfQ756sl5eswE2azm0IxZKRHSC5p2RoO67rcfaLlhUHNYNYcZi2DUqW0/VjuRUgaEENcD7wI24HEp5SYhxF3ASinl683sfj/whBBiE6rhxhNSyvWdOuC+0EO6mSf8WBo6sGT3ggULuOmmm1i9ejX19fXk5ORwzz33sGLFCrKzs1m0aBEej6flA8Vh0aJFvPrqq0yZMoUnn3yy3T4bs+R3Z5X77sV3T9PsH3ohfPOR5jeyGT0SMgdBWj7csgumXt54u4nnq4lw4gVqQj7+WvVU60yB45SKSPoAdYzzH2i5N3F6gRIOVQfaN0lnDlFO7exhyqkc6xuI9Tmk9kOSpJ4m05sJI02EE2+C81uoidgewQDRmsPIeTD9ivYdr51IKd+SUo6VUo6SUv7BWPbreIJBSjnP0BqQUtYa0XcTpZQTpJQtOJM6AN3sp82kpaUxf/58rr76ahYuXEh1dTWpqalkZmZSXFzM22+/3ez+c+fO5dVXX6WhoYGamhreeOON8LqamhoGDBiA3+/nmWeeCS9PT0+P63weN24chYWF7NypSsY8/fTTnHJKx4ZpN0ef1BzaRFNRNWn94Gdb4z8pz/q+iuoZ1IqqiekDoPJ15RRvj+Ywch78dEt03oGVsOZg+CFsdpVzkZyamH22OfLGqFdnEtYc2ilkjkZ0KGu7WLhwIRdccAGLFy9m/PjxTJs2jfHjxzNkyBDmzp3b7L5Tp07l0ksvZcqUKfTr14/jjov0Kvnd737H7Nmzyc/PZ/bs2WGBcNlll/H973+f++67jxdffDG8vdvt5oknnuDiiy8OO6R/+MMfds6PjoMWDonQVCRMah7cuKZ1x5pxlSqPcWhdpMpqWxCiacEAyieRZI8Sah53P1zZvaQ7WsHkSFSYpnWYYdJaOLSJ888/H2mpsNxUUx+rWci0+dfU1PDLX/6SX/7yl422v/baa+PmTMydOzfKj2E932mnncaaNY3nGKuPYebMmZ1SPFILh64mbzT84GMVCZXVic1ohICv/wGGHh9etG3cDcyafXwzO/UgjjlXvTStZ+A0mHNj1N9eo2ktWjh0B0J0dkav4vhoFbQ+dXCkJamm7+Jww9d/192j0PRytN6p0Wg0mkZo4aDRaHoMsrluippW057rqYWDRqPpEbjdbsrKyrSA6CCklJSVleF2u9u0v/Y5aDSaHsHgwYMpKiqipKSkVft5PJ42T4AdTU8bS1ZWFoMHty2fSgsHjUbTI3A4HIwY0fpAjaVLl7apsFxn0JfGos1KGo1Go2mEFg4ajUajaYQWDhqNRqNphOjNkQFCiBJgb5xVeUBpFw+nKfRY4tNTxtLcOIZJKbul3kgT93ZPuWagx9IUvWUsLd7bvVo4NIUQYqWUshXV8DoPPZb49JSx9JRxJEJPGqseS3z60li0WUmj0Wg0jdDCQaPRaDSN6KvC4eHuHoAFPZb49JSx9JRxJEJPGqseS3z6zFj6pM9Bo9FoNO2jr2oOGo1Go2kHfUo4CCHOFEJsE0LsFEL8oovPPUQI8ZEQYrMQYpMQ4sfG8juFEAeEEGuN19ldNJ5CIcQG45wrjWU5Qoj3hRA7jPfsLhjHOMtvXyuEqBZC/KSrrosQ4nEhxBEhxEbLsrjXQSjuM+6f9UKI6Z0xprag7+2o8eh7my64t6WUfeIF2IBdwEjACawDJnTh+QcA043P6cB2YAJwJ3BzN1yPQiAvZtmfgV8Yn38B/Kkb/kaHgWFddV2Ak4HpwMaWrgNwNvA2IIDjgS+7+u/WzHXT93ZkPPrelp1/b/clzWEWsFNKuVtK6QMWAwu66uRSykNSytXG5xpgCzCoq86fIAuAp4zPTwHnd/H5TwN2SSnjJS52ClLKZUB5zOKmrsMC4N9S8QWQJYQY0CUDbR59b7eMvrcVHXZv9yXhMAjYb/leRDfdwEKI4cA04Etj0fWGKvd4V6i7BhJ4TwixSgjxA2NZfynlIePzYaB/F43F5DLgWcv37rgu0PR16DH3UAw9Zlz63m6SPndv9yXh0CMQQqQBLwE/kVJWAw8Co4CpwCHgL100lBOllNOBs4DrhBAnW1dKpWt2WaiaEMIJnAe8YCzqrusSRVdfh96Mvrfj01fv7b4kHA4AQyzfBxvLugwhhAP1z/OMlPJlACllsZQyKKUMAY+gTASdjpTygPF+BHjFOG+xqUoa70e6YiwGZwGrpZTFxri65boYNHUduv0eaoJuH5e+t5ulT97bfUk4rADGCCFGGJL8MuD1rjq5EEIAjwFbpJR/tSy32vUuADbG7tsJY0kVQqSbn4GvG+d9HbjS2OxK4LXOHouFhVjU7u64Lhaaug6vA1cYkR3HA1UWFb070fd25Jz63m6ejru3u9Kj3wXe+7NRkRS7gF928blPRKlw64G1xuts4Glgg7H8dWBAF4xlJCqiZR2wybwWQC6wBNgBfADkdNG1SQXKgEzLsi65Lqh/2kOAH2Vn/W5T1wEVyXG/cf9sAGZ25T3Uwu/Q97bU93bMuTv13tYZ0hqNRqNpRF8yK2k0Go2mg9DCQaPRaDSN0MJBo9FoNI3QwkGj0Wg0jdDCQaPRaDSN0MKhFyKECMZUg+ywKp1CiOHWKo8aTVei7+2eg727B6BpEw1SyqndPQiNphPQ93YPQWsOfQijzv2fjVr3XwkhRhvLhwshPjQKgS0RQgw1lvcXQrwihFhnvOYYh7IJIR4Rqnb/e0KI5G77URoN+t7uDrRw6J0kx6jel1rWVUkpJwP/BO41lv0DeEpKeSzwDHCfsfw+4GMp5RRUXfhNxvIxwP1SyolAJfDNTv01Gk0EfW/3EHSGdC9ECFErpUyLs7wQOFVKudsolHZYSpkrhChFpfD7jeWHpJR5QogSYLCU0ms5xnDgfSnlGOP7zwGHlPL3XfDTNEc5+t7uOWjNoe8hm/jcGryWz0G0b0rTM9D3dheihUPf41LL++fG589QlTwBvgUsNz4vAa4FEELYhBCZXTVIjaYN6Hu7C9FSs3eSLIRYa/n+jpTSDPnLFkKsRz0hLTSW3QA8IYS4BSgBrjKW/xh4WAjxXdRT1LWoKo8aTXeh7+0egvY59CEMu+xMKWVpd49Fo+lI9L3d9Wizkkaj0WgaoTUHjUaj0TRCaw4ajUajaYQWDhqNRqNphBYOGo1Go2mEFg4ajUajaYQWDhqNRqNphBYOGo1Go2nE/wf9+0/QYldiuwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.5522\n",
      "Validation AUC: 0.5588\n"
     ]
    }
   ],
   "source": [
    "#model = create_model()\n",
    "K=2\n",
    "R=5\n",
    "NUM_RUNS = 10\n",
    "N_EPOCHS = 100\n",
    "val_acc = np.zeros(NUM_RUNS)\n",
    "AUC= np.zeros(NUM_RUNS)\n",
    "\n",
    "for i in range(NUM_RUNS):\n",
    "  MA = MultipleAnnotators_Classification(2, 5, 0.1)\n",
    "  model =  create_model()\n",
    "  model = MA.fit(model, train_batches_MA, val_batches_MA, N_EPOCHS)\n",
    "  #model = MA.fit(model, Data_train_MA, N_EPOCHS)\n",
    "  val_acc[i] = MA.eval_model(test_batches_MA)\n",
    "  print(\"Validation acc: %.4f\" % (float(val_acc[i]),))\n",
    "    \n",
    " #AUC =======================\n",
    "  val_AUC_metric = tf.keras.metrics.AUC( from_logits = True)\n",
    "  for x_batch_val, y_batch_val in test_batches_MA:\n",
    "      val_logits = model(x_batch_val.numpy(), training=False)\n",
    "      # tf.print(y_batch_val)\n",
    "      val_AUC_metric.update_state(y_batch_val, val_logits[:,:K].numpy().argmax(axis=1).astype('float'))\n",
    "\n",
    "  val_AUC = val_AUC_metric.result()\n",
    "  val_AUC_metric.reset_states()\n",
    "  val_AUC = val_AUC.numpy()\n",
    "  print(\"Validation AUC: %.4f\" % (float(val_AUC),))\n",
    "  AUC[i] = val_AUC\n",
    "  #===================================================\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(val_acc)\n",
    "#df.to_csv('/content/CatDogs_MA_InceptionV3.csv',index=False) # save to notebook output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57dc2b48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T17:52:34.399843Z",
     "iopub.status.busy": "2023-01-03T17:52:34.398840Z",
     "iopub.status.idle": "2023-01-03T17:52:34.407543Z",
     "shell.execute_reply": "2023-01-03T17:52:34.406474Z"
    },
    "papermill": {
     "duration": 0.668598,
     "end_time": "2023-01-03T17:52:34.410591",
     "exception": false,
     "start_time": "2023-01-03T17:52:33.741993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75816852, 0.63048154, 0.69926912, 0.65563196, 0.66509026,\n",
       "       0.5522356 , 0.73688734, 0.77063626, 0.76483232, 0.5522356 ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9959bdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T17:52:35.775263Z",
     "iopub.status.busy": "2023-01-03T17:52:35.774896Z",
     "iopub.status.idle": "2023-01-03T17:52:35.781529Z",
     "shell.execute_reply": "2023-01-03T17:52:35.780602Z"
    },
    "id": "Uq3Ki4uQxcVj",
    "papermill": {
     "duration": 0.66046,
     "end_time": "2023-01-03T17:52:35.783778",
     "exception": false,
     "start_time": "2023-01-03T17:52:35.123318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75788254, 0.63359034, 0.70049107, 0.6586358 , 0.66890585,\n",
       "       0.55950713, 0.7374053 , 0.77141839, 0.76399177, 0.5587756 ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6208c62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T17:52:37.146770Z",
     "iopub.status.busy": "2023-01-03T17:52:37.146401Z",
     "iopub.status.idle": "2023-01-03T17:52:37.155302Z",
     "shell.execute_reply": "2023-01-03T17:52:37.154324Z"
    },
    "papermill": {
     "duration": 0.662183,
     "end_time": "2023-01-03T17:52:37.157549",
     "exception": false,
     "start_time": "2023-01-03T17:52:36.495366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy:  67.85\n",
      "Average std:  7.8100000000000005\n"
     ]
    }
   ],
   "source": [
    "print('Average Accuracy: ', np.round(val_acc.mean(),4)*100) \n",
    "print('Average std: ',np.round(np.std(val_acc),4)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31c1294a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T17:52:38.585717Z",
     "iopub.status.busy": "2023-01-03T17:52:38.585351Z",
     "iopub.status.idle": "2023-01-03T17:52:38.591987Z",
     "shell.execute_reply": "2023-01-03T17:52:38.590691Z"
    },
    "papermill": {
     "duration": 0.781898,
     "end_time": "2023-01-03T17:52:38.594043",
     "exception": false,
     "start_time": "2023-01-03T17:52:37.812145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy:  68.11\n",
      "Average std:  7.5600000000000005\n"
     ]
    }
   ],
   "source": [
    "print('Average Accuracy: ', np.round( AUC.mean(),4)*100) \n",
    "print('Average std: ',np.round(np.std( AUC),4)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef70257f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T17:52:40.156775Z",
     "iopub.status.busy": "2023-01-03T17:52:40.156409Z",
     "iopub.status.idle": "2023-01-03T17:52:40.160842Z",
     "shell.execute_reply": "2023-01-03T17:52:40.159845Z"
    },
    "id": "cxSh9vktxcVj",
    "papermill": {
     "duration": 0.787196,
     "end_time": "2023-01-03T17:52:40.162868",
     "exception": false,
     "start_time": "2023-01-03T17:52:39.375672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # accuracy\n",
    "# val_acc_GCCE  = np.zeros(NUM_RUNS)\n",
    "\n",
    "# for i in range(len(classification_report_r)):\n",
    "   \n",
    "#   val_acc_GCCE[i] = classification_report_r[i]['accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39ce742d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T17:52:41.479776Z",
     "iopub.status.busy": "2023-01-03T17:52:41.479089Z",
     "iopub.status.idle": "2023-01-03T17:52:41.485457Z",
     "shell.execute_reply": "2023-01-03T17:52:41.484493Z"
    },
    "id": "Ak1z-BteyMF6",
    "outputId": "8b14abfb-4940-45fb-b50a-df0a4f7a731b",
    "papermill": {
     "duration": 0.663399,
     "end_time": "2023-01-03T17:52:41.487728",
     "exception": false,
     "start_time": "2023-01-03T17:52:40.824329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75816852, 0.63048154, 0.69926912, 0.65563196, 0.66509026,\n",
       "       0.5522356 , 0.73688734, 0.77063626, 0.76483232, 0.5522356 ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59a6944e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T17:52:42.855863Z",
     "iopub.status.busy": "2023-01-03T17:52:42.855034Z",
     "iopub.status.idle": "2023-01-03T17:52:42.860692Z",
     "shell.execute_reply": "2023-01-03T17:52:42.859678Z"
    },
    "id": "K-EeM9bqyI-w",
    "outputId": "b59f0070-7e4c-4480-cd00-da90044724ed",
    "papermill": {
     "duration": 0.659389,
     "end_time": "2023-01-03T17:52:42.863347",
     "exception": false,
     "start_time": "2023-01-03T17:52:42.203958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy:  67.85\n"
     ]
    }
   ],
   "source": [
    "print('Average Accuracy: ', np.round(val_acc.mean(),4)*100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ff09bb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T17:52:44.591355Z",
     "iopub.status.busy": "2023-01-03T17:52:44.590876Z",
     "iopub.status.idle": "2023-01-03T17:52:44.595724Z",
     "shell.execute_reply": "2023-01-03T17:52:44.594783Z"
    },
    "id": "I0E-qXsr1RFC",
    "papermill": {
     "duration": 1.090967,
     "end_time": "2023-01-03T17:52:44.602868",
     "exception": false,
     "start_time": "2023-01-03T17:52:43.511901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y_test = np.asarray([aux[1].numpy() for aux  in validation_data])\n",
    "# X_test = np.asarray([aux[0].numpy() for aux  in validation_data])\n",
    "# # N = len(y_true)\n",
    "# # #test_batches_MA\n",
    "# # aux1 = [test_batches_MA[i][0] for i in range(N)]\n",
    "# # aux2 = [test_batches_MA[i][1] for i in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ad8f1dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T17:52:46.002199Z",
     "iopub.status.busy": "2023-01-03T17:52:46.001831Z",
     "iopub.status.idle": "2023-01-03T17:52:46.006279Z",
     "shell.execute_reply": "2023-01-03T17:52:46.005146Z"
    },
    "id": "9P-KeFKp2TEr",
    "outputId": "1998d331-5546-4bc4-c007-8180b58dd574",
    "papermill": {
     "duration": 0.665813,
     "end_time": "2023-01-03T17:52:46.008341",
     "exception": false,
     "start_time": "2023-01-03T17:52:45.342528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec9c0373",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T17:52:47.369926Z",
     "iopub.status.busy": "2023-01-03T17:52:47.369556Z",
     "iopub.status.idle": "2023-01-03T17:52:47.373802Z",
     "shell.execute_reply": "2023-01-03T17:52:47.372777Z"
    },
    "id": "6pWXVQnK1GXo",
    "outputId": "89294013-e100-4cc0-9a9e-3f7bad8f7729",
    "papermill": {
     "duration": 0.655273,
     "end_time": "2023-01-03T17:52:47.375703",
     "exception": false,
     "start_time": "2023-01-03T17:52:46.720430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pred = model.predict(X_test)\n",
    "# pred[:, :2].argmax(axis=1)\n",
    "# print(classification_report(pred[:, :2].argmax(axis=1), y_test ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65053f0a",
   "metadata": {
    "id": "STPfoIdfxcVj",
    "papermill": {
     "duration": 0.708892,
     "end_time": "2023-01-03T17:52:48.736587",
     "exception": false,
     "start_time": "2023-01-03T17:52:48.027695",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17f9dc8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T17:52:50.112086Z",
     "iopub.status.busy": "2023-01-03T17:52:50.111059Z",
     "iopub.status.idle": "2023-01-03T17:52:50.120586Z",
     "shell.execute_reply": "2023-01-03T17:52:50.119499Z"
    },
    "id": "z1b6tNcdxcVj",
    "outputId": "fcf6fada-a56b-45e9-ab8b-e3562b97621e",
    "papermill": {
     "duration": 0.707337,
     "end_time": "2023-01-03T17:52:50.124208",
     "exception": false,
     "start_time": "2023-01-03T17:52:49.416871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6785468518733978"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4808cc0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T17:52:51.659754Z",
     "iopub.status.busy": "2023-01-03T17:52:51.659399Z",
     "iopub.status.idle": "2023-01-03T17:52:51.664913Z",
     "shell.execute_reply": "2023-01-03T17:52:51.663919Z"
    },
    "papermill": {
     "duration": 0.684643,
     "end_time": "2023-01-03T17:52:51.667529",
     "exception": false,
     "start_time": "2023-01-03T17:52:50.982886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD Accuracy:  7.8100000000000005\n"
     ]
    }
   ],
   "source": [
    "print('STD Accuracy: ', np.round(np.std(val_acc),4)*100) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8e853e",
   "metadata": {
    "id": "xxyE_WnFxcVj",
    "papermill": {
     "duration": 0.652092,
     "end_time": "2023-01-03T17:52:53.036863",
     "exception": false,
     "start_time": "2023-01-03T17:52:52.384771",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "MC droput run this in a loop with training layer set to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83067cf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T17:52:54.458823Z",
     "iopub.status.busy": "2023-01-03T17:52:54.456834Z",
     "iopub.status.idle": "2023-01-03T17:52:54.462209Z",
     "shell.execute_reply": "2023-01-03T17:52:54.461298Z"
    },
    "id": "I6R3im8_xcVk",
    "papermill": {
     "duration": 0.742119,
     "end_time": "2023-01-03T17:52:54.464385",
     "exception": false,
     "start_time": "2023-01-03T17:52:53.722266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y_probas = np.stack([MA.eval_model((test_batches,training=True) # se activa training en True para que el Dropout se aplique\n",
    "#                    for sample in range(100)])\n",
    "\n",
    "# y_proba = y_probas.mean(axis=0)\n",
    "# y_std = y_probas.std(axis=0)\n",
    "# y_probas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2dbf8fbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T17:52:55.829709Z",
     "iopub.status.busy": "2023-01-03T17:52:55.829313Z",
     "iopub.status.idle": "2023-01-03T17:52:55.834219Z",
     "shell.execute_reply": "2023-01-03T17:52:55.833328Z"
    },
    "id": "uLKmvJlpxcVk",
    "papermill": {
     "duration": 0.7226,
     "end_time": "2023-01-03T17:52:55.836140",
     "exception": false,
     "start_time": "2023-01-03T17:52:55.113540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y_ped =np.argmax(y_proba,axis=1)\n",
    "# accuracy=np.sum(y_pred==test_label)/len(test_label)\n",
    "# accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc096b81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T17:52:57.210972Z",
     "iopub.status.busy": "2023-01-03T17:52:57.208959Z",
     "iopub.status.idle": "2023-01-03T17:52:57.214543Z",
     "shell.execute_reply": "2023-01-03T17:52:57.213567Z"
    },
    "id": "Jl1sFNOf1KgC",
    "papermill": {
     "duration": 0.734004,
     "end_time": "2023-01-03T17:52:57.216567",
     "exception": false,
     "start_time": "2023-01-03T17:52:56.482563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "   \n",
    "# r1 = np.mean(val_acc)\n",
    "# print(\"\\nMean: \", r1)\n",
    "  \n",
    "# r2 = np.std(val_acc)\n",
    "# print(\"\\nstd: \", r2)\n",
    "  \n",
    "# r3 = np.var(val_acc)\n",
    "# print(\"\\nvariance: \", r3)\n",
    "# #MA.eval_model(test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd15106a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-03T17:52:58.518555Z",
     "iopub.status.busy": "2023-01-03T17:52:58.518157Z",
     "iopub.status.idle": "2023-01-03T17:52:58.522354Z",
     "shell.execute_reply": "2023-01-03T17:52:58.521414Z"
    },
    "id": "KqeZpkxuxcVk",
    "papermill": {
     "duration": 0.660724,
     "end_time": "2023-01-03T17:52:58.524329",
     "exception": false,
     "start_time": "2023-01-03T17:52:57.863605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f515d8",
   "metadata": {
    "id": "6cjM2VJJkuKK",
    "papermill": {
     "duration": 0.657914,
     "end_time": "2023-01-03T17:52:59.894627",
     "exception": false,
     "start_time": "2023-01-03T17:52:59.236713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "VGG19 --> acc:0.8613  --> 0.894454 --> 0.772356"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10812.992918,
   "end_time": "2023-01-03T17:53:04.002327",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-03T14:52:51.009409",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
