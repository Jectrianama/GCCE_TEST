{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d5ad9d0",
   "metadata": {
    "id": "oAuRT75GdLFw",
    "papermill": {
     "duration": 0.007173,
     "end_time": "2023-02-04T05:02:32.954608",
     "exception": false,
     "start_time": "2023-02-04T05:02:32.947435",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cats vs. Dogs Class dataset for multiple annotators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e45c0cf",
   "metadata": {
    "id": "9rK94t33nwDC",
    "papermill": {
     "duration": 0.005377,
     "end_time": "2023-02-04T05:02:32.965933",
     "exception": false,
     "start_time": "2023-02-04T05:02:32.960556",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3c44a64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T05:02:32.979147Z",
     "iopub.status.busy": "2023-02-04T05:02:32.978390Z",
     "iopub.status.idle": "2023-02-04T05:02:39.332893Z",
     "shell.execute_reply": "2023-02-04T05:02:39.331908Z"
    },
    "id": "zSyMHuCVys-O",
    "papermill": {
     "duration": 6.364133,
     "end_time": "2023-02-04T05:02:39.335577",
     "exception": false,
     "start_time": "2023-02-04T05:02:32.971444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense,Conv2D,Flatten,MaxPooling2D,GlobalAveragePooling2D\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07874846",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T05:02:39.349525Z",
     "iopub.status.busy": "2023-02-04T05:02:39.348351Z",
     "iopub.status.idle": "2023-02-04T05:02:39.353038Z",
     "shell.execute_reply": "2023-02-04T05:02:39.352181Z"
    },
    "id": "-E1MJt8cxlwg",
    "outputId": "ea43c1c9-075f-44de-d2d8-e135799b6630",
    "papermill": {
     "duration": 0.01326,
     "end_time": "2023-02-04T05:02:39.354834",
     "exception": false,
     "start_time": "2023-02-04T05:02:39.341574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "023792c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T05:02:39.367909Z",
     "iopub.status.busy": "2023-02-04T05:02:39.367606Z",
     "iopub.status.idle": "2023-02-04T05:02:39.371675Z",
     "shell.execute_reply": "2023-02-04T05:02:39.370779Z"
    },
    "id": "QJPvjdZ-f8ca",
    "papermill": {
     "duration": 0.012533,
     "end_time": "2023-02-04T05:02:39.373624",
     "exception": false,
     "start_time": "2023-02-04T05:02:39.361091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# os.chdir('/content/drive/Shareddrives/Multiple Anotators/CrowdLayer/Notebooks')\n",
    "# cwd = os.getcwd()\n",
    "# sys.path.append(\"../Models\")\n",
    "\n",
    "\n",
    "# from Multiple_Annotators_C import MultipleAnnotators_Classification\n",
    "\n",
    "#import sys\n",
    "#sys.path.insert(1, '../input/multiple-annotators-c/')\n",
    "#os.chdir('/Multiple Anotators-c/')\n",
    "#cwd = os.getcwd()\n",
    "#sys.path.append('/input/multiple-annotators-c')\n",
    "#from Multiple_Annotators_C import MultipleAnnotators_Classification\n",
    "\n",
    "# seed_value= 12321 \n",
    "# from numpy.random import seed\n",
    "# seed(seed_value)\n",
    "# tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59cc4d1",
   "metadata": {
    "id": "6Un5nFWgnyem",
    "papermill": {
     "duration": 0.005386,
     "end_time": "2023-02-04T05:02:39.384543",
     "exception": false,
     "start_time": "2023-02-04T05:02:39.379157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Download and Prepare the Dataset\n",
    "\n",
    "We will use the [Cats vs Dogs](https://www.tensorflow.org/datasets/catalog/cats_vs_dogs) dataset and we can load it via Tensorflow Datasets. The images are labeled 0 for cats and 1 for dogs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2317ff3",
   "metadata": {
    "id": "Gw6K2Uey06kh",
    "papermill": {
     "duration": 0.005764,
     "end_time": "2023-02-04T05:02:39.395962",
     "exception": false,
     "start_time": "2023-02-04T05:02:39.390198",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Multiple annotators model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f42e1f33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T05:02:39.409814Z",
     "iopub.status.busy": "2023-02-04T05:02:39.409552Z",
     "iopub.status.idle": "2023-02-04T05:02:42.422599Z",
     "shell.execute_reply": "2023-02-04T05:02:42.421441Z"
    },
    "id": "xam4REp209Sd",
    "papermill": {
     "duration": 3.023755,
     "end_time": "2023-02-04T05:02:42.425748",
     "exception": false,
     "start_time": "2023-02-04T05:02:39.401993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-04 05:02:39.504986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-04 05:02:39.660879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-04 05:02:39.661664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-04 05:02:39.663272: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-04 05:02:39.669899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-04 05:02:39.670569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-04 05:02:39.671250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-04 05:02:41.881018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-04 05:02:41.882229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-04 05:02:41.883242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-04 05:02:41.884112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "validation_data = tf.data.experimental.load('/kaggle/input/cat-vs-dog-ma-sin/cats_dogs_Te')\n",
    "train_data_MA = tf.data.experimental.load('/kaggle/input/cat-vs-dog-ma-sin/cats_dogs_MA_sin_Tr_1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d73efe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T05:02:42.448774Z",
     "iopub.status.busy": "2023-02-04T05:02:42.448393Z",
     "iopub.status.idle": "2023-02-04T05:02:42.464020Z",
     "shell.execute_reply": "2023-02-04T05:02:42.463015Z"
    },
    "id": "D_S0EJ3mFdfK",
    "outputId": "9ed3c2c7-50b4-4445-a01e-c9a3d780c403",
    "papermill": {
     "duration": 0.030364,
     "end_time": "2023-02-04T05:02:42.466128",
     "exception": false,
     "start_time": "2023-02-04T05:02:42.435764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18610"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_count = tf.data.experimental.cardinality(train_data_MA).numpy() # los datos de training son 18610 usar subconjunto de 5000\n",
    "image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f1b65ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T05:02:42.479989Z",
     "iopub.status.busy": "2023-02-04T05:02:42.479186Z",
     "iopub.status.idle": "2023-02-04T05:02:42.486367Z",
     "shell.execute_reply": "2023-02-04T05:02:42.485262Z"
    },
    "id": "ctjLei0TxcVh",
    "outputId": "6f578b73-ebdf-4465-91c7-2adb7d127174",
    "papermill": {
     "duration": 0.021325,
     "end_time": "2023-02-04T05:02:42.493466",
     "exception": false,
     "start_time": "2023-02-04T05:02:42.472141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4652"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_count1 = tf.data.experimental.cardinality(validation_data).numpy() # los datos de training son 18610\n",
    "image_count1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e91a031",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T05:02:42.518488Z",
     "iopub.status.busy": "2023-02-04T05:02:42.517556Z",
     "iopub.status.idle": "2023-02-04T05:02:59.431026Z",
     "shell.execute_reply": "2023-02-04T05:02:59.429982Z"
    },
    "id": "opk5MXl4IwjC",
    "papermill": {
     "duration": 16.923248,
     "end_time": "2023-02-04T05:02:59.433531",
     "exception": false,
     "start_time": "2023-02-04T05:02:42.510283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-04 05:02:42.545208: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "#X_test = [validation_data[i][0] for i in range(image_count1)]\n",
    "#Y_true_test = [validation_data[i][1] for i in range(image_count1)]\n",
    "Y_true_test = np.asarray([aux[1].numpy() for aux  in validation_data])\n",
    "X_test = np.asarray([aux[0].numpy() for aux  in validation_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "664e34fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T05:02:59.447259Z",
     "iopub.status.busy": "2023-02-04T05:02:59.446960Z",
     "iopub.status.idle": "2023-02-04T05:02:59.453797Z",
     "shell.execute_reply": "2023-02-04T05:02:59.452915Z"
    },
    "id": "-BydcVOQxcVh",
    "outputId": "8c1b4ed2-7c43-4675-f055-f9e4e3f5b3dd",
    "papermill": {
     "duration": 0.016083,
     "end_time": "2023-02-04T05:02:59.455893",
     "exception": false,
     "start_time": "2023-02-04T05:02:59.439810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18610"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "978dcce9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T05:02:59.469120Z",
     "iopub.status.busy": "2023-02-04T05:02:59.468479Z",
     "iopub.status.idle": "2023-02-04T05:02:59.475808Z",
     "shell.execute_reply": "2023-02-04T05:02:59.475023Z"
    },
    "id": "HdFme6fdxcVh",
    "papermill": {
     "duration": 0.016214,
     "end_time": "2023-02-04T05:02:59.477922",
     "exception": false,
     "start_time": "2023-02-04T05:02:59.461708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_size = int(image_count * 0.2)\n",
    "train_ds_MA = train_data_MA.skip(val_size)\n",
    "val_ds_MA = train_data_MA.take(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b0e4c56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T05:02:59.491095Z",
     "iopub.status.busy": "2023-02-04T05:02:59.490510Z",
     "iopub.status.idle": "2023-02-04T05:02:59.500632Z",
     "shell.execute_reply": "2023-02-04T05:02:59.499810Z"
    },
    "id": "aVHIlFpgxcVi",
    "papermill": {
     "duration": 0.019061,
     "end_time": "2023-02-04T05:02:59.502749",
     "exception": false,
     "start_time": "2023-02-04T05:02:59.483688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_batches_MA = train_ds_MA.shuffle(1024).batch(batch_size)\n",
    "val_batches_MA = val_ds_MA.shuffle(1024).batch(batch_size)\n",
    "test_batches_MA = validation_data.shuffle(1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef0a7a97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T05:02:59.517106Z",
     "iopub.status.busy": "2023-02-04T05:02:59.515326Z",
     "iopub.status.idle": "2023-02-04T05:02:59.522967Z",
     "shell.execute_reply": "2023-02-04T05:02:59.521974Z"
    },
    "id": "GsB4EA2-xcVi",
    "outputId": "2d45809e-a9cc-408f-9a8b-745e8fe850e9",
    "papermill": {
     "duration": 0.016479,
     "end_time": "2023-02-04T05:02:59.525038",
     "exception": false,
     "start_time": "2023-02-04T05:02:59.508559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14888"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_count = tf.data.experimental.cardinality(train_ds_MA).numpy() # los datos de training son 18610 usar subconjunto de 5000\n",
    "image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3107596b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T05:02:59.538160Z",
     "iopub.status.busy": "2023-02-04T05:02:59.537895Z",
     "iopub.status.idle": "2023-02-04T05:02:59.544489Z",
     "shell.execute_reply": "2023-02-04T05:02:59.543585Z"
    },
    "id": "Hk33DzwkxcVi",
    "outputId": "aad91eec-842c-4995-de90-5bb715539b6a",
    "papermill": {
     "duration": 0.015346,
     "end_time": "2023-02-04T05:02:59.546490",
     "exception": false,
     "start_time": "2023-02-04T05:02:59.531144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3722"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_count_val = tf.data.experimental.cardinality(val_ds_MA).numpy() # los datos de training son 18610 usar subconjunto de 5000\n",
    "image_count_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1335e3e",
   "metadata": {
    "id": "UMeK3NG3xcVi",
    "papermill": {
     "duration": 0.005852,
     "end_time": "2023-02-04T05:02:59.558489",
     "exception": false,
     "start_time": "2023-02-04T05:02:59.552637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9d12d66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T05:02:59.571640Z",
     "iopub.status.busy": "2023-02-04T05:02:59.571386Z",
     "iopub.status.idle": "2023-02-04T05:03:16.411701Z",
     "shell.execute_reply": "2023-02-04T05:03:16.410774Z"
    },
    "id": "uvwc7eixxcVi",
    "outputId": "d7766078-8c40-41ed-fb01-66b5f62a07f1",
    "papermill": {
     "duration": 16.849269,
     "end_time": "2023-02-04T05:03:16.413802",
     "exception": false,
     "start_time": "2023-02-04T05:02:59.564533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-04 05:03:11.166308: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 1 of 1024\n",
      "2023-02-04 05:03:14.396813: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:228] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotator 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.75      0.74        60\n",
      "         1.0       0.77      0.75      0.76        68\n",
      "\n",
      "    accuracy                           0.75       128\n",
      "   macro avg       0.75      0.75      0.75       128\n",
      "weighted avg       0.75      0.75      0.75       128\n",
      "\n",
      "annotator 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.55      0.57        60\n",
      "         1.0       0.62      0.66      0.64        68\n",
      "\n",
      "    accuracy                           0.61       128\n",
      "   macro avg       0.61      0.61      0.61       128\n",
      "weighted avg       0.61      0.61      0.61       128\n",
      "\n",
      "annotator 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.62      0.57        60\n",
      "         1.0       0.60      0.51      0.56        68\n",
      "\n",
      "    accuracy                           0.56       128\n",
      "   macro avg       0.57      0.57      0.56       128\n",
      "weighted avg       0.57      0.56      0.56       128\n",
      "\n",
      "annotator 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.57      0.54        60\n",
      "         1.0       0.58      0.53      0.55        68\n",
      "\n",
      "    accuracy                           0.55       128\n",
      "   macro avg       0.55      0.55      0.55       128\n",
      "weighted avg       0.55      0.55      0.55       128\n",
      "\n",
      "annotator 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.21      0.25      0.23        60\n",
      "         1.0       0.20      0.16      0.18        68\n",
      "\n",
      "    accuracy                           0.20       128\n",
      "   macro avg       0.20      0.21      0.20       128\n",
      "weighted avg       0.20      0.20      0.20       128\n",
      "\n",
      "annotator 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.85      0.84        61\n",
      "         1.0       0.86      0.84      0.85        67\n",
      "\n",
      "    accuracy                           0.84       128\n",
      "   macro avg       0.84      0.84      0.84       128\n",
      "weighted avg       0.84      0.84      0.84       128\n",
      "\n",
      "annotator 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.51      0.51        61\n",
      "         1.0       0.56      0.57      0.56        67\n",
      "\n",
      "    accuracy                           0.54       128\n",
      "   macro avg       0.54      0.54      0.54       128\n",
      "weighted avg       0.54      0.54      0.54       128\n",
      "\n",
      "annotator 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.70      0.66        61\n",
      "         1.0       0.69      0.61      0.65        67\n",
      "\n",
      "    accuracy                           0.66       128\n",
      "   macro avg       0.66      0.66      0.66       128\n",
      "weighted avg       0.66      0.66      0.66       128\n",
      "\n",
      "annotator 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.54      0.52        61\n",
      "         1.0       0.55      0.51      0.53        67\n",
      "\n",
      "    accuracy                           0.52       128\n",
      "   macro avg       0.52      0.52      0.52       128\n",
      "weighted avg       0.53      0.52      0.52       128\n",
      "\n",
      "annotator 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.23      0.26      0.24        61\n",
      "         1.0       0.22      0.19      0.21        67\n",
      "\n",
      "    accuracy                           0.23       128\n",
      "   macro avg       0.23      0.23      0.23       128\n",
      "weighted avg       0.23      0.23      0.23       128\n",
      "\n",
      "annotator 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.79      0.77        53\n",
      "         1.0       0.85      0.81      0.83        75\n",
      "\n",
      "    accuracy                           0.80       128\n",
      "   macro avg       0.80      0.80      0.80       128\n",
      "weighted avg       0.81      0.80      0.81       128\n",
      "\n",
      "annotator 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.47      0.60      0.53        53\n",
      "         1.0       0.65      0.52      0.58        75\n",
      "\n",
      "    accuracy                           0.55       128\n",
      "   macro avg       0.56      0.56      0.55       128\n",
      "weighted avg       0.58      0.55      0.56       128\n",
      "\n",
      "annotator 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.66      0.64        53\n",
      "         1.0       0.75      0.72      0.73        75\n",
      "\n",
      "    accuracy                           0.70       128\n",
      "   macro avg       0.69      0.69      0.69       128\n",
      "weighted avg       0.70      0.70      0.70       128\n",
      "\n",
      "annotator 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      0.53      0.49        53\n",
      "         1.0       0.62      0.55      0.58        75\n",
      "\n",
      "    accuracy                           0.54       128\n",
      "   macro avg       0.54      0.54      0.53       128\n",
      "weighted avg       0.55      0.54      0.54       128\n",
      "\n",
      "annotator 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.22      0.28      0.25        53\n",
      "         1.0       0.37      0.29      0.33        75\n",
      "\n",
      "    accuracy                           0.29       128\n",
      "   macro avg       0.29      0.29      0.29       128\n",
      "weighted avg       0.31      0.29      0.29       128\n",
      "\n",
      "annotator 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.81      0.85        64\n",
      "         1.0       0.83      0.89      0.86        64\n",
      "\n",
      "    accuracy                           0.85       128\n",
      "   macro avg       0.85      0.85      0.85       128\n",
      "weighted avg       0.85      0.85      0.85       128\n",
      "\n",
      "annotator 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.61      0.61        64\n",
      "         1.0       0.62      0.62      0.62        64\n",
      "\n",
      "    accuracy                           0.62       128\n",
      "   macro avg       0.62      0.62      0.62       128\n",
      "weighted avg       0.62      0.62      0.62       128\n",
      "\n",
      "annotator 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.61      0.61        64\n",
      "         1.0       0.61      0.61      0.61        64\n",
      "\n",
      "    accuracy                           0.61       128\n",
      "   macro avg       0.61      0.61      0.61       128\n",
      "weighted avg       0.61      0.61      0.61       128\n",
      "\n",
      "annotator 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.46      0.45      0.46        64\n",
      "         1.0       0.46      0.47      0.47        64\n",
      "\n",
      "    accuracy                           0.46       128\n",
      "   macro avg       0.46      0.46      0.46       128\n",
      "weighted avg       0.46      0.46      0.46       128\n",
      "\n",
      "annotator 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.24      0.25      0.24        64\n",
      "         1.0       0.21      0.20      0.21        64\n",
      "\n",
      "    accuracy                           0.23       128\n",
      "   macro avg       0.23      0.23      0.23       128\n",
      "weighted avg       0.23      0.23      0.23       128\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "i = 0\n",
    "fig, ax = plt.subplots(1, 4)\n",
    "for image, label, label2 in train_batches_MA.take(4):\n",
    "   # predictedLabel = int(predictions[i] >= 0.5)\n",
    "   # print(label2)\n",
    "    ax[i].axis('off')\n",
    "   # ax[i].set_title(classNames[label[i]])\n",
    "    ax[i].imshow(image[0])\n",
    "    i += 1\n",
    "    for j in range(label2.shape[1]):\n",
    "      print('annotator',j+1)\n",
    "      print(classification_report(label ,label2[:,j]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2de073d",
   "metadata": {
    "id": "9AgOHREc1bmd",
    "papermill": {
     "duration": 0.007121,
     "end_time": "2023-02-04T05:03:16.428603",
     "exception": false,
     "start_time": "2023-02-04T05:03:16.421482",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Build the classifier from multiple annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd20cc05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T05:03:16.444780Z",
     "iopub.status.busy": "2023-02-04T05:03:16.444492Z",
     "iopub.status.idle": "2023-02-04T05:03:16.470346Z",
     "shell.execute_reply": "2023-02-04T05:03:16.469357Z"
    },
    "id": "k-ePr0-fxcVi",
    "papermill": {
     "duration": 0.036664,
     "end_time": "2023-02-04T05:03:16.472559",
     "exception": false,
     "start_time": "2023-02-04T05:03:16.435895",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense,Conv2D,Flatten,MaxPooling2D,GlobalAveragePooling2D\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "class MultipleAnnotators_Classification():\n",
    "    def __init__(self, output_dim, num_annotators, q= 0.0001):\n",
    "        self.K = output_dim\n",
    "        self.R = num_annotators\n",
    "        self.q = q\n",
    "        #self.callbacks #=callbacks\n",
    "        #self.l1_param=l1_param \n",
    "        #self.l2_param=l1_param\n",
    "\n",
    "    def CrowdLayer(self, input):\n",
    "       #x = keras.layers.Dense(self.R + self.K, kernel_regularizer=regularizers.L1L2(l1= 1e-2, l2=1e-3),  activation='tanh')(input)\n",
    "        output_cla = keras.layers.Dense(self.K,  activation='softmax')(input)\n",
    "        output_ann = keras.layers.Dense(self.R,  activation='sigmoid')(input)\n",
    "        output = keras.layers.Concatenate()([output_cla, output_ann])\n",
    "        \n",
    "        return output\n",
    "#RCDNN   \n",
    "    def loss(self):\n",
    "        def custom_loss(y_true, y_pred):\n",
    "            # print(y_true,y_pred)\n",
    "            pred = y_pred[:, :self.K]\n",
    "            pred = tf.clip_by_value(pred, clip_value_min=1e-9, clip_value_max=1-1e-9) #estabilidad numerica de la funcion de costo\n",
    "            ann_ = y_pred[:, self.K:]\n",
    "            Y_true = tf.one_hot(tf.cast(y_true, dtype=tf.int32), depth=self.K, axis=1)\n",
    "            Y_hat = tf.repeat(tf.expand_dims(pred,-1), self.R, axis = -1)\n",
    "            p_logreg = tf.math.reduce_prod(tf.math.pow(Y_hat, Y_true), axis=1)\n",
    "            temp1 = ann_*tf.math.log(p_logreg)  \n",
    "            temp2 = (1 - ann_)*tf.math.log(1/self.K)*tf.reduce_sum(Y_true,axis=1)\n",
    "            # temp2 = (tf.ones(tf.shape(ann_)) - ann_)*tf.math.log(1/K)\n",
    "            # print(tf.reduce_mean(Y_true,axis=1).numpy())\n",
    "            return -tf.math.reduce_sum((temp1 + temp2))\n",
    "        return custom_loss\n",
    "    \n",
    "#     def loss(self):\n",
    "#         def custom_loss(y_true, y_pred):\n",
    "#                # print(y_true,y_pred)\n",
    "#            # q = 0.1\n",
    "#             pred = y_pred[:, :self.K]\n",
    "#             pred = tf.clip_by_value(pred, clip_value_min=1e-9, clip_value_max=1)\n",
    "#             ann_ = y_pred[:, self.K:]\n",
    "#             # ann_ = tf.clip_by_value(ann_, clip_value_min=1e-9, clip_value_max=1-1e-9)\n",
    "#             Y_true = tf.one_hot(tf.cast(y_true, dtype=tf.int32), depth=self.K, axis=1)\n",
    "#             Y_hat = tf.repeat(tf.expand_dims(pred,-1), self.R, axis = -1)\n",
    "\n",
    "#             p_gcce = Y_true*(1 - Y_hat**self.q)/self.q\n",
    "#             temp1 = ann_*tf.math.reduce_sum(p_gcce, axis=1)\n",
    "#             temp2 = (1 - ann_)*(1-(1/self.K)**self.q)/self.q*tf.reduce_sum(Y_true,axis=1)\n",
    "#             return tf.math.reduce_sum((temp1 + temp2))\n",
    "#         return custom_loss\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, x, Y, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.model(x, training=True)\n",
    "            loss_value = self.loss_fn(Y, logits)\n",
    "        grads = tape.gradient(loss_value, self.model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "        self.train_acc_metric.update_state(y, logits[:, :self.K])\n",
    "        return loss_value\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, x, y):\n",
    "        val_logits = self.model(x, training=False)\n",
    "        self.val_acc_metric.update_state(y, val_logits[:,:self.K])\n",
    "\n",
    "    def fit(self, model, Data_tr, Data_Val, epochs):\n",
    "        self.model = model\n",
    "        #++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        # Instantiate an optimizer.\n",
    "        #self.optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-4)\n",
    "        self.optimizer =  tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        #self.optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4, clipnorm=1.0)\n",
    "        #++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        # Instantiate a loss function.\n",
    "        self.loss_fn = self.loss()\n",
    "        self.train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "        self.val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "        train_loss = np.zeros(epochs)\n",
    "        train_accur = np.zeros(epochs)\n",
    "        val_accur = np.zeros(epochs)\n",
    "        val_loss = np.zeros(epochs)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Iterate over the batches of the dataset.\n",
    "            for step, (x_batch_train, y_batch_train, Y_batch_train) in enumerate(Data_tr):\n",
    "                # print(y_batch_train, Y_batch_train)\n",
    "                loss_value = self.train_step(x_batch_train, Y_batch_train, y_batch_train)\n",
    "\n",
    "                # Log every 200 batches.\n",
    "                if step % 10 == 0:\n",
    "                    train_acc = self.train_acc_metric.result()\n",
    "                    print(\n",
    "                      \"Training loss (for one batch) at step %d: %.4f, Accuracy: %.4f\"\n",
    "                      % (step, float(loss_value), float(train_acc))\n",
    "                            )\n",
    "                # print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "\n",
    "\n",
    "\n",
    "            # Run a validation loop at the end of each epoch.\n",
    "            for x_batch_val, y_batch_val,Y_batch_val in Data_Val:\n",
    "\n",
    "                val_logits = model(x_batch_val, training=False)\n",
    "\n",
    "                val_loss_value = self.loss_fn(Y_batch_val, val_logits)\n",
    "\n",
    "                self.val_acc_metric.update_state(y_batch_val, val_logits[:,:self.K])\n",
    "                \n",
    "               # np.round(np.mean([model(x_batch_val, training= True) for sample in range(100)]), 2)\n",
    "\n",
    "\n",
    "             # Display metrics at the end of each epoch.\n",
    "            train_acc = self.train_acc_metric.result()\n",
    "            val_acc = self.val_acc_metric.result()\n",
    "\n",
    "\n",
    "            print('---- Training ----')\n",
    "            print(\"Training loss: %.4f\" % (float(loss_value),))\n",
    "            print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "            # Reset training metrics at the end of each epoch\n",
    "            self.train_acc_metric.reset_states()\n",
    "            self.val_acc_metric.reset_states()\n",
    "\n",
    "\n",
    "            train_loss[epoch] = float(loss_value)\n",
    "            train_accur[epoch] = float(train_acc)\n",
    "\n",
    "            val_accur[epoch] = float(val_acc)\n",
    "            val_loss[epoch] = float(val_loss_value) \n",
    "\n",
    "\n",
    "            print('---- Validation ----')\n",
    "            print(\"Validation loss: %.4f\" % (float(val_loss_value),))\n",
    "            print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "\n",
    "            print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.suptitle('Loss and accuracy')\n",
    "        ax1.plot(range(1,epochs+1),train_loss)\n",
    "        ax1.plot(range(1,epochs+1), val_loss)\n",
    "        ax2.plot(range(1,epochs+1),train_accur)\n",
    "        ax2.plot(range(1,epochs+1),val_accur)\n",
    "        #plt.figure(figsize=(16,9))\n",
    "        ax1.set(xlabel= 'Epoch', ylabel=\"Loss\")\n",
    "        ax2.set(xlabel= 'Epoch',ylabel=\"Accuracy\")\n",
    "        ax1.legend(['Training_loss', 'Validation_loss'])\n",
    "        ax2.legend(['Training', 'Validation'])\n",
    "        ax1.grid()\n",
    "        ax2.grid()\n",
    "        plt.show()\n",
    "        return self.model\n",
    "\n",
    "    def eval_model(self, Data):\n",
    "        self.val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "        for x_batch_val, y_batch_val in Data:\n",
    "            self.test_step(x_batch_val, y_batch_val)\n",
    "\n",
    "        val_acc = self.val_acc_metric.result()\n",
    "        self.val_acc_metric.reset_states()\n",
    "        return val_acc\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c238ba9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T05:03:16.488125Z",
     "iopub.status.busy": "2023-02-04T05:03:16.487832Z",
     "iopub.status.idle": "2023-02-04T05:03:16.494647Z",
     "shell.execute_reply": "2023-02-04T05:03:16.493716Z"
    },
    "id": "4l-_pkpaBkSv",
    "papermill": {
     "duration": 0.016851,
     "end_time": "2023-02-04T05:03:16.496597",
     "exception": false,
     "start_time": "2023-02-04T05:03:16.479746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "  # print(y_true,y_pred)\n",
    "  K = 2 #len(np.unique(y_true))\n",
    "  R = 5\n",
    "  q = 0.1\n",
    "  pred = y_pred[:, K]\n",
    "  pred = tf.clip_by_value(pred, clip_value_min=1e-9, clip_value_max=1)\n",
    "  ann_ = y_pred[:,  K:]\n",
    "  # ann_ = tf.clip_by_value(ann_, clip_value_min=1e-9, clip_value_max=1-1e-9)\n",
    "  Y_true = tf.one_hot(tf.cast(y_true, dtype=tf.int32), depth=K, axis=1)\n",
    "  Y_hat = tf.repeat(tf.expand_dims(pred,-1), R, axis = -1)\n",
    "\n",
    "  p_gcce = Y_true*(1 - Y_hat**q)/q\n",
    "  temp1 = ann_*tf.math.reduce_sum(p_gcce, axis=1)\n",
    "  temp2 = (1 - ann_)*(1-(1/K)**q)/q*tf.reduce_sum(Y_true,axis=1)\n",
    "  return tf.math.reduce_sum((temp1 + temp2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb85e24e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T05:03:16.512682Z",
     "iopub.status.busy": "2023-02-04T05:03:16.511959Z",
     "iopub.status.idle": "2023-02-04T05:03:16.523284Z",
     "shell.execute_reply": "2023-02-04T05:03:16.522464Z"
    },
    "id": "0I4Rrc5TxcVj",
    "papermill": {
     "duration": 0.021265,
     "end_time": "2023-02-04T05:03:16.525231",
     "exception": false,
     "start_time": "2023-02-04T05:03:16.503966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MA = MultipleAnnotators_Classification(2, 5, 0.001)\n",
    " \n",
    "def create_model():\n",
    "   \n",
    "    l1 = 1e-2\n",
    "    # Block 1\n",
    "    inputs = keras.layers.Input(shape=(150, 150, 3), name='entrada')\n",
    "    x = keras.layers.BatchNormalization()(inputs)\n",
    "    x = keras.layers.Conv2D(32, (3, 3), activation=\"relu\" , name=\"block1_conv1\")(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name=\"block1_pool\")(x)\n",
    "\n",
    "\n",
    "    # Block 2\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Conv2D(32, (3, 3), activation=\"relu\", name=\"block2_conv1\")(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    #x = keras.layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name=\"block2_pool\")(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Conv2D(64, (3, 3), activation=\"relu\", name=\"block3_conv1\" )(x)             \n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "   # x = keras.layers.Dropout(0.2)(x)\n",
    "   \n",
    "    x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name=\"block3_pool\")(x)\n",
    "    \n",
    "    # Block 4\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Conv2D(64, (3, 3), activation=\"relu\", name=\"block4_conv1\")(x)            \n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name=\"block4_pool\")(x)\n",
    "    #x = keras.layers.Dropout(0.2)(x)\n",
    "    \n",
    "    #x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "   \n",
    "    x = keras.layers.Flatten()(x)\n",
    "    #x = keras.layers.Dropout(0.5)(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    output = MA.CrowdLayer(x)\n",
    "    model = keras.Model(inputs=inputs,outputs=output)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7d159f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T05:03:16.541234Z",
     "iopub.status.busy": "2023-02-04T05:03:16.540754Z",
     "iopub.status.idle": "2023-02-04T05:03:16.544457Z",
     "shell.execute_reply": "2023-02-04T05:03:16.543577Z"
    },
    "id": "iZAxrNF3_hE_",
    "papermill": {
     "duration": 0.013706,
     "end_time": "2023-02-04T05:03:16.546446",
     "exception": false,
     "start_time": "2023-02-04T05:03:16.532740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "# callbacks = [\n",
    "#     EarlyStopping(patience=10, verbose=1),\n",
    "#     ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00001, verbose=1),\n",
    "#     ModelCheckpoint('model1.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d58c264",
   "metadata": {
    "id": "Z-fV95n3GEqa",
    "papermill": {
     "duration": 0.00699,
     "end_time": "2023-02-04T05:03:16.560649",
     "exception": false,
     "start_time": "2023-02-04T05:03:16.553659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ac89bf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T05:03:16.576339Z",
     "iopub.status.busy": "2023-02-04T05:03:16.576079Z",
     "iopub.status.idle": "2023-02-04T05:03:16.580567Z",
     "shell.execute_reply": "2023-02-04T05:03:16.579681Z"
    },
    "id": "_H_sb1cl1FC_",
    "outputId": "59d957da-9223-4a01-e4d9-33933f7a2f4a",
    "papermill": {
     "duration": 0.014434,
     "end_time": "2023-02-04T05:03:16.582544",
     "exception": false,
     "start_time": "2023-02-04T05:03:16.568110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# classification_report_r= []\n",
    "# model = create_model()\n",
    "# K=2\n",
    "# R=5\n",
    "# NUM_RUNS = 5\n",
    "# N_EPOCHS = 30\n",
    "# val_acc = np.zeros(NUM_RUNS)\n",
    "# for i in range(NUM_RUNS):\n",
    "#   MA = MultipleAnnotators_Classification(K, R, 0.1)\n",
    "#   model = create_model()\n",
    "#   optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, clipnorm=1.0)\n",
    "#   model.compile(optimizer=optimizer, loss= MA.loss())\n",
    "#   history_model = model.fit(train_batches_MA, validation_data=val_batches_MA, epochs= N_EPOCHS, callbacks=callbacks, verbose=0)\n",
    "#   #model = MA.fit(model, Data_train_MA, N_EPOCHS)\n",
    "#   pred_2 = model.predict(X_test)\n",
    "\n",
    "#   lambda_R_ = pred_2[:, K:] #annotators reliability prediction N x R   \n",
    "#   classification_report_r += [classification_report( pred_2[:,:K].argmax(axis=1),Y_true_test.ravel(),output_dict=True)]\n",
    "#   print(classification_report( pred_2[:,:K].argmax(axis=1),Y_true_test.ravel()))\n",
    "#   #val_acc[i] = MA.eval_model(test_batches_MA)\n",
    "#   #print(\"Validation acc: %.4f\" % (float(val_acc[i]),))\n",
    "#   # Create the history figure\n",
    "#   plt.figure(figsize=(16,9))\n",
    "#   for i in  history_model.history:\n",
    "#       plt.plot(history_model.history[i],label=i)\n",
    "#   plt.title('Model history')\n",
    "#   plt.legend()\n",
    "#   plt.grid()\n",
    "\n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame(val_acc)\n",
    "# #df.to_csimport pandas as pddf = pd.DataFrame(val_acc)#df.to_csv('/kaggle/working/CatDogs_MA_InceptionV3.csv',index=False) # save to notebook output​v('/kaggle/working/CatDogs_MA_InceptionV3.csv',index=False) # save to notebook output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea8c4448",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T05:03:16.598360Z",
     "iopub.status.busy": "2023-02-04T05:03:16.598097Z",
     "iopub.status.idle": "2023-02-04T08:00:29.655417Z",
     "shell.execute_reply": "2023-02-04T08:00:29.654412Z"
    },
    "id": "Mu0lyAUIGSTB",
    "outputId": "cb82872d-c3ba-4d76-a28c-237eb266e78b",
    "papermill": {
     "duration": 10633.069703,
     "end_time": "2023-02-04T08:00:29.659654",
     "exception": false,
     "start_time": "2023-02-04T05:03:16.589951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-04 05:03:20.027316: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 0: 649.9142, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 623.9696, Accuracy: 0.5348\n",
      "Training loss (for one batch) at step 20: 577.8041, Accuracy: 0.5275\n",
      "Training loss (for one batch) at step 30: 535.6826, Accuracy: 0.5252\n",
      "Training loss (for one batch) at step 40: 527.7786, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 50: 492.9263, Accuracy: 0.5279\n",
      "Training loss (for one batch) at step 60: 505.6358, Accuracy: 0.5247\n",
      "Training loss (for one batch) at step 70: 490.0894, Accuracy: 0.5262\n",
      "Training loss (for one batch) at step 80: 472.4005, Accuracy: 0.5277\n",
      "Training loss (for one batch) at step 90: 491.4037, Accuracy: 0.5270\n",
      "Training loss (for one batch) at step 100: 463.9149, Accuracy: 0.5256\n",
      "Training loss (for one batch) at step 110: 482.3610, Accuracy: 0.5240\n",
      "---- Training ----\n",
      "Training loss: 144.2348\n",
      "Training acc over epoch: 0.5225\n",
      "---- Validation ----\n",
      "Validation loss: 34.6669\n",
      "Validation acc: 0.4769\n",
      "Time taken: 67.11s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 454.2283, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 461.1741, Accuracy: 0.5199\n",
      "Training loss (for one batch) at step 20: 457.7478, Accuracy: 0.5227\n",
      "Training loss (for one batch) at step 30: 458.6119, Accuracy: 0.5295\n",
      "Training loss (for one batch) at step 40: 449.5756, Accuracy: 0.5295\n",
      "Training loss (for one batch) at step 50: 456.3244, Accuracy: 0.5288\n",
      "Training loss (for one batch) at step 60: 458.9780, Accuracy: 0.5296\n",
      "Training loss (for one batch) at step 70: 454.1492, Accuracy: 0.5296\n",
      "Training loss (for one batch) at step 80: 458.4155, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 90: 447.6113, Accuracy: 0.5346\n",
      "Training loss (for one batch) at step 100: 445.3442, Accuracy: 0.5337\n",
      "Training loss (for one batch) at step 110: 442.1274, Accuracy: 0.5348\n",
      "---- Training ----\n",
      "Training loss: 140.4400\n",
      "Training acc over epoch: 0.5341\n",
      "---- Validation ----\n",
      "Validation loss: 34.5414\n",
      "Validation acc: 0.4995\n",
      "Time taken: 13.89s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 450.5325, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 450.4073, Accuracy: 0.5007\n",
      "Training loss (for one batch) at step 20: 446.3307, Accuracy: 0.5134\n",
      "Training loss (for one batch) at step 30: 443.1499, Accuracy: 0.5255\n",
      "Training loss (for one batch) at step 40: 444.6661, Accuracy: 0.5274\n",
      "Training loss (for one batch) at step 50: 445.2421, Accuracy: 0.5339\n",
      "Training loss (for one batch) at step 60: 441.6832, Accuracy: 0.5309\n",
      "Training loss (for one batch) at step 70: 446.2522, Accuracy: 0.5332\n",
      "Training loss (for one batch) at step 80: 442.2828, Accuracy: 0.5353\n",
      "Training loss (for one batch) at step 90: 442.7913, Accuracy: 0.5373\n",
      "Training loss (for one batch) at step 100: 445.4742, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 110: 444.6809, Accuracy: 0.5412\n",
      "---- Training ----\n",
      "Training loss: 138.9564\n",
      "Training acc over epoch: 0.5417\n",
      "---- Validation ----\n",
      "Validation loss: 34.8120\n",
      "Validation acc: 0.5349\n",
      "Time taken: 10.71s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 441.2382, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 447.6499, Accuracy: 0.5682\n",
      "Training loss (for one batch) at step 20: 441.4564, Accuracy: 0.5818\n",
      "Training loss (for one batch) at step 30: 443.7147, Accuracy: 0.5832\n",
      "Training loss (for one batch) at step 40: 440.8571, Accuracy: 0.5737\n",
      "Training loss (for one batch) at step 50: 439.5444, Accuracy: 0.5731\n",
      "Training loss (for one batch) at step 60: 446.4913, Accuracy: 0.5724\n",
      "Training loss (for one batch) at step 70: 444.7516, Accuracy: 0.5766\n",
      "Training loss (for one batch) at step 80: 447.0611, Accuracy: 0.5775\n",
      "Training loss (for one batch) at step 90: 443.0368, Accuracy: 0.5769\n",
      "Training loss (for one batch) at step 100: 444.5820, Accuracy: 0.5749\n",
      "Training loss (for one batch) at step 110: 446.3011, Accuracy: 0.5735\n",
      "---- Training ----\n",
      "Training loss: 138.4789\n",
      "Training acc over epoch: 0.5737\n",
      "---- Validation ----\n",
      "Validation loss: 34.6496\n",
      "Validation acc: 0.6013\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 445.7239, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 447.2430, Accuracy: 0.5760\n",
      "Training loss (for one batch) at step 20: 444.5995, Accuracy: 0.5666\n",
      "Training loss (for one batch) at step 30: 444.4415, Accuracy: 0.5743\n",
      "Training loss (for one batch) at step 40: 440.6062, Accuracy: 0.5846\n",
      "Training loss (for one batch) at step 50: 443.6095, Accuracy: 0.5918\n",
      "Training loss (for one batch) at step 60: 440.3912, Accuracy: 0.5941\n",
      "Training loss (for one batch) at step 70: 443.8450, Accuracy: 0.5965\n",
      "Training loss (for one batch) at step 80: 443.9813, Accuracy: 0.5946\n",
      "Training loss (for one batch) at step 90: 442.0719, Accuracy: 0.5943\n",
      "Training loss (for one batch) at step 100: 441.9098, Accuracy: 0.5922\n",
      "Training loss (for one batch) at step 110: 442.1848, Accuracy: 0.5956\n",
      "---- Training ----\n",
      "Training loss: 138.7154\n",
      "Training acc over epoch: 0.5948\n",
      "---- Validation ----\n",
      "Validation loss: 33.8691\n",
      "Validation acc: 0.6217\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 443.9882, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 442.9418, Accuracy: 0.5895\n",
      "Training loss (for one batch) at step 20: 443.8833, Accuracy: 0.5930\n",
      "Training loss (for one batch) at step 30: 439.5399, Accuracy: 0.5958\n",
      "Training loss (for one batch) at step 40: 436.6901, Accuracy: 0.6044\n",
      "Training loss (for one batch) at step 50: 443.6563, Accuracy: 0.6063\n",
      "Training loss (for one batch) at step 60: 438.1695, Accuracy: 0.6103\n",
      "Training loss (for one batch) at step 70: 447.0942, Accuracy: 0.6164\n",
      "Training loss (for one batch) at step 80: 440.8799, Accuracy: 0.6146\n",
      "Training loss (for one batch) at step 90: 443.8840, Accuracy: 0.6119\n",
      "Training loss (for one batch) at step 100: 443.3055, Accuracy: 0.6091\n",
      "Training loss (for one batch) at step 110: 442.0175, Accuracy: 0.6090\n",
      "---- Training ----\n",
      "Training loss: 137.4839\n",
      "Training acc over epoch: 0.6096\n",
      "---- Validation ----\n",
      "Validation loss: 35.2584\n",
      "Validation acc: 0.6408\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 443.7498, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 443.7153, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 20: 440.2875, Accuracy: 0.6060\n",
      "Training loss (for one batch) at step 30: 442.2803, Accuracy: 0.6096\n",
      "Training loss (for one batch) at step 40: 440.4282, Accuracy: 0.6117\n",
      "Training loss (for one batch) at step 50: 442.2083, Accuracy: 0.6163\n",
      "Training loss (for one batch) at step 60: 436.4212, Accuracy: 0.6247\n",
      "Training loss (for one batch) at step 70: 444.7436, Accuracy: 0.6273\n",
      "Training loss (for one batch) at step 80: 443.0854, Accuracy: 0.6264\n",
      "Training loss (for one batch) at step 90: 442.3906, Accuracy: 0.6212\n",
      "Training loss (for one batch) at step 100: 439.6077, Accuracy: 0.6204\n",
      "Training loss (for one batch) at step 110: 438.5840, Accuracy: 0.6211\n",
      "---- Training ----\n",
      "Training loss: 137.2943\n",
      "Training acc over epoch: 0.6208\n",
      "---- Validation ----\n",
      "Validation loss: 34.9750\n",
      "Validation acc: 0.6263\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 442.1277, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 440.8593, Accuracy: 0.6278\n",
      "Training loss (for one batch) at step 20: 439.2757, Accuracy: 0.5997\n",
      "Training loss (for one batch) at step 30: 436.6375, Accuracy: 0.6058\n",
      "Training loss (for one batch) at step 40: 438.5338, Accuracy: 0.6170\n",
      "Training loss (for one batch) at step 50: 436.3534, Accuracy: 0.6242\n",
      "Training loss (for one batch) at step 60: 440.0976, Accuracy: 0.6265\n",
      "Training loss (for one batch) at step 70: 441.9768, Accuracy: 0.6309\n",
      "Training loss (for one batch) at step 80: 443.3456, Accuracy: 0.6329\n",
      "Training loss (for one batch) at step 90: 440.7710, Accuracy: 0.6277\n",
      "Training loss (for one batch) at step 100: 436.9119, Accuracy: 0.6249\n",
      "Training loss (for one batch) at step 110: 441.3560, Accuracy: 0.6267\n",
      "---- Training ----\n",
      "Training loss: 138.1184\n",
      "Training acc over epoch: 0.6265\n",
      "---- Validation ----\n",
      "Validation loss: 34.7761\n",
      "Validation acc: 0.6445\n",
      "Time taken: 10.68s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 444.3752, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 441.2147, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 20: 439.9821, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 30: 440.9512, Accuracy: 0.6184\n",
      "Training loss (for one batch) at step 40: 433.7266, Accuracy: 0.6239\n",
      "Training loss (for one batch) at step 50: 434.9016, Accuracy: 0.6360\n",
      "Training loss (for one batch) at step 60: 433.7120, Accuracy: 0.6413\n",
      "Training loss (for one batch) at step 70: 438.4556, Accuracy: 0.6452\n",
      "Training loss (for one batch) at step 80: 443.5513, Accuracy: 0.6454\n",
      "Training loss (for one batch) at step 90: 439.8924, Accuracy: 0.6416\n",
      "Training loss (for one batch) at step 100: 437.6534, Accuracy: 0.6397\n",
      "Training loss (for one batch) at step 110: 441.3313, Accuracy: 0.6427\n",
      "---- Training ----\n",
      "Training loss: 138.4264\n",
      "Training acc over epoch: 0.6427\n",
      "---- Validation ----\n",
      "Validation loss: 34.3486\n",
      "Validation acc: 0.6607\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 445.8204, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 443.8558, Accuracy: 0.6335\n",
      "Training loss (for one batch) at step 20: 437.0974, Accuracy: 0.6321\n",
      "Training loss (for one batch) at step 30: 431.6437, Accuracy: 0.6381\n",
      "Training loss (for one batch) at step 40: 434.0936, Accuracy: 0.6431\n",
      "Training loss (for one batch) at step 50: 432.0283, Accuracy: 0.6478\n",
      "Training loss (for one batch) at step 60: 442.7010, Accuracy: 0.6547\n",
      "Training loss (for one batch) at step 70: 441.5699, Accuracy: 0.6564\n",
      "Training loss (for one batch) at step 80: 442.4536, Accuracy: 0.6548\n",
      "Training loss (for one batch) at step 90: 441.1350, Accuracy: 0.6492\n",
      "Training loss (for one batch) at step 100: 438.1998, Accuracy: 0.6483\n",
      "Training loss (for one batch) at step 110: 444.2199, Accuracy: 0.6480\n",
      "---- Training ----\n",
      "Training loss: 137.4286\n",
      "Training acc over epoch: 0.6487\n",
      "---- Validation ----\n",
      "Validation loss: 34.8613\n",
      "Validation acc: 0.6319\n",
      "Time taken: 10.61s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 443.7684, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 439.6591, Accuracy: 0.6385\n",
      "Training loss (for one batch) at step 20: 438.8206, Accuracy: 0.6246\n",
      "Training loss (for one batch) at step 30: 434.1056, Accuracy: 0.6237\n",
      "Training loss (for one batch) at step 40: 429.1769, Accuracy: 0.6378\n",
      "Training loss (for one batch) at step 50: 441.2547, Accuracy: 0.6483\n",
      "Training loss (for one batch) at step 60: 439.3232, Accuracy: 0.6552\n",
      "Training loss (for one batch) at step 70: 441.2411, Accuracy: 0.6579\n",
      "Training loss (for one batch) at step 80: 438.6893, Accuracy: 0.6572\n",
      "Training loss (for one batch) at step 90: 439.6129, Accuracy: 0.6505\n",
      "Training loss (for one batch) at step 100: 433.4632, Accuracy: 0.6510\n",
      "Training loss (for one batch) at step 110: 434.8181, Accuracy: 0.6534\n",
      "---- Training ----\n",
      "Training loss: 140.1432\n",
      "Training acc over epoch: 0.6552\n",
      "---- Validation ----\n",
      "Validation loss: 35.7899\n",
      "Validation acc: 0.6830\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 447.0024, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 434.8585, Accuracy: 0.6527\n",
      "Training loss (for one batch) at step 20: 437.2932, Accuracy: 0.6425\n",
      "Training loss (for one batch) at step 30: 428.6678, Accuracy: 0.6409\n",
      "Training loss (for one batch) at step 40: 429.2305, Accuracy: 0.6467\n",
      "Training loss (for one batch) at step 50: 432.1740, Accuracy: 0.6576\n",
      "Training loss (for one batch) at step 60: 433.4066, Accuracy: 0.6637\n",
      "Training loss (for one batch) at step 70: 441.1894, Accuracy: 0.6633\n",
      "Training loss (for one batch) at step 80: 443.2390, Accuracy: 0.6608\n",
      "Training loss (for one batch) at step 90: 437.4915, Accuracy: 0.6550\n",
      "Training loss (for one batch) at step 100: 430.1309, Accuracy: 0.6523\n",
      "Training loss (for one batch) at step 110: 433.0542, Accuracy: 0.6548\n",
      "---- Training ----\n",
      "Training loss: 134.3382\n",
      "Training acc over epoch: 0.6552\n",
      "---- Validation ----\n",
      "Validation loss: 33.7480\n",
      "Validation acc: 0.6660\n",
      "Time taken: 10.63s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 448.2722, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 442.8277, Accuracy: 0.6520\n",
      "Training loss (for one batch) at step 20: 438.2600, Accuracy: 0.6432\n",
      "Training loss (for one batch) at step 30: 430.6486, Accuracy: 0.6505\n",
      "Training loss (for one batch) at step 40: 426.7176, Accuracy: 0.6618\n",
      "Training loss (for one batch) at step 50: 424.1327, Accuracy: 0.6751\n",
      "Training loss (for one batch) at step 60: 434.1447, Accuracy: 0.6816\n",
      "Training loss (for one batch) at step 70: 433.9783, Accuracy: 0.6833\n",
      "Training loss (for one batch) at step 80: 441.6202, Accuracy: 0.6790\n",
      "Training loss (for one batch) at step 90: 433.3351, Accuracy: 0.6720\n",
      "Training loss (for one batch) at step 100: 433.7149, Accuracy: 0.6695\n",
      "Training loss (for one batch) at step 110: 438.8758, Accuracy: 0.6723\n",
      "---- Training ----\n",
      "Training loss: 137.9378\n",
      "Training acc over epoch: 0.6738\n",
      "---- Validation ----\n",
      "Validation loss: 35.2485\n",
      "Validation acc: 0.7106\n",
      "Time taken: 12.52s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 442.7477, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 438.3509, Accuracy: 0.6989\n",
      "Training loss (for one batch) at step 20: 438.3425, Accuracy: 0.6782\n",
      "Training loss (for one batch) at step 30: 425.5596, Accuracy: 0.6802\n",
      "Training loss (for one batch) at step 40: 421.7290, Accuracy: 0.6911\n",
      "Training loss (for one batch) at step 50: 421.4007, Accuracy: 0.6982\n",
      "Training loss (for one batch) at step 60: 418.2028, Accuracy: 0.7043\n",
      "Training loss (for one batch) at step 70: 435.0544, Accuracy: 0.7083\n",
      "Training loss (for one batch) at step 80: 442.7555, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 90: 436.6979, Accuracy: 0.6974\n",
      "Training loss (for one batch) at step 100: 432.2347, Accuracy: 0.6940\n",
      "Training loss (for one batch) at step 110: 429.6226, Accuracy: 0.6938\n",
      "---- Training ----\n",
      "Training loss: 135.2011\n",
      "Training acc over epoch: 0.6945\n",
      "---- Validation ----\n",
      "Validation loss: 35.3007\n",
      "Validation acc: 0.6967\n",
      "Time taken: 17.05s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 445.9349, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 435.7283, Accuracy: 0.6911\n",
      "Training loss (for one batch) at step 20: 431.9761, Accuracy: 0.6771\n",
      "Training loss (for one batch) at step 30: 429.5081, Accuracy: 0.6832\n",
      "Training loss (for one batch) at step 40: 424.4117, Accuracy: 0.6936\n",
      "Training loss (for one batch) at step 50: 416.7951, Accuracy: 0.7047\n",
      "Training loss (for one batch) at step 60: 425.6502, Accuracy: 0.7100\n",
      "Training loss (for one batch) at step 70: 438.5755, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 80: 438.8674, Accuracy: 0.7037\n",
      "Training loss (for one batch) at step 90: 432.4482, Accuracy: 0.6966\n",
      "Training loss (for one batch) at step 100: 426.2119, Accuracy: 0.6981\n",
      "Training loss (for one batch) at step 110: 436.4685, Accuracy: 0.7009\n",
      "---- Training ----\n",
      "Training loss: 136.6203\n",
      "Training acc over epoch: 0.7020\n",
      "---- Validation ----\n",
      "Validation loss: 35.7735\n",
      "Validation acc: 0.7195\n",
      "Time taken: 11.37s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 443.8708, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 438.8994, Accuracy: 0.6996\n",
      "Training loss (for one batch) at step 20: 434.4510, Accuracy: 0.6864\n",
      "Training loss (for one batch) at step 30: 425.3264, Accuracy: 0.7016\n",
      "Training loss (for one batch) at step 40: 410.0921, Accuracy: 0.7146\n",
      "Training loss (for one batch) at step 50: 413.1781, Accuracy: 0.7249\n",
      "Training loss (for one batch) at step 60: 425.8668, Accuracy: 0.7319\n",
      "Training loss (for one batch) at step 70: 439.4861, Accuracy: 0.7309\n",
      "Training loss (for one batch) at step 80: 440.2538, Accuracy: 0.7265\n",
      "Training loss (for one batch) at step 90: 426.8430, Accuracy: 0.7213\n",
      "Training loss (for one batch) at step 100: 421.8450, Accuracy: 0.7217\n",
      "Training loss (for one batch) at step 110: 424.1400, Accuracy: 0.7214\n",
      "---- Training ----\n",
      "Training loss: 132.0026\n",
      "Training acc over epoch: 0.7223\n",
      "---- Validation ----\n",
      "Validation loss: 35.4115\n",
      "Validation acc: 0.7133\n",
      "Time taken: 10.15s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 439.9235, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 438.5272, Accuracy: 0.7010\n",
      "Training loss (for one batch) at step 20: 430.0683, Accuracy: 0.6853\n",
      "Training loss (for one batch) at step 30: 415.8665, Accuracy: 0.6910\n",
      "Training loss (for one batch) at step 40: 416.4510, Accuracy: 0.6987\n",
      "Training loss (for one batch) at step 50: 414.9506, Accuracy: 0.7120\n",
      "Training loss (for one batch) at step 60: 415.7068, Accuracy: 0.7268\n",
      "Training loss (for one batch) at step 70: 441.2941, Accuracy: 0.7316\n",
      "Training loss (for one batch) at step 80: 438.4570, Accuracy: 0.7270\n",
      "Training loss (for one batch) at step 90: 437.8386, Accuracy: 0.7218\n",
      "Training loss (for one batch) at step 100: 419.6845, Accuracy: 0.7201\n",
      "Training loss (for one batch) at step 110: 418.8273, Accuracy: 0.7208\n",
      "---- Training ----\n",
      "Training loss: 136.8178\n",
      "Training acc over epoch: 0.7225\n",
      "---- Validation ----\n",
      "Validation loss: 36.2700\n",
      "Validation acc: 0.7104\n",
      "Time taken: 14.33s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 443.6488, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 431.9836, Accuracy: 0.7251\n",
      "Training loss (for one batch) at step 20: 416.1439, Accuracy: 0.7195\n",
      "Training loss (for one batch) at step 30: 415.5641, Accuracy: 0.7316\n",
      "Training loss (for one batch) at step 40: 401.0819, Accuracy: 0.7365\n",
      "Training loss (for one batch) at step 50: 415.5976, Accuracy: 0.7459\n",
      "Training loss (for one batch) at step 60: 430.7641, Accuracy: 0.7527\n",
      "Training loss (for one batch) at step 70: 428.9550, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 80: 424.8697, Accuracy: 0.7483\n",
      "Training loss (for one batch) at step 90: 427.8789, Accuracy: 0.7418\n",
      "Training loss (for one batch) at step 100: 419.0844, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 110: 420.9312, Accuracy: 0.7435\n",
      "---- Training ----\n",
      "Training loss: 134.0666\n",
      "Training acc over epoch: 0.7440\n",
      "---- Validation ----\n",
      "Validation loss: 34.6673\n",
      "Validation acc: 0.7289\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 427.2465, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 435.0675, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 20: 425.3609, Accuracy: 0.7094\n",
      "Training loss (for one batch) at step 30: 411.7209, Accuracy: 0.7253\n",
      "Training loss (for one batch) at step 40: 417.0309, Accuracy: 0.7374\n",
      "Training loss (for one batch) at step 50: 390.4622, Accuracy: 0.7506\n",
      "Training loss (for one batch) at step 60: 412.4887, Accuracy: 0.7583\n",
      "Training loss (for one batch) at step 70: 427.2080, Accuracy: 0.7605\n",
      "Training loss (for one batch) at step 80: 433.5673, Accuracy: 0.7535\n",
      "Training loss (for one batch) at step 90: 428.5305, Accuracy: 0.7465\n",
      "Training loss (for one batch) at step 100: 412.1768, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 110: 420.8346, Accuracy: 0.7449\n",
      "---- Training ----\n",
      "Training loss: 134.9808\n",
      "Training acc over epoch: 0.7456\n",
      "---- Validation ----\n",
      "Validation loss: 34.8321\n",
      "Validation acc: 0.7472\n",
      "Time taken: 11.65s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 440.3880, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 430.9813, Accuracy: 0.7259\n",
      "Training loss (for one batch) at step 20: 415.9119, Accuracy: 0.7262\n",
      "Training loss (for one batch) at step 30: 403.7553, Accuracy: 0.7404\n",
      "Training loss (for one batch) at step 40: 403.0846, Accuracy: 0.7529\n",
      "Training loss (for one batch) at step 50: 396.7643, Accuracy: 0.7587\n",
      "Training loss (for one batch) at step 60: 408.8968, Accuracy: 0.7660\n",
      "Training loss (for one batch) at step 70: 447.6960, Accuracy: 0.7687\n",
      "Training loss (for one batch) at step 80: 437.0462, Accuracy: 0.7606\n",
      "Training loss (for one batch) at step 90: 423.8581, Accuracy: 0.7569\n",
      "Training loss (for one batch) at step 100: 407.6104, Accuracy: 0.7531\n",
      "Training loss (for one batch) at step 110: 422.9739, Accuracy: 0.7551\n",
      "---- Training ----\n",
      "Training loss: 132.8342\n",
      "Training acc over epoch: 0.7550\n",
      "---- Validation ----\n",
      "Validation loss: 35.1899\n",
      "Validation acc: 0.7243\n",
      "Time taken: 11.78s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 428.1876, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 429.0757, Accuracy: 0.7273\n",
      "Training loss (for one batch) at step 20: 422.0381, Accuracy: 0.7154\n",
      "Training loss (for one batch) at step 30: 425.4700, Accuracy: 0.7387\n",
      "Training loss (for one batch) at step 40: 385.9907, Accuracy: 0.7523\n",
      "Training loss (for one batch) at step 50: 385.0818, Accuracy: 0.7699\n",
      "Training loss (for one batch) at step 60: 394.6544, Accuracy: 0.7806\n",
      "Training loss (for one batch) at step 70: 418.5965, Accuracy: 0.7804\n",
      "Training loss (for one batch) at step 80: 417.1031, Accuracy: 0.7735\n",
      "Training loss (for one batch) at step 90: 402.8402, Accuracy: 0.7663\n",
      "Training loss (for one batch) at step 100: 408.4078, Accuracy: 0.7643\n",
      "Training loss (for one batch) at step 110: 409.6804, Accuracy: 0.7634\n",
      "---- Training ----\n",
      "Training loss: 120.8680\n",
      "Training acc over epoch: 0.7637\n",
      "---- Validation ----\n",
      "Validation loss: 40.2422\n",
      "Validation acc: 0.7225\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 446.9691, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 438.3026, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 20: 416.7644, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 30: 405.4127, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 40: 408.1969, Accuracy: 0.7643\n",
      "Training loss (for one batch) at step 50: 378.3280, Accuracy: 0.7791\n",
      "Training loss (for one batch) at step 60: 394.8980, Accuracy: 0.7873\n",
      "Training loss (for one batch) at step 70: 425.7560, Accuracy: 0.7864\n",
      "Training loss (for one batch) at step 80: 436.8197, Accuracy: 0.7763\n",
      "Training loss (for one batch) at step 90: 419.7968, Accuracy: 0.7728\n",
      "Training loss (for one batch) at step 100: 406.0755, Accuracy: 0.7704\n",
      "Training loss (for one batch) at step 110: 405.8799, Accuracy: 0.7688\n",
      "---- Training ----\n",
      "Training loss: 132.6017\n",
      "Training acc over epoch: 0.7687\n",
      "---- Validation ----\n",
      "Validation loss: 39.3315\n",
      "Validation acc: 0.7276\n",
      "Time taken: 13.42s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 437.6360, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 424.3319, Accuracy: 0.7287\n",
      "Training loss (for one batch) at step 20: 420.5715, Accuracy: 0.7292\n",
      "Training loss (for one batch) at step 30: 402.7767, Accuracy: 0.7419\n",
      "Training loss (for one batch) at step 40: 367.7471, Accuracy: 0.7584\n",
      "Training loss (for one batch) at step 50: 378.4313, Accuracy: 0.7721\n",
      "Training loss (for one batch) at step 60: 396.6397, Accuracy: 0.7789\n",
      "Training loss (for one batch) at step 70: 419.9825, Accuracy: 0.7757\n",
      "Training loss (for one batch) at step 80: 430.0054, Accuracy: 0.7690\n",
      "Training loss (for one batch) at step 90: 403.8874, Accuracy: 0.7653\n",
      "Training loss (for one batch) at step 100: 385.4548, Accuracy: 0.7666\n",
      "Training loss (for one batch) at step 110: 401.2012, Accuracy: 0.7661\n",
      "---- Training ----\n",
      "Training loss: 126.1722\n",
      "Training acc over epoch: 0.7648\n",
      "---- Validation ----\n",
      "Validation loss: 34.2454\n",
      "Validation acc: 0.7184\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 417.1777, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 416.6475, Accuracy: 0.7479\n",
      "Training loss (for one batch) at step 20: 397.8766, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 30: 404.7100, Accuracy: 0.7525\n",
      "Training loss (for one batch) at step 40: 384.8606, Accuracy: 0.7717\n",
      "Training loss (for one batch) at step 50: 369.7773, Accuracy: 0.7828\n",
      "Training loss (for one batch) at step 60: 384.9880, Accuracy: 0.7907\n",
      "Training loss (for one batch) at step 70: 427.2253, Accuracy: 0.7901\n",
      "Training loss (for one batch) at step 80: 408.2187, Accuracy: 0.7809\n",
      "Training loss (for one batch) at step 90: 412.5154, Accuracy: 0.7736\n",
      "Training loss (for one batch) at step 100: 371.7524, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 110: 400.1018, Accuracy: 0.7729\n",
      "---- Training ----\n",
      "Training loss: 134.7065\n",
      "Training acc over epoch: 0.7732\n",
      "---- Validation ----\n",
      "Validation loss: 38.0866\n",
      "Validation acc: 0.7286\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 430.1121, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 415.4585, Accuracy: 0.7244\n",
      "Training loss (for one batch) at step 20: 394.7000, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 30: 384.4603, Accuracy: 0.7629\n",
      "Training loss (for one batch) at step 40: 366.4897, Accuracy: 0.7763\n",
      "Training loss (for one batch) at step 50: 363.2236, Accuracy: 0.7862\n",
      "Training loss (for one batch) at step 60: 386.1805, Accuracy: 0.7946\n",
      "Training loss (for one batch) at step 70: 430.0972, Accuracy: 0.7909\n",
      "Training loss (for one batch) at step 80: 417.2495, Accuracy: 0.7841\n",
      "Training loss (for one batch) at step 90: 415.0281, Accuracy: 0.7788\n",
      "Training loss (for one batch) at step 100: 387.3101, Accuracy: 0.7796\n",
      "Training loss (for one batch) at step 110: 396.8998, Accuracy: 0.7785\n",
      "---- Training ----\n",
      "Training loss: 124.4855\n",
      "Training acc over epoch: 0.7770\n",
      "---- Validation ----\n",
      "Validation loss: 37.2440\n",
      "Validation acc: 0.7316\n",
      "Time taken: 16.75s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 439.9091, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 422.5253, Accuracy: 0.7209\n",
      "Training loss (for one batch) at step 20: 400.8058, Accuracy: 0.7400\n",
      "Training loss (for one batch) at step 30: 393.9205, Accuracy: 0.7576\n",
      "Training loss (for one batch) at step 40: 391.5912, Accuracy: 0.7715\n",
      "Training loss (for one batch) at step 50: 374.7682, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 60: 390.0992, Accuracy: 0.7934\n",
      "Training loss (for one batch) at step 70: 391.7919, Accuracy: 0.7906\n",
      "Training loss (for one batch) at step 80: 416.9772, Accuracy: 0.7836\n",
      "Training loss (for one batch) at step 90: 378.4150, Accuracy: 0.7788\n",
      "Training loss (for one batch) at step 100: 374.8755, Accuracy: 0.7780\n",
      "Training loss (for one batch) at step 110: 394.7163, Accuracy: 0.7776\n",
      "---- Training ----\n",
      "Training loss: 124.2478\n",
      "Training acc over epoch: 0.7773\n",
      "---- Validation ----\n",
      "Validation loss: 35.7304\n",
      "Validation acc: 0.7235\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 423.4424, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 401.9861, Accuracy: 0.7379\n",
      "Training loss (for one batch) at step 20: 414.8163, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 30: 372.5659, Accuracy: 0.7523\n",
      "Training loss (for one batch) at step 40: 356.6920, Accuracy: 0.7708\n",
      "Training loss (for one batch) at step 50: 363.8447, Accuracy: 0.7848\n",
      "Training loss (for one batch) at step 60: 374.8494, Accuracy: 0.7947\n",
      "Training loss (for one batch) at step 70: 410.0674, Accuracy: 0.7906\n",
      "Training loss (for one batch) at step 80: 396.3080, Accuracy: 0.7846\n",
      "Training loss (for one batch) at step 90: 396.7740, Accuracy: 0.7824\n",
      "Training loss (for one batch) at step 100: 373.2065, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 110: 372.5315, Accuracy: 0.7818\n",
      "---- Training ----\n",
      "Training loss: 118.4304\n",
      "Training acc over epoch: 0.7808\n",
      "---- Validation ----\n",
      "Validation loss: 36.8921\n",
      "Validation acc: 0.7200\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 430.8599, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 403.0647, Accuracy: 0.7294\n",
      "Training loss (for one batch) at step 20: 390.5827, Accuracy: 0.7452\n",
      "Training loss (for one batch) at step 30: 383.5762, Accuracy: 0.7623\n",
      "Training loss (for one batch) at step 40: 363.2874, Accuracy: 0.7799\n",
      "Training loss (for one batch) at step 50: 350.2077, Accuracy: 0.7934\n",
      "Training loss (for one batch) at step 60: 361.2009, Accuracy: 0.8038\n",
      "Training loss (for one batch) at step 70: 399.5546, Accuracy: 0.7991\n",
      "Training loss (for one batch) at step 80: 411.6675, Accuracy: 0.7886\n",
      "Training loss (for one batch) at step 90: 382.5620, Accuracy: 0.7849\n",
      "Training loss (for one batch) at step 100: 352.0375, Accuracy: 0.7853\n",
      "Training loss (for one batch) at step 110: 374.9194, Accuracy: 0.7855\n",
      "---- Training ----\n",
      "Training loss: 128.6162\n",
      "Training acc over epoch: 0.7843\n",
      "---- Validation ----\n",
      "Validation loss: 32.1833\n",
      "Validation acc: 0.7174\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 404.2090, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 401.3541, Accuracy: 0.7273\n",
      "Training loss (for one batch) at step 20: 383.3345, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 30: 352.1066, Accuracy: 0.7674\n",
      "Training loss (for one batch) at step 40: 366.1886, Accuracy: 0.7837\n",
      "Training loss (for one batch) at step 50: 360.5757, Accuracy: 0.8002\n",
      "Training loss (for one batch) at step 60: 388.0302, Accuracy: 0.8072\n",
      "Training loss (for one batch) at step 70: 402.4739, Accuracy: 0.7990\n",
      "Training loss (for one batch) at step 80: 389.6151, Accuracy: 0.7906\n",
      "Training loss (for one batch) at step 90: 373.1966, Accuracy: 0.7857\n",
      "Training loss (for one batch) at step 100: 354.1280, Accuracy: 0.7847\n",
      "Training loss (for one batch) at step 110: 372.6463, Accuracy: 0.7844\n",
      "---- Training ----\n",
      "Training loss: 122.4908\n",
      "Training acc over epoch: 0.7843\n",
      "---- Validation ----\n",
      "Validation loss: 36.1519\n",
      "Validation acc: 0.7179\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 410.8618, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 420.5742, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 20: 377.6324, Accuracy: 0.7455\n",
      "Training loss (for one batch) at step 30: 369.0600, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 40: 353.7594, Accuracy: 0.7786\n",
      "Training loss (for one batch) at step 50: 349.9115, Accuracy: 0.7923\n",
      "Training loss (for one batch) at step 60: 359.6510, Accuracy: 0.7999\n",
      "Training loss (for one batch) at step 70: 410.0534, Accuracy: 0.7932\n",
      "Training loss (for one batch) at step 80: 392.9049, Accuracy: 0.7840\n",
      "Training loss (for one batch) at step 90: 376.4682, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 100: 372.1790, Accuracy: 0.7806\n",
      "Training loss (for one batch) at step 110: 381.6401, Accuracy: 0.7817\n",
      "---- Training ----\n",
      "Training loss: 121.3401\n",
      "Training acc over epoch: 0.7798\n",
      "---- Validation ----\n",
      "Validation loss: 35.3844\n",
      "Validation acc: 0.7305\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 398.7192, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 388.0371, Accuracy: 0.7216\n",
      "Training loss (for one batch) at step 20: 374.4917, Accuracy: 0.7448\n",
      "Training loss (for one batch) at step 30: 364.4930, Accuracy: 0.7674\n",
      "Training loss (for one batch) at step 40: 367.0064, Accuracy: 0.7832\n",
      "Training loss (for one batch) at step 50: 343.4974, Accuracy: 0.7966\n",
      "Training loss (for one batch) at step 60: 349.1188, Accuracy: 0.8038\n",
      "Training loss (for one batch) at step 70: 383.6689, Accuracy: 0.7965\n",
      "Training loss (for one batch) at step 80: 372.8784, Accuracy: 0.7876\n",
      "Training loss (for one batch) at step 90: 361.4261, Accuracy: 0.7830\n",
      "Training loss (for one batch) at step 100: 351.5172, Accuracy: 0.7823\n",
      "Training loss (for one batch) at step 110: 378.6954, Accuracy: 0.7827\n",
      "---- Training ----\n",
      "Training loss: 112.7293\n",
      "Training acc over epoch: 0.7819\n",
      "---- Validation ----\n",
      "Validation loss: 45.0837\n",
      "Validation acc: 0.7141\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 399.0100, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 391.7573, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 20: 365.6896, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 30: 363.6899, Accuracy: 0.7623\n",
      "Training loss (for one batch) at step 40: 353.4592, Accuracy: 0.7839\n",
      "Training loss (for one batch) at step 50: 331.3382, Accuracy: 0.7989\n",
      "Training loss (for one batch) at step 60: 329.6464, Accuracy: 0.8060\n",
      "Training loss (for one batch) at step 70: 367.2009, Accuracy: 0.7980\n",
      "Training loss (for one batch) at step 80: 381.3195, Accuracy: 0.7867\n",
      "Training loss (for one batch) at step 90: 381.7965, Accuracy: 0.7834\n",
      "Training loss (for one batch) at step 100: 356.5107, Accuracy: 0.7848\n",
      "Training loss (for one batch) at step 110: 348.1505, Accuracy: 0.7862\n",
      "---- Training ----\n",
      "Training loss: 120.1469\n",
      "Training acc over epoch: 0.7863\n",
      "---- Validation ----\n",
      "Validation loss: 34.8803\n",
      "Validation acc: 0.7311\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 401.4000, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 380.9937, Accuracy: 0.7116\n",
      "Training loss (for one batch) at step 20: 354.1107, Accuracy: 0.7288\n",
      "Training loss (for one batch) at step 30: 340.8582, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 40: 343.0101, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 50: 335.8883, Accuracy: 0.7981\n",
      "Training loss (for one batch) at step 60: 348.1086, Accuracy: 0.8032\n",
      "Training loss (for one batch) at step 70: 371.6720, Accuracy: 0.7960\n",
      "Training loss (for one batch) at step 80: 390.2902, Accuracy: 0.7860\n",
      "Training loss (for one batch) at step 90: 350.3753, Accuracy: 0.7811\n",
      "Training loss (for one batch) at step 100: 352.3443, Accuracy: 0.7847\n",
      "Training loss (for one batch) at step 110: 356.1749, Accuracy: 0.7842\n",
      "---- Training ----\n",
      "Training loss: 121.6702\n",
      "Training acc over epoch: 0.7823\n",
      "---- Validation ----\n",
      "Validation loss: 39.4491\n",
      "Validation acc: 0.7120\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 392.5131, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 402.6529, Accuracy: 0.7003\n",
      "Training loss (for one batch) at step 20: 357.0237, Accuracy: 0.7321\n",
      "Training loss (for one batch) at step 30: 337.2899, Accuracy: 0.7659\n",
      "Training loss (for one batch) at step 40: 326.0762, Accuracy: 0.7893\n",
      "Training loss (for one batch) at step 50: 323.6243, Accuracy: 0.8024\n",
      "Training loss (for one batch) at step 60: 348.6767, Accuracy: 0.8092\n",
      "Training loss (for one batch) at step 70: 377.8724, Accuracy: 0.8005\n",
      "Training loss (for one batch) at step 80: 365.7549, Accuracy: 0.7892\n",
      "Training loss (for one batch) at step 90: 362.9695, Accuracy: 0.7841\n",
      "Training loss (for one batch) at step 100: 353.2800, Accuracy: 0.7860\n",
      "Training loss (for one batch) at step 110: 370.9617, Accuracy: 0.7863\n",
      "---- Training ----\n",
      "Training loss: 122.2972\n",
      "Training acc over epoch: 0.7845\n",
      "---- Validation ----\n",
      "Validation loss: 39.8128\n",
      "Validation acc: 0.7238\n",
      "Time taken: 10.40s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 385.1551, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 386.6278, Accuracy: 0.7294\n",
      "Training loss (for one batch) at step 20: 355.6781, Accuracy: 0.7489\n",
      "Training loss (for one batch) at step 30: 343.7349, Accuracy: 0.7694\n",
      "Training loss (for one batch) at step 40: 324.0793, Accuracy: 0.7893\n",
      "Training loss (for one batch) at step 50: 350.6133, Accuracy: 0.8038\n",
      "Training loss (for one batch) at step 60: 342.0482, Accuracy: 0.8116\n",
      "Training loss (for one batch) at step 70: 404.8712, Accuracy: 0.8019\n",
      "Training loss (for one batch) at step 80: 380.3372, Accuracy: 0.7903\n",
      "Training loss (for one batch) at step 90: 359.4896, Accuracy: 0.7867\n",
      "Training loss (for one batch) at step 100: 345.2764, Accuracy: 0.7896\n",
      "Training loss (for one batch) at step 110: 356.2169, Accuracy: 0.7891\n",
      "---- Training ----\n",
      "Training loss: 106.3111\n",
      "Training acc over epoch: 0.7884\n",
      "---- Validation ----\n",
      "Validation loss: 46.2828\n",
      "Validation acc: 0.7026\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 382.5943, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 382.8449, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 20: 338.7579, Accuracy: 0.7333\n",
      "Training loss (for one batch) at step 30: 338.2953, Accuracy: 0.7611\n",
      "Training loss (for one batch) at step 40: 329.0730, Accuracy: 0.7797\n",
      "Training loss (for one batch) at step 50: 317.7628, Accuracy: 0.7963\n",
      "Training loss (for one batch) at step 60: 344.9736, Accuracy: 0.8026\n",
      "Training loss (for one batch) at step 70: 366.0204, Accuracy: 0.7936\n",
      "Training loss (for one batch) at step 80: 366.1497, Accuracy: 0.7814\n",
      "Training loss (for one batch) at step 90: 350.4884, Accuracy: 0.7773\n",
      "Training loss (for one batch) at step 100: 359.2061, Accuracy: 0.7815\n",
      "Training loss (for one batch) at step 110: 351.7597, Accuracy: 0.7806\n",
      "---- Training ----\n",
      "Training loss: 118.1318\n",
      "Training acc over epoch: 0.7792\n",
      "---- Validation ----\n",
      "Validation loss: 40.1737\n",
      "Validation acc: 0.7114\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 396.3771, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 364.6466, Accuracy: 0.7067\n",
      "Training loss (for one batch) at step 20: 342.9930, Accuracy: 0.7407\n",
      "Training loss (for one batch) at step 30: 328.3867, Accuracy: 0.7671\n",
      "Training loss (for one batch) at step 40: 322.3736, Accuracy: 0.7856\n",
      "Training loss (for one batch) at step 50: 326.5285, Accuracy: 0.8033\n",
      "Training loss (for one batch) at step 60: 348.3880, Accuracy: 0.8097\n",
      "Training loss (for one batch) at step 70: 361.6366, Accuracy: 0.7981\n",
      "Training loss (for one batch) at step 80: 384.8402, Accuracy: 0.7847\n",
      "Training loss (for one batch) at step 90: 338.3930, Accuracy: 0.7833\n",
      "Training loss (for one batch) at step 100: 339.0360, Accuracy: 0.7855\n",
      "Training loss (for one batch) at step 110: 373.4886, Accuracy: 0.7858\n",
      "---- Training ----\n",
      "Training loss: 108.0496\n",
      "Training acc over epoch: 0.7845\n",
      "---- Validation ----\n",
      "Validation loss: 53.8043\n",
      "Validation acc: 0.7149\n",
      "Time taken: 10.62s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 387.5393, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 395.7502, Accuracy: 0.7166\n",
      "Training loss (for one batch) at step 20: 335.3502, Accuracy: 0.7396\n",
      "Training loss (for one batch) at step 30: 339.9932, Accuracy: 0.7659\n",
      "Training loss (for one batch) at step 40: 319.4235, Accuracy: 0.7881\n",
      "Training loss (for one batch) at step 50: 328.3142, Accuracy: 0.8012\n",
      "Training loss (for one batch) at step 60: 349.6500, Accuracy: 0.8064\n",
      "Training loss (for one batch) at step 70: 361.2872, Accuracy: 0.7972\n",
      "Training loss (for one batch) at step 80: 351.9774, Accuracy: 0.7832\n",
      "Training loss (for one batch) at step 90: 336.4984, Accuracy: 0.7797\n",
      "Training loss (for one batch) at step 100: 331.9302, Accuracy: 0.7843\n",
      "Training loss (for one batch) at step 110: 338.2154, Accuracy: 0.7837\n",
      "---- Training ----\n",
      "Training loss: 111.0032\n",
      "Training acc over epoch: 0.7830\n",
      "---- Validation ----\n",
      "Validation loss: 43.8479\n",
      "Validation acc: 0.7133\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 397.4232, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 371.3782, Accuracy: 0.7060\n",
      "Training loss (for one batch) at step 20: 355.2670, Accuracy: 0.7288\n",
      "Training loss (for one batch) at step 30: 322.2930, Accuracy: 0.7576\n",
      "Training loss (for one batch) at step 40: 316.7607, Accuracy: 0.7832\n",
      "Training loss (for one batch) at step 50: 319.5723, Accuracy: 0.7984\n",
      "Training loss (for one batch) at step 60: 333.7477, Accuracy: 0.8039\n",
      "Training loss (for one batch) at step 70: 361.6245, Accuracy: 0.7923\n",
      "Training loss (for one batch) at step 80: 358.1122, Accuracy: 0.7801\n",
      "Training loss (for one batch) at step 90: 321.5510, Accuracy: 0.7771\n",
      "Training loss (for one batch) at step 100: 328.7782, Accuracy: 0.7792\n",
      "Training loss (for one batch) at step 110: 329.2638, Accuracy: 0.7796\n",
      "---- Training ----\n",
      "Training loss: 109.3573\n",
      "Training acc over epoch: 0.7788\n",
      "---- Validation ----\n",
      "Validation loss: 39.2183\n",
      "Validation acc: 0.7045\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 364.2082, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 353.3870, Accuracy: 0.6974\n",
      "Training loss (for one batch) at step 20: 326.5025, Accuracy: 0.7321\n",
      "Training loss (for one batch) at step 30: 306.8969, Accuracy: 0.7664\n",
      "Training loss (for one batch) at step 40: 307.3707, Accuracy: 0.7872\n",
      "Training loss (for one batch) at step 50: 345.9391, Accuracy: 0.8018\n",
      "Training loss (for one batch) at step 60: 327.9459, Accuracy: 0.8085\n",
      "Training loss (for one batch) at step 70: 340.6928, Accuracy: 0.7967\n",
      "Training loss (for one batch) at step 80: 379.3388, Accuracy: 0.7817\n",
      "Training loss (for one batch) at step 90: 331.3202, Accuracy: 0.7787\n",
      "Training loss (for one batch) at step 100: 325.6620, Accuracy: 0.7826\n",
      "Training loss (for one batch) at step 110: 344.8326, Accuracy: 0.7842\n",
      "---- Training ----\n",
      "Training loss: 117.6786\n",
      "Training acc over epoch: 0.7830\n",
      "---- Validation ----\n",
      "Validation loss: 36.7953\n",
      "Validation acc: 0.7131\n",
      "Time taken: 10.65s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 368.0061, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 361.9383, Accuracy: 0.7038\n",
      "Training loss (for one batch) at step 20: 326.1999, Accuracy: 0.7325\n",
      "Training loss (for one batch) at step 30: 318.9931, Accuracy: 0.7618\n",
      "Training loss (for one batch) at step 40: 318.6155, Accuracy: 0.7807\n",
      "Training loss (for one batch) at step 50: 292.7701, Accuracy: 0.8002\n",
      "Training loss (for one batch) at step 60: 330.7238, Accuracy: 0.8066\n",
      "Training loss (for one batch) at step 70: 362.8725, Accuracy: 0.7993\n",
      "Training loss (for one batch) at step 80: 372.0673, Accuracy: 0.7870\n",
      "Training loss (for one batch) at step 90: 324.9302, Accuracy: 0.7837\n",
      "Training loss (for one batch) at step 100: 316.2475, Accuracy: 0.7864\n",
      "Training loss (for one batch) at step 110: 336.4194, Accuracy: 0.7865\n",
      "---- Training ----\n",
      "Training loss: 117.6323\n",
      "Training acc over epoch: 0.7852\n",
      "---- Validation ----\n",
      "Validation loss: 51.2691\n",
      "Validation acc: 0.7090\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 356.9204, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 351.4560, Accuracy: 0.6939\n",
      "Training loss (for one batch) at step 20: 318.2015, Accuracy: 0.7295\n",
      "Training loss (for one batch) at step 30: 312.7978, Accuracy: 0.7571\n",
      "Training loss (for one batch) at step 40: 300.5757, Accuracy: 0.7830\n",
      "Training loss (for one batch) at step 50: 312.3021, Accuracy: 0.7979\n",
      "Training loss (for one batch) at step 60: 332.5652, Accuracy: 0.8053\n",
      "Training loss (for one batch) at step 70: 349.4112, Accuracy: 0.7959\n",
      "Training loss (for one batch) at step 80: 376.6765, Accuracy: 0.7824\n",
      "Training loss (for one batch) at step 90: 337.5461, Accuracy: 0.7781\n",
      "Training loss (for one batch) at step 100: 336.0176, Accuracy: 0.7829\n",
      "Training loss (for one batch) at step 110: 355.0168, Accuracy: 0.7843\n",
      "---- Training ----\n",
      "Training loss: 112.2307\n",
      "Training acc over epoch: 0.7832\n",
      "---- Validation ----\n",
      "Validation loss: 37.0155\n",
      "Validation acc: 0.7074\n",
      "Time taken: 10.14s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 365.9375, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 377.2533, Accuracy: 0.6989\n",
      "Training loss (for one batch) at step 20: 325.0180, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 30: 318.9576, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 40: 317.4783, Accuracy: 0.7860\n",
      "Training loss (for one batch) at step 50: 294.3445, Accuracy: 0.8015\n",
      "Training loss (for one batch) at step 60: 318.3325, Accuracy: 0.8087\n",
      "Training loss (for one batch) at step 70: 344.5936, Accuracy: 0.7993\n",
      "Training loss (for one batch) at step 80: 357.3641, Accuracy: 0.7860\n",
      "Training loss (for one batch) at step 90: 324.7198, Accuracy: 0.7819\n",
      "Training loss (for one batch) at step 100: 323.5796, Accuracy: 0.7831\n",
      "Training loss (for one batch) at step 110: 336.5395, Accuracy: 0.7840\n",
      "---- Training ----\n",
      "Training loss: 107.4303\n",
      "Training acc over epoch: 0.7830\n",
      "---- Validation ----\n",
      "Validation loss: 48.8589\n",
      "Validation acc: 0.7133\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 353.5725, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 353.3508, Accuracy: 0.6925\n",
      "Training loss (for one batch) at step 20: 322.4931, Accuracy: 0.7325\n",
      "Training loss (for one batch) at step 30: 315.1772, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 40: 301.6228, Accuracy: 0.7875\n",
      "Training loss (for one batch) at step 50: 306.9995, Accuracy: 0.8039\n",
      "Training loss (for one batch) at step 60: 344.2722, Accuracy: 0.8102\n",
      "Training loss (for one batch) at step 70: 340.0991, Accuracy: 0.7987\n",
      "Training loss (for one batch) at step 80: 358.0834, Accuracy: 0.7825\n",
      "Training loss (for one batch) at step 90: 326.5112, Accuracy: 0.7782\n",
      "Training loss (for one batch) at step 100: 312.5142, Accuracy: 0.7815\n",
      "Training loss (for one batch) at step 110: 324.7672, Accuracy: 0.7827\n",
      "---- Training ----\n",
      "Training loss: 108.3831\n",
      "Training acc over epoch: 0.7827\n",
      "---- Validation ----\n",
      "Validation loss: 45.5239\n",
      "Validation acc: 0.7171\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 368.1309, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 371.8279, Accuracy: 0.6839\n",
      "Training loss (for one batch) at step 20: 322.0050, Accuracy: 0.7307\n",
      "Training loss (for one batch) at step 30: 318.5127, Accuracy: 0.7676\n",
      "Training loss (for one batch) at step 40: 308.9210, Accuracy: 0.7873\n",
      "Training loss (for one batch) at step 50: 320.0241, Accuracy: 0.8021\n",
      "Training loss (for one batch) at step 60: 341.3917, Accuracy: 0.8096\n",
      "Training loss (for one batch) at step 70: 363.5002, Accuracy: 0.7970\n",
      "Training loss (for one batch) at step 80: 355.5427, Accuracy: 0.7847\n",
      "Training loss (for one batch) at step 90: 339.7138, Accuracy: 0.7795\n",
      "Training loss (for one batch) at step 100: 314.3813, Accuracy: 0.7830\n",
      "Training loss (for one batch) at step 110: 337.5984, Accuracy: 0.7833\n",
      "---- Training ----\n",
      "Training loss: 97.6968\n",
      "Training acc over epoch: 0.7818\n",
      "---- Validation ----\n",
      "Validation loss: 59.2101\n",
      "Validation acc: 0.7080\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 371.1914, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 335.3978, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 20: 313.1156, Accuracy: 0.7269\n",
      "Training loss (for one batch) at step 30: 313.2277, Accuracy: 0.7568\n",
      "Training loss (for one batch) at step 40: 294.5104, Accuracy: 0.7820\n",
      "Training loss (for one batch) at step 50: 281.4489, Accuracy: 0.8027\n",
      "Training loss (for one batch) at step 60: 320.7560, Accuracy: 0.8056\n",
      "Training loss (for one batch) at step 70: 349.8386, Accuracy: 0.7948\n",
      "Training loss (for one batch) at step 80: 343.1470, Accuracy: 0.7825\n",
      "Training loss (for one batch) at step 90: 329.1672, Accuracy: 0.7779\n",
      "Training loss (for one batch) at step 100: 294.5111, Accuracy: 0.7815\n",
      "Training loss (for one batch) at step 110: 331.6172, Accuracy: 0.7834\n",
      "---- Training ----\n",
      "Training loss: 100.7002\n",
      "Training acc over epoch: 0.7819\n",
      "---- Validation ----\n",
      "Validation loss: 36.3268\n",
      "Validation acc: 0.7128\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 381.2399, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 357.3823, Accuracy: 0.6982\n",
      "Training loss (for one batch) at step 20: 317.1390, Accuracy: 0.7307\n",
      "Training loss (for one batch) at step 30: 311.9084, Accuracy: 0.7616\n",
      "Training loss (for one batch) at step 40: 310.2948, Accuracy: 0.7868\n",
      "Training loss (for one batch) at step 50: 297.4786, Accuracy: 0.8007\n",
      "Training loss (for one batch) at step 60: 323.8029, Accuracy: 0.8072\n",
      "Training loss (for one batch) at step 70: 360.5292, Accuracy: 0.7952\n",
      "Training loss (for one batch) at step 80: 343.4215, Accuracy: 0.7780\n",
      "Training loss (for one batch) at step 90: 322.0151, Accuracy: 0.7731\n",
      "Training loss (for one batch) at step 100: 319.5551, Accuracy: 0.7778\n",
      "Training loss (for one batch) at step 110: 316.9720, Accuracy: 0.7793\n",
      "---- Training ----\n",
      "Training loss: 98.9693\n",
      "Training acc over epoch: 0.7787\n",
      "---- Validation ----\n",
      "Validation loss: 52.4658\n",
      "Validation acc: 0.7179\n",
      "Time taken: 10.12s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 364.6313, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 360.4630, Accuracy: 0.7053\n",
      "Training loss (for one batch) at step 20: 320.4192, Accuracy: 0.7448\n",
      "Training loss (for one batch) at step 30: 289.2547, Accuracy: 0.7722\n",
      "Training loss (for one batch) at step 40: 285.0229, Accuracy: 0.7973\n",
      "Training loss (for one batch) at step 50: 308.8292, Accuracy: 0.8090\n",
      "Training loss (for one batch) at step 60: 312.6680, Accuracy: 0.8137\n",
      "Training loss (for one batch) at step 70: 340.2729, Accuracy: 0.7991\n",
      "Training loss (for one batch) at step 80: 325.1676, Accuracy: 0.7839\n",
      "Training loss (for one batch) at step 90: 308.2973, Accuracy: 0.7827\n",
      "Training loss (for one batch) at step 100: 320.6919, Accuracy: 0.7849\n",
      "Training loss (for one batch) at step 110: 323.0318, Accuracy: 0.7854\n",
      "---- Training ----\n",
      "Training loss: 109.1409\n",
      "Training acc over epoch: 0.7849\n",
      "---- Validation ----\n",
      "Validation loss: 47.4352\n",
      "Validation acc: 0.7123\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 352.9526, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 353.2190, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 20: 322.3916, Accuracy: 0.7247\n",
      "Training loss (for one batch) at step 30: 335.8047, Accuracy: 0.7553\n",
      "Training loss (for one batch) at step 40: 296.9144, Accuracy: 0.7839\n",
      "Training loss (for one batch) at step 50: 286.1550, Accuracy: 0.8004\n",
      "Training loss (for one batch) at step 60: 331.8913, Accuracy: 0.8065\n",
      "Training loss (for one batch) at step 70: 344.4162, Accuracy: 0.7937\n",
      "Training loss (for one batch) at step 80: 344.1830, Accuracy: 0.7791\n",
      "Training loss (for one batch) at step 90: 315.7982, Accuracy: 0.7764\n",
      "Training loss (for one batch) at step 100: 296.9310, Accuracy: 0.7814\n",
      "Training loss (for one batch) at step 110: 332.9384, Accuracy: 0.7820\n",
      "---- Training ----\n",
      "Training loss: 116.6810\n",
      "Training acc over epoch: 0.7818\n",
      "---- Validation ----\n",
      "Validation loss: 53.8788\n",
      "Validation acc: 0.7080\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 342.1082, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 353.5556, Accuracy: 0.6989\n",
      "Training loss (for one batch) at step 20: 312.6945, Accuracy: 0.7292\n",
      "Training loss (for one batch) at step 30: 296.4380, Accuracy: 0.7661\n",
      "Training loss (for one batch) at step 40: 305.0436, Accuracy: 0.7853\n",
      "Training loss (for one batch) at step 50: 303.0605, Accuracy: 0.8021\n",
      "Training loss (for one batch) at step 60: 310.8675, Accuracy: 0.8093\n",
      "Training loss (for one batch) at step 70: 326.8605, Accuracy: 0.7953\n",
      "Training loss (for one batch) at step 80: 337.7625, Accuracy: 0.7809\n",
      "Training loss (for one batch) at step 90: 313.7427, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 100: 307.3734, Accuracy: 0.7790\n",
      "Training loss (for one batch) at step 110: 319.7821, Accuracy: 0.7808\n",
      "---- Training ----\n",
      "Training loss: 102.1608\n",
      "Training acc over epoch: 0.7786\n",
      "---- Validation ----\n",
      "Validation loss: 40.8293\n",
      "Validation acc: 0.7131\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 358.9594, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 330.9082, Accuracy: 0.6911\n",
      "Training loss (for one batch) at step 20: 316.9175, Accuracy: 0.7258\n",
      "Training loss (for one batch) at step 30: 283.0635, Accuracy: 0.7641\n",
      "Training loss (for one batch) at step 40: 303.0879, Accuracy: 0.7864\n",
      "Training loss (for one batch) at step 50: 283.2741, Accuracy: 0.8021\n",
      "Training loss (for one batch) at step 60: 311.9714, Accuracy: 0.8058\n",
      "Training loss (for one batch) at step 70: 351.6056, Accuracy: 0.7924\n",
      "Training loss (for one batch) at step 80: 328.2673, Accuracy: 0.7787\n",
      "Training loss (for one batch) at step 90: 300.4608, Accuracy: 0.7755\n",
      "Training loss (for one batch) at step 100: 303.3888, Accuracy: 0.7778\n",
      "Training loss (for one batch) at step 110: 317.6196, Accuracy: 0.7780\n",
      "---- Training ----\n",
      "Training loss: 97.7132\n",
      "Training acc over epoch: 0.7773\n",
      "---- Validation ----\n",
      "Validation loss: 47.7829\n",
      "Validation acc: 0.7002\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 372.7417, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 341.7560, Accuracy: 0.6697\n",
      "Training loss (for one batch) at step 20: 305.2590, Accuracy: 0.7195\n",
      "Training loss (for one batch) at step 30: 304.2872, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 40: 287.6330, Accuracy: 0.7786\n",
      "Training loss (for one batch) at step 50: 291.6704, Accuracy: 0.7950\n",
      "Training loss (for one batch) at step 60: 323.3835, Accuracy: 0.8033\n",
      "Training loss (for one batch) at step 70: 318.6822, Accuracy: 0.7910\n",
      "Training loss (for one batch) at step 80: 352.2546, Accuracy: 0.7785\n",
      "Training loss (for one batch) at step 90: 298.2405, Accuracy: 0.7746\n",
      "Training loss (for one batch) at step 100: 280.2213, Accuracy: 0.7778\n",
      "Training loss (for one batch) at step 110: 320.7909, Accuracy: 0.7790\n",
      "---- Training ----\n",
      "Training loss: 106.2196\n",
      "Training acc over epoch: 0.7777\n",
      "---- Validation ----\n",
      "Validation loss: 50.4436\n",
      "Validation acc: 0.7155\n",
      "Time taken: 10.64s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 338.5556, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 333.4221, Accuracy: 0.6832\n",
      "Training loss (for one batch) at step 20: 327.8781, Accuracy: 0.7161\n",
      "Training loss (for one batch) at step 30: 309.7267, Accuracy: 0.7581\n",
      "Training loss (for one batch) at step 40: 300.4424, Accuracy: 0.7845\n",
      "Training loss (for one batch) at step 50: 301.7230, Accuracy: 0.8018\n",
      "Training loss (for one batch) at step 60: 316.8272, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 70: 329.7258, Accuracy: 0.7959\n",
      "Training loss (for one batch) at step 80: 350.0075, Accuracy: 0.7803\n",
      "Training loss (for one batch) at step 90: 300.9575, Accuracy: 0.7777\n",
      "Training loss (for one batch) at step 100: 313.7195, Accuracy: 0.7809\n",
      "Training loss (for one batch) at step 110: 323.3026, Accuracy: 0.7819\n",
      "---- Training ----\n",
      "Training loss: 98.8979\n",
      "Training acc over epoch: 0.7796\n",
      "---- Validation ----\n",
      "Validation loss: 47.6515\n",
      "Validation acc: 0.7120\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 366.4275, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 336.2029, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 20: 316.4906, Accuracy: 0.7303\n",
      "Training loss (for one batch) at step 30: 294.1734, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 40: 281.7784, Accuracy: 0.7875\n",
      "Training loss (for one batch) at step 50: 285.9162, Accuracy: 0.8056\n",
      "Training loss (for one batch) at step 60: 308.8582, Accuracy: 0.8102\n",
      "Training loss (for one batch) at step 70: 330.7273, Accuracy: 0.7985\n",
      "Training loss (for one batch) at step 80: 330.7534, Accuracy: 0.7835\n",
      "Training loss (for one batch) at step 90: 339.3707, Accuracy: 0.7764\n",
      "Training loss (for one batch) at step 100: 328.4203, Accuracy: 0.7822\n",
      "Training loss (for one batch) at step 110: 331.4489, Accuracy: 0.7826\n",
      "---- Training ----\n",
      "Training loss: 116.3728\n",
      "Training acc over epoch: 0.7818\n",
      "---- Validation ----\n",
      "Validation loss: 42.0027\n",
      "Validation acc: 0.7093\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 358.7311, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 331.9468, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 20: 299.4380, Accuracy: 0.7329\n",
      "Training loss (for one batch) at step 30: 294.2625, Accuracy: 0.7644\n",
      "Training loss (for one batch) at step 40: 285.1797, Accuracy: 0.7837\n",
      "Training loss (for one batch) at step 50: 292.5579, Accuracy: 0.7999\n",
      "Training loss (for one batch) at step 60: 310.9433, Accuracy: 0.8066\n",
      "Training loss (for one batch) at step 70: 327.3058, Accuracy: 0.7945\n",
      "Training loss (for one batch) at step 80: 328.6557, Accuracy: 0.7773\n",
      "Training loss (for one batch) at step 90: 315.1719, Accuracy: 0.7728\n",
      "Training loss (for one batch) at step 100: 316.0103, Accuracy: 0.7758\n",
      "Training loss (for one batch) at step 110: 330.5243, Accuracy: 0.7779\n",
      "---- Training ----\n",
      "Training loss: 108.4937\n",
      "Training acc over epoch: 0.7768\n",
      "---- Validation ----\n",
      "Validation loss: 59.0208\n",
      "Validation acc: 0.7136\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 353.0398, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 334.8580, Accuracy: 0.6896\n",
      "Training loss (for one batch) at step 20: 313.5704, Accuracy: 0.7292\n",
      "Training loss (for one batch) at step 30: 295.1775, Accuracy: 0.7636\n",
      "Training loss (for one batch) at step 40: 299.4344, Accuracy: 0.7837\n",
      "Training loss (for one batch) at step 50: 285.7628, Accuracy: 0.8038\n",
      "Training loss (for one batch) at step 60: 324.7107, Accuracy: 0.8093\n",
      "Training loss (for one batch) at step 70: 333.1982, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 80: 338.7752, Accuracy: 0.7804\n",
      "Training loss (for one batch) at step 90: 304.1812, Accuracy: 0.7779\n",
      "Training loss (for one batch) at step 100: 304.1706, Accuracy: 0.7826\n",
      "Training loss (for one batch) at step 110: 309.4194, Accuracy: 0.7827\n",
      "---- Training ----\n",
      "Training loss: 110.5298\n",
      "Training acc over epoch: 0.7804\n",
      "---- Validation ----\n",
      "Validation loss: 54.9810\n",
      "Validation acc: 0.7074\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 337.1067, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 334.5137, Accuracy: 0.6960\n",
      "Training loss (for one batch) at step 20: 298.6241, Accuracy: 0.7321\n",
      "Training loss (for one batch) at step 30: 304.0356, Accuracy: 0.7664\n",
      "Training loss (for one batch) at step 40: 298.1237, Accuracy: 0.7860\n",
      "Training loss (for one batch) at step 50: 288.6298, Accuracy: 0.8025\n",
      "Training loss (for one batch) at step 60: 319.9652, Accuracy: 0.8083\n",
      "Training loss (for one batch) at step 70: 321.1723, Accuracy: 0.7978\n",
      "Training loss (for one batch) at step 80: 347.5978, Accuracy: 0.7828\n",
      "Training loss (for one batch) at step 90: 302.9834, Accuracy: 0.7794\n",
      "Training loss (for one batch) at step 100: 325.4305, Accuracy: 0.7821\n",
      "Training loss (for one batch) at step 110: 316.7602, Accuracy: 0.7816\n",
      "---- Training ----\n",
      "Training loss: 95.6332\n",
      "Training acc over epoch: 0.7808\n",
      "---- Validation ----\n",
      "Validation loss: 49.9379\n",
      "Validation acc: 0.7047\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 343.0773, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 338.3051, Accuracy: 0.6776\n",
      "Training loss (for one batch) at step 20: 305.2411, Accuracy: 0.7184\n",
      "Training loss (for one batch) at step 30: 287.4287, Accuracy: 0.7613\n",
      "Training loss (for one batch) at step 40: 293.5005, Accuracy: 0.7822\n",
      "Training loss (for one batch) at step 50: 277.0620, Accuracy: 0.7978\n",
      "Training loss (for one batch) at step 60: 321.7826, Accuracy: 0.8052\n",
      "Training loss (for one batch) at step 70: 350.0836, Accuracy: 0.7939\n",
      "Training loss (for one batch) at step 80: 346.6969, Accuracy: 0.7772\n",
      "Training loss (for one batch) at step 90: 308.8063, Accuracy: 0.7736\n",
      "Training loss (for one batch) at step 100: 303.2119, Accuracy: 0.7775\n",
      "Training loss (for one batch) at step 110: 311.1168, Accuracy: 0.7781\n",
      "---- Training ----\n",
      "Training loss: 99.3041\n",
      "Training acc over epoch: 0.7777\n",
      "---- Validation ----\n",
      "Validation loss: 40.1072\n",
      "Validation acc: 0.7031\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 357.4257, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 339.0724, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 20: 296.5074, Accuracy: 0.7228\n",
      "Training loss (for one batch) at step 30: 288.4870, Accuracy: 0.7601\n",
      "Training loss (for one batch) at step 40: 299.5207, Accuracy: 0.7820\n",
      "Training loss (for one batch) at step 50: 285.3392, Accuracy: 0.7989\n",
      "Training loss (for one batch) at step 60: 290.5177, Accuracy: 0.8057\n",
      "Training loss (for one batch) at step 70: 319.6434, Accuracy: 0.7926\n",
      "Training loss (for one batch) at step 80: 317.9034, Accuracy: 0.7805\n",
      "Training loss (for one batch) at step 90: 303.4303, Accuracy: 0.7751\n",
      "Training loss (for one batch) at step 100: 283.7892, Accuracy: 0.7795\n",
      "Training loss (for one batch) at step 110: 336.5342, Accuracy: 0.7805\n",
      "---- Training ----\n",
      "Training loss: 89.6487\n",
      "Training acc over epoch: 0.7785\n",
      "---- Validation ----\n",
      "Validation loss: 46.8284\n",
      "Validation acc: 0.7031\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 335.9524, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 320.6763, Accuracy: 0.6790\n",
      "Training loss (for one batch) at step 20: 298.5046, Accuracy: 0.7284\n",
      "Training loss (for one batch) at step 30: 296.2455, Accuracy: 0.7631\n",
      "Training loss (for one batch) at step 40: 277.9846, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 50: 271.8895, Accuracy: 0.8041\n",
      "Training loss (for one batch) at step 60: 289.7780, Accuracy: 0.8075\n",
      "Training loss (for one batch) at step 70: 328.2971, Accuracy: 0.7923\n",
      "Training loss (for one batch) at step 80: 321.5018, Accuracy: 0.7785\n",
      "Training loss (for one batch) at step 90: 316.1289, Accuracy: 0.7751\n",
      "Training loss (for one batch) at step 100: 282.5676, Accuracy: 0.7791\n",
      "Training loss (for one batch) at step 110: 290.8856, Accuracy: 0.7793\n",
      "---- Training ----\n",
      "Training loss: 86.8717\n",
      "Training acc over epoch: 0.7778\n",
      "---- Validation ----\n",
      "Validation loss: 61.7031\n",
      "Validation acc: 0.7053\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 356.5508, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 338.9736, Accuracy: 0.6839\n",
      "Training loss (for one batch) at step 20: 304.6536, Accuracy: 0.7307\n",
      "Training loss (for one batch) at step 30: 307.4889, Accuracy: 0.7603\n",
      "Training loss (for one batch) at step 40: 284.1404, Accuracy: 0.7858\n",
      "Training loss (for one batch) at step 50: 298.0157, Accuracy: 0.8018\n",
      "Training loss (for one batch) at step 60: 290.3485, Accuracy: 0.8060\n",
      "Training loss (for one batch) at step 70: 309.5562, Accuracy: 0.7946\n",
      "Training loss (for one batch) at step 80: 340.9583, Accuracy: 0.7808\n",
      "Training loss (for one batch) at step 90: 297.8381, Accuracy: 0.7771\n",
      "Training loss (for one batch) at step 100: 298.0839, Accuracy: 0.7807\n",
      "Training loss (for one batch) at step 110: 284.0474, Accuracy: 0.7812\n",
      "---- Training ----\n",
      "Training loss: 102.3585\n",
      "Training acc over epoch: 0.7806\n",
      "---- Validation ----\n",
      "Validation loss: 40.0490\n",
      "Validation acc: 0.6908\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 347.0299, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 344.8822, Accuracy: 0.6591\n",
      "Training loss (for one batch) at step 20: 309.0594, Accuracy: 0.7098\n",
      "Training loss (for one batch) at step 30: 275.4822, Accuracy: 0.7523\n",
      "Training loss (for one batch) at step 40: 283.6073, Accuracy: 0.7799\n",
      "Training loss (for one batch) at step 50: 286.8638, Accuracy: 0.7941\n",
      "Training loss (for one batch) at step 60: 286.2755, Accuracy: 0.8011\n",
      "Training loss (for one batch) at step 70: 333.3690, Accuracy: 0.7917\n",
      "Training loss (for one batch) at step 80: 316.1850, Accuracy: 0.7753\n",
      "Training loss (for one batch) at step 90: 281.7556, Accuracy: 0.7721\n",
      "Training loss (for one batch) at step 100: 313.9862, Accuracy: 0.7764\n",
      "Training loss (for one batch) at step 110: 312.2247, Accuracy: 0.7779\n",
      "---- Training ----\n",
      "Training loss: 103.1843\n",
      "Training acc over epoch: 0.7773\n",
      "---- Validation ----\n",
      "Validation loss: 33.7421\n",
      "Validation acc: 0.6964\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 353.8062, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 314.7464, Accuracy: 0.6974\n",
      "Training loss (for one batch) at step 20: 300.3846, Accuracy: 0.7355\n",
      "Training loss (for one batch) at step 30: 294.3508, Accuracy: 0.7659\n",
      "Training loss (for one batch) at step 40: 287.4448, Accuracy: 0.7904\n",
      "Training loss (for one batch) at step 50: 293.9421, Accuracy: 0.8053\n",
      "Training loss (for one batch) at step 60: 284.9515, Accuracy: 0.8110\n",
      "Training loss (for one batch) at step 70: 323.9178, Accuracy: 0.8008\n",
      "Training loss (for one batch) at step 80: 312.9151, Accuracy: 0.7855\n",
      "Training loss (for one batch) at step 90: 299.9487, Accuracy: 0.7788\n",
      "Training loss (for one batch) at step 100: 273.4209, Accuracy: 0.7840\n",
      "Training loss (for one batch) at step 110: 305.7272, Accuracy: 0.7841\n",
      "---- Training ----\n",
      "Training loss: 110.0080\n",
      "Training acc over epoch: 0.7831\n",
      "---- Validation ----\n",
      "Validation loss: 78.3782\n",
      "Validation acc: 0.7007\n",
      "Time taken: 10.05s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 329.1416, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 327.4567, Accuracy: 0.6832\n",
      "Training loss (for one batch) at step 20: 283.7875, Accuracy: 0.7299\n",
      "Training loss (for one batch) at step 30: 282.8660, Accuracy: 0.7616\n",
      "Training loss (for one batch) at step 40: 285.3308, Accuracy: 0.7883\n",
      "Training loss (for one batch) at step 50: 277.3217, Accuracy: 0.8025\n",
      "Training loss (for one batch) at step 60: 326.3192, Accuracy: 0.8081\n",
      "Training loss (for one batch) at step 70: 336.6969, Accuracy: 0.7960\n",
      "Training loss (for one batch) at step 80: 316.9503, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 90: 322.1856, Accuracy: 0.7769\n",
      "Training loss (for one batch) at step 100: 281.6624, Accuracy: 0.7814\n",
      "Training loss (for one batch) at step 110: 298.8849, Accuracy: 0.7799\n",
      "---- Training ----\n",
      "Training loss: 92.5153\n",
      "Training acc over epoch: 0.7802\n",
      "---- Validation ----\n",
      "Validation loss: 36.1672\n",
      "Validation acc: 0.7002\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 326.3055, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 333.3645, Accuracy: 0.6839\n",
      "Training loss (for one batch) at step 20: 301.4611, Accuracy: 0.7284\n",
      "Training loss (for one batch) at step 30: 278.1788, Accuracy: 0.7631\n",
      "Training loss (for one batch) at step 40: 296.8632, Accuracy: 0.7868\n",
      "Training loss (for one batch) at step 50: 289.7233, Accuracy: 0.8013\n",
      "Training loss (for one batch) at step 60: 295.5261, Accuracy: 0.8081\n",
      "Training loss (for one batch) at step 70: 315.4982, Accuracy: 0.7947\n",
      "Training loss (for one batch) at step 80: 337.2786, Accuracy: 0.7789\n",
      "Training loss (for one batch) at step 90: 304.4618, Accuracy: 0.7774\n",
      "Training loss (for one batch) at step 100: 300.5490, Accuracy: 0.7787\n",
      "Training loss (for one batch) at step 110: 302.4275, Accuracy: 0.7796\n",
      "---- Training ----\n",
      "Training loss: 100.1483\n",
      "Training acc over epoch: 0.7798\n",
      "---- Validation ----\n",
      "Validation loss: 69.9973\n",
      "Validation acc: 0.7071\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 351.6763, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 334.8056, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 20: 288.8269, Accuracy: 0.7292\n",
      "Training loss (for one batch) at step 30: 290.4948, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 40: 283.3950, Accuracy: 0.7858\n",
      "Training loss (for one batch) at step 50: 292.0289, Accuracy: 0.8038\n",
      "Training loss (for one batch) at step 60: 313.0176, Accuracy: 0.8090\n",
      "Training loss (for one batch) at step 70: 321.2459, Accuracy: 0.7971\n",
      "Training loss (for one batch) at step 80: 325.6989, Accuracy: 0.7815\n",
      "Training loss (for one batch) at step 90: 313.3631, Accuracy: 0.7776\n",
      "Training loss (for one batch) at step 100: 289.8297, Accuracy: 0.7806\n",
      "Training loss (for one batch) at step 110: 293.8722, Accuracy: 0.7812\n",
      "---- Training ----\n",
      "Training loss: 95.3580\n",
      "Training acc over epoch: 0.7811\n",
      "---- Validation ----\n",
      "Validation loss: 53.3307\n",
      "Validation acc: 0.7101\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 345.4522, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 326.9010, Accuracy: 0.6761\n",
      "Training loss (for one batch) at step 20: 309.4842, Accuracy: 0.7243\n",
      "Training loss (for one batch) at step 30: 283.4511, Accuracy: 0.7586\n",
      "Training loss (for one batch) at step 40: 302.2896, Accuracy: 0.7839\n",
      "Training loss (for one batch) at step 50: 275.7845, Accuracy: 0.8016\n",
      "Training loss (for one batch) at step 60: 272.8856, Accuracy: 0.8067\n",
      "Training loss (for one batch) at step 70: 306.8596, Accuracy: 0.7938\n",
      "Training loss (for one batch) at step 80: 337.0290, Accuracy: 0.7781\n",
      "Training loss (for one batch) at step 90: 300.2148, Accuracy: 0.7759\n",
      "Training loss (for one batch) at step 100: 278.5474, Accuracy: 0.7790\n",
      "Training loss (for one batch) at step 110: 304.0227, Accuracy: 0.7789\n",
      "---- Training ----\n",
      "Training loss: 86.5169\n",
      "Training acc over epoch: 0.7798\n",
      "---- Validation ----\n",
      "Validation loss: 55.7012\n",
      "Validation acc: 0.7149\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 331.4756, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 327.7912, Accuracy: 0.6662\n",
      "Training loss (for one batch) at step 20: 279.2722, Accuracy: 0.7135\n",
      "Training loss (for one batch) at step 30: 307.2551, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 40: 303.0963, Accuracy: 0.7765\n",
      "Training loss (for one batch) at step 50: 269.0208, Accuracy: 0.7947\n",
      "Training loss (for one batch) at step 60: 302.1749, Accuracy: 0.8021\n",
      "Training loss (for one batch) at step 70: 315.8674, Accuracy: 0.7923\n",
      "Training loss (for one batch) at step 80: 317.4011, Accuracy: 0.7796\n",
      "Training loss (for one batch) at step 90: 306.4219, Accuracy: 0.7752\n",
      "Training loss (for one batch) at step 100: 285.0222, Accuracy: 0.7800\n",
      "Training loss (for one batch) at step 110: 292.8103, Accuracy: 0.7812\n",
      "---- Training ----\n",
      "Training loss: 108.3343\n",
      "Training acc over epoch: 0.7789\n",
      "---- Validation ----\n",
      "Validation loss: 48.3425\n",
      "Validation acc: 0.7144\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 322.2017, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 329.9881, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 20: 276.3704, Accuracy: 0.7236\n",
      "Training loss (for one batch) at step 30: 268.4285, Accuracy: 0.7644\n",
      "Training loss (for one batch) at step 40: 285.4574, Accuracy: 0.7872\n",
      "Training loss (for one batch) at step 50: 278.0211, Accuracy: 0.8055\n",
      "Training loss (for one batch) at step 60: 302.2989, Accuracy: 0.8105\n",
      "Training loss (for one batch) at step 70: 333.1981, Accuracy: 0.7994\n",
      "Training loss (for one batch) at step 80: 313.3793, Accuracy: 0.7833\n",
      "Training loss (for one batch) at step 90: 290.2978, Accuracy: 0.7795\n",
      "Training loss (for one batch) at step 100: 283.7732, Accuracy: 0.7836\n",
      "Training loss (for one batch) at step 110: 287.8106, Accuracy: 0.7843\n",
      "---- Training ----\n",
      "Training loss: 96.9580\n",
      "Training acc over epoch: 0.7826\n",
      "---- Validation ----\n",
      "Validation loss: 44.1348\n",
      "Validation acc: 0.7077\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 341.3019, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 303.4094, Accuracy: 0.6662\n",
      "Training loss (for one batch) at step 20: 294.6853, Accuracy: 0.7173\n",
      "Training loss (for one batch) at step 30: 283.2124, Accuracy: 0.7571\n",
      "Training loss (for one batch) at step 40: 268.5905, Accuracy: 0.7784\n",
      "Training loss (for one batch) at step 50: 274.9081, Accuracy: 0.7960\n",
      "Training loss (for one batch) at step 60: 297.4634, Accuracy: 0.8046\n",
      "Training loss (for one batch) at step 70: 344.2718, Accuracy: 0.7926\n",
      "Training loss (for one batch) at step 80: 324.8985, Accuracy: 0.7760\n",
      "Training loss (for one batch) at step 90: 275.3100, Accuracy: 0.7729\n",
      "Training loss (for one batch) at step 100: 291.9802, Accuracy: 0.7779\n",
      "Training loss (for one batch) at step 110: 294.7752, Accuracy: 0.7782\n",
      "---- Training ----\n",
      "Training loss: 113.2296\n",
      "Training acc over epoch: 0.7771\n",
      "---- Validation ----\n",
      "Validation loss: 34.6454\n",
      "Validation acc: 0.7093\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 329.7628, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 324.8076, Accuracy: 0.6690\n",
      "Training loss (for one batch) at step 20: 290.3047, Accuracy: 0.7221\n",
      "Training loss (for one batch) at step 30: 294.6638, Accuracy: 0.7576\n",
      "Training loss (for one batch) at step 40: 263.1177, Accuracy: 0.7818\n",
      "Training loss (for one batch) at step 50: 276.0479, Accuracy: 0.7978\n",
      "Training loss (for one batch) at step 60: 281.8248, Accuracy: 0.8079\n",
      "Training loss (for one batch) at step 70: 311.5952, Accuracy: 0.7946\n",
      "Training loss (for one batch) at step 80: 328.8052, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 90: 313.8903, Accuracy: 0.7773\n",
      "Training loss (for one batch) at step 100: 281.5370, Accuracy: 0.7810\n",
      "Training loss (for one batch) at step 110: 293.9123, Accuracy: 0.7820\n",
      "---- Training ----\n",
      "Training loss: 87.8708\n",
      "Training acc over epoch: 0.7806\n",
      "---- Validation ----\n",
      "Validation loss: 36.1338\n",
      "Validation acc: 0.7136\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 324.1920, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 325.4016, Accuracy: 0.6790\n",
      "Training loss (for one batch) at step 20: 307.8800, Accuracy: 0.7210\n",
      "Training loss (for one batch) at step 30: 299.5954, Accuracy: 0.7623\n",
      "Training loss (for one batch) at step 40: 287.7399, Accuracy: 0.7872\n",
      "Training loss (for one batch) at step 50: 277.0078, Accuracy: 0.8045\n",
      "Training loss (for one batch) at step 60: 311.1454, Accuracy: 0.8078\n",
      "Training loss (for one batch) at step 70: 310.0545, Accuracy: 0.7970\n",
      "Training loss (for one batch) at step 80: 317.0924, Accuracy: 0.7799\n",
      "Training loss (for one batch) at step 90: 287.5277, Accuracy: 0.7775\n",
      "Training loss (for one batch) at step 100: 312.3981, Accuracy: 0.7815\n",
      "Training loss (for one batch) at step 110: 308.1937, Accuracy: 0.7817\n",
      "---- Training ----\n",
      "Training loss: 97.9400\n",
      "Training acc over epoch: 0.7806\n",
      "---- Validation ----\n",
      "Validation loss: 49.8092\n",
      "Validation acc: 0.7090\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 322.9560, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 318.8601, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 20: 295.9298, Accuracy: 0.7221\n",
      "Training loss (for one batch) at step 30: 274.4937, Accuracy: 0.7591\n",
      "Training loss (for one batch) at step 40: 267.6085, Accuracy: 0.7858\n",
      "Training loss (for one batch) at step 50: 269.6709, Accuracy: 0.8019\n",
      "Training loss (for one batch) at step 60: 291.8978, Accuracy: 0.8052\n",
      "Training loss (for one batch) at step 70: 327.6444, Accuracy: 0.7924\n",
      "Training loss (for one batch) at step 80: 313.4348, Accuracy: 0.7785\n",
      "Training loss (for one batch) at step 90: 289.9826, Accuracy: 0.7718\n",
      "Training loss (for one batch) at step 100: 285.2351, Accuracy: 0.7778\n",
      "Training loss (for one batch) at step 110: 311.0366, Accuracy: 0.7768\n",
      "---- Training ----\n",
      "Training loss: 110.5655\n",
      "Training acc over epoch: 0.7761\n",
      "---- Validation ----\n",
      "Validation loss: 35.5781\n",
      "Validation acc: 0.7050\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 327.0204, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 337.9746, Accuracy: 0.6733\n",
      "Training loss (for one batch) at step 20: 289.1972, Accuracy: 0.7202\n",
      "Training loss (for one batch) at step 30: 282.1177, Accuracy: 0.7571\n",
      "Training loss (for one batch) at step 40: 262.1750, Accuracy: 0.7809\n",
      "Training loss (for one batch) at step 50: 278.8803, Accuracy: 0.7999\n",
      "Training loss (for one batch) at step 60: 292.2328, Accuracy: 0.8065\n",
      "Training loss (for one batch) at step 70: 357.6573, Accuracy: 0.7930\n",
      "Training loss (for one batch) at step 80: 334.0648, Accuracy: 0.7797\n",
      "Training loss (for one batch) at step 90: 267.2138, Accuracy: 0.7764\n",
      "Training loss (for one batch) at step 100: 295.0788, Accuracy: 0.7795\n",
      "Training loss (for one batch) at step 110: 289.1876, Accuracy: 0.7805\n",
      "---- Training ----\n",
      "Training loss: 116.2967\n",
      "Training acc over epoch: 0.7792\n",
      "---- Validation ----\n",
      "Validation loss: 39.7670\n",
      "Validation acc: 0.6969\n",
      "Time taken: 10.58s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 342.2655, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 309.5292, Accuracy: 0.6761\n",
      "Training loss (for one batch) at step 20: 300.5234, Accuracy: 0.7232\n",
      "Training loss (for one batch) at step 30: 277.2151, Accuracy: 0.7603\n",
      "Training loss (for one batch) at step 40: 272.3841, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 50: 276.0768, Accuracy: 0.8019\n",
      "Training loss (for one batch) at step 60: 296.3195, Accuracy: 0.8080\n",
      "Training loss (for one batch) at step 70: 322.7840, Accuracy: 0.7961\n",
      "Training loss (for one batch) at step 80: 307.4139, Accuracy: 0.7809\n",
      "Training loss (for one batch) at step 90: 290.5305, Accuracy: 0.7785\n",
      "Training loss (for one batch) at step 100: 288.5331, Accuracy: 0.7813\n",
      "Training loss (for one batch) at step 110: 304.3793, Accuracy: 0.7809\n",
      "---- Training ----\n",
      "Training loss: 100.7132\n",
      "Training acc over epoch: 0.7801\n",
      "---- Validation ----\n",
      "Validation loss: 48.5261\n",
      "Validation acc: 0.6999\n",
      "Time taken: 10.14s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 321.7270, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 327.3261, Accuracy: 0.6655\n",
      "Training loss (for one batch) at step 20: 286.6352, Accuracy: 0.7169\n",
      "Training loss (for one batch) at step 30: 268.7772, Accuracy: 0.7545\n",
      "Training loss (for one batch) at step 40: 275.8487, Accuracy: 0.7793\n",
      "Training loss (for one batch) at step 50: 273.8666, Accuracy: 0.7972\n",
      "Training loss (for one batch) at step 60: 298.8484, Accuracy: 0.8061\n",
      "Training loss (for one batch) at step 70: 318.4348, Accuracy: 0.7934\n",
      "Training loss (for one batch) at step 80: 335.5814, Accuracy: 0.7778\n",
      "Training loss (for one batch) at step 90: 273.7920, Accuracy: 0.7727\n",
      "Training loss (for one batch) at step 100: 292.9444, Accuracy: 0.7777\n",
      "Training loss (for one batch) at step 110: 291.6235, Accuracy: 0.7791\n",
      "---- Training ----\n",
      "Training loss: 95.5365\n",
      "Training acc over epoch: 0.7785\n",
      "---- Validation ----\n",
      "Validation loss: 62.0060\n",
      "Validation acc: 0.7080\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 336.3977, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 308.6561, Accuracy: 0.6818\n",
      "Training loss (for one batch) at step 20: 288.6927, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 30: 278.0716, Accuracy: 0.7593\n",
      "Training loss (for one batch) at step 40: 278.4932, Accuracy: 0.7809\n",
      "Training loss (for one batch) at step 50: 279.2844, Accuracy: 0.8018\n",
      "Training loss (for one batch) at step 60: 288.3361, Accuracy: 0.8101\n",
      "Training loss (for one batch) at step 70: 317.6779, Accuracy: 0.7985\n",
      "Training loss (for one batch) at step 80: 317.5068, Accuracy: 0.7819\n",
      "Training loss (for one batch) at step 90: 292.8636, Accuracy: 0.7786\n",
      "Training loss (for one batch) at step 100: 297.3278, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 110: 301.6510, Accuracy: 0.7824\n",
      "---- Training ----\n",
      "Training loss: 108.5792\n",
      "Training acc over epoch: 0.7819\n",
      "---- Validation ----\n",
      "Validation loss: 72.2942\n",
      "Validation acc: 0.6961\n",
      "Time taken: 10.65s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 307.8007, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 291.0151, Accuracy: 0.6839\n",
      "Training loss (for one batch) at step 20: 280.0344, Accuracy: 0.7273\n",
      "Training loss (for one batch) at step 30: 256.2664, Accuracy: 0.7555\n",
      "Training loss (for one batch) at step 40: 269.2776, Accuracy: 0.7839\n",
      "Training loss (for one batch) at step 50: 271.8792, Accuracy: 0.7998\n",
      "Training loss (for one batch) at step 60: 279.9007, Accuracy: 0.8071\n",
      "Training loss (for one batch) at step 70: 314.3946, Accuracy: 0.7965\n",
      "Training loss (for one batch) at step 80: 311.2642, Accuracy: 0.7806\n",
      "Training loss (for one batch) at step 90: 287.6599, Accuracy: 0.7768\n",
      "Training loss (for one batch) at step 100: 277.7695, Accuracy: 0.7811\n",
      "Training loss (for one batch) at step 110: 290.3466, Accuracy: 0.7819\n",
      "---- Training ----\n",
      "Training loss: 115.0783\n",
      "Training acc over epoch: 0.7805\n",
      "---- Validation ----\n",
      "Validation loss: 53.6375\n",
      "Validation acc: 0.7023\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 359.0773, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 322.3935, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 20: 282.3166, Accuracy: 0.7206\n",
      "Training loss (for one batch) at step 30: 265.9211, Accuracy: 0.7563\n",
      "Training loss (for one batch) at step 40: 261.0264, Accuracy: 0.7814\n",
      "Training loss (for one batch) at step 50: 268.8613, Accuracy: 0.7983\n",
      "Training loss (for one batch) at step 60: 289.4892, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 70: 316.4912, Accuracy: 0.7929\n",
      "Training loss (for one batch) at step 80: 324.9340, Accuracy: 0.7782\n",
      "Training loss (for one batch) at step 90: 279.2423, Accuracy: 0.7725\n",
      "Training loss (for one batch) at step 100: 265.3071, Accuracy: 0.7776\n",
      "Training loss (for one batch) at step 110: 297.2563, Accuracy: 0.7786\n",
      "---- Training ----\n",
      "Training loss: 88.7178\n",
      "Training acc over epoch: 0.7781\n",
      "---- Validation ----\n",
      "Validation loss: 36.9945\n",
      "Validation acc: 0.7012\n",
      "Time taken: 10.12s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 317.6462, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 326.7925, Accuracy: 0.6690\n",
      "Training loss (for one batch) at step 20: 283.2415, Accuracy: 0.7254\n",
      "Training loss (for one batch) at step 30: 293.9697, Accuracy: 0.7618\n",
      "Training loss (for one batch) at step 40: 272.0329, Accuracy: 0.7843\n",
      "Training loss (for one batch) at step 50: 272.7976, Accuracy: 0.7995\n",
      "Training loss (for one batch) at step 60: 296.9223, Accuracy: 0.8074\n",
      "Training loss (for one batch) at step 70: 323.6106, Accuracy: 0.7957\n",
      "Training loss (for one batch) at step 80: 342.4817, Accuracy: 0.7790\n",
      "Training loss (for one batch) at step 90: 295.6268, Accuracy: 0.7743\n",
      "Training loss (for one batch) at step 100: 273.1933, Accuracy: 0.7792\n",
      "Training loss (for one batch) at step 110: 307.4641, Accuracy: 0.7799\n",
      "---- Training ----\n",
      "Training loss: 93.7840\n",
      "Training acc over epoch: 0.7786\n",
      "---- Validation ----\n",
      "Validation loss: 40.3468\n",
      "Validation acc: 0.6991\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 325.3875, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 332.1793, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 20: 264.6837, Accuracy: 0.7158\n",
      "Training loss (for one batch) at step 30: 292.3320, Accuracy: 0.7545\n",
      "Training loss (for one batch) at step 40: 276.5134, Accuracy: 0.7832\n",
      "Training loss (for one batch) at step 50: 277.8756, Accuracy: 0.7964\n",
      "Training loss (for one batch) at step 60: 294.7186, Accuracy: 0.8044\n",
      "Training loss (for one batch) at step 70: 317.9373, Accuracy: 0.7916\n",
      "Training loss (for one batch) at step 80: 316.2039, Accuracy: 0.7768\n",
      "Training loss (for one batch) at step 90: 282.6895, Accuracy: 0.7737\n",
      "Training loss (for one batch) at step 100: 298.1930, Accuracy: 0.7785\n",
      "Training loss (for one batch) at step 110: 278.4693, Accuracy: 0.7804\n",
      "---- Training ----\n",
      "Training loss: 100.3523\n",
      "Training acc over epoch: 0.7789\n",
      "---- Validation ----\n",
      "Validation loss: 41.0788\n",
      "Validation acc: 0.7157\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 312.0364, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 323.1185, Accuracy: 0.6818\n",
      "Training loss (for one batch) at step 20: 297.7749, Accuracy: 0.7202\n",
      "Training loss (for one batch) at step 30: 276.0793, Accuracy: 0.7560\n",
      "Training loss (for one batch) at step 40: 252.5957, Accuracy: 0.7801\n",
      "Training loss (for one batch) at step 50: 267.6115, Accuracy: 0.7989\n",
      "Training loss (for one batch) at step 60: 291.9678, Accuracy: 0.8058\n",
      "Training loss (for one batch) at step 70: 329.9042, Accuracy: 0.7931\n",
      "Training loss (for one batch) at step 80: 328.7624, Accuracy: 0.7758\n",
      "Training loss (for one batch) at step 90: 278.6568, Accuracy: 0.7732\n",
      "Training loss (for one batch) at step 100: 284.1208, Accuracy: 0.7777\n",
      "Training loss (for one batch) at step 110: 268.5583, Accuracy: 0.7781\n",
      "---- Training ----\n",
      "Training loss: 86.7822\n",
      "Training acc over epoch: 0.7772\n",
      "---- Validation ----\n",
      "Validation loss: 51.4742\n",
      "Validation acc: 0.7088\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 320.8182, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 307.4749, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 20: 262.8220, Accuracy: 0.7214\n",
      "Training loss (for one batch) at step 30: 271.0931, Accuracy: 0.7583\n",
      "Training loss (for one batch) at step 40: 276.3638, Accuracy: 0.7839\n",
      "Training loss (for one batch) at step 50: 259.7399, Accuracy: 0.7989\n",
      "Training loss (for one batch) at step 60: 280.5660, Accuracy: 0.8051\n",
      "Training loss (for one batch) at step 70: 296.2633, Accuracy: 0.7908\n",
      "Training loss (for one batch) at step 80: 314.8066, Accuracy: 0.7767\n",
      "Training loss (for one batch) at step 90: 284.9463, Accuracy: 0.7752\n",
      "Training loss (for one batch) at step 100: 261.7066, Accuracy: 0.7792\n",
      "Training loss (for one batch) at step 110: 280.8388, Accuracy: 0.7801\n",
      "---- Training ----\n",
      "Training loss: 90.9448\n",
      "Training acc over epoch: 0.7788\n",
      "---- Validation ----\n",
      "Validation loss: 57.2691\n",
      "Validation acc: 0.7050\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 328.9011, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 309.9404, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 20: 280.1516, Accuracy: 0.7240\n",
      "Training loss (for one batch) at step 30: 271.7927, Accuracy: 0.7613\n",
      "Training loss (for one batch) at step 40: 263.6841, Accuracy: 0.7843\n",
      "Training loss (for one batch) at step 50: 268.1762, Accuracy: 0.7996\n",
      "Training loss (for one batch) at step 60: 288.3073, Accuracy: 0.8069\n",
      "Training loss (for one batch) at step 70: 314.9096, Accuracy: 0.7954\n",
      "Training loss (for one batch) at step 80: 310.8852, Accuracy: 0.7789\n",
      "Training loss (for one batch) at step 90: 291.5481, Accuracy: 0.7758\n",
      "Training loss (for one batch) at step 100: 260.9677, Accuracy: 0.7808\n",
      "Training loss (for one batch) at step 110: 289.6947, Accuracy: 0.7809\n",
      "---- Training ----\n",
      "Training loss: 88.8704\n",
      "Training acc over epoch: 0.7807\n",
      "---- Validation ----\n",
      "Validation loss: 52.6878\n",
      "Validation acc: 0.7066\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 330.7007, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 307.5172, Accuracy: 0.6591\n",
      "Training loss (for one batch) at step 20: 292.8388, Accuracy: 0.7106\n",
      "Training loss (for one batch) at step 30: 281.3053, Accuracy: 0.7492\n",
      "Training loss (for one batch) at step 40: 249.4588, Accuracy: 0.7761\n",
      "Training loss (for one batch) at step 50: 286.1291, Accuracy: 0.7932\n",
      "Training loss (for one batch) at step 60: 286.1355, Accuracy: 0.8034\n",
      "Training loss (for one batch) at step 70: 319.4688, Accuracy: 0.7904\n",
      "Training loss (for one batch) at step 80: 305.0248, Accuracy: 0.7783\n",
      "Training loss (for one batch) at step 90: 303.9206, Accuracy: 0.7741\n",
      "Training loss (for one batch) at step 100: 280.5249, Accuracy: 0.7789\n",
      "Training loss (for one batch) at step 110: 283.5741, Accuracy: 0.7793\n",
      "---- Training ----\n",
      "Training loss: 112.7974\n",
      "Training acc over epoch: 0.7771\n",
      "---- Validation ----\n",
      "Validation loss: 45.0491\n",
      "Validation acc: 0.7096\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 343.0388, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 290.9695, Accuracy: 0.6847\n",
      "Training loss (for one batch) at step 20: 268.4388, Accuracy: 0.7251\n",
      "Training loss (for one batch) at step 30: 270.9921, Accuracy: 0.7616\n",
      "Training loss (for one batch) at step 40: 259.4072, Accuracy: 0.7830\n",
      "Training loss (for one batch) at step 50: 261.7568, Accuracy: 0.7989\n",
      "Training loss (for one batch) at step 60: 282.1040, Accuracy: 0.8040\n",
      "Training loss (for one batch) at step 70: 288.7566, Accuracy: 0.7916\n",
      "Training loss (for one batch) at step 80: 290.7768, Accuracy: 0.7763\n",
      "Training loss (for one batch) at step 90: 263.3934, Accuracy: 0.7745\n",
      "Training loss (for one batch) at step 100: 292.7056, Accuracy: 0.7794\n",
      "Training loss (for one batch) at step 110: 287.2780, Accuracy: 0.7804\n",
      "---- Training ----\n",
      "Training loss: 98.5009\n",
      "Training acc over epoch: 0.7786\n",
      "---- Validation ----\n",
      "Validation loss: 59.5922\n",
      "Validation acc: 0.7053\n",
      "Time taken: 10.51s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 322.7448, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 316.8782, Accuracy: 0.6882\n",
      "Training loss (for one batch) at step 20: 286.1872, Accuracy: 0.7240\n",
      "Training loss (for one batch) at step 30: 277.6934, Accuracy: 0.7608\n",
      "Training loss (for one batch) at step 40: 275.7050, Accuracy: 0.7856\n",
      "Training loss (for one batch) at step 50: 259.1114, Accuracy: 0.8015\n",
      "Training loss (for one batch) at step 60: 285.7713, Accuracy: 0.8098\n",
      "Training loss (for one batch) at step 70: 302.5820, Accuracy: 0.7992\n",
      "Training loss (for one batch) at step 80: 322.2225, Accuracy: 0.7810\n",
      "Training loss (for one batch) at step 90: 285.8963, Accuracy: 0.7783\n",
      "Training loss (for one batch) at step 100: 260.5944, Accuracy: 0.7819\n",
      "Training loss (for one batch) at step 110: 296.0821, Accuracy: 0.7839\n",
      "---- Training ----\n",
      "Training loss: 98.4400\n",
      "Training acc over epoch: 0.7814\n",
      "---- Validation ----\n",
      "Validation loss: 63.7285\n",
      "Validation acc: 0.7090\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 318.8404, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 324.2404, Accuracy: 0.6705\n",
      "Training loss (for one batch) at step 20: 272.8454, Accuracy: 0.7173\n",
      "Training loss (for one batch) at step 30: 268.4353, Accuracy: 0.7596\n",
      "Training loss (for one batch) at step 40: 280.9064, Accuracy: 0.7833\n",
      "Training loss (for one batch) at step 50: 278.2037, Accuracy: 0.7999\n",
      "Training loss (for one batch) at step 60: 293.4392, Accuracy: 0.8065\n",
      "Training loss (for one batch) at step 70: 303.4164, Accuracy: 0.7923\n",
      "Training loss (for one batch) at step 80: 310.1465, Accuracy: 0.7758\n",
      "Training loss (for one batch) at step 90: 296.8314, Accuracy: 0.7721\n",
      "Training loss (for one batch) at step 100: 274.7652, Accuracy: 0.7774\n",
      "Training loss (for one batch) at step 110: 277.6557, Accuracy: 0.7791\n",
      "---- Training ----\n",
      "Training loss: 94.0346\n",
      "Training acc over epoch: 0.7773\n",
      "---- Validation ----\n",
      "Validation loss: 67.1860\n",
      "Validation acc: 0.7109\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 308.3555, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 292.5086, Accuracy: 0.6790\n",
      "Training loss (for one batch) at step 20: 274.6740, Accuracy: 0.7262\n",
      "Training loss (for one batch) at step 30: 266.9110, Accuracy: 0.7593\n",
      "Training loss (for one batch) at step 40: 258.1792, Accuracy: 0.7847\n",
      "Training loss (for one batch) at step 50: 294.8330, Accuracy: 0.7996\n",
      "Training loss (for one batch) at step 60: 281.4181, Accuracy: 0.8066\n",
      "Training loss (for one batch) at step 70: 285.2186, Accuracy: 0.7951\n",
      "Training loss (for one batch) at step 80: 301.7388, Accuracy: 0.7786\n",
      "Training loss (for one batch) at step 90: 264.8528, Accuracy: 0.7744\n",
      "Training loss (for one batch) at step 100: 257.4183, Accuracy: 0.7780\n",
      "Training loss (for one batch) at step 110: 308.3593, Accuracy: 0.7789\n",
      "---- Training ----\n",
      "Training loss: 92.0981\n",
      "Training acc over epoch: 0.7773\n",
      "---- Validation ----\n",
      "Validation loss: 50.1868\n",
      "Validation acc: 0.7061\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 330.9704, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 305.4757, Accuracy: 0.6896\n",
      "Training loss (for one batch) at step 20: 272.2694, Accuracy: 0.7225\n",
      "Training loss (for one batch) at step 30: 257.3016, Accuracy: 0.7576\n",
      "Training loss (for one batch) at step 40: 269.7526, Accuracy: 0.7856\n",
      "Training loss (for one batch) at step 50: 251.4713, Accuracy: 0.7992\n",
      "Training loss (for one batch) at step 60: 284.3163, Accuracy: 0.8060\n",
      "Training loss (for one batch) at step 70: 297.4152, Accuracy: 0.7965\n",
      "Training loss (for one batch) at step 80: 290.4946, Accuracy: 0.7798\n",
      "Training loss (for one batch) at step 90: 275.1843, Accuracy: 0.7756\n",
      "Training loss (for one batch) at step 100: 291.1057, Accuracy: 0.7797\n",
      "Training loss (for one batch) at step 110: 283.3358, Accuracy: 0.7786\n",
      "---- Training ----\n",
      "Training loss: 95.6144\n",
      "Training acc over epoch: 0.7779\n",
      "---- Validation ----\n",
      "Validation loss: 33.8748\n",
      "Validation acc: 0.6967\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 316.8030, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 277.1084, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 20: 273.8339, Accuracy: 0.7199\n",
      "Training loss (for one batch) at step 30: 267.2975, Accuracy: 0.7583\n",
      "Training loss (for one batch) at step 40: 255.4351, Accuracy: 0.7864\n",
      "Training loss (for one batch) at step 50: 259.8820, Accuracy: 0.8027\n",
      "Training loss (for one batch) at step 60: 297.4191, Accuracy: 0.8096\n",
      "Training loss (for one batch) at step 70: 296.3192, Accuracy: 0.7978\n",
      "Training loss (for one batch) at step 80: 283.7073, Accuracy: 0.7793\n",
      "Training loss (for one batch) at step 90: 292.8810, Accuracy: 0.7767\n",
      "Training loss (for one batch) at step 100: 274.6683, Accuracy: 0.7795\n",
      "Training loss (for one batch) at step 110: 294.7532, Accuracy: 0.7803\n",
      "---- Training ----\n",
      "Training loss: 96.5795\n",
      "Training acc over epoch: 0.7792\n",
      "---- Validation ----\n",
      "Validation loss: 43.5395\n",
      "Validation acc: 0.6999\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 304.5859, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 309.8898, Accuracy: 0.6776\n",
      "Training loss (for one batch) at step 20: 276.0461, Accuracy: 0.7240\n",
      "Training loss (for one batch) at step 30: 278.7917, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 40: 265.0476, Accuracy: 0.7851\n",
      "Training loss (for one batch) at step 50: 261.9264, Accuracy: 0.8012\n",
      "Training loss (for one batch) at step 60: 303.0671, Accuracy: 0.8062\n",
      "Training loss (for one batch) at step 70: 294.4182, Accuracy: 0.7957\n",
      "Training loss (for one batch) at step 80: 320.3120, Accuracy: 0.7799\n",
      "Training loss (for one batch) at step 90: 276.8090, Accuracy: 0.7752\n",
      "Training loss (for one batch) at step 100: 278.1428, Accuracy: 0.7788\n",
      "Training loss (for one batch) at step 110: 280.8575, Accuracy: 0.7800\n",
      "---- Training ----\n",
      "Training loss: 96.8726\n",
      "Training acc over epoch: 0.7774\n",
      "---- Validation ----\n",
      "Validation loss: 47.6803\n",
      "Validation acc: 0.7176\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 315.7255, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 300.9673, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 20: 279.9586, Accuracy: 0.7236\n",
      "Training loss (for one batch) at step 30: 261.1109, Accuracy: 0.7581\n",
      "Training loss (for one batch) at step 40: 286.2263, Accuracy: 0.7833\n",
      "Training loss (for one batch) at step 50: 252.4990, Accuracy: 0.8013\n",
      "Training loss (for one batch) at step 60: 287.0732, Accuracy: 0.8062\n",
      "Training loss (for one batch) at step 70: 291.9195, Accuracy: 0.7929\n",
      "Training loss (for one batch) at step 80: 305.2523, Accuracy: 0.7759\n",
      "Training loss (for one batch) at step 90: 278.9911, Accuracy: 0.7756\n",
      "Training loss (for one batch) at step 100: 272.5906, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 110: 281.7602, Accuracy: 0.7819\n",
      "---- Training ----\n",
      "Training loss: 103.1735\n",
      "Training acc over epoch: 0.7797\n",
      "---- Validation ----\n",
      "Validation loss: 57.0141\n",
      "Validation acc: 0.6972\n",
      "Time taken: 10.13s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 308.8421, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 306.1981, Accuracy: 0.6662\n",
      "Training loss (for one batch) at step 20: 296.1126, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 30: 261.5687, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 40: 264.9520, Accuracy: 0.7830\n",
      "Training loss (for one batch) at step 50: 253.8509, Accuracy: 0.7992\n",
      "Training loss (for one batch) at step 60: 280.9199, Accuracy: 0.8090\n",
      "Training loss (for one batch) at step 70: 284.0575, Accuracy: 0.7935\n",
      "Training loss (for one batch) at step 80: 297.8666, Accuracy: 0.7782\n",
      "Training loss (for one batch) at step 90: 265.7804, Accuracy: 0.7750\n",
      "Training loss (for one batch) at step 100: 275.8586, Accuracy: 0.7802\n",
      "Training loss (for one batch) at step 110: 296.8728, Accuracy: 0.7810\n",
      "---- Training ----\n",
      "Training loss: 84.1302\n",
      "Training acc over epoch: 0.7801\n",
      "---- Validation ----\n",
      "Validation loss: 50.4238\n",
      "Validation acc: 0.7088\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 325.6981, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 332.0941, Accuracy: 0.6754\n",
      "Training loss (for one batch) at step 20: 265.9923, Accuracy: 0.7217\n",
      "Training loss (for one batch) at step 30: 284.0523, Accuracy: 0.7538\n",
      "Training loss (for one batch) at step 40: 258.8260, Accuracy: 0.7803\n",
      "Training loss (for one batch) at step 50: 251.5041, Accuracy: 0.7993\n",
      "Training loss (for one batch) at step 60: 267.8789, Accuracy: 0.8043\n",
      "Training loss (for one batch) at step 70: 282.8337, Accuracy: 0.7927\n",
      "Training loss (for one batch) at step 80: 294.3617, Accuracy: 0.7773\n",
      "Training loss (for one batch) at step 90: 257.2667, Accuracy: 0.7735\n",
      "Training loss (for one batch) at step 100: 277.8827, Accuracy: 0.7776\n",
      "Training loss (for one batch) at step 110: 265.6989, Accuracy: 0.7788\n",
      "---- Training ----\n",
      "Training loss: 88.0002\n",
      "Training acc over epoch: 0.7775\n",
      "---- Validation ----\n",
      "Validation loss: 100.4631\n",
      "Validation acc: 0.7069\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 300.0789, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 319.9183, Accuracy: 0.6697\n",
      "Training loss (for one batch) at step 20: 274.2776, Accuracy: 0.7161\n",
      "Training loss (for one batch) at step 30: 283.8121, Accuracy: 0.7545\n",
      "Training loss (for one batch) at step 40: 254.4525, Accuracy: 0.7803\n",
      "Training loss (for one batch) at step 50: 260.7226, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 60: 273.4481, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 70: 312.3564, Accuracy: 0.7903\n",
      "Training loss (for one batch) at step 80: 286.6519, Accuracy: 0.7757\n",
      "Training loss (for one batch) at step 90: 269.1471, Accuracy: 0.7719\n",
      "Training loss (for one batch) at step 100: 267.7868, Accuracy: 0.7768\n",
      "Training loss (for one batch) at step 110: 296.8180, Accuracy: 0.7790\n",
      "---- Training ----\n",
      "Training loss: 99.3304\n",
      "Training acc over epoch: 0.7775\n",
      "---- Validation ----\n",
      "Validation loss: 44.5184\n",
      "Validation acc: 0.7047\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 345.4312, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 311.4486, Accuracy: 0.6697\n",
      "Training loss (for one batch) at step 20: 269.7627, Accuracy: 0.7232\n",
      "Training loss (for one batch) at step 30: 254.2325, Accuracy: 0.7581\n",
      "Training loss (for one batch) at step 40: 272.0236, Accuracy: 0.7839\n",
      "Training loss (for one batch) at step 50: 251.9706, Accuracy: 0.8009\n",
      "Training loss (for one batch) at step 60: 290.7592, Accuracy: 0.8081\n",
      "Training loss (for one batch) at step 70: 302.9670, Accuracy: 0.7946\n",
      "Training loss (for one batch) at step 80: 279.4432, Accuracy: 0.7781\n",
      "Training loss (for one batch) at step 90: 292.7122, Accuracy: 0.7737\n",
      "Training loss (for one batch) at step 100: 263.2110, Accuracy: 0.7787\n",
      "Training loss (for one batch) at step 110: 297.0027, Accuracy: 0.7786\n",
      "---- Training ----\n",
      "Training loss: 105.9226\n",
      "Training acc over epoch: 0.7779\n",
      "---- Validation ----\n",
      "Validation loss: 60.6821\n",
      "Validation acc: 0.7106\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 315.3639, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 312.1501, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 20: 281.1975, Accuracy: 0.7269\n",
      "Training loss (for one batch) at step 30: 250.0262, Accuracy: 0.7631\n",
      "Training loss (for one batch) at step 40: 264.9337, Accuracy: 0.7879\n",
      "Training loss (for one batch) at step 50: 273.6519, Accuracy: 0.8028\n",
      "Training loss (for one batch) at step 60: 271.1992, Accuracy: 0.8102\n",
      "Training loss (for one batch) at step 70: 300.5293, Accuracy: 0.7978\n",
      "Training loss (for one batch) at step 80: 301.2908, Accuracy: 0.7811\n",
      "Training loss (for one batch) at step 90: 283.4140, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 100: 284.6868, Accuracy: 0.7802\n",
      "Training loss (for one batch) at step 110: 285.9648, Accuracy: 0.7810\n",
      "---- Training ----\n",
      "Training loss: 86.3590\n",
      "Training acc over epoch: 0.7789\n",
      "---- Validation ----\n",
      "Validation loss: 27.6085\n",
      "Validation acc: 0.6975\n",
      "Time taken: 10.56s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 303.8009, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 296.7928, Accuracy: 0.6683\n",
      "Training loss (for one batch) at step 20: 275.2894, Accuracy: 0.7184\n",
      "Training loss (for one batch) at step 30: 278.4754, Accuracy: 0.7558\n",
      "Training loss (for one batch) at step 40: 258.2108, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 50: 269.0051, Accuracy: 0.7950\n",
      "Training loss (for one batch) at step 60: 264.3769, Accuracy: 0.8042\n",
      "Training loss (for one batch) at step 70: 296.2624, Accuracy: 0.7929\n",
      "Training loss (for one batch) at step 80: 309.4335, Accuracy: 0.7746\n",
      "Training loss (for one batch) at step 90: 278.5563, Accuracy: 0.7716\n",
      "Training loss (for one batch) at step 100: 264.0486, Accuracy: 0.7772\n",
      "Training loss (for one batch) at step 110: 276.8924, Accuracy: 0.7792\n",
      "---- Training ----\n",
      "Training loss: 84.2094\n",
      "Training acc over epoch: 0.7766\n",
      "---- Validation ----\n",
      "Validation loss: 56.8066\n",
      "Validation acc: 0.7098\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 320.8472, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 330.7485, Accuracy: 0.6811\n",
      "Training loss (for one batch) at step 20: 267.3943, Accuracy: 0.7269\n",
      "Training loss (for one batch) at step 30: 282.5130, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 40: 269.9123, Accuracy: 0.7853\n",
      "Training loss (for one batch) at step 50: 249.5893, Accuracy: 0.8038\n",
      "Training loss (for one batch) at step 60: 264.8512, Accuracy: 0.8105\n",
      "Training loss (for one batch) at step 70: 324.2114, Accuracy: 0.7951\n",
      "Training loss (for one batch) at step 80: 301.3081, Accuracy: 0.7805\n",
      "Training loss (for one batch) at step 90: 278.0191, Accuracy: 0.7781\n",
      "Training loss (for one batch) at step 100: 266.2384, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 110: 271.0694, Accuracy: 0.7817\n",
      "---- Training ----\n",
      "Training loss: 88.7188\n",
      "Training acc over epoch: 0.7800\n",
      "---- Validation ----\n",
      "Validation loss: 46.6723\n",
      "Validation acc: 0.7085\n",
      "Time taken: 10.23s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.7259\n",
      "Validation AUC: 0.7256\n",
      "Validation Balanced_ACC: 0.4530\n",
      "Validation MI: 0.1101\n",
      "Validation Normalized MI: 0.1595\n",
      "Validation Adjusted MI: 0.1595\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 629.0930, Accuracy: 0.4531\n",
      "Training loss (for one batch) at step 10: 593.8155, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 20: 548.7513, Accuracy: 0.5223\n",
      "Training loss (for one batch) at step 30: 522.7498, Accuracy: 0.5164\n",
      "Training loss (for one batch) at step 40: 504.2539, Accuracy: 0.5145\n",
      "Training loss (for one batch) at step 50: 486.3815, Accuracy: 0.5199\n",
      "Training loss (for one batch) at step 60: 491.9373, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 70: 467.2225, Accuracy: 0.5232\n",
      "Training loss (for one batch) at step 80: 474.7616, Accuracy: 0.5199\n",
      "Training loss (for one batch) at step 90: 464.2637, Accuracy: 0.5180\n",
      "Training loss (for one batch) at step 100: 478.4964, Accuracy: 0.5185\n",
      "Training loss (for one batch) at step 110: 461.6579, Accuracy: 0.5180\n",
      "---- Training ----\n",
      "Training loss: 148.6317\n",
      "Training acc over epoch: 0.5177\n",
      "---- Validation ----\n",
      "Validation loss: 35.0675\n",
      "Validation acc: 0.4866\n",
      "Time taken: 12.43s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 465.0520, Accuracy: 0.4766\n",
      "Training loss (for one batch) at step 10: 460.2374, Accuracy: 0.5270\n",
      "Training loss (for one batch) at step 20: 454.4692, Accuracy: 0.5279\n",
      "Training loss (for one batch) at step 30: 459.8855, Accuracy: 0.5318\n",
      "Training loss (for one batch) at step 40: 451.4705, Accuracy: 0.5271\n",
      "Training loss (for one batch) at step 50: 449.3209, Accuracy: 0.5253\n",
      "Training loss (for one batch) at step 60: 450.2392, Accuracy: 0.5259\n",
      "Training loss (for one batch) at step 70: 447.7588, Accuracy: 0.5284\n",
      "Training loss (for one batch) at step 80: 453.1617, Accuracy: 0.5285\n",
      "Training loss (for one batch) at step 90: 458.2364, Accuracy: 0.5285\n",
      "Training loss (for one batch) at step 100: 453.8937, Accuracy: 0.5287\n",
      "Training loss (for one batch) at step 110: 457.4832, Accuracy: 0.5278\n",
      "---- Training ----\n",
      "Training loss: 139.4594\n",
      "Training acc over epoch: 0.5302\n",
      "---- Validation ----\n",
      "Validation loss: 34.2739\n",
      "Validation acc: 0.5134\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 448.6634, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 449.9237, Accuracy: 0.5412\n",
      "Training loss (for one batch) at step 20: 445.1620, Accuracy: 0.5253\n",
      "Training loss (for one batch) at step 30: 446.5793, Accuracy: 0.5295\n",
      "Training loss (for one batch) at step 40: 446.5093, Accuracy: 0.5316\n",
      "Training loss (for one batch) at step 50: 445.1844, Accuracy: 0.5354\n",
      "Training loss (for one batch) at step 60: 444.3487, Accuracy: 0.5398\n",
      "Training loss (for one batch) at step 70: 445.6506, Accuracy: 0.5428\n",
      "Training loss (for one batch) at step 80: 446.0038, Accuracy: 0.5441\n",
      "Training loss (for one batch) at step 90: 447.8054, Accuracy: 0.5427\n",
      "Training loss (for one batch) at step 100: 449.2628, Accuracy: 0.5428\n",
      "Training loss (for one batch) at step 110: 444.2419, Accuracy: 0.5426\n",
      "---- Training ----\n",
      "Training loss: 137.9390\n",
      "Training acc over epoch: 0.5431\n",
      "---- Validation ----\n",
      "Validation loss: 34.6171\n",
      "Validation acc: 0.5038\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 446.2219, Accuracy: 0.4453\n",
      "Training loss (for one batch) at step 10: 445.1117, Accuracy: 0.5199\n",
      "Training loss (for one batch) at step 20: 443.5581, Accuracy: 0.5454\n",
      "Training loss (for one batch) at step 30: 442.3009, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 40: 446.2754, Accuracy: 0.5553\n",
      "Training loss (for one batch) at step 50: 441.5685, Accuracy: 0.5636\n",
      "Training loss (for one batch) at step 60: 442.6210, Accuracy: 0.5644\n",
      "Training loss (for one batch) at step 70: 442.4260, Accuracy: 0.5691\n",
      "Training loss (for one batch) at step 80: 445.8820, Accuracy: 0.5698\n",
      "Training loss (for one batch) at step 90: 443.0528, Accuracy: 0.5717\n",
      "Training loss (for one batch) at step 100: 444.6546, Accuracy: 0.5711\n",
      "Training loss (for one batch) at step 110: 443.4136, Accuracy: 0.5712\n",
      "---- Training ----\n",
      "Training loss: 139.6490\n",
      "Training acc over epoch: 0.5718\n",
      "---- Validation ----\n",
      "Validation loss: 34.7964\n",
      "Validation acc: 0.5602\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 443.4165, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 443.3966, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 20: 441.6308, Accuracy: 0.5763\n",
      "Training loss (for one batch) at step 30: 442.6816, Accuracy: 0.5791\n",
      "Training loss (for one batch) at step 40: 439.1763, Accuracy: 0.5821\n",
      "Training loss (for one batch) at step 50: 436.1879, Accuracy: 0.5853\n",
      "Training loss (for one batch) at step 60: 444.6515, Accuracy: 0.5822\n",
      "Training loss (for one batch) at step 70: 447.8951, Accuracy: 0.5847\n",
      "Training loss (for one batch) at step 80: 445.2191, Accuracy: 0.5833\n",
      "Training loss (for one batch) at step 90: 442.0749, Accuracy: 0.5843\n",
      "Training loss (for one batch) at step 100: 444.7830, Accuracy: 0.5850\n",
      "Training loss (for one batch) at step 110: 443.7030, Accuracy: 0.5865\n",
      "---- Training ----\n",
      "Training loss: 138.9082\n",
      "Training acc over epoch: 0.5870\n",
      "---- Validation ----\n",
      "Validation loss: 34.5078\n",
      "Validation acc: 0.6042\n",
      "Time taken: 10.69s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 442.3443, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 445.8316, Accuracy: 0.5980\n",
      "Training loss (for one batch) at step 20: 442.7048, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 30: 438.6463, Accuracy: 0.5887\n",
      "Training loss (for one batch) at step 40: 441.5037, Accuracy: 0.5957\n",
      "Training loss (for one batch) at step 50: 440.1659, Accuracy: 0.5991\n",
      "Training loss (for one batch) at step 60: 442.3309, Accuracy: 0.6046\n",
      "Training loss (for one batch) at step 70: 443.9649, Accuracy: 0.6081\n",
      "Training loss (for one batch) at step 80: 442.9579, Accuracy: 0.6055\n",
      "Training loss (for one batch) at step 90: 441.6566, Accuracy: 0.6074\n",
      "Training loss (for one batch) at step 100: 440.0798, Accuracy: 0.6069\n",
      "Training loss (for one batch) at step 110: 442.4187, Accuracy: 0.6078\n",
      "---- Training ----\n",
      "Training loss: 137.7317\n",
      "Training acc over epoch: 0.6094\n",
      "---- Validation ----\n",
      "Validation loss: 34.3259\n",
      "Validation acc: 0.6368\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 440.9910, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 442.0013, Accuracy: 0.6058\n",
      "Training loss (for one batch) at step 20: 442.6805, Accuracy: 0.6004\n",
      "Training loss (for one batch) at step 30: 440.3908, Accuracy: 0.6109\n",
      "Training loss (for one batch) at step 40: 439.6467, Accuracy: 0.6054\n",
      "Training loss (for one batch) at step 50: 432.8356, Accuracy: 0.6124\n",
      "Training loss (for one batch) at step 60: 441.7168, Accuracy: 0.6192\n",
      "Training loss (for one batch) at step 70: 443.7360, Accuracy: 0.6264\n",
      "Training loss (for one batch) at step 80: 444.8798, Accuracy: 0.6206\n",
      "Training loss (for one batch) at step 90: 437.8934, Accuracy: 0.6179\n",
      "Training loss (for one batch) at step 100: 439.9554, Accuracy: 0.6183\n",
      "Training loss (for one batch) at step 110: 440.0150, Accuracy: 0.6189\n",
      "---- Training ----\n",
      "Training loss: 138.3438\n",
      "Training acc over epoch: 0.6186\n",
      "---- Validation ----\n",
      "Validation loss: 34.5725\n",
      "Validation acc: 0.6421\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 442.7897, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 443.0286, Accuracy: 0.6122\n",
      "Training loss (for one batch) at step 20: 437.6554, Accuracy: 0.6161\n",
      "Training loss (for one batch) at step 30: 437.3428, Accuracy: 0.6149\n",
      "Training loss (for one batch) at step 40: 439.0724, Accuracy: 0.6151\n",
      "Training loss (for one batch) at step 50: 433.7483, Accuracy: 0.6239\n",
      "Training loss (for one batch) at step 60: 440.7818, Accuracy: 0.6286\n",
      "Training loss (for one batch) at step 70: 447.3914, Accuracy: 0.6337\n",
      "Training loss (for one batch) at step 80: 440.2251, Accuracy: 0.6302\n",
      "Training loss (for one batch) at step 90: 443.8228, Accuracy: 0.6270\n",
      "Training loss (for one batch) at step 100: 438.0897, Accuracy: 0.6273\n",
      "Training loss (for one batch) at step 110: 445.7960, Accuracy: 0.6294\n",
      "---- Training ----\n",
      "Training loss: 135.7361\n",
      "Training acc over epoch: 0.6308\n",
      "---- Validation ----\n",
      "Validation loss: 33.6393\n",
      "Validation acc: 0.6561\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 442.7598, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 443.1966, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 20: 440.1430, Accuracy: 0.6380\n",
      "Training loss (for one batch) at step 30: 436.5897, Accuracy: 0.6442\n",
      "Training loss (for one batch) at step 40: 430.2614, Accuracy: 0.6511\n",
      "Training loss (for one batch) at step 50: 433.9238, Accuracy: 0.6509\n",
      "Training loss (for one batch) at step 60: 433.2263, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 70: 441.1698, Accuracy: 0.6581\n",
      "Training loss (for one batch) at step 80: 441.9590, Accuracy: 0.6545\n",
      "Training loss (for one batch) at step 90: 439.6934, Accuracy: 0.6525\n",
      "Training loss (for one batch) at step 100: 434.5380, Accuracy: 0.6539\n",
      "Training loss (for one batch) at step 110: 437.8456, Accuracy: 0.6534\n",
      "---- Training ----\n",
      "Training loss: 139.0878\n",
      "Training acc over epoch: 0.6538\n",
      "---- Validation ----\n",
      "Validation loss: 34.3770\n",
      "Validation acc: 0.6499\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 442.1662, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 441.8563, Accuracy: 0.6534\n",
      "Training loss (for one batch) at step 20: 435.4228, Accuracy: 0.6496\n",
      "Training loss (for one batch) at step 30: 434.7343, Accuracy: 0.6517\n",
      "Training loss (for one batch) at step 40: 437.0898, Accuracy: 0.6612\n",
      "Training loss (for one batch) at step 50: 431.7278, Accuracy: 0.6661\n",
      "Training loss (for one batch) at step 60: 434.0267, Accuracy: 0.6739\n",
      "Training loss (for one batch) at step 70: 447.4751, Accuracy: 0.6762\n",
      "Training loss (for one batch) at step 80: 443.8210, Accuracy: 0.6721\n",
      "Training loss (for one batch) at step 90: 439.3525, Accuracy: 0.6705\n",
      "Training loss (for one batch) at step 100: 433.9619, Accuracy: 0.6701\n",
      "Training loss (for one batch) at step 110: 439.2344, Accuracy: 0.6724\n",
      "---- Training ----\n",
      "Training loss: 134.9461\n",
      "Training acc over epoch: 0.6724\n",
      "---- Validation ----\n",
      "Validation loss: 33.9148\n",
      "Validation acc: 0.6625\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 443.6086, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 443.1956, Accuracy: 0.6520\n",
      "Training loss (for one batch) at step 20: 442.5491, Accuracy: 0.6514\n",
      "Training loss (for one batch) at step 30: 438.3370, Accuracy: 0.6532\n",
      "Training loss (for one batch) at step 40: 433.7854, Accuracy: 0.6667\n",
      "Training loss (for one batch) at step 50: 427.3583, Accuracy: 0.6752\n",
      "Training loss (for one batch) at step 60: 434.7349, Accuracy: 0.6796\n",
      "Training loss (for one batch) at step 70: 444.9385, Accuracy: 0.6832\n",
      "Training loss (for one batch) at step 80: 443.1418, Accuracy: 0.6783\n",
      "Training loss (for one batch) at step 90: 441.7040, Accuracy: 0.6753\n",
      "Training loss (for one batch) at step 100: 443.9366, Accuracy: 0.6747\n",
      "Training loss (for one batch) at step 110: 435.6956, Accuracy: 0.6781\n",
      "---- Training ----\n",
      "Training loss: 136.4184\n",
      "Training acc over epoch: 0.6793\n",
      "---- Validation ----\n",
      "Validation loss: 34.2309\n",
      "Validation acc: 0.6996\n",
      "Time taken: 10.56s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 450.7537, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 440.1937, Accuracy: 0.6761\n",
      "Training loss (for one batch) at step 20: 437.6853, Accuracy: 0.6652\n",
      "Training loss (for one batch) at step 30: 425.5624, Accuracy: 0.6696\n",
      "Training loss (for one batch) at step 40: 429.9008, Accuracy: 0.6728\n",
      "Training loss (for one batch) at step 50: 421.4374, Accuracy: 0.6828\n",
      "Training loss (for one batch) at step 60: 444.9606, Accuracy: 0.6878\n",
      "Training loss (for one batch) at step 70: 439.5040, Accuracy: 0.6887\n",
      "Training loss (for one batch) at step 80: 439.2416, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 90: 431.0599, Accuracy: 0.6858\n",
      "Training loss (for one batch) at step 100: 430.1057, Accuracy: 0.6846\n",
      "Training loss (for one batch) at step 110: 435.3887, Accuracy: 0.6858\n",
      "---- Training ----\n",
      "Training loss: 135.3587\n",
      "Training acc over epoch: 0.6875\n",
      "---- Validation ----\n",
      "Validation loss: 35.4289\n",
      "Validation acc: 0.6918\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 440.1874, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 439.3262, Accuracy: 0.7038\n",
      "Training loss (for one batch) at step 20: 441.2194, Accuracy: 0.6856\n",
      "Training loss (for one batch) at step 30: 436.0043, Accuracy: 0.6888\n",
      "Training loss (for one batch) at step 40: 429.4899, Accuracy: 0.6978\n",
      "Training loss (for one batch) at step 50: 419.2722, Accuracy: 0.7057\n",
      "Training loss (for one batch) at step 60: 438.9731, Accuracy: 0.7118\n",
      "Training loss (for one batch) at step 70: 435.2473, Accuracy: 0.7093\n",
      "Training loss (for one batch) at step 80: 437.6366, Accuracy: 0.7050\n",
      "Training loss (for one batch) at step 90: 428.4803, Accuracy: 0.7045\n",
      "Training loss (for one batch) at step 100: 429.3609, Accuracy: 0.7046\n",
      "Training loss (for one batch) at step 110: 431.3797, Accuracy: 0.7047\n",
      "---- Training ----\n",
      "Training loss: 135.9239\n",
      "Training acc over epoch: 0.7063\n",
      "---- Validation ----\n",
      "Validation loss: 34.5598\n",
      "Validation acc: 0.7131\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 441.1386, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 435.6737, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 20: 431.1542, Accuracy: 0.7154\n",
      "Training loss (for one batch) at step 30: 427.6912, Accuracy: 0.7107\n",
      "Training loss (for one batch) at step 40: 415.8351, Accuracy: 0.7184\n",
      "Training loss (for one batch) at step 50: 411.1779, Accuracy: 0.7275\n",
      "Training loss (for one batch) at step 60: 422.4758, Accuracy: 0.7291\n",
      "Training loss (for one batch) at step 70: 433.8667, Accuracy: 0.7317\n",
      "Training loss (for one batch) at step 80: 443.7064, Accuracy: 0.7288\n",
      "Training loss (for one batch) at step 90: 432.5148, Accuracy: 0.7260\n",
      "Training loss (for one batch) at step 100: 423.0186, Accuracy: 0.7259\n",
      "Training loss (for one batch) at step 110: 441.1724, Accuracy: 0.7262\n",
      "---- Training ----\n",
      "Training loss: 132.1348\n",
      "Training acc over epoch: 0.7265\n",
      "---- Validation ----\n",
      "Validation loss: 34.8402\n",
      "Validation acc: 0.6805\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 444.1240, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 436.5464, Accuracy: 0.7337\n",
      "Training loss (for one batch) at step 20: 433.5191, Accuracy: 0.7035\n",
      "Training loss (for one batch) at step 30: 426.2878, Accuracy: 0.7130\n",
      "Training loss (for one batch) at step 40: 418.9245, Accuracy: 0.7250\n",
      "Training loss (for one batch) at step 50: 412.2616, Accuracy: 0.7376\n",
      "Training loss (for one batch) at step 60: 438.4958, Accuracy: 0.7444\n",
      "Training loss (for one batch) at step 70: 444.7147, Accuracy: 0.7465\n",
      "Training loss (for one batch) at step 80: 435.4478, Accuracy: 0.7387\n",
      "Training loss (for one batch) at step 90: 435.5448, Accuracy: 0.7345\n",
      "Training loss (for one batch) at step 100: 428.5214, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 110: 429.8194, Accuracy: 0.7369\n",
      "---- Training ----\n",
      "Training loss: 132.8114\n",
      "Training acc over epoch: 0.7380\n",
      "---- Validation ----\n",
      "Validation loss: 36.0939\n",
      "Validation acc: 0.7555\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 444.1855, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 430.3524, Accuracy: 0.7379\n",
      "Training loss (for one batch) at step 20: 428.8106, Accuracy: 0.7228\n",
      "Training loss (for one batch) at step 30: 417.3427, Accuracy: 0.7288\n",
      "Training loss (for one batch) at step 40: 419.7840, Accuracy: 0.7428\n",
      "Training loss (for one batch) at step 50: 421.8924, Accuracy: 0.7531\n",
      "Training loss (for one batch) at step 60: 436.7064, Accuracy: 0.7573\n",
      "Training loss (for one batch) at step 70: 435.7255, Accuracy: 0.7558\n",
      "Training loss (for one batch) at step 80: 439.5874, Accuracy: 0.7525\n",
      "Training loss (for one batch) at step 90: 420.7862, Accuracy: 0.7480\n",
      "Training loss (for one batch) at step 100: 418.9057, Accuracy: 0.7487\n",
      "Training loss (for one batch) at step 110: 425.5767, Accuracy: 0.7510\n",
      "---- Training ----\n",
      "Training loss: 125.1937\n",
      "Training acc over epoch: 0.7503\n",
      "---- Validation ----\n",
      "Validation loss: 39.3158\n",
      "Validation acc: 0.7319\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 453.5369, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 433.0654, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 20: 435.1207, Accuracy: 0.7478\n",
      "Training loss (for one batch) at step 30: 428.2110, Accuracy: 0.7447\n",
      "Training loss (for one batch) at step 40: 412.0541, Accuracy: 0.7487\n",
      "Training loss (for one batch) at step 50: 413.6382, Accuracy: 0.7603\n",
      "Training loss (for one batch) at step 60: 425.5927, Accuracy: 0.7690\n",
      "Training loss (for one batch) at step 70: 438.1342, Accuracy: 0.7718\n",
      "Training loss (for one batch) at step 80: 437.3973, Accuracy: 0.7661\n",
      "Training loss (for one batch) at step 90: 431.5387, Accuracy: 0.7620\n",
      "Training loss (for one batch) at step 100: 429.4427, Accuracy: 0.7600\n",
      "Training loss (for one batch) at step 110: 428.7170, Accuracy: 0.7613\n",
      "---- Training ----\n",
      "Training loss: 135.7439\n",
      "Training acc over epoch: 0.7618\n",
      "---- Validation ----\n",
      "Validation loss: 34.1761\n",
      "Validation acc: 0.7217\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 438.7935, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 433.8022, Accuracy: 0.7628\n",
      "Training loss (for one batch) at step 20: 425.5181, Accuracy: 0.7504\n",
      "Training loss (for one batch) at step 30: 419.2727, Accuracy: 0.7533\n",
      "Training loss (for one batch) at step 40: 406.7767, Accuracy: 0.7647\n",
      "Training loss (for one batch) at step 50: 392.9424, Accuracy: 0.7751\n",
      "Training loss (for one batch) at step 60: 434.7661, Accuracy: 0.7819\n",
      "Training loss (for one batch) at step 70: 435.1890, Accuracy: 0.7822\n",
      "Training loss (for one batch) at step 80: 426.1613, Accuracy: 0.7741\n",
      "Training loss (for one batch) at step 90: 429.3940, Accuracy: 0.7710\n",
      "Training loss (for one batch) at step 100: 417.2466, Accuracy: 0.7712\n",
      "Training loss (for one batch) at step 110: 418.9797, Accuracy: 0.7721\n",
      "---- Training ----\n",
      "Training loss: 128.5810\n",
      "Training acc over epoch: 0.7718\n",
      "---- Validation ----\n",
      "Validation loss: 35.7647\n",
      "Validation acc: 0.7563\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 437.4753, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 434.6608, Accuracy: 0.7727\n",
      "Training loss (for one batch) at step 20: 426.7060, Accuracy: 0.7712\n",
      "Training loss (for one batch) at step 30: 413.1904, Accuracy: 0.7729\n",
      "Training loss (for one batch) at step 40: 412.2447, Accuracy: 0.7820\n",
      "Training loss (for one batch) at step 50: 391.5570, Accuracy: 0.7929\n",
      "Training loss (for one batch) at step 60: 419.6282, Accuracy: 0.7984\n",
      "Training loss (for one batch) at step 70: 436.1507, Accuracy: 0.7985\n",
      "Training loss (for one batch) at step 80: 434.8660, Accuracy: 0.7892\n",
      "Training loss (for one batch) at step 90: 418.4752, Accuracy: 0.7839\n",
      "Training loss (for one batch) at step 100: 411.7469, Accuracy: 0.7840\n",
      "Training loss (for one batch) at step 110: 428.2245, Accuracy: 0.7856\n",
      "---- Training ----\n",
      "Training loss: 137.6389\n",
      "Training acc over epoch: 0.7847\n",
      "---- Validation ----\n",
      "Validation loss: 35.7399\n",
      "Validation acc: 0.7689\n",
      "Time taken: 10.14s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 439.1966, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 425.2950, Accuracy: 0.7862\n",
      "Training loss (for one batch) at step 20: 424.0279, Accuracy: 0.7675\n",
      "Training loss (for one batch) at step 30: 408.1975, Accuracy: 0.7739\n",
      "Training loss (for one batch) at step 40: 410.8205, Accuracy: 0.7820\n",
      "Training loss (for one batch) at step 50: 382.7869, Accuracy: 0.7898\n",
      "Training loss (for one batch) at step 60: 417.5717, Accuracy: 0.7974\n",
      "Training loss (for one batch) at step 70: 417.5464, Accuracy: 0.8003\n",
      "Training loss (for one batch) at step 80: 430.7880, Accuracy: 0.7939\n",
      "Training loss (for one batch) at step 90: 421.9644, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 100: 412.6755, Accuracy: 0.7901\n",
      "Training loss (for one batch) at step 110: 410.3754, Accuracy: 0.7913\n",
      "---- Training ----\n",
      "Training loss: 133.1541\n",
      "Training acc over epoch: 0.7908\n",
      "---- Validation ----\n",
      "Validation loss: 34.1971\n",
      "Validation acc: 0.7555\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 442.7833, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 10: 429.3001, Accuracy: 0.7848\n",
      "Training loss (for one batch) at step 20: 420.9571, Accuracy: 0.7764\n",
      "Training loss (for one batch) at step 30: 413.0024, Accuracy: 0.7860\n",
      "Training loss (for one batch) at step 40: 391.0966, Accuracy: 0.7961\n",
      "Training loss (for one batch) at step 50: 379.9874, Accuracy: 0.8073\n",
      "Training loss (for one batch) at step 60: 414.9443, Accuracy: 0.8093\n",
      "Training loss (for one batch) at step 70: 433.4053, Accuracy: 0.8094\n",
      "Training loss (for one batch) at step 80: 428.4781, Accuracy: 0.8053\n",
      "Training loss (for one batch) at step 90: 411.0992, Accuracy: 0.7996\n",
      "Training loss (for one batch) at step 100: 404.6089, Accuracy: 0.7983\n",
      "Training loss (for one batch) at step 110: 414.3857, Accuracy: 0.7988\n",
      "---- Training ----\n",
      "Training loss: 134.3922\n",
      "Training acc over epoch: 0.7978\n",
      "---- Validation ----\n",
      "Validation loss: 40.0649\n",
      "Validation acc: 0.7504\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 434.4288, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 423.6665, Accuracy: 0.7784\n",
      "Training loss (for one batch) at step 20: 411.5162, Accuracy: 0.7820\n",
      "Training loss (for one batch) at step 30: 405.2806, Accuracy: 0.7916\n",
      "Training loss (for one batch) at step 40: 396.3540, Accuracy: 0.8026\n",
      "Training loss (for one batch) at step 50: 386.8889, Accuracy: 0.8097\n",
      "Training loss (for one batch) at step 60: 409.5045, Accuracy: 0.8169\n",
      "Training loss (for one batch) at step 70: 449.6658, Accuracy: 0.8144\n",
      "Training loss (for one batch) at step 80: 418.7667, Accuracy: 0.8107\n",
      "Training loss (for one batch) at step 90: 409.8595, Accuracy: 0.8080\n",
      "Training loss (for one batch) at step 100: 403.1064, Accuracy: 0.8061\n",
      "Training loss (for one batch) at step 110: 406.9872, Accuracy: 0.8060\n",
      "---- Training ----\n",
      "Training loss: 121.9243\n",
      "Training acc over epoch: 0.8060\n",
      "---- Validation ----\n",
      "Validation loss: 34.4245\n",
      "Validation acc: 0.7544\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 437.8192, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 436.2916, Accuracy: 0.7756\n",
      "Training loss (for one batch) at step 20: 408.9595, Accuracy: 0.7824\n",
      "Training loss (for one batch) at step 30: 392.6035, Accuracy: 0.7956\n",
      "Training loss (for one batch) at step 40: 382.5813, Accuracy: 0.8072\n",
      "Training loss (for one batch) at step 50: 368.1565, Accuracy: 0.8165\n",
      "Training loss (for one batch) at step 60: 406.7204, Accuracy: 0.8217\n",
      "Training loss (for one batch) at step 70: 412.0635, Accuracy: 0.8184\n",
      "Training loss (for one batch) at step 80: 418.5594, Accuracy: 0.8110\n",
      "Training loss (for one batch) at step 90: 414.9027, Accuracy: 0.8059\n",
      "Training loss (for one batch) at step 100: 388.1180, Accuracy: 0.8038\n",
      "Training loss (for one batch) at step 110: 407.9369, Accuracy: 0.8041\n",
      "---- Training ----\n",
      "Training loss: 123.5187\n",
      "Training acc over epoch: 0.8047\n",
      "---- Validation ----\n",
      "Validation loss: 42.2497\n",
      "Validation acc: 0.7652\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 433.9672, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 426.1015, Accuracy: 0.7962\n",
      "Training loss (for one batch) at step 20: 402.9151, Accuracy: 0.7924\n",
      "Training loss (for one batch) at step 30: 404.7761, Accuracy: 0.7961\n",
      "Training loss (for one batch) at step 40: 390.6015, Accuracy: 0.8102\n",
      "Training loss (for one batch) at step 50: 373.4621, Accuracy: 0.8195\n",
      "Training loss (for one batch) at step 60: 398.8325, Accuracy: 0.8252\n",
      "Training loss (for one batch) at step 70: 429.8329, Accuracy: 0.8221\n",
      "Training loss (for one batch) at step 80: 420.3940, Accuracy: 0.8162\n",
      "Training loss (for one batch) at step 90: 389.1276, Accuracy: 0.8128\n",
      "Training loss (for one batch) at step 100: 400.1552, Accuracy: 0.8119\n",
      "Training loss (for one batch) at step 110: 390.2860, Accuracy: 0.8145\n",
      "---- Training ----\n",
      "Training loss: 125.7929\n",
      "Training acc over epoch: 0.8144\n",
      "---- Validation ----\n",
      "Validation loss: 36.5098\n",
      "Validation acc: 0.7783\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 416.3833, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 413.9932, Accuracy: 0.7727\n",
      "Training loss (for one batch) at step 20: 391.0508, Accuracy: 0.7809\n",
      "Training loss (for one batch) at step 30: 385.4478, Accuracy: 0.7959\n",
      "Training loss (for one batch) at step 40: 372.2494, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 50: 370.7588, Accuracy: 0.8191\n",
      "Training loss (for one batch) at step 60: 390.6555, Accuracy: 0.8251\n",
      "Training loss (for one batch) at step 70: 411.8155, Accuracy: 0.8220\n",
      "Training loss (for one batch) at step 80: 413.8483, Accuracy: 0.8139\n",
      "Training loss (for one batch) at step 90: 393.3067, Accuracy: 0.8079\n",
      "Training loss (for one batch) at step 100: 399.0188, Accuracy: 0.8066\n",
      "Training loss (for one batch) at step 110: 398.0657, Accuracy: 0.8077\n",
      "---- Training ----\n",
      "Training loss: 124.1050\n",
      "Training acc over epoch: 0.8077\n",
      "---- Validation ----\n",
      "Validation loss: 34.3192\n",
      "Validation acc: 0.7711\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 420.3010, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 415.5869, Accuracy: 0.7798\n",
      "Training loss (for one batch) at step 20: 395.0918, Accuracy: 0.7879\n",
      "Training loss (for one batch) at step 30: 390.9545, Accuracy: 0.7999\n",
      "Training loss (for one batch) at step 40: 372.7943, Accuracy: 0.8115\n",
      "Training loss (for one batch) at step 50: 357.3597, Accuracy: 0.8237\n",
      "Training loss (for one batch) at step 60: 384.5931, Accuracy: 0.8288\n",
      "Training loss (for one batch) at step 70: 417.1443, Accuracy: 0.8234\n",
      "Training loss (for one batch) at step 80: 422.8838, Accuracy: 0.8149\n",
      "Training loss (for one batch) at step 90: 409.3764, Accuracy: 0.8098\n",
      "Training loss (for one batch) at step 100: 386.4855, Accuracy: 0.8099\n",
      "Training loss (for one batch) at step 110: 401.2593, Accuracy: 0.8103\n",
      "---- Training ----\n",
      "Training loss: 126.2973\n",
      "Training acc over epoch: 0.8096\n",
      "---- Validation ----\n",
      "Validation loss: 38.6947\n",
      "Validation acc: 0.7692\n",
      "Time taken: 10.60s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 412.9544, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 409.8723, Accuracy: 0.7834\n",
      "Training loss (for one batch) at step 20: 405.3466, Accuracy: 0.7861\n",
      "Training loss (for one batch) at step 30: 384.1584, Accuracy: 0.8037\n",
      "Training loss (for one batch) at step 40: 373.5709, Accuracy: 0.8142\n",
      "Training loss (for one batch) at step 50: 353.0088, Accuracy: 0.8254\n",
      "Training loss (for one batch) at step 60: 373.2528, Accuracy: 0.8318\n",
      "Training loss (for one batch) at step 70: 410.7011, Accuracy: 0.8279\n",
      "Training loss (for one batch) at step 80: 415.3431, Accuracy: 0.8194\n",
      "Training loss (for one batch) at step 90: 397.5240, Accuracy: 0.8144\n",
      "Training loss (for one batch) at step 100: 399.2133, Accuracy: 0.8128\n",
      "Training loss (for one batch) at step 110: 384.9522, Accuracy: 0.8130\n",
      "---- Training ----\n",
      "Training loss: 128.3881\n",
      "Training acc over epoch: 0.8122\n",
      "---- Validation ----\n",
      "Validation loss: 39.0139\n",
      "Validation acc: 0.7740\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 421.3476, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 413.3294, Accuracy: 0.7713\n",
      "Training loss (for one batch) at step 20: 373.4298, Accuracy: 0.7723\n",
      "Training loss (for one batch) at step 30: 376.4818, Accuracy: 0.7906\n",
      "Training loss (for one batch) at step 40: 359.9554, Accuracy: 0.8058\n",
      "Training loss (for one batch) at step 50: 351.0731, Accuracy: 0.8163\n",
      "Training loss (for one batch) at step 60: 377.7136, Accuracy: 0.8233\n",
      "Training loss (for one batch) at step 70: 412.4116, Accuracy: 0.8193\n",
      "Training loss (for one batch) at step 80: 418.9043, Accuracy: 0.8119\n",
      "Training loss (for one batch) at step 90: 384.9239, Accuracy: 0.8089\n",
      "Training loss (for one batch) at step 100: 389.7138, Accuracy: 0.8108\n",
      "Training loss (for one batch) at step 110: 381.8746, Accuracy: 0.8129\n",
      "---- Training ----\n",
      "Training loss: 125.2437\n",
      "Training acc over epoch: 0.8129\n",
      "---- Validation ----\n",
      "Validation loss: 32.2390\n",
      "Validation acc: 0.7695\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 402.5116, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 419.6042, Accuracy: 0.7607\n",
      "Training loss (for one batch) at step 20: 377.5179, Accuracy: 0.7846\n",
      "Training loss (for one batch) at step 30: 383.0119, Accuracy: 0.8065\n",
      "Training loss (for one batch) at step 40: 361.5356, Accuracy: 0.8136\n",
      "Training loss (for one batch) at step 50: 344.5487, Accuracy: 0.8254\n",
      "Training loss (for one batch) at step 60: 388.0897, Accuracy: 0.8315\n",
      "Training loss (for one batch) at step 70: 408.9416, Accuracy: 0.8264\n",
      "Training loss (for one batch) at step 80: 396.5567, Accuracy: 0.8180\n",
      "Training loss (for one batch) at step 90: 376.4083, Accuracy: 0.8146\n",
      "Training loss (for one batch) at step 100: 354.0775, Accuracy: 0.8170\n",
      "Training loss (for one batch) at step 110: 388.3345, Accuracy: 0.8169\n",
      "---- Training ----\n",
      "Training loss: 120.4241\n",
      "Training acc over epoch: 0.8165\n",
      "---- Validation ----\n",
      "Validation loss: 36.5519\n",
      "Validation acc: 0.7611\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 397.6115, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 393.4565, Accuracy: 0.7805\n",
      "Training loss (for one batch) at step 20: 382.6182, Accuracy: 0.7760\n",
      "Training loss (for one batch) at step 30: 363.8216, Accuracy: 0.7979\n",
      "Training loss (for one batch) at step 40: 359.3187, Accuracy: 0.8117\n",
      "Training loss (for one batch) at step 50: 337.3141, Accuracy: 0.8244\n",
      "Training loss (for one batch) at step 60: 377.9967, Accuracy: 0.8307\n",
      "Training loss (for one batch) at step 70: 400.2628, Accuracy: 0.8248\n",
      "Training loss (for one batch) at step 80: 394.4218, Accuracy: 0.8171\n",
      "Training loss (for one batch) at step 90: 367.2099, Accuracy: 0.8128\n",
      "Training loss (for one batch) at step 100: 353.9940, Accuracy: 0.8144\n",
      "Training loss (for one batch) at step 110: 366.3347, Accuracy: 0.8155\n",
      "---- Training ----\n",
      "Training loss: 134.0046\n",
      "Training acc over epoch: 0.8143\n",
      "---- Validation ----\n",
      "Validation loss: 45.7102\n",
      "Validation acc: 0.7649\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 410.9194, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 401.7534, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 20: 366.0529, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 30: 360.5480, Accuracy: 0.8057\n",
      "Training loss (for one batch) at step 40: 346.4678, Accuracy: 0.8234\n",
      "Training loss (for one batch) at step 50: 345.6010, Accuracy: 0.8316\n",
      "Training loss (for one batch) at step 60: 343.3979, Accuracy: 0.8379\n",
      "Training loss (for one batch) at step 70: 385.8621, Accuracy: 0.8303\n",
      "Training loss (for one batch) at step 80: 414.2641, Accuracy: 0.8220\n",
      "Training loss (for one batch) at step 90: 373.2257, Accuracy: 0.8171\n",
      "Training loss (for one batch) at step 100: 375.3640, Accuracy: 0.8158\n",
      "Training loss (for one batch) at step 110: 370.8646, Accuracy: 0.8163\n",
      "---- Training ----\n",
      "Training loss: 130.5777\n",
      "Training acc over epoch: 0.8150\n",
      "---- Validation ----\n",
      "Validation loss: 42.4785\n",
      "Validation acc: 0.7716\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 402.9725, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 390.9507, Accuracy: 0.7642\n",
      "Training loss (for one batch) at step 20: 361.4950, Accuracy: 0.7857\n",
      "Training loss (for one batch) at step 30: 357.6931, Accuracy: 0.8029\n",
      "Training loss (for one batch) at step 40: 365.5909, Accuracy: 0.8161\n",
      "Training loss (for one batch) at step 50: 352.0325, Accuracy: 0.8280\n",
      "Training loss (for one batch) at step 60: 354.2228, Accuracy: 0.8366\n",
      "Training loss (for one batch) at step 70: 383.6553, Accuracy: 0.8304\n",
      "Training loss (for one batch) at step 80: 402.0943, Accuracy: 0.8240\n",
      "Training loss (for one batch) at step 90: 356.9130, Accuracy: 0.8180\n",
      "Training loss (for one batch) at step 100: 378.4007, Accuracy: 0.8201\n",
      "Training loss (for one batch) at step 110: 382.5660, Accuracy: 0.8196\n",
      "---- Training ----\n",
      "Training loss: 106.4122\n",
      "Training acc over epoch: 0.8184\n",
      "---- Validation ----\n",
      "Validation loss: 34.2685\n",
      "Validation acc: 0.7660\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 395.7114, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 10: 386.1961, Accuracy: 0.7763\n",
      "Training loss (for one batch) at step 20: 364.5610, Accuracy: 0.7917\n",
      "Training loss (for one batch) at step 30: 347.8542, Accuracy: 0.8100\n",
      "Training loss (for one batch) at step 40: 349.5616, Accuracy: 0.8255\n",
      "Training loss (for one batch) at step 50: 331.0731, Accuracy: 0.8358\n",
      "Training loss (for one batch) at step 60: 364.5315, Accuracy: 0.8427\n",
      "Training loss (for one batch) at step 70: 383.7850, Accuracy: 0.8366\n",
      "Training loss (for one batch) at step 80: 380.2093, Accuracy: 0.8267\n",
      "Training loss (for one batch) at step 90: 340.2870, Accuracy: 0.8231\n",
      "Training loss (for one batch) at step 100: 332.3312, Accuracy: 0.8245\n",
      "Training loss (for one batch) at step 110: 372.8825, Accuracy: 0.8255\n",
      "---- Training ----\n",
      "Training loss: 128.2634\n",
      "Training acc over epoch: 0.8229\n",
      "---- Validation ----\n",
      "Validation loss: 61.4472\n",
      "Validation acc: 0.7606\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 389.6327, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 386.0984, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 20: 334.5871, Accuracy: 0.7861\n",
      "Training loss (for one batch) at step 30: 343.2624, Accuracy: 0.8102\n",
      "Training loss (for one batch) at step 40: 342.8542, Accuracy: 0.8266\n",
      "Training loss (for one batch) at step 50: 347.2248, Accuracy: 0.8355\n",
      "Training loss (for one batch) at step 60: 350.3001, Accuracy: 0.8416\n",
      "Training loss (for one batch) at step 70: 374.2542, Accuracy: 0.8323\n",
      "Training loss (for one batch) at step 80: 390.5798, Accuracy: 0.8211\n",
      "Training loss (for one batch) at step 90: 347.0083, Accuracy: 0.8174\n",
      "Training loss (for one batch) at step 100: 361.0764, Accuracy: 0.8192\n",
      "Training loss (for one batch) at step 110: 368.7817, Accuracy: 0.8193\n",
      "---- Training ----\n",
      "Training loss: 123.7466\n",
      "Training acc over epoch: 0.8189\n",
      "---- Validation ----\n",
      "Validation loss: 34.7140\n",
      "Validation acc: 0.7751\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 400.7753, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 389.2941, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 20: 354.8211, Accuracy: 0.7790\n",
      "Training loss (for one batch) at step 30: 346.6763, Accuracy: 0.8034\n",
      "Training loss (for one batch) at step 40: 324.3455, Accuracy: 0.8222\n",
      "Training loss (for one batch) at step 50: 333.1851, Accuracy: 0.8376\n",
      "Training loss (for one batch) at step 60: 353.0035, Accuracy: 0.8422\n",
      "Training loss (for one batch) at step 70: 375.0635, Accuracy: 0.8344\n",
      "Training loss (for one batch) at step 80: 402.1428, Accuracy: 0.8235\n",
      "Training loss (for one batch) at step 90: 352.3596, Accuracy: 0.8201\n",
      "Training loss (for one batch) at step 100: 341.3841, Accuracy: 0.8222\n",
      "Training loss (for one batch) at step 110: 366.9194, Accuracy: 0.8244\n",
      "---- Training ----\n",
      "Training loss: 123.8897\n",
      "Training acc over epoch: 0.8228\n",
      "---- Validation ----\n",
      "Validation loss: 42.0367\n",
      "Validation acc: 0.7654\n",
      "Time taken: 10.76s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 399.6471, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 390.9607, Accuracy: 0.7393\n",
      "Training loss (for one batch) at step 20: 349.9544, Accuracy: 0.7798\n",
      "Training loss (for one batch) at step 30: 351.8098, Accuracy: 0.8009\n",
      "Training loss (for one batch) at step 40: 327.5345, Accuracy: 0.8157\n",
      "Training loss (for one batch) at step 50: 325.9926, Accuracy: 0.8321\n",
      "Training loss (for one batch) at step 60: 339.0419, Accuracy: 0.8390\n",
      "Training loss (for one batch) at step 70: 362.2582, Accuracy: 0.8323\n",
      "Training loss (for one batch) at step 80: 389.1473, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 90: 363.1469, Accuracy: 0.8154\n",
      "Training loss (for one batch) at step 100: 350.7944, Accuracy: 0.8182\n",
      "Training loss (for one batch) at step 110: 359.4373, Accuracy: 0.8193\n",
      "---- Training ----\n",
      "Training loss: 105.2779\n",
      "Training acc over epoch: 0.8190\n",
      "---- Validation ----\n",
      "Validation loss: 33.8492\n",
      "Validation acc: 0.7611\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 406.6529, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 359.7885, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 20: 356.6539, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 30: 360.6161, Accuracy: 0.8057\n",
      "Training loss (for one batch) at step 40: 327.1194, Accuracy: 0.8247\n",
      "Training loss (for one batch) at step 50: 332.2184, Accuracy: 0.8358\n",
      "Training loss (for one batch) at step 60: 343.2827, Accuracy: 0.8423\n",
      "Training loss (for one batch) at step 70: 372.8232, Accuracy: 0.8331\n",
      "Training loss (for one batch) at step 80: 379.1091, Accuracy: 0.8220\n",
      "Training loss (for one batch) at step 90: 345.2025, Accuracy: 0.8201\n",
      "Training loss (for one batch) at step 100: 334.4846, Accuracy: 0.8230\n",
      "Training loss (for one batch) at step 110: 343.1726, Accuracy: 0.8247\n",
      "---- Training ----\n",
      "Training loss: 112.3924\n",
      "Training acc over epoch: 0.8234\n",
      "---- Validation ----\n",
      "Validation loss: 47.4961\n",
      "Validation acc: 0.7654\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 390.2255, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 387.3395, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 20: 338.2783, Accuracy: 0.7831\n",
      "Training loss (for one batch) at step 30: 345.6799, Accuracy: 0.8150\n",
      "Training loss (for one batch) at step 40: 329.4579, Accuracy: 0.8276\n",
      "Training loss (for one batch) at step 50: 345.5810, Accuracy: 0.8404\n",
      "Training loss (for one batch) at step 60: 358.6260, Accuracy: 0.8457\n",
      "Training loss (for one batch) at step 70: 382.2412, Accuracy: 0.8355\n",
      "Training loss (for one batch) at step 80: 370.5615, Accuracy: 0.8238\n",
      "Training loss (for one batch) at step 90: 344.6006, Accuracy: 0.8200\n",
      "Training loss (for one batch) at step 100: 330.7220, Accuracy: 0.8216\n",
      "Training loss (for one batch) at step 110: 358.2612, Accuracy: 0.8226\n",
      "---- Training ----\n",
      "Training loss: 129.5680\n",
      "Training acc over epoch: 0.8218\n",
      "---- Validation ----\n",
      "Validation loss: 46.3476\n",
      "Validation acc: 0.7716\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 382.5670, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 369.8905, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 20: 349.6165, Accuracy: 0.7760\n",
      "Training loss (for one batch) at step 30: 334.0429, Accuracy: 0.8085\n",
      "Training loss (for one batch) at step 40: 331.6457, Accuracy: 0.8260\n",
      "Training loss (for one batch) at step 50: 310.5894, Accuracy: 0.8410\n",
      "Training loss (for one batch) at step 60: 346.5589, Accuracy: 0.8476\n",
      "Training loss (for one batch) at step 70: 371.3108, Accuracy: 0.8384\n",
      "Training loss (for one batch) at step 80: 379.3639, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 90: 342.7823, Accuracy: 0.8223\n",
      "Training loss (for one batch) at step 100: 344.8723, Accuracy: 0.8246\n",
      "Training loss (for one batch) at step 110: 363.7562, Accuracy: 0.8262\n",
      "---- Training ----\n",
      "Training loss: 107.4634\n",
      "Training acc over epoch: 0.8244\n",
      "---- Validation ----\n",
      "Validation loss: 44.5076\n",
      "Validation acc: 0.7711\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 368.8759, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 387.6461, Accuracy: 0.7585\n",
      "Training loss (for one batch) at step 20: 356.5183, Accuracy: 0.7902\n",
      "Training loss (for one batch) at step 30: 333.9767, Accuracy: 0.8085\n",
      "Training loss (for one batch) at step 40: 302.8524, Accuracy: 0.8258\n",
      "Training loss (for one batch) at step 50: 322.2857, Accuracy: 0.8352\n",
      "Training loss (for one batch) at step 60: 324.5943, Accuracy: 0.8421\n",
      "Training loss (for one batch) at step 70: 367.8777, Accuracy: 0.8358\n",
      "Training loss (for one batch) at step 80: 386.5699, Accuracy: 0.8243\n",
      "Training loss (for one batch) at step 90: 352.8509, Accuracy: 0.8196\n",
      "Training loss (for one batch) at step 100: 349.0214, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 110: 344.0049, Accuracy: 0.8209\n",
      "---- Training ----\n",
      "Training loss: 114.2012\n",
      "Training acc over epoch: 0.8217\n",
      "---- Validation ----\n",
      "Validation loss: 36.4168\n",
      "Validation acc: 0.7665\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 376.4558, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 364.5806, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 20: 355.2989, Accuracy: 0.7794\n",
      "Training loss (for one batch) at step 30: 324.9760, Accuracy: 0.8007\n",
      "Training loss (for one batch) at step 40: 323.1765, Accuracy: 0.8180\n",
      "Training loss (for one batch) at step 50: 315.3462, Accuracy: 0.8339\n",
      "Training loss (for one batch) at step 60: 327.9259, Accuracy: 0.8411\n",
      "Training loss (for one batch) at step 70: 379.0039, Accuracy: 0.8327\n",
      "Training loss (for one batch) at step 80: 377.5089, Accuracy: 0.8243\n",
      "Training loss (for one batch) at step 90: 334.7980, Accuracy: 0.8201\n",
      "Training loss (for one batch) at step 100: 326.9257, Accuracy: 0.8218\n",
      "Training loss (for one batch) at step 110: 338.6705, Accuracy: 0.8243\n",
      "---- Training ----\n",
      "Training loss: 106.6159\n",
      "Training acc over epoch: 0.8242\n",
      "---- Validation ----\n",
      "Validation loss: 38.5136\n",
      "Validation acc: 0.7716\n",
      "Time taken: 10.54s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 359.8540, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 361.2888, Accuracy: 0.7607\n",
      "Training loss (for one batch) at step 20: 351.7259, Accuracy: 0.7835\n",
      "Training loss (for one batch) at step 30: 328.3144, Accuracy: 0.8102\n",
      "Training loss (for one batch) at step 40: 322.6774, Accuracy: 0.8243\n",
      "Training loss (for one batch) at step 50: 308.8871, Accuracy: 0.8396\n",
      "Training loss (for one batch) at step 60: 349.0054, Accuracy: 0.8455\n",
      "Training loss (for one batch) at step 70: 373.1580, Accuracy: 0.8371\n",
      "Training loss (for one batch) at step 80: 372.4597, Accuracy: 0.8235\n",
      "Training loss (for one batch) at step 90: 350.1680, Accuracy: 0.8214\n",
      "Training loss (for one batch) at step 100: 329.9304, Accuracy: 0.8240\n",
      "Training loss (for one batch) at step 110: 337.4787, Accuracy: 0.8246\n",
      "---- Training ----\n",
      "Training loss: 112.4615\n",
      "Training acc over epoch: 0.8243\n",
      "---- Validation ----\n",
      "Validation loss: 42.5891\n",
      "Validation acc: 0.7644\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 365.3787, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 366.8825, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 20: 327.4034, Accuracy: 0.7790\n",
      "Training loss (for one batch) at step 30: 321.3904, Accuracy: 0.8075\n",
      "Training loss (for one batch) at step 40: 328.8405, Accuracy: 0.8272\n",
      "Training loss (for one batch) at step 50: 299.8372, Accuracy: 0.8404\n",
      "Training loss (for one batch) at step 60: 316.0071, Accuracy: 0.8457\n",
      "Training loss (for one batch) at step 70: 366.7788, Accuracy: 0.8352\n",
      "Training loss (for one batch) at step 80: 363.0708, Accuracy: 0.8242\n",
      "Training loss (for one batch) at step 90: 341.5624, Accuracy: 0.8208\n",
      "Training loss (for one batch) at step 100: 341.3978, Accuracy: 0.8248\n",
      "Training loss (for one batch) at step 110: 338.9992, Accuracy: 0.8266\n",
      "---- Training ----\n",
      "Training loss: 118.5380\n",
      "Training acc over epoch: 0.8252\n",
      "---- Validation ----\n",
      "Validation loss: 50.9929\n",
      "Validation acc: 0.7563\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 381.0363, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 351.6849, Accuracy: 0.7479\n",
      "Training loss (for one batch) at step 20: 328.6111, Accuracy: 0.7850\n",
      "Training loss (for one batch) at step 30: 311.2246, Accuracy: 0.8077\n",
      "Training loss (for one batch) at step 40: 315.7582, Accuracy: 0.8262\n",
      "Training loss (for one batch) at step 50: 314.0382, Accuracy: 0.8384\n",
      "Training loss (for one batch) at step 60: 333.2291, Accuracy: 0.8467\n",
      "Training loss (for one batch) at step 70: 357.5491, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 80: 369.4057, Accuracy: 0.8243\n",
      "Training loss (for one batch) at step 90: 338.5002, Accuracy: 0.8225\n",
      "Training loss (for one batch) at step 100: 325.0608, Accuracy: 0.8253\n",
      "Training loss (for one batch) at step 110: 334.6976, Accuracy: 0.8275\n",
      "---- Training ----\n",
      "Training loss: 113.9497\n",
      "Training acc over epoch: 0.8264\n",
      "---- Validation ----\n",
      "Validation loss: 48.5546\n",
      "Validation acc: 0.7657\n",
      "Time taken: 10.68s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 363.0258, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 349.4485, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 20: 324.9570, Accuracy: 0.7853\n",
      "Training loss (for one batch) at step 30: 319.0565, Accuracy: 0.8107\n",
      "Training loss (for one batch) at step 40: 317.8078, Accuracy: 0.8289\n",
      "Training loss (for one batch) at step 50: 303.3403, Accuracy: 0.8421\n",
      "Training loss (for one batch) at step 60: 335.0945, Accuracy: 0.8496\n",
      "Training loss (for one batch) at step 70: 336.4124, Accuracy: 0.8386\n",
      "Training loss (for one batch) at step 80: 352.1663, Accuracy: 0.8266\n",
      "Training loss (for one batch) at step 90: 315.0703, Accuracy: 0.8241\n",
      "Training loss (for one batch) at step 100: 310.7509, Accuracy: 0.8284\n",
      "Training loss (for one batch) at step 110: 345.7628, Accuracy: 0.8284\n",
      "---- Training ----\n",
      "Training loss: 120.6773\n",
      "Training acc over epoch: 0.8268\n",
      "---- Validation ----\n",
      "Validation loss: 46.0743\n",
      "Validation acc: 0.7472\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 357.6402, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 413.1649, Accuracy: 0.7358\n",
      "Training loss (for one batch) at step 20: 331.3315, Accuracy: 0.7783\n",
      "Training loss (for one batch) at step 30: 315.1119, Accuracy: 0.8052\n",
      "Training loss (for one batch) at step 40: 340.1221, Accuracy: 0.8230\n",
      "Training loss (for one batch) at step 50: 307.2189, Accuracy: 0.8358\n",
      "Training loss (for one batch) at step 60: 324.5818, Accuracy: 0.8431\n",
      "Training loss (for one batch) at step 70: 365.1481, Accuracy: 0.8333\n",
      "Training loss (for one batch) at step 80: 355.3950, Accuracy: 0.8225\n",
      "Training loss (for one batch) at step 90: 335.7712, Accuracy: 0.8212\n",
      "Training loss (for one batch) at step 100: 300.7120, Accuracy: 0.8229\n",
      "Training loss (for one batch) at step 110: 343.4947, Accuracy: 0.8237\n",
      "---- Training ----\n",
      "Training loss: 116.6898\n",
      "Training acc over epoch: 0.8227\n",
      "---- Validation ----\n",
      "Validation loss: 35.6476\n",
      "Validation acc: 0.7569\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 387.2384, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 364.0166, Accuracy: 0.7507\n",
      "Training loss (for one batch) at step 20: 316.6370, Accuracy: 0.7868\n",
      "Training loss (for one batch) at step 30: 321.1756, Accuracy: 0.8135\n",
      "Training loss (for one batch) at step 40: 309.2137, Accuracy: 0.8291\n",
      "Training loss (for one batch) at step 50: 310.8970, Accuracy: 0.8387\n",
      "Training loss (for one batch) at step 60: 335.1428, Accuracy: 0.8466\n",
      "Training loss (for one batch) at step 70: 356.7922, Accuracy: 0.8375\n",
      "Training loss (for one batch) at step 80: 383.6831, Accuracy: 0.8262\n",
      "Training loss (for one batch) at step 90: 314.3120, Accuracy: 0.8250\n",
      "Training loss (for one batch) at step 100: 314.9688, Accuracy: 0.8259\n",
      "Training loss (for one batch) at step 110: 335.9706, Accuracy: 0.8269\n",
      "---- Training ----\n",
      "Training loss: 111.4840\n",
      "Training acc over epoch: 0.8264\n",
      "---- Validation ----\n",
      "Validation loss: 44.5574\n",
      "Validation acc: 0.7493\n",
      "Time taken: 10.69s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 361.2532, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 361.2623, Accuracy: 0.7358\n",
      "Training loss (for one batch) at step 20: 322.4151, Accuracy: 0.7775\n",
      "Training loss (for one batch) at step 30: 308.3118, Accuracy: 0.8097\n",
      "Training loss (for one batch) at step 40: 311.6486, Accuracy: 0.8247\n",
      "Training loss (for one batch) at step 50: 293.4777, Accuracy: 0.8388\n",
      "Training loss (for one batch) at step 60: 313.1318, Accuracy: 0.8478\n",
      "Training loss (for one batch) at step 70: 341.0297, Accuracy: 0.8366\n",
      "Training loss (for one batch) at step 80: 369.2681, Accuracy: 0.8228\n",
      "Training loss (for one batch) at step 90: 330.7555, Accuracy: 0.8208\n",
      "Training loss (for one batch) at step 100: 316.0087, Accuracy: 0.8241\n",
      "Training loss (for one batch) at step 110: 341.8988, Accuracy: 0.8253\n",
      "---- Training ----\n",
      "Training loss: 112.0806\n",
      "Training acc over epoch: 0.8264\n",
      "---- Validation ----\n",
      "Validation loss: 38.7417\n",
      "Validation acc: 0.7673\n",
      "Time taken: 10.12s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 357.9603, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 365.3253, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 20: 324.1718, Accuracy: 0.7961\n",
      "Training loss (for one batch) at step 30: 306.2281, Accuracy: 0.8155\n",
      "Training loss (for one batch) at step 40: 328.1428, Accuracy: 0.8327\n",
      "Training loss (for one batch) at step 50: 300.4891, Accuracy: 0.8457\n",
      "Training loss (for one batch) at step 60: 339.8554, Accuracy: 0.8496\n",
      "Training loss (for one batch) at step 70: 360.4621, Accuracy: 0.8398\n",
      "Training loss (for one batch) at step 80: 351.8618, Accuracy: 0.8260\n",
      "Training loss (for one batch) at step 90: 306.4261, Accuracy: 0.8216\n",
      "Training loss (for one batch) at step 100: 309.0904, Accuracy: 0.8259\n",
      "Training loss (for one batch) at step 110: 336.8713, Accuracy: 0.8273\n",
      "---- Training ----\n",
      "Training loss: 102.2013\n",
      "Training acc over epoch: 0.8269\n",
      "---- Validation ----\n",
      "Validation loss: 34.7624\n",
      "Validation acc: 0.7679\n",
      "Time taken: 10.12s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 369.3796, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 359.8644, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 20: 318.7711, Accuracy: 0.7913\n",
      "Training loss (for one batch) at step 30: 318.0033, Accuracy: 0.8135\n",
      "Training loss (for one batch) at step 40: 321.5680, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 50: 306.6481, Accuracy: 0.8424\n",
      "Training loss (for one batch) at step 60: 327.7111, Accuracy: 0.8504\n",
      "Training loss (for one batch) at step 70: 356.6911, Accuracy: 0.8404\n",
      "Training loss (for one batch) at step 80: 351.1997, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 90: 328.4234, Accuracy: 0.8251\n",
      "Training loss (for one batch) at step 100: 290.0188, Accuracy: 0.8280\n",
      "Training loss (for one batch) at step 110: 316.3725, Accuracy: 0.8292\n",
      "---- Training ----\n",
      "Training loss: 105.0098\n",
      "Training acc over epoch: 0.8277\n",
      "---- Validation ----\n",
      "Validation loss: 43.2013\n",
      "Validation acc: 0.7673\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 361.3082, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 352.2912, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 20: 305.4680, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 30: 322.1755, Accuracy: 0.8082\n",
      "Training loss (for one batch) at step 40: 283.0569, Accuracy: 0.8255\n",
      "Training loss (for one batch) at step 50: 315.8888, Accuracy: 0.8396\n",
      "Training loss (for one batch) at step 60: 318.8291, Accuracy: 0.8445\n",
      "Training loss (for one batch) at step 70: 358.8054, Accuracy: 0.8351\n",
      "Training loss (for one batch) at step 80: 349.4532, Accuracy: 0.8216\n",
      "Training loss (for one batch) at step 90: 325.2648, Accuracy: 0.8198\n",
      "Training loss (for one batch) at step 100: 319.6262, Accuracy: 0.8223\n",
      "Training loss (for one batch) at step 110: 305.1544, Accuracy: 0.8253\n",
      "---- Training ----\n",
      "Training loss: 117.3618\n",
      "Training acc over epoch: 0.8242\n",
      "---- Validation ----\n",
      "Validation loss: 49.6570\n",
      "Validation acc: 0.7714\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 372.0954, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 362.7298, Accuracy: 0.7443\n",
      "Training loss (for one batch) at step 20: 317.7337, Accuracy: 0.7846\n",
      "Training loss (for one batch) at step 30: 314.0476, Accuracy: 0.8110\n",
      "Training loss (for one batch) at step 40: 304.4305, Accuracy: 0.8247\n",
      "Training loss (for one batch) at step 50: 299.2413, Accuracy: 0.8379\n",
      "Training loss (for one batch) at step 60: 310.2961, Accuracy: 0.8425\n",
      "Training loss (for one batch) at step 70: 346.5535, Accuracy: 0.8334\n",
      "Training loss (for one batch) at step 80: 334.7028, Accuracy: 0.8226\n",
      "Training loss (for one batch) at step 90: 319.7990, Accuracy: 0.8198\n",
      "Training loss (for one batch) at step 100: 334.4625, Accuracy: 0.8234\n",
      "Training loss (for one batch) at step 110: 342.9336, Accuracy: 0.8248\n",
      "---- Training ----\n",
      "Training loss: 110.7349\n",
      "Training acc over epoch: 0.8244\n",
      "---- Validation ----\n",
      "Validation loss: 46.6167\n",
      "Validation acc: 0.7552\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 369.8072, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 353.3212, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 20: 312.6667, Accuracy: 0.7824\n",
      "Training loss (for one batch) at step 30: 306.6994, Accuracy: 0.8100\n",
      "Training loss (for one batch) at step 40: 284.4506, Accuracy: 0.8317\n",
      "Training loss (for one batch) at step 50: 288.9412, Accuracy: 0.8427\n",
      "Training loss (for one batch) at step 60: 319.8155, Accuracy: 0.8476\n",
      "Training loss (for one batch) at step 70: 355.1042, Accuracy: 0.8391\n",
      "Training loss (for one batch) at step 80: 330.3227, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 90: 323.5770, Accuracy: 0.8259\n",
      "Training loss (for one batch) at step 100: 289.1790, Accuracy: 0.8282\n",
      "Training loss (for one batch) at step 110: 321.8954, Accuracy: 0.8295\n",
      "---- Training ----\n",
      "Training loss: 99.3186\n",
      "Training acc over epoch: 0.8278\n",
      "---- Validation ----\n",
      "Validation loss: 45.2695\n",
      "Validation acc: 0.7571\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 338.6659, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 366.7281, Accuracy: 0.7337\n",
      "Training loss (for one batch) at step 20: 323.6627, Accuracy: 0.7723\n",
      "Training loss (for one batch) at step 30: 315.2777, Accuracy: 0.8032\n",
      "Training loss (for one batch) at step 40: 300.4496, Accuracy: 0.8213\n",
      "Training loss (for one batch) at step 50: 283.8807, Accuracy: 0.8364\n",
      "Training loss (for one batch) at step 60: 305.2957, Accuracy: 0.8446\n",
      "Training loss (for one batch) at step 70: 337.7886, Accuracy: 0.8334\n",
      "Training loss (for one batch) at step 80: 346.7679, Accuracy: 0.8215\n",
      "Training loss (for one batch) at step 90: 310.4225, Accuracy: 0.8186\n",
      "Training loss (for one batch) at step 100: 301.4190, Accuracy: 0.8212\n",
      "Training loss (for one batch) at step 110: 324.4800, Accuracy: 0.8240\n",
      "---- Training ----\n",
      "Training loss: 108.8886\n",
      "Training acc over epoch: 0.8240\n",
      "---- Validation ----\n",
      "Validation loss: 40.6227\n",
      "Validation acc: 0.7689\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 357.9106, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 354.0702, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 20: 327.0212, Accuracy: 0.7865\n",
      "Training loss (for one batch) at step 30: 313.1923, Accuracy: 0.8120\n",
      "Training loss (for one batch) at step 40: 328.7873, Accuracy: 0.8262\n",
      "Training loss (for one batch) at step 50: 303.9174, Accuracy: 0.8418\n",
      "Training loss (for one batch) at step 60: 320.9507, Accuracy: 0.8484\n",
      "Training loss (for one batch) at step 70: 337.7019, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 80: 344.5368, Accuracy: 0.8253\n",
      "Training loss (for one batch) at step 90: 322.1783, Accuracy: 0.8217\n",
      "Training loss (for one batch) at step 100: 311.4999, Accuracy: 0.8264\n",
      "Training loss (for one batch) at step 110: 324.3391, Accuracy: 0.8275\n",
      "---- Training ----\n",
      "Training loss: 100.7526\n",
      "Training acc over epoch: 0.8263\n",
      "---- Validation ----\n",
      "Validation loss: 49.1368\n",
      "Validation acc: 0.7781\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 374.5628, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 356.7946, Accuracy: 0.7372\n",
      "Training loss (for one batch) at step 20: 324.6697, Accuracy: 0.7768\n",
      "Training loss (for one batch) at step 30: 331.8557, Accuracy: 0.8062\n",
      "Training loss (for one batch) at step 40: 280.4917, Accuracy: 0.8264\n",
      "Training loss (for one batch) at step 50: 308.2919, Accuracy: 0.8398\n",
      "Training loss (for one batch) at step 60: 308.5100, Accuracy: 0.8458\n",
      "Training loss (for one batch) at step 70: 329.8564, Accuracy: 0.8341\n",
      "Training loss (for one batch) at step 80: 351.4973, Accuracy: 0.8220\n",
      "Training loss (for one batch) at step 90: 317.5287, Accuracy: 0.8188\n",
      "Training loss (for one batch) at step 100: 308.0923, Accuracy: 0.8224\n",
      "Training loss (for one batch) at step 110: 316.0018, Accuracy: 0.8242\n",
      "---- Training ----\n",
      "Training loss: 98.9093\n",
      "Training acc over epoch: 0.8252\n",
      "---- Validation ----\n",
      "Validation loss: 37.3042\n",
      "Validation acc: 0.7657\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 370.6392, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 341.0258, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 20: 314.6769, Accuracy: 0.7827\n",
      "Training loss (for one batch) at step 30: 309.7072, Accuracy: 0.8070\n",
      "Training loss (for one batch) at step 40: 293.4636, Accuracy: 0.8241\n",
      "Training loss (for one batch) at step 50: 297.7015, Accuracy: 0.8382\n",
      "Training loss (for one batch) at step 60: 303.2360, Accuracy: 0.8467\n",
      "Training loss (for one batch) at step 70: 366.5431, Accuracy: 0.8368\n",
      "Training loss (for one batch) at step 80: 372.0790, Accuracy: 0.8230\n",
      "Training loss (for one batch) at step 90: 298.9816, Accuracy: 0.8206\n",
      "Training loss (for one batch) at step 100: 326.4162, Accuracy: 0.8233\n",
      "Training loss (for one batch) at step 110: 325.3345, Accuracy: 0.8257\n",
      "---- Training ----\n",
      "Training loss: 96.0639\n",
      "Training acc over epoch: 0.8252\n",
      "---- Validation ----\n",
      "Validation loss: 51.4129\n",
      "Validation acc: 0.7646\n",
      "Time taken: 10.11s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 363.2242, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 342.2083, Accuracy: 0.7408\n",
      "Training loss (for one batch) at step 20: 315.8965, Accuracy: 0.7805\n",
      "Training loss (for one batch) at step 30: 308.2967, Accuracy: 0.8133\n",
      "Training loss (for one batch) at step 40: 294.4041, Accuracy: 0.8316\n",
      "Training loss (for one batch) at step 50: 290.8914, Accuracy: 0.8457\n",
      "Training loss (for one batch) at step 60: 318.6763, Accuracy: 0.8512\n",
      "Training loss (for one batch) at step 70: 357.7898, Accuracy: 0.8398\n",
      "Training loss (for one batch) at step 80: 376.8727, Accuracy: 0.8272\n",
      "Training loss (for one batch) at step 90: 315.2191, Accuracy: 0.8235\n",
      "Training loss (for one batch) at step 100: 297.2030, Accuracy: 0.8264\n",
      "Training loss (for one batch) at step 110: 324.5929, Accuracy: 0.8283\n",
      "---- Training ----\n",
      "Training loss: 96.3028\n",
      "Training acc over epoch: 0.8274\n",
      "---- Validation ----\n",
      "Validation loss: 50.9244\n",
      "Validation acc: 0.7714\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 362.8386, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 349.2838, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 20: 320.7461, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 30: 311.1990, Accuracy: 0.8080\n",
      "Training loss (for one batch) at step 40: 302.4221, Accuracy: 0.8300\n",
      "Training loss (for one batch) at step 50: 304.5800, Accuracy: 0.8430\n",
      "Training loss (for one batch) at step 60: 303.5139, Accuracy: 0.8478\n",
      "Training loss (for one batch) at step 70: 329.6975, Accuracy: 0.8385\n",
      "Training loss (for one batch) at step 80: 336.6745, Accuracy: 0.8251\n",
      "Training loss (for one batch) at step 90: 318.1486, Accuracy: 0.8217\n",
      "Training loss (for one batch) at step 100: 286.6918, Accuracy: 0.8260\n",
      "Training loss (for one batch) at step 110: 314.5630, Accuracy: 0.8270\n",
      "---- Training ----\n",
      "Training loss: 96.6201\n",
      "Training acc over epoch: 0.8264\n",
      "---- Validation ----\n",
      "Validation loss: 32.2488\n",
      "Validation acc: 0.7746\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 356.5856, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 324.7354, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 20: 309.9620, Accuracy: 0.7760\n",
      "Training loss (for one batch) at step 30: 314.2469, Accuracy: 0.8077\n",
      "Training loss (for one batch) at step 40: 319.7079, Accuracy: 0.8241\n",
      "Training loss (for one batch) at step 50: 276.4962, Accuracy: 0.8388\n",
      "Training loss (for one batch) at step 60: 318.0420, Accuracy: 0.8443\n",
      "Training loss (for one batch) at step 70: 331.4489, Accuracy: 0.8358\n",
      "Training loss (for one batch) at step 80: 364.3376, Accuracy: 0.8234\n",
      "Training loss (for one batch) at step 90: 296.2419, Accuracy: 0.8207\n",
      "Training loss (for one batch) at step 100: 295.9628, Accuracy: 0.8242\n",
      "Training loss (for one batch) at step 110: 320.9548, Accuracy: 0.8253\n",
      "---- Training ----\n",
      "Training loss: 99.6041\n",
      "Training acc over epoch: 0.8259\n",
      "---- Validation ----\n",
      "Validation loss: 64.9859\n",
      "Validation acc: 0.7673\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 341.1726, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 345.8802, Accuracy: 0.7599\n",
      "Training loss (for one batch) at step 20: 322.6273, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 30: 297.2075, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 40: 296.6529, Accuracy: 0.8306\n",
      "Training loss (for one batch) at step 50: 277.3783, Accuracy: 0.8460\n",
      "Training loss (for one batch) at step 60: 297.6322, Accuracy: 0.8528\n",
      "Training loss (for one batch) at step 70: 317.3134, Accuracy: 0.8439\n",
      "Training loss (for one batch) at step 80: 348.1431, Accuracy: 0.8285\n",
      "Training loss (for one batch) at step 90: 309.0127, Accuracy: 0.8256\n",
      "Training loss (for one batch) at step 100: 294.0040, Accuracy: 0.8288\n",
      "Training loss (for one batch) at step 110: 304.2353, Accuracy: 0.8302\n",
      "---- Training ----\n",
      "Training loss: 107.5679\n",
      "Training acc over epoch: 0.8290\n",
      "---- Validation ----\n",
      "Validation loss: 54.2883\n",
      "Validation acc: 0.7649\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 355.3100, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 353.6688, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 20: 311.0676, Accuracy: 0.7824\n",
      "Training loss (for one batch) at step 30: 296.4978, Accuracy: 0.8090\n",
      "Training loss (for one batch) at step 40: 284.1971, Accuracy: 0.8306\n",
      "Training loss (for one batch) at step 50: 313.2198, Accuracy: 0.8434\n",
      "Training loss (for one batch) at step 60: 312.5489, Accuracy: 0.8482\n",
      "Training loss (for one batch) at step 70: 344.6011, Accuracy: 0.8360\n",
      "Training loss (for one batch) at step 80: 320.3801, Accuracy: 0.8231\n",
      "Training loss (for one batch) at step 90: 326.2632, Accuracy: 0.8206\n",
      "Training loss (for one batch) at step 100: 304.8445, Accuracy: 0.8242\n",
      "Training loss (for one batch) at step 110: 310.3860, Accuracy: 0.8267\n",
      "---- Training ----\n",
      "Training loss: 117.0416\n",
      "Training acc over epoch: 0.8253\n",
      "---- Validation ----\n",
      "Validation loss: 38.9398\n",
      "Validation acc: 0.7652\n",
      "Time taken: 10.63s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 346.8205, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 362.1784, Accuracy: 0.7443\n",
      "Training loss (for one batch) at step 20: 303.5717, Accuracy: 0.7805\n",
      "Training loss (for one batch) at step 30: 290.9539, Accuracy: 0.8092\n",
      "Training loss (for one batch) at step 40: 292.9736, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 50: 290.7270, Accuracy: 0.8407\n",
      "Training loss (for one batch) at step 60: 295.8858, Accuracy: 0.8498\n",
      "Training loss (for one batch) at step 70: 318.1054, Accuracy: 0.8387\n",
      "Training loss (for one batch) at step 80: 317.0777, Accuracy: 0.8246\n",
      "Training loss (for one batch) at step 90: 306.5010, Accuracy: 0.8224\n",
      "Training loss (for one batch) at step 100: 278.1688, Accuracy: 0.8266\n",
      "Training loss (for one batch) at step 110: 328.0690, Accuracy: 0.8262\n",
      "---- Training ----\n",
      "Training loss: 107.5970\n",
      "Training acc over epoch: 0.8256\n",
      "---- Validation ----\n",
      "Validation loss: 49.2446\n",
      "Validation acc: 0.7654\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 341.3527, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 329.2048, Accuracy: 0.7379\n",
      "Training loss (for one batch) at step 20: 302.7951, Accuracy: 0.7768\n",
      "Training loss (for one batch) at step 30: 290.3250, Accuracy: 0.8097\n",
      "Training loss (for one batch) at step 40: 274.8032, Accuracy: 0.8279\n",
      "Training loss (for one batch) at step 50: 283.1915, Accuracy: 0.8398\n",
      "Training loss (for one batch) at step 60: 308.8611, Accuracy: 0.8459\n",
      "Training loss (for one batch) at step 70: 337.3323, Accuracy: 0.8375\n",
      "Training loss (for one batch) at step 80: 328.7552, Accuracy: 0.8234\n",
      "Training loss (for one batch) at step 90: 312.9406, Accuracy: 0.8193\n",
      "Training loss (for one batch) at step 100: 307.6139, Accuracy: 0.8230\n",
      "Training loss (for one batch) at step 110: 309.5728, Accuracy: 0.8239\n",
      "---- Training ----\n",
      "Training loss: 99.3687\n",
      "Training acc over epoch: 0.8237\n",
      "---- Validation ----\n",
      "Validation loss: 41.1486\n",
      "Validation acc: 0.7638\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 345.6122, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 323.3428, Accuracy: 0.7493\n",
      "Training loss (for one batch) at step 20: 302.9901, Accuracy: 0.7902\n",
      "Training loss (for one batch) at step 30: 290.6500, Accuracy: 0.8160\n",
      "Training loss (for one batch) at step 40: 306.5128, Accuracy: 0.8317\n",
      "Training loss (for one batch) at step 50: 285.3815, Accuracy: 0.8460\n",
      "Training loss (for one batch) at step 60: 312.1774, Accuracy: 0.8526\n",
      "Training loss (for one batch) at step 70: 359.3316, Accuracy: 0.8411\n",
      "Training loss (for one batch) at step 80: 325.0653, Accuracy: 0.8282\n",
      "Training loss (for one batch) at step 90: 305.2013, Accuracy: 0.8251\n",
      "Training loss (for one batch) at step 100: 291.6869, Accuracy: 0.8282\n",
      "Training loss (for one batch) at step 110: 325.7259, Accuracy: 0.8291\n",
      "---- Training ----\n",
      "Training loss: 102.9826\n",
      "Training acc over epoch: 0.8288\n",
      "---- Validation ----\n",
      "Validation loss: 40.8568\n",
      "Validation acc: 0.7620\n",
      "Time taken: 10.60s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 349.2158, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 332.2362, Accuracy: 0.7408\n",
      "Training loss (for one batch) at step 20: 305.0018, Accuracy: 0.7723\n",
      "Training loss (for one batch) at step 30: 321.9932, Accuracy: 0.8072\n",
      "Training loss (for one batch) at step 40: 287.0529, Accuracy: 0.8253\n",
      "Training loss (for one batch) at step 50: 287.0241, Accuracy: 0.8388\n",
      "Training loss (for one batch) at step 60: 298.2575, Accuracy: 0.8444\n",
      "Training loss (for one batch) at step 70: 339.4178, Accuracy: 0.8337\n",
      "Training loss (for one batch) at step 80: 341.3380, Accuracy: 0.8211\n",
      "Training loss (for one batch) at step 90: 324.1540, Accuracy: 0.8183\n",
      "Training loss (for one batch) at step 100: 306.8052, Accuracy: 0.8226\n",
      "Training loss (for one batch) at step 110: 345.9859, Accuracy: 0.8238\n",
      "---- Training ----\n",
      "Training loss: 99.0629\n",
      "Training acc over epoch: 0.8227\n",
      "---- Validation ----\n",
      "Validation loss: 40.5107\n",
      "Validation acc: 0.7603\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 338.8139, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 339.4330, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 20: 321.2321, Accuracy: 0.7872\n",
      "Training loss (for one batch) at step 30: 295.3868, Accuracy: 0.8178\n",
      "Training loss (for one batch) at step 40: 296.9607, Accuracy: 0.8310\n",
      "Training loss (for one batch) at step 50: 309.0739, Accuracy: 0.8441\n",
      "Training loss (for one batch) at step 60: 312.1367, Accuracy: 0.8505\n",
      "Training loss (for one batch) at step 70: 314.0323, Accuracy: 0.8397\n",
      "Training loss (for one batch) at step 80: 324.9630, Accuracy: 0.8225\n",
      "Training loss (for one batch) at step 90: 300.1536, Accuracy: 0.8197\n",
      "Training loss (for one batch) at step 100: 280.7730, Accuracy: 0.8239\n",
      "Training loss (for one batch) at step 110: 300.1114, Accuracy: 0.8249\n",
      "---- Training ----\n",
      "Training loss: 95.7986\n",
      "Training acc over epoch: 0.8248\n",
      "---- Validation ----\n",
      "Validation loss: 48.5444\n",
      "Validation acc: 0.7663\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 351.2671, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 319.4035, Accuracy: 0.7479\n",
      "Training loss (for one batch) at step 20: 285.5657, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 30: 275.8674, Accuracy: 0.8110\n",
      "Training loss (for one batch) at step 40: 287.9982, Accuracy: 0.8302\n",
      "Training loss (for one batch) at step 50: 286.0970, Accuracy: 0.8441\n",
      "Training loss (for one batch) at step 60: 299.3647, Accuracy: 0.8519\n",
      "Training loss (for one batch) at step 70: 344.2668, Accuracy: 0.8370\n",
      "Training loss (for one batch) at step 80: 357.9603, Accuracy: 0.8242\n",
      "Training loss (for one batch) at step 90: 313.5882, Accuracy: 0.8202\n",
      "Training loss (for one batch) at step 100: 315.6842, Accuracy: 0.8246\n",
      "Training loss (for one batch) at step 110: 312.9385, Accuracy: 0.8269\n",
      "---- Training ----\n",
      "Training loss: 120.6369\n",
      "Training acc over epoch: 0.8250\n",
      "---- Validation ----\n",
      "Validation loss: 69.5185\n",
      "Validation acc: 0.7660\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 354.7923, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 328.0976, Accuracy: 0.7663\n",
      "Training loss (for one batch) at step 20: 304.4857, Accuracy: 0.7868\n",
      "Training loss (for one batch) at step 30: 292.5193, Accuracy: 0.8130\n",
      "Training loss (for one batch) at step 40: 276.0436, Accuracy: 0.8333\n",
      "Training loss (for one batch) at step 50: 276.2302, Accuracy: 0.8454\n",
      "Training loss (for one batch) at step 60: 314.6089, Accuracy: 0.8509\n",
      "Training loss (for one batch) at step 70: 316.1220, Accuracy: 0.8406\n",
      "Training loss (for one batch) at step 80: 321.9883, Accuracy: 0.8300\n",
      "Training loss (for one batch) at step 90: 298.1322, Accuracy: 0.8257\n",
      "Training loss (for one batch) at step 100: 275.4924, Accuracy: 0.8285\n",
      "Training loss (for one batch) at step 110: 306.7585, Accuracy: 0.8288\n",
      "---- Training ----\n",
      "Training loss: 97.3990\n",
      "Training acc over epoch: 0.8283\n",
      "---- Validation ----\n",
      "Validation loss: 55.2842\n",
      "Validation acc: 0.7711\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 321.1483, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 338.4041, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 20: 299.6107, Accuracy: 0.7857\n",
      "Training loss (for one batch) at step 30: 299.9384, Accuracy: 0.8133\n",
      "Training loss (for one batch) at step 40: 275.6397, Accuracy: 0.8293\n",
      "Training loss (for one batch) at step 50: 285.4651, Accuracy: 0.8433\n",
      "Training loss (for one batch) at step 60: 274.5916, Accuracy: 0.8505\n",
      "Training loss (for one batch) at step 70: 330.4927, Accuracy: 0.8407\n",
      "Training loss (for one batch) at step 80: 309.9629, Accuracy: 0.8261\n",
      "Training loss (for one batch) at step 90: 283.1979, Accuracy: 0.8222\n",
      "Training loss (for one batch) at step 100: 281.0419, Accuracy: 0.8258\n",
      "Training loss (for one batch) at step 110: 303.1070, Accuracy: 0.8271\n",
      "---- Training ----\n",
      "Training loss: 94.8117\n",
      "Training acc over epoch: 0.8266\n",
      "---- Validation ----\n",
      "Validation loss: 48.7997\n",
      "Validation acc: 0.7695\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 350.6594, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 326.1755, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 20: 296.8660, Accuracy: 0.7801\n",
      "Training loss (for one batch) at step 30: 283.4233, Accuracy: 0.8145\n",
      "Training loss (for one batch) at step 40: 278.8888, Accuracy: 0.8300\n",
      "Training loss (for one batch) at step 50: 290.9264, Accuracy: 0.8462\n",
      "Training loss (for one batch) at step 60: 316.5139, Accuracy: 0.8516\n",
      "Training loss (for one batch) at step 70: 303.3360, Accuracy: 0.8407\n",
      "Training loss (for one batch) at step 80: 324.8929, Accuracy: 0.8254\n",
      "Training loss (for one batch) at step 90: 275.2503, Accuracy: 0.8225\n",
      "Training loss (for one batch) at step 100: 287.3199, Accuracy: 0.8266\n",
      "Training loss (for one batch) at step 110: 291.5916, Accuracy: 0.8279\n",
      "---- Training ----\n",
      "Training loss: 100.9298\n",
      "Training acc over epoch: 0.8271\n",
      "---- Validation ----\n",
      "Validation loss: 41.3375\n",
      "Validation acc: 0.7630\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 339.8873, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 337.9593, Accuracy: 0.7330\n",
      "Training loss (for one batch) at step 20: 287.4076, Accuracy: 0.7775\n",
      "Training loss (for one batch) at step 30: 275.9119, Accuracy: 0.8115\n",
      "Training loss (for one batch) at step 40: 293.1497, Accuracy: 0.8262\n",
      "Training loss (for one batch) at step 50: 294.0922, Accuracy: 0.8421\n",
      "Training loss (for one batch) at step 60: 304.7523, Accuracy: 0.8475\n",
      "Training loss (for one batch) at step 70: 307.1025, Accuracy: 0.8403\n",
      "Training loss (for one batch) at step 80: 328.0219, Accuracy: 0.8247\n",
      "Training loss (for one batch) at step 90: 305.7413, Accuracy: 0.8232\n",
      "Training loss (for one batch) at step 100: 299.3690, Accuracy: 0.8246\n",
      "Training loss (for one batch) at step 110: 298.6500, Accuracy: 0.8264\n",
      "---- Training ----\n",
      "Training loss: 103.8568\n",
      "Training acc over epoch: 0.8259\n",
      "---- Validation ----\n",
      "Validation loss: 35.6042\n",
      "Validation acc: 0.7724\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 339.4284, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 340.3965, Accuracy: 0.7507\n",
      "Training loss (for one batch) at step 20: 296.9120, Accuracy: 0.7939\n",
      "Training loss (for one batch) at step 30: 275.5601, Accuracy: 0.8170\n",
      "Training loss (for one batch) at step 40: 308.5431, Accuracy: 0.8325\n",
      "Training loss (for one batch) at step 50: 300.3895, Accuracy: 0.8447\n",
      "Training loss (for one batch) at step 60: 344.4313, Accuracy: 0.8493\n",
      "Training loss (for one batch) at step 70: 315.6784, Accuracy: 0.8380\n",
      "Training loss (for one batch) at step 80: 333.9782, Accuracy: 0.8249\n",
      "Training loss (for one batch) at step 90: 316.9197, Accuracy: 0.8239\n",
      "Training loss (for one batch) at step 100: 286.5396, Accuracy: 0.8272\n",
      "Training loss (for one batch) at step 110: 317.2814, Accuracy: 0.8281\n",
      "---- Training ----\n",
      "Training loss: 107.8588\n",
      "Training acc over epoch: 0.8275\n",
      "---- Validation ----\n",
      "Validation loss: 47.6923\n",
      "Validation acc: 0.7808\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 327.5808, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 360.1593, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 20: 283.2534, Accuracy: 0.7865\n",
      "Training loss (for one batch) at step 30: 283.5998, Accuracy: 0.8153\n",
      "Training loss (for one batch) at step 40: 281.3909, Accuracy: 0.8346\n",
      "Training loss (for one batch) at step 50: 267.0394, Accuracy: 0.8487\n",
      "Training loss (for one batch) at step 60: 299.5416, Accuracy: 0.8555\n",
      "Training loss (for one batch) at step 70: 325.7032, Accuracy: 0.8423\n",
      "Training loss (for one batch) at step 80: 326.1639, Accuracy: 0.8284\n",
      "Training loss (for one batch) at step 90: 301.0216, Accuracy: 0.8249\n",
      "Training loss (for one batch) at step 100: 266.2473, Accuracy: 0.8294\n",
      "Training loss (for one batch) at step 110: 291.7405, Accuracy: 0.8311\n",
      "---- Training ----\n",
      "Training loss: 107.7838\n",
      "Training acc over epoch: 0.8303\n",
      "---- Validation ----\n",
      "Validation loss: 41.2530\n",
      "Validation acc: 0.7700\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 339.1259, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 323.7537, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 20: 304.4224, Accuracy: 0.7798\n",
      "Training loss (for one batch) at step 30: 293.0231, Accuracy: 0.8117\n",
      "Training loss (for one batch) at step 40: 292.5775, Accuracy: 0.8296\n",
      "Training loss (for one batch) at step 50: 279.9254, Accuracy: 0.8439\n",
      "Training loss (for one batch) at step 60: 298.8788, Accuracy: 0.8511\n",
      "Training loss (for one batch) at step 70: 303.4199, Accuracy: 0.8407\n",
      "Training loss (for one batch) at step 80: 323.1980, Accuracy: 0.8280\n",
      "Training loss (for one batch) at step 90: 288.5946, Accuracy: 0.8242\n",
      "Training loss (for one batch) at step 100: 286.4880, Accuracy: 0.8280\n",
      "Training loss (for one batch) at step 110: 313.7159, Accuracy: 0.8302\n",
      "---- Training ----\n",
      "Training loss: 105.2521\n",
      "Training acc over epoch: 0.8286\n",
      "---- Validation ----\n",
      "Validation loss: 63.1980\n",
      "Validation acc: 0.7585\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 335.0013, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 350.2736, Accuracy: 0.7536\n",
      "Training loss (for one batch) at step 20: 298.1221, Accuracy: 0.7861\n",
      "Training loss (for one batch) at step 30: 291.5973, Accuracy: 0.8145\n",
      "Training loss (for one batch) at step 40: 279.2985, Accuracy: 0.8325\n",
      "Training loss (for one batch) at step 50: 286.6149, Accuracy: 0.8453\n",
      "Training loss (for one batch) at step 60: 294.0550, Accuracy: 0.8535\n",
      "Training loss (for one batch) at step 70: 344.4709, Accuracy: 0.8420\n",
      "Training loss (for one batch) at step 80: 344.4281, Accuracy: 0.8290\n",
      "Training loss (for one batch) at step 90: 299.4128, Accuracy: 0.8259\n",
      "Training loss (for one batch) at step 100: 290.9328, Accuracy: 0.8294\n",
      "Training loss (for one batch) at step 110: 308.8699, Accuracy: 0.8310\n",
      "---- Training ----\n",
      "Training loss: 88.0662\n",
      "Training acc over epoch: 0.8302\n",
      "---- Validation ----\n",
      "Validation loss: 42.5270\n",
      "Validation acc: 0.7614\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 330.9240, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 320.0502, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 20: 293.0092, Accuracy: 0.7835\n",
      "Training loss (for one batch) at step 30: 274.2947, Accuracy: 0.8120\n",
      "Training loss (for one batch) at step 40: 290.8918, Accuracy: 0.8314\n",
      "Training loss (for one batch) at step 50: 278.7483, Accuracy: 0.8454\n",
      "Training loss (for one batch) at step 60: 285.9708, Accuracy: 0.8516\n",
      "Training loss (for one batch) at step 70: 320.4274, Accuracy: 0.8398\n",
      "Training loss (for one batch) at step 80: 326.1048, Accuracy: 0.8264\n",
      "Training loss (for one batch) at step 90: 284.2087, Accuracy: 0.8246\n",
      "Training loss (for one batch) at step 100: 302.5747, Accuracy: 0.8282\n",
      "Training loss (for one batch) at step 110: 298.0541, Accuracy: 0.8307\n",
      "---- Training ----\n",
      "Training loss: 100.8445\n",
      "Training acc over epoch: 0.8293\n",
      "---- Validation ----\n",
      "Validation loss: 41.7527\n",
      "Validation acc: 0.7665\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 335.0692, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 314.8089, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 20: 298.9641, Accuracy: 0.7876\n",
      "Training loss (for one batch) at step 30: 280.1249, Accuracy: 0.8130\n",
      "Training loss (for one batch) at step 40: 286.2692, Accuracy: 0.8314\n",
      "Training loss (for one batch) at step 50: 292.9140, Accuracy: 0.8459\n",
      "Training loss (for one batch) at step 60: 300.5866, Accuracy: 0.8517\n",
      "Training loss (for one batch) at step 70: 310.1667, Accuracy: 0.8431\n",
      "Training loss (for one batch) at step 80: 328.4013, Accuracy: 0.8286\n",
      "Training loss (for one batch) at step 90: 303.7146, Accuracy: 0.8240\n",
      "Training loss (for one batch) at step 100: 273.9824, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 110: 291.3680, Accuracy: 0.8292\n",
      "---- Training ----\n",
      "Training loss: 106.2711\n",
      "Training acc over epoch: 0.8284\n",
      "---- Validation ----\n",
      "Validation loss: 72.3273\n",
      "Validation acc: 0.7711\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 337.5734, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 326.7939, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 20: 286.6057, Accuracy: 0.7902\n",
      "Training loss (for one batch) at step 30: 288.3700, Accuracy: 0.8180\n",
      "Training loss (for one batch) at step 40: 286.0898, Accuracy: 0.8335\n",
      "Training loss (for one batch) at step 50: 272.8136, Accuracy: 0.8448\n",
      "Training loss (for one batch) at step 60: 301.0800, Accuracy: 0.8495\n",
      "Training loss (for one batch) at step 70: 321.3507, Accuracy: 0.8382\n",
      "Training loss (for one batch) at step 80: 329.9081, Accuracy: 0.8244\n",
      "Training loss (for one batch) at step 90: 301.4502, Accuracy: 0.8231\n",
      "Training loss (for one batch) at step 100: 274.4914, Accuracy: 0.8261\n",
      "Training loss (for one batch) at step 110: 314.1325, Accuracy: 0.8274\n",
      "---- Training ----\n",
      "Training loss: 103.7878\n",
      "Training acc over epoch: 0.8275\n",
      "---- Validation ----\n",
      "Validation loss: 44.0432\n",
      "Validation acc: 0.7765\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 345.1920, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 332.2541, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 20: 286.3806, Accuracy: 0.7786\n",
      "Training loss (for one batch) at step 30: 273.2642, Accuracy: 0.8158\n",
      "Training loss (for one batch) at step 40: 273.3845, Accuracy: 0.8302\n",
      "Training loss (for one batch) at step 50: 277.8285, Accuracy: 0.8454\n",
      "Training loss (for one batch) at step 60: 282.9695, Accuracy: 0.8528\n",
      "Training loss (for one batch) at step 70: 312.5901, Accuracy: 0.8411\n",
      "Training loss (for one batch) at step 80: 331.9115, Accuracy: 0.8298\n",
      "Training loss (for one batch) at step 90: 303.2941, Accuracy: 0.8240\n",
      "Training loss (for one batch) at step 100: 275.0703, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 110: 303.2192, Accuracy: 0.8289\n",
      "---- Training ----\n",
      "Training loss: 96.4083\n",
      "Training acc over epoch: 0.8278\n",
      "---- Validation ----\n",
      "Validation loss: 42.8664\n",
      "Validation acc: 0.7700\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 362.8928, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 328.8174, Accuracy: 0.7443\n",
      "Training loss (for one batch) at step 20: 272.3247, Accuracy: 0.7853\n",
      "Training loss (for one batch) at step 30: 290.9014, Accuracy: 0.8135\n",
      "Training loss (for one batch) at step 40: 272.4705, Accuracy: 0.8285\n",
      "Training loss (for one batch) at step 50: 278.6757, Accuracy: 0.8422\n",
      "Training loss (for one batch) at step 60: 272.2005, Accuracy: 0.8480\n",
      "Training loss (for one batch) at step 70: 316.0972, Accuracy: 0.8365\n",
      "Training loss (for one batch) at step 80: 323.0665, Accuracy: 0.8234\n",
      "Training loss (for one batch) at step 90: 288.9268, Accuracy: 0.8202\n",
      "Training loss (for one batch) at step 100: 288.4862, Accuracy: 0.8245\n",
      "Training loss (for one batch) at step 110: 294.3551, Accuracy: 0.8266\n",
      "---- Training ----\n",
      "Training loss: 96.3230\n",
      "Training acc over epoch: 0.8263\n",
      "---- Validation ----\n",
      "Validation loss: 32.6230\n",
      "Validation acc: 0.7628\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 329.2424, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 347.3758, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 20: 292.7105, Accuracy: 0.7779\n",
      "Training loss (for one batch) at step 30: 285.8724, Accuracy: 0.8052\n",
      "Training loss (for one batch) at step 40: 278.5533, Accuracy: 0.8241\n",
      "Training loss (for one batch) at step 50: 274.6726, Accuracy: 0.8392\n",
      "Training loss (for one batch) at step 60: 290.7326, Accuracy: 0.8454\n",
      "Training loss (for one batch) at step 70: 303.1884, Accuracy: 0.8373\n",
      "Training loss (for one batch) at step 80: 307.0940, Accuracy: 0.8238\n",
      "Training loss (for one batch) at step 90: 314.7970, Accuracy: 0.8207\n",
      "Training loss (for one batch) at step 100: 281.8167, Accuracy: 0.8249\n",
      "Training loss (for one batch) at step 110: 317.1157, Accuracy: 0.8262\n",
      "---- Training ----\n",
      "Training loss: 106.7773\n",
      "Training acc over epoch: 0.8256\n",
      "---- Validation ----\n",
      "Validation loss: 50.4264\n",
      "Validation acc: 0.7418\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 334.2924, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 334.9461, Accuracy: 0.7401\n",
      "Training loss (for one batch) at step 20: 296.6549, Accuracy: 0.7768\n",
      "Training loss (for one batch) at step 30: 277.5042, Accuracy: 0.8107\n",
      "Training loss (for one batch) at step 40: 283.6996, Accuracy: 0.8289\n",
      "Training loss (for one batch) at step 50: 272.6099, Accuracy: 0.8428\n",
      "Training loss (for one batch) at step 60: 308.2446, Accuracy: 0.8505\n",
      "Training loss (for one batch) at step 70: 320.5938, Accuracy: 0.8391\n",
      "Training loss (for one batch) at step 80: 320.1970, Accuracy: 0.8271\n",
      "Training loss (for one batch) at step 90: 297.3925, Accuracy: 0.8249\n",
      "Training loss (for one batch) at step 100: 277.7245, Accuracy: 0.8277\n",
      "Training loss (for one batch) at step 110: 296.9557, Accuracy: 0.8278\n",
      "---- Training ----\n",
      "Training loss: 107.6593\n",
      "Training acc over epoch: 0.8272\n",
      "---- Validation ----\n",
      "Validation loss: 35.9385\n",
      "Validation acc: 0.7684\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 328.2514, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 318.1189, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 20: 289.1583, Accuracy: 0.7850\n",
      "Training loss (for one batch) at step 30: 280.0818, Accuracy: 0.8168\n",
      "Training loss (for one batch) at step 40: 286.8659, Accuracy: 0.8310\n",
      "Training loss (for one batch) at step 50: 282.3947, Accuracy: 0.8441\n",
      "Training loss (for one batch) at step 60: 298.5439, Accuracy: 0.8514\n",
      "Training loss (for one batch) at step 70: 321.9009, Accuracy: 0.8385\n",
      "Training loss (for one batch) at step 80: 310.3689, Accuracy: 0.8234\n",
      "Training loss (for one batch) at step 90: 277.1786, Accuracy: 0.8243\n",
      "Training loss (for one batch) at step 100: 274.8701, Accuracy: 0.8272\n",
      "Training loss (for one batch) at step 110: 288.2171, Accuracy: 0.8287\n",
      "---- Training ----\n",
      "Training loss: 107.4572\n",
      "Training acc over epoch: 0.8276\n",
      "---- Validation ----\n",
      "Validation loss: 51.1555\n",
      "Validation acc: 0.7711\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 309.1677, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 323.6490, Accuracy: 0.7564\n",
      "Training loss (for one batch) at step 20: 278.3504, Accuracy: 0.7853\n",
      "Training loss (for one batch) at step 30: 270.5869, Accuracy: 0.8110\n",
      "Training loss (for one batch) at step 40: 292.4424, Accuracy: 0.8266\n",
      "Training loss (for one batch) at step 50: 275.1836, Accuracy: 0.8407\n",
      "Training loss (for one batch) at step 60: 288.2856, Accuracy: 0.8477\n",
      "Training loss (for one batch) at step 70: 347.6660, Accuracy: 0.8366\n",
      "Training loss (for one batch) at step 80: 325.4137, Accuracy: 0.8225\n",
      "Training loss (for one batch) at step 90: 291.3064, Accuracy: 0.8212\n",
      "Training loss (for one batch) at step 100: 277.2984, Accuracy: 0.8242\n",
      "Training loss (for one batch) at step 110: 296.2324, Accuracy: 0.8265\n",
      "---- Training ----\n",
      "Training loss: 99.2949\n",
      "Training acc over epoch: 0.8257\n",
      "---- Validation ----\n",
      "Validation loss: 38.1243\n",
      "Validation acc: 0.7724\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 321.5295, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 313.4746, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 20: 278.9531, Accuracy: 0.7876\n",
      "Training loss (for one batch) at step 30: 277.8747, Accuracy: 0.8143\n",
      "Training loss (for one batch) at step 40: 269.3205, Accuracy: 0.8319\n",
      "Training loss (for one batch) at step 50: 268.1906, Accuracy: 0.8467\n",
      "Training loss (for one batch) at step 60: 273.1842, Accuracy: 0.8537\n",
      "Training loss (for one batch) at step 70: 329.3773, Accuracy: 0.8439\n",
      "Training loss (for one batch) at step 80: 314.3265, Accuracy: 0.8285\n",
      "Training loss (for one batch) at step 90: 298.2430, Accuracy: 0.8261\n",
      "Training loss (for one batch) at step 100: 288.2742, Accuracy: 0.8296\n",
      "Training loss (for one batch) at step 110: 291.2092, Accuracy: 0.8307\n",
      "---- Training ----\n",
      "Training loss: 92.8330\n",
      "Training acc over epoch: 0.8297\n",
      "---- Validation ----\n",
      "Validation loss: 53.7843\n",
      "Validation acc: 0.7689\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 333.9485, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 320.0409, Accuracy: 0.7486\n",
      "Training loss (for one batch) at step 20: 274.6422, Accuracy: 0.7857\n",
      "Training loss (for one batch) at step 30: 296.2491, Accuracy: 0.8183\n",
      "Training loss (for one batch) at step 40: 275.4973, Accuracy: 0.8331\n",
      "Training loss (for one batch) at step 50: 285.8056, Accuracy: 0.8465\n",
      "Training loss (for one batch) at step 60: 283.8584, Accuracy: 0.8534\n",
      "Training loss (for one batch) at step 70: 309.3834, Accuracy: 0.8425\n",
      "Training loss (for one batch) at step 80: 330.8015, Accuracy: 0.8291\n",
      "Training loss (for one batch) at step 90: 304.6376, Accuracy: 0.8257\n",
      "Training loss (for one batch) at step 100: 288.0734, Accuracy: 0.8291\n",
      "Training loss (for one batch) at step 110: 288.5407, Accuracy: 0.8321\n",
      "---- Training ----\n",
      "Training loss: 102.1122\n",
      "Training acc over epoch: 0.8303\n",
      "---- Validation ----\n",
      "Validation loss: 51.2149\n",
      "Validation acc: 0.7730\n",
      "Time taken: 10.16s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 305.8549, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 313.4974, Accuracy: 0.7507\n",
      "Training loss (for one batch) at step 20: 285.8146, Accuracy: 0.7924\n",
      "Training loss (for one batch) at step 30: 290.8503, Accuracy: 0.8180\n",
      "Training loss (for one batch) at step 40: 285.4120, Accuracy: 0.8325\n",
      "Training loss (for one batch) at step 50: 265.5305, Accuracy: 0.8493\n",
      "Training loss (for one batch) at step 60: 292.2905, Accuracy: 0.8558\n",
      "Training loss (for one batch) at step 70: 318.9577, Accuracy: 0.8450\n",
      "Training loss (for one batch) at step 80: 329.2936, Accuracy: 0.8298\n",
      "Training loss (for one batch) at step 90: 275.6071, Accuracy: 0.8283\n",
      "Training loss (for one batch) at step 100: 272.3191, Accuracy: 0.8312\n",
      "Training loss (for one batch) at step 110: 307.1316, Accuracy: 0.8334\n",
      "---- Training ----\n",
      "Training loss: 102.6407\n",
      "Training acc over epoch: 0.8329\n",
      "---- Validation ----\n",
      "Validation loss: 45.9753\n",
      "Validation acc: 0.7644\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 304.5786, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 323.9412, Accuracy: 0.7322\n",
      "Training loss (for one batch) at step 20: 293.2970, Accuracy: 0.7801\n",
      "Training loss (for one batch) at step 30: 279.7788, Accuracy: 0.8135\n",
      "Training loss (for one batch) at step 40: 258.7722, Accuracy: 0.8308\n",
      "Training loss (for one batch) at step 50: 281.2790, Accuracy: 0.8473\n",
      "Training loss (for one batch) at step 60: 292.6292, Accuracy: 0.8525\n",
      "Training loss (for one batch) at step 70: 311.5214, Accuracy: 0.8411\n",
      "Training loss (for one batch) at step 80: 321.3705, Accuracy: 0.8271\n",
      "Training loss (for one batch) at step 90: 285.0806, Accuracy: 0.8247\n",
      "Training loss (for one batch) at step 100: 290.0393, Accuracy: 0.8287\n",
      "Training loss (for one batch) at step 110: 288.4461, Accuracy: 0.8307\n",
      "---- Training ----\n",
      "Training loss: 90.9980\n",
      "Training acc over epoch: 0.8293\n",
      "---- Validation ----\n",
      "Validation loss: 31.4105\n",
      "Validation acc: 0.7550\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 324.5233, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 325.3244, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 20: 282.6377, Accuracy: 0.7801\n",
      "Training loss (for one batch) at step 30: 295.6343, Accuracy: 0.8133\n",
      "Training loss (for one batch) at step 40: 281.0856, Accuracy: 0.8312\n",
      "Training loss (for one batch) at step 50: 267.0364, Accuracy: 0.8428\n",
      "Training loss (for one batch) at step 60: 302.4099, Accuracy: 0.8502\n",
      "Training loss (for one batch) at step 70: 303.8986, Accuracy: 0.8371\n",
      "Training loss (for one batch) at step 80: 337.3555, Accuracy: 0.8234\n",
      "Training loss (for one batch) at step 90: 296.4720, Accuracy: 0.8219\n",
      "Training loss (for one batch) at step 100: 275.1212, Accuracy: 0.8269\n",
      "Training loss (for one batch) at step 110: 301.3240, Accuracy: 0.8289\n",
      "---- Training ----\n",
      "Training loss: 98.2780\n",
      "Training acc over epoch: 0.8284\n",
      "---- Validation ----\n",
      "Validation loss: 58.6999\n",
      "Validation acc: 0.7689\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 305.7942, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 319.9124, Accuracy: 0.7514\n",
      "Training loss (for one batch) at step 20: 285.0501, Accuracy: 0.7846\n",
      "Training loss (for one batch) at step 30: 287.6456, Accuracy: 0.8170\n",
      "Training loss (for one batch) at step 40: 271.5081, Accuracy: 0.8356\n",
      "Training loss (for one batch) at step 50: 274.0825, Accuracy: 0.8480\n",
      "Training loss (for one batch) at step 60: 288.4084, Accuracy: 0.8548\n",
      "Training loss (for one batch) at step 70: 314.1817, Accuracy: 0.8443\n",
      "Training loss (for one batch) at step 80: 301.1326, Accuracy: 0.8282\n",
      "Training loss (for one batch) at step 90: 279.7602, Accuracy: 0.8251\n",
      "Training loss (for one batch) at step 100: 280.4705, Accuracy: 0.8287\n",
      "Training loss (for one batch) at step 110: 316.1707, Accuracy: 0.8307\n",
      "---- Training ----\n",
      "Training loss: 91.3530\n",
      "Training acc over epoch: 0.8301\n",
      "---- Validation ----\n",
      "Validation loss: 37.8589\n",
      "Validation acc: 0.7735\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 309.4823, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 306.4190, Accuracy: 0.7486\n",
      "Training loss (for one batch) at step 20: 282.3048, Accuracy: 0.7801\n",
      "Training loss (for one batch) at step 30: 263.4144, Accuracy: 0.8130\n",
      "Training loss (for one batch) at step 40: 266.1139, Accuracy: 0.8335\n",
      "Training loss (for one batch) at step 50: 275.8649, Accuracy: 0.8434\n",
      "Training loss (for one batch) at step 60: 286.7984, Accuracy: 0.8503\n",
      "Training loss (for one batch) at step 70: 316.5114, Accuracy: 0.8380\n",
      "Training loss (for one batch) at step 80: 319.7007, Accuracy: 0.8241\n",
      "Training loss (for one batch) at step 90: 263.1223, Accuracy: 0.8225\n",
      "Training loss (for one batch) at step 100: 278.7318, Accuracy: 0.8280\n",
      "Training loss (for one batch) at step 110: 288.6955, Accuracy: 0.8290\n",
      "---- Training ----\n",
      "Training loss: 93.6065\n",
      "Training acc over epoch: 0.8285\n",
      "---- Validation ----\n",
      "Validation loss: 41.5951\n",
      "Validation acc: 0.7724\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 317.4858, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 303.6161, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 20: 294.2919, Accuracy: 0.7935\n",
      "Training loss (for one batch) at step 30: 290.3406, Accuracy: 0.8193\n",
      "Training loss (for one batch) at step 40: 284.4923, Accuracy: 0.8377\n",
      "Training loss (for one batch) at step 50: 295.4597, Accuracy: 0.8511\n",
      "Training loss (for one batch) at step 60: 294.3119, Accuracy: 0.8551\n",
      "Training loss (for one batch) at step 70: 317.1734, Accuracy: 0.8439\n",
      "Training loss (for one batch) at step 80: 316.5372, Accuracy: 0.8307\n",
      "Training loss (for one batch) at step 90: 290.9286, Accuracy: 0.8292\n",
      "Training loss (for one batch) at step 100: 270.9799, Accuracy: 0.8338\n",
      "Training loss (for one batch) at step 110: 284.5343, Accuracy: 0.8345\n",
      "---- Training ----\n",
      "Training loss: 98.2762\n",
      "Training acc over epoch: 0.8334\n",
      "---- Validation ----\n",
      "Validation loss: 40.1604\n",
      "Validation acc: 0.7563\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 341.2215, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 334.4091, Accuracy: 0.7358\n",
      "Training loss (for one batch) at step 20: 277.9656, Accuracy: 0.7772\n",
      "Training loss (for one batch) at step 30: 287.7793, Accuracy: 0.8133\n",
      "Training loss (for one batch) at step 40: 276.6035, Accuracy: 0.8335\n",
      "Training loss (for one batch) at step 50: 259.1036, Accuracy: 0.8476\n",
      "Training loss (for one batch) at step 60: 301.8568, Accuracy: 0.8532\n",
      "Training loss (for one batch) at step 70: 332.7013, Accuracy: 0.8398\n",
      "Training loss (for one batch) at step 80: 310.9799, Accuracy: 0.8261\n",
      "Training loss (for one batch) at step 90: 274.7678, Accuracy: 0.8235\n",
      "Training loss (for one batch) at step 100: 268.4157, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 110: 297.0298, Accuracy: 0.8288\n",
      "---- Training ----\n",
      "Training loss: 85.0013\n",
      "Training acc over epoch: 0.8285\n",
      "---- Validation ----\n",
      "Validation loss: 43.2475\n",
      "Validation acc: 0.7732\n",
      "Time taken: 10.16s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 346.7013, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 321.6675, Accuracy: 0.7507\n",
      "Training loss (for one batch) at step 20: 284.7083, Accuracy: 0.7872\n",
      "Training loss (for one batch) at step 30: 276.9652, Accuracy: 0.8150\n",
      "Training loss (for one batch) at step 40: 280.1596, Accuracy: 0.8316\n",
      "Training loss (for one batch) at step 50: 287.5107, Accuracy: 0.8457\n",
      "Training loss (for one batch) at step 60: 284.4917, Accuracy: 0.8527\n",
      "Training loss (for one batch) at step 70: 301.1485, Accuracy: 0.8392\n",
      "Training loss (for one batch) at step 80: 311.7669, Accuracy: 0.8247\n",
      "Training loss (for one batch) at step 90: 314.2822, Accuracy: 0.8219\n",
      "Training loss (for one batch) at step 100: 274.5754, Accuracy: 0.8273\n",
      "Training loss (for one batch) at step 110: 292.0804, Accuracy: 0.8291\n",
      "---- Training ----\n",
      "Training loss: 104.8891\n",
      "Training acc over epoch: 0.8286\n",
      "---- Validation ----\n",
      "Validation loss: 57.1963\n",
      "Validation acc: 0.7749\n",
      "Time taken: 10.68s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 321.2875, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 313.5817, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 20: 283.6456, Accuracy: 0.7943\n",
      "Training loss (for one batch) at step 30: 266.7451, Accuracy: 0.8223\n",
      "Training loss (for one batch) at step 40: 264.7609, Accuracy: 0.8380\n",
      "Training loss (for one batch) at step 50: 251.3899, Accuracy: 0.8525\n",
      "Training loss (for one batch) at step 60: 278.7470, Accuracy: 0.8573\n",
      "Training loss (for one batch) at step 70: 315.2325, Accuracy: 0.8446\n",
      "Training loss (for one batch) at step 80: 325.2720, Accuracy: 0.8320\n",
      "Training loss (for one batch) at step 90: 276.3203, Accuracy: 0.8286\n",
      "Training loss (for one batch) at step 100: 278.1935, Accuracy: 0.8304\n",
      "Training loss (for one batch) at step 110: 280.7586, Accuracy: 0.8325\n",
      "---- Training ----\n",
      "Training loss: 86.5722\n",
      "Training acc over epoch: 0.8317\n",
      "---- Validation ----\n",
      "Validation loss: 56.1630\n",
      "Validation acc: 0.7617\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 327.6305, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 318.9946, Accuracy: 0.7472\n",
      "Training loss (for one batch) at step 20: 287.1403, Accuracy: 0.7831\n",
      "Training loss (for one batch) at step 30: 275.1927, Accuracy: 0.8138\n",
      "Training loss (for one batch) at step 40: 262.5515, Accuracy: 0.8316\n",
      "Training loss (for one batch) at step 50: 268.7849, Accuracy: 0.8442\n",
      "Training loss (for one batch) at step 60: 303.2584, Accuracy: 0.8528\n",
      "Training loss (for one batch) at step 70: 286.9696, Accuracy: 0.8411\n",
      "Training loss (for one batch) at step 80: 297.0026, Accuracy: 0.8270\n",
      "Training loss (for one batch) at step 90: 287.2312, Accuracy: 0.8243\n",
      "Training loss (for one batch) at step 100: 281.0907, Accuracy: 0.8285\n",
      "Training loss (for one batch) at step 110: 292.6421, Accuracy: 0.8302\n",
      "---- Training ----\n",
      "Training loss: 91.6504\n",
      "Training acc over epoch: 0.8286\n",
      "---- Validation ----\n",
      "Validation loss: 48.9989\n",
      "Validation acc: 0.7646\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 315.1461, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 310.4874, Accuracy: 0.7585\n",
      "Training loss (for one batch) at step 20: 288.9855, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 30: 272.2464, Accuracy: 0.8193\n",
      "Training loss (for one batch) at step 40: 263.0427, Accuracy: 0.8346\n",
      "Training loss (for one batch) at step 50: 268.0461, Accuracy: 0.8485\n",
      "Training loss (for one batch) at step 60: 275.0041, Accuracy: 0.8545\n",
      "Training loss (for one batch) at step 70: 316.3039, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 80: 306.9005, Accuracy: 0.8305\n",
      "Training loss (for one batch) at step 90: 290.5355, Accuracy: 0.8254\n",
      "Training loss (for one batch) at step 100: 288.6334, Accuracy: 0.8291\n",
      "Training loss (for one batch) at step 110: 271.9719, Accuracy: 0.8314\n",
      "---- Training ----\n",
      "Training loss: 91.3892\n",
      "Training acc over epoch: 0.8311\n",
      "---- Validation ----\n",
      "Validation loss: 42.0001\n",
      "Validation acc: 0.7681\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 318.0497, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 320.6121, Accuracy: 0.7415\n",
      "Training loss (for one batch) at step 20: 295.8659, Accuracy: 0.7835\n",
      "Training loss (for one batch) at step 30: 292.9089, Accuracy: 0.8155\n",
      "Training loss (for one batch) at step 40: 253.5867, Accuracy: 0.8331\n",
      "Training loss (for one batch) at step 50: 259.5296, Accuracy: 0.8460\n",
      "Training loss (for one batch) at step 60: 297.1741, Accuracy: 0.8530\n",
      "Training loss (for one batch) at step 70: 303.6623, Accuracy: 0.8411\n",
      "Training loss (for one batch) at step 80: 324.9968, Accuracy: 0.8265\n",
      "Training loss (for one batch) at step 90: 291.5413, Accuracy: 0.8248\n",
      "Training loss (for one batch) at step 100: 274.1513, Accuracy: 0.8280\n",
      "Training loss (for one batch) at step 110: 290.9075, Accuracy: 0.8297\n",
      "---- Training ----\n",
      "Training loss: 82.7580\n",
      "Training acc over epoch: 0.8278\n",
      "---- Validation ----\n",
      "Validation loss: 61.7173\n",
      "Validation acc: 0.7636\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 330.2444, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 300.5263, Accuracy: 0.7443\n",
      "Training loss (for one batch) at step 20: 282.7460, Accuracy: 0.7786\n",
      "Training loss (for one batch) at step 30: 278.3980, Accuracy: 0.8122\n",
      "Training loss (for one batch) at step 40: 264.0153, Accuracy: 0.8323\n",
      "Training loss (for one batch) at step 50: 270.8849, Accuracy: 0.8468\n",
      "Training loss (for one batch) at step 60: 304.2344, Accuracy: 0.8537\n",
      "Training loss (for one batch) at step 70: 308.7607, Accuracy: 0.8423\n",
      "Training loss (for one batch) at step 80: 310.1859, Accuracy: 0.8272\n",
      "Training loss (for one batch) at step 90: 291.9781, Accuracy: 0.8241\n",
      "Training loss (for one batch) at step 100: 273.4222, Accuracy: 0.8289\n",
      "Training loss (for one batch) at step 110: 299.3181, Accuracy: 0.8315\n",
      "---- Training ----\n",
      "Training loss: 87.7118\n",
      "Training acc over epoch: 0.8301\n",
      "---- Validation ----\n",
      "Validation loss: 38.1642\n",
      "Validation acc: 0.7598\n",
      "Time taken: 10.14s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.7569\n",
      "Validation AUC: 0.7578\n",
      "Validation Balanced_ACC: 0.4848\n",
      "Validation MI: 0.1284\n",
      "Validation Normalized MI: 0.1864\n",
      "Validation Adjusted MI: 0.1864\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 600.0139, Accuracy: 0.4922\n",
      "Training loss (for one batch) at step 10: 590.8316, Accuracy: 0.5348\n",
      "Training loss (for one batch) at step 20: 560.1121, Accuracy: 0.5294\n",
      "Training loss (for one batch) at step 30: 546.3358, Accuracy: 0.5171\n",
      "Training loss (for one batch) at step 40: 517.0790, Accuracy: 0.5118\n",
      "Training loss (for one batch) at step 50: 508.0912, Accuracy: 0.5077\n",
      "Training loss (for one batch) at step 60: 507.1490, Accuracy: 0.5087\n",
      "Training loss (for one batch) at step 70: 493.8525, Accuracy: 0.5087\n",
      "Training loss (for one batch) at step 80: 492.0990, Accuracy: 0.5083\n",
      "Training loss (for one batch) at step 90: 468.1234, Accuracy: 0.5082\n",
      "Training loss (for one batch) at step 100: 468.4988, Accuracy: 0.5070\n",
      "Training loss (for one batch) at step 110: 461.0213, Accuracy: 0.5087\n",
      "---- Training ----\n",
      "Training loss: 147.7763\n",
      "Training acc over epoch: 0.5084\n",
      "---- Validation ----\n",
      "Validation loss: 34.7429\n",
      "Validation acc: 0.5134\n",
      "Time taken: 11.80s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 468.1673, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 472.0798, Accuracy: 0.5128\n",
      "Training loss (for one batch) at step 20: 465.2831, Accuracy: 0.5007\n",
      "Training loss (for one batch) at step 30: 458.4656, Accuracy: 0.4995\n",
      "Training loss (for one batch) at step 40: 457.3387, Accuracy: 0.5074\n",
      "Training loss (for one batch) at step 50: 450.2000, Accuracy: 0.5093\n",
      "Training loss (for one batch) at step 60: 454.6582, Accuracy: 0.5158\n",
      "Training loss (for one batch) at step 70: 449.6710, Accuracy: 0.5184\n",
      "Training loss (for one batch) at step 80: 445.7210, Accuracy: 0.5172\n",
      "Training loss (for one batch) at step 90: 458.3477, Accuracy: 0.5163\n",
      "Training loss (for one batch) at step 100: 450.6244, Accuracy: 0.5168\n",
      "Training loss (for one batch) at step 110: 451.4444, Accuracy: 0.5163\n",
      "---- Training ----\n",
      "Training loss: 139.5797\n",
      "Training acc over epoch: 0.5169\n",
      "---- Validation ----\n",
      "Validation loss: 34.3616\n",
      "Validation acc: 0.5134\n",
      "Time taken: 10.15s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 451.7689, Accuracy: 0.4219\n",
      "Training loss (for one batch) at step 10: 446.9929, Accuracy: 0.5099\n",
      "Training loss (for one batch) at step 20: 446.6544, Accuracy: 0.5119\n",
      "Training loss (for one batch) at step 30: 441.7336, Accuracy: 0.5151\n",
      "Training loss (for one batch) at step 40: 444.5818, Accuracy: 0.5154\n",
      "Training loss (for one batch) at step 50: 447.8230, Accuracy: 0.5149\n",
      "Training loss (for one batch) at step 60: 445.0687, Accuracy: 0.5182\n",
      "Training loss (for one batch) at step 70: 442.8147, Accuracy: 0.5176\n",
      "Training loss (for one batch) at step 80: 447.3970, Accuracy: 0.5198\n",
      "Training loss (for one batch) at step 90: 442.6445, Accuracy: 0.5237\n",
      "Training loss (for one batch) at step 100: 445.0948, Accuracy: 0.5250\n",
      "Training loss (for one batch) at step 110: 448.0407, Accuracy: 0.5260\n",
      "---- Training ----\n",
      "Training loss: 139.9258\n",
      "Training acc over epoch: 0.5260\n",
      "---- Validation ----\n",
      "Validation loss: 34.8746\n",
      "Validation acc: 0.5185\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 447.9345, Accuracy: 0.4844\n",
      "Training loss (for one batch) at step 10: 443.6956, Accuracy: 0.5334\n",
      "Training loss (for one batch) at step 20: 442.7908, Accuracy: 0.5472\n",
      "Training loss (for one batch) at step 30: 443.3200, Accuracy: 0.5451\n",
      "Training loss (for one batch) at step 40: 444.2297, Accuracy: 0.5351\n",
      "Training loss (for one batch) at step 50: 444.3977, Accuracy: 0.5389\n",
      "Training loss (for one batch) at step 60: 444.4037, Accuracy: 0.5397\n",
      "Training loss (for one batch) at step 70: 443.2505, Accuracy: 0.5450\n",
      "Training loss (for one batch) at step 80: 443.1033, Accuracy: 0.5502\n",
      "Training loss (for one batch) at step 90: 442.4578, Accuracy: 0.5497\n",
      "Training loss (for one batch) at step 100: 444.5964, Accuracy: 0.5510\n",
      "Training loss (for one batch) at step 110: 444.7890, Accuracy: 0.5533\n",
      "---- Training ----\n",
      "Training loss: 138.6541\n",
      "Training acc over epoch: 0.5538\n",
      "---- Validation ----\n",
      "Validation loss: 34.5501\n",
      "Validation acc: 0.5685\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 445.8590, Accuracy: 0.4922\n",
      "Training loss (for one batch) at step 10: 444.0259, Accuracy: 0.5348\n",
      "Training loss (for one batch) at step 20: 442.6039, Accuracy: 0.5361\n",
      "Training loss (for one batch) at step 30: 443.1053, Accuracy: 0.5524\n",
      "Training loss (for one batch) at step 40: 441.8745, Accuracy: 0.5495\n",
      "Training loss (for one batch) at step 50: 440.0437, Accuracy: 0.5509\n",
      "Training loss (for one batch) at step 60: 445.9299, Accuracy: 0.5589\n",
      "Training loss (for one batch) at step 70: 443.4835, Accuracy: 0.5649\n",
      "Training loss (for one batch) at step 80: 446.7664, Accuracy: 0.5663\n",
      "Training loss (for one batch) at step 90: 442.2840, Accuracy: 0.5634\n",
      "Training loss (for one batch) at step 100: 441.9522, Accuracy: 0.5664\n",
      "Training loss (for one batch) at step 110: 445.7421, Accuracy: 0.5690\n",
      "---- Training ----\n",
      "Training loss: 139.6550\n",
      "Training acc over epoch: 0.5699\n",
      "---- Validation ----\n",
      "Validation loss: 34.6872\n",
      "Validation acc: 0.5943\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 444.6592, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 443.9940, Accuracy: 0.5710\n",
      "Training loss (for one batch) at step 20: 442.1735, Accuracy: 0.5699\n",
      "Training loss (for one batch) at step 30: 445.3812, Accuracy: 0.5638\n",
      "Training loss (for one batch) at step 40: 439.5744, Accuracy: 0.5663\n",
      "Training loss (for one batch) at step 50: 440.0693, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 60: 440.8280, Accuracy: 0.5767\n",
      "Training loss (for one batch) at step 70: 445.7906, Accuracy: 0.5785\n",
      "Training loss (for one batch) at step 80: 441.8957, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 90: 442.2614, Accuracy: 0.5744\n",
      "Training loss (for one batch) at step 100: 442.2144, Accuracy: 0.5763\n",
      "Training loss (for one batch) at step 110: 440.9873, Accuracy: 0.5772\n",
      "---- Training ----\n",
      "Training loss: 138.2854\n",
      "Training acc over epoch: 0.5782\n",
      "---- Validation ----\n",
      "Validation loss: 34.7138\n",
      "Validation acc: 0.5841\n",
      "Time taken: 10.74s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 446.2700, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 442.3925, Accuracy: 0.5604\n",
      "Training loss (for one batch) at step 20: 442.7572, Accuracy: 0.5458\n",
      "Training loss (for one batch) at step 30: 437.5027, Accuracy: 0.5587\n",
      "Training loss (for one batch) at step 40: 441.8454, Accuracy: 0.5598\n",
      "Training loss (for one batch) at step 50: 434.5241, Accuracy: 0.5669\n",
      "Training loss (for one batch) at step 60: 447.9704, Accuracy: 0.5695\n",
      "Training loss (for one batch) at step 70: 441.5663, Accuracy: 0.5731\n",
      "Training loss (for one batch) at step 80: 439.1133, Accuracy: 0.5698\n",
      "Training loss (for one batch) at step 90: 443.1899, Accuracy: 0.5683\n",
      "Training loss (for one batch) at step 100: 437.7563, Accuracy: 0.5673\n",
      "Training loss (for one batch) at step 110: 440.1612, Accuracy: 0.5695\n",
      "---- Training ----\n",
      "Training loss: 141.8578\n",
      "Training acc over epoch: 0.5693\n",
      "---- Validation ----\n",
      "Validation loss: 34.9062\n",
      "Validation acc: 0.5916\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 443.0213, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 439.4445, Accuracy: 0.5774\n",
      "Training loss (for one batch) at step 20: 443.4760, Accuracy: 0.5584\n",
      "Training loss (for one batch) at step 30: 437.5231, Accuracy: 0.5628\n",
      "Training loss (for one batch) at step 40: 438.1138, Accuracy: 0.5675\n",
      "Training loss (for one batch) at step 50: 430.1154, Accuracy: 0.5743\n",
      "Training loss (for one batch) at step 60: 438.6995, Accuracy: 0.5790\n",
      "Training loss (for one batch) at step 70: 441.5609, Accuracy: 0.5823\n",
      "Training loss (for one batch) at step 80: 439.9249, Accuracy: 0.5807\n",
      "Training loss (for one batch) at step 90: 440.1203, Accuracy: 0.5783\n",
      "Training loss (for one batch) at step 100: 433.4965, Accuracy: 0.5777\n",
      "Training loss (for one batch) at step 110: 437.3548, Accuracy: 0.5796\n",
      "---- Training ----\n",
      "Training loss: 136.8403\n",
      "Training acc over epoch: 0.5785\n",
      "---- Validation ----\n",
      "Validation loss: 34.5500\n",
      "Validation acc: 0.6005\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 435.2901, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 437.4934, Accuracy: 0.5866\n",
      "Training loss (for one batch) at step 20: 441.9099, Accuracy: 0.5610\n",
      "Training loss (for one batch) at step 30: 435.1566, Accuracy: 0.5678\n",
      "Training loss (for one batch) at step 40: 436.1873, Accuracy: 0.5686\n",
      "Training loss (for one batch) at step 50: 435.8297, Accuracy: 0.5752\n",
      "Training loss (for one batch) at step 60: 440.6953, Accuracy: 0.5812\n",
      "Training loss (for one batch) at step 70: 437.6739, Accuracy: 0.5865\n",
      "Training loss (for one batch) at step 80: 436.3991, Accuracy: 0.5852\n",
      "Training loss (for one batch) at step 90: 437.8813, Accuracy: 0.5821\n",
      "Training loss (for one batch) at step 100: 432.1472, Accuracy: 0.5810\n",
      "Training loss (for one batch) at step 110: 437.6103, Accuracy: 0.5807\n",
      "---- Training ----\n",
      "Training loss: 137.9330\n",
      "Training acc over epoch: 0.5816\n",
      "---- Validation ----\n",
      "Validation loss: 34.1105\n",
      "Validation acc: 0.5868\n",
      "Time taken: 10.54s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 437.8532, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 437.7933, Accuracy: 0.5526\n",
      "Training loss (for one batch) at step 20: 439.8916, Accuracy: 0.5350\n",
      "Training loss (for one batch) at step 30: 439.3106, Accuracy: 0.5444\n",
      "Training loss (for one batch) at step 40: 434.3280, Accuracy: 0.5535\n",
      "Training loss (for one batch) at step 50: 435.1176, Accuracy: 0.5668\n",
      "Training loss (for one batch) at step 60: 436.4636, Accuracy: 0.5731\n",
      "Training loss (for one batch) at step 70: 434.1376, Accuracy: 0.5775\n",
      "Training loss (for one batch) at step 80: 432.4300, Accuracy: 0.5791\n",
      "Training loss (for one batch) at step 90: 435.7707, Accuracy: 0.5762\n",
      "Training loss (for one batch) at step 100: 434.6033, Accuracy: 0.5772\n",
      "Training loss (for one batch) at step 110: 437.6866, Accuracy: 0.5797\n",
      "---- Training ----\n",
      "Training loss: 136.0358\n",
      "Training acc over epoch: 0.5793\n",
      "---- Validation ----\n",
      "Validation loss: 33.6836\n",
      "Validation acc: 0.5672\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 438.3939, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 429.6862, Accuracy: 0.5653\n",
      "Training loss (for one batch) at step 20: 431.6527, Accuracy: 0.5744\n",
      "Training loss (for one batch) at step 30: 432.6771, Accuracy: 0.5837\n",
      "Training loss (for one batch) at step 40: 426.7643, Accuracy: 0.5911\n",
      "Training loss (for one batch) at step 50: 435.0202, Accuracy: 0.5967\n",
      "Training loss (for one batch) at step 60: 438.6117, Accuracy: 0.5987\n",
      "Training loss (for one batch) at step 70: 437.5697, Accuracy: 0.6059\n",
      "Training loss (for one batch) at step 80: 439.7672, Accuracy: 0.6044\n",
      "Training loss (for one batch) at step 90: 436.9757, Accuracy: 0.6023\n",
      "Training loss (for one batch) at step 100: 434.4755, Accuracy: 0.6004\n",
      "Training loss (for one batch) at step 110: 429.8388, Accuracy: 0.6005\n",
      "---- Training ----\n",
      "Training loss: 138.9102\n",
      "Training acc over epoch: 0.6006\n",
      "---- Validation ----\n",
      "Validation loss: 37.5791\n",
      "Validation acc: 0.5003\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 440.6414, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 436.7238, Accuracy: 0.5767\n",
      "Training loss (for one batch) at step 20: 426.6797, Accuracy: 0.5818\n",
      "Training loss (for one batch) at step 30: 424.5682, Accuracy: 0.5849\n",
      "Training loss (for one batch) at step 40: 434.5320, Accuracy: 0.5924\n",
      "Training loss (for one batch) at step 50: 433.9119, Accuracy: 0.5990\n",
      "Training loss (for one batch) at step 60: 426.6375, Accuracy: 0.6032\n",
      "Training loss (for one batch) at step 70: 438.4854, Accuracy: 0.6078\n",
      "Training loss (for one batch) at step 80: 439.6072, Accuracy: 0.6043\n",
      "Training loss (for one batch) at step 90: 432.3854, Accuracy: 0.6014\n",
      "Training loss (for one batch) at step 100: 433.9160, Accuracy: 0.6000\n",
      "Training loss (for one batch) at step 110: 423.5139, Accuracy: 0.6011\n",
      "---- Training ----\n",
      "Training loss: 134.7806\n",
      "Training acc over epoch: 0.6010\n",
      "---- Validation ----\n",
      "Validation loss: 32.2410\n",
      "Validation acc: 0.5924\n",
      "Time taken: 10.63s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 431.1143, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 429.4105, Accuracy: 0.5760\n",
      "Training loss (for one batch) at step 20: 433.0181, Accuracy: 0.5766\n",
      "Training loss (for one batch) at step 30: 433.7084, Accuracy: 0.5897\n",
      "Training loss (for one batch) at step 40: 420.8600, Accuracy: 0.5957\n",
      "Training loss (for one batch) at step 50: 428.9864, Accuracy: 0.6032\n",
      "Training loss (for one batch) at step 60: 435.4363, Accuracy: 0.6095\n",
      "Training loss (for one batch) at step 70: 429.1376, Accuracy: 0.6136\n",
      "Training loss (for one batch) at step 80: 434.4765, Accuracy: 0.6148\n",
      "Training loss (for one batch) at step 90: 434.8994, Accuracy: 0.6126\n",
      "Training loss (for one batch) at step 100: 424.9663, Accuracy: 0.6102\n",
      "Training loss (for one batch) at step 110: 429.0681, Accuracy: 0.6117\n",
      "---- Training ----\n",
      "Training loss: 132.1824\n",
      "Training acc over epoch: 0.6108\n",
      "---- Validation ----\n",
      "Validation loss: 36.9584\n",
      "Validation acc: 0.6032\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 432.7041, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 428.5954, Accuracy: 0.5852\n",
      "Training loss (for one batch) at step 20: 426.2189, Accuracy: 0.5990\n",
      "Training loss (for one batch) at step 30: 429.2982, Accuracy: 0.6026\n",
      "Training loss (for one batch) at step 40: 417.0480, Accuracy: 0.6122\n",
      "Training loss (for one batch) at step 50: 420.1758, Accuracy: 0.6210\n",
      "Training loss (for one batch) at step 60: 421.8577, Accuracy: 0.6291\n",
      "Training loss (for one batch) at step 70: 429.6765, Accuracy: 0.6334\n",
      "Training loss (for one batch) at step 80: 436.4292, Accuracy: 0.6310\n",
      "Training loss (for one batch) at step 90: 427.5682, Accuracy: 0.6263\n",
      "Training loss (for one batch) at step 100: 421.7879, Accuracy: 0.6240\n",
      "Training loss (for one batch) at step 110: 426.3961, Accuracy: 0.6237\n",
      "---- Training ----\n",
      "Training loss: 130.9994\n",
      "Training acc over epoch: 0.6232\n",
      "---- Validation ----\n",
      "Validation loss: 32.7229\n",
      "Validation acc: 0.6303\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 436.1447, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 429.6102, Accuracy: 0.6236\n",
      "Training loss (for one batch) at step 20: 427.2802, Accuracy: 0.6187\n",
      "Training loss (for one batch) at step 30: 421.1607, Accuracy: 0.6139\n",
      "Training loss (for one batch) at step 40: 420.4250, Accuracy: 0.6197\n",
      "Training loss (for one batch) at step 50: 416.0442, Accuracy: 0.6291\n",
      "Training loss (for one batch) at step 60: 411.9363, Accuracy: 0.6365\n",
      "Training loss (for one batch) at step 70: 425.2559, Accuracy: 0.6439\n",
      "Training loss (for one batch) at step 80: 433.7363, Accuracy: 0.6399\n",
      "Training loss (for one batch) at step 90: 423.0284, Accuracy: 0.6357\n",
      "Training loss (for one batch) at step 100: 424.5413, Accuracy: 0.6362\n",
      "Training loss (for one batch) at step 110: 424.0814, Accuracy: 0.6365\n",
      "---- Training ----\n",
      "Training loss: 135.4766\n",
      "Training acc over epoch: 0.6347\n",
      "---- Validation ----\n",
      "Validation loss: 34.8072\n",
      "Validation acc: 0.6322\n",
      "Time taken: 10.62s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 437.2611, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 428.3488, Accuracy: 0.6278\n",
      "Training loss (for one batch) at step 20: 428.5575, Accuracy: 0.6239\n",
      "Training loss (for one batch) at step 30: 434.2988, Accuracy: 0.6321\n",
      "Training loss (for one batch) at step 40: 428.2791, Accuracy: 0.6378\n",
      "Training loss (for one batch) at step 50: 414.3079, Accuracy: 0.6389\n",
      "Training loss (for one batch) at step 60: 419.1654, Accuracy: 0.6408\n",
      "Training loss (for one batch) at step 70: 434.8970, Accuracy: 0.6411\n",
      "Training loss (for one batch) at step 80: 427.9600, Accuracy: 0.6395\n",
      "Training loss (for one batch) at step 90: 425.2738, Accuracy: 0.6340\n",
      "Training loss (for one batch) at step 100: 420.5196, Accuracy: 0.6309\n",
      "Training loss (for one batch) at step 110: 427.4725, Accuracy: 0.6307\n",
      "---- Training ----\n",
      "Training loss: 133.4005\n",
      "Training acc over epoch: 0.6313\n",
      "---- Validation ----\n",
      "Validation loss: 33.4454\n",
      "Validation acc: 0.6295\n",
      "Time taken: 10.13s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 434.3109, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 424.7307, Accuracy: 0.6243\n",
      "Training loss (for one batch) at step 20: 420.2127, Accuracy: 0.6265\n",
      "Training loss (for one batch) at step 30: 422.5062, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 40: 410.5278, Accuracy: 0.6343\n",
      "Training loss (for one batch) at step 50: 399.4123, Accuracy: 0.6437\n",
      "Training loss (for one batch) at step 60: 412.1601, Accuracy: 0.6518\n",
      "Training loss (for one batch) at step 70: 413.7583, Accuracy: 0.6547\n",
      "Training loss (for one batch) at step 80: 424.6158, Accuracy: 0.6511\n",
      "Training loss (for one batch) at step 90: 421.1969, Accuracy: 0.6490\n",
      "Training loss (for one batch) at step 100: 415.9266, Accuracy: 0.6473\n",
      "Training loss (for one batch) at step 110: 429.7949, Accuracy: 0.6469\n",
      "---- Training ----\n",
      "Training loss: 130.2290\n",
      "Training acc over epoch: 0.6454\n",
      "---- Validation ----\n",
      "Validation loss: 36.2204\n",
      "Validation acc: 0.6445\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 423.7155, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 419.3299, Accuracy: 0.6349\n",
      "Training loss (for one batch) at step 20: 431.2827, Accuracy: 0.6283\n",
      "Training loss (for one batch) at step 30: 417.6040, Accuracy: 0.6381\n",
      "Training loss (for one batch) at step 40: 402.6603, Accuracy: 0.6441\n",
      "Training loss (for one batch) at step 50: 399.3087, Accuracy: 0.6509\n",
      "Training loss (for one batch) at step 60: 418.2111, Accuracy: 0.6574\n",
      "Training loss (for one batch) at step 70: 428.4061, Accuracy: 0.6591\n",
      "Training loss (for one batch) at step 80: 417.2461, Accuracy: 0.6534\n",
      "Training loss (for one batch) at step 90: 412.9025, Accuracy: 0.6497\n",
      "Training loss (for one batch) at step 100: 417.7356, Accuracy: 0.6489\n",
      "Training loss (for one batch) at step 110: 419.7408, Accuracy: 0.6482\n",
      "---- Training ----\n",
      "Training loss: 132.4311\n",
      "Training acc over epoch: 0.6477\n",
      "---- Validation ----\n",
      "Validation loss: 37.2281\n",
      "Validation acc: 0.6067\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 421.2570, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 424.4171, Accuracy: 0.5980\n",
      "Training loss (for one batch) at step 20: 415.2859, Accuracy: 0.6127\n",
      "Training loss (for one batch) at step 30: 412.9878, Accuracy: 0.6343\n",
      "Training loss (for one batch) at step 40: 400.9039, Accuracy: 0.6431\n",
      "Training loss (for one batch) at step 50: 395.9702, Accuracy: 0.6478\n",
      "Training loss (for one batch) at step 60: 402.0960, Accuracy: 0.6537\n",
      "Training loss (for one batch) at step 70: 402.3496, Accuracy: 0.6572\n",
      "Training loss (for one batch) at step 80: 411.8796, Accuracy: 0.6529\n",
      "Training loss (for one batch) at step 90: 418.8905, Accuracy: 0.6492\n",
      "Training loss (for one batch) at step 100: 413.8579, Accuracy: 0.6460\n",
      "Training loss (for one batch) at step 110: 421.9191, Accuracy: 0.6453\n",
      "---- Training ----\n",
      "Training loss: 140.7400\n",
      "Training acc over epoch: 0.6447\n",
      "---- Validation ----\n",
      "Validation loss: 34.2740\n",
      "Validation acc: 0.6145\n",
      "Time taken: 10.16s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 414.3486, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 432.8497, Accuracy: 0.6399\n",
      "Training loss (for one batch) at step 20: 413.7491, Accuracy: 0.6373\n",
      "Training loss (for one batch) at step 30: 404.6923, Accuracy: 0.6487\n",
      "Training loss (for one batch) at step 40: 400.4617, Accuracy: 0.6572\n",
      "Training loss (for one batch) at step 50: 385.2818, Accuracy: 0.6650\n",
      "Training loss (for one batch) at step 60: 410.8409, Accuracy: 0.6670\n",
      "Training loss (for one batch) at step 70: 408.3144, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 80: 417.8060, Accuracy: 0.6597\n",
      "Training loss (for one batch) at step 90: 407.8566, Accuracy: 0.6564\n",
      "Training loss (for one batch) at step 100: 413.6224, Accuracy: 0.6545\n",
      "Training loss (for one batch) at step 110: 420.2554, Accuracy: 0.6529\n",
      "---- Training ----\n",
      "Training loss: 128.8133\n",
      "Training acc over epoch: 0.6526\n",
      "---- Validation ----\n",
      "Validation loss: 32.8229\n",
      "Validation acc: 0.6354\n",
      "Time taken: 10.09s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 435.5570, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 409.6095, Accuracy: 0.6364\n",
      "Training loss (for one batch) at step 20: 408.7142, Accuracy: 0.6458\n",
      "Training loss (for one batch) at step 30: 397.0293, Accuracy: 0.6487\n",
      "Training loss (for one batch) at step 40: 399.3954, Accuracy: 0.6629\n",
      "Training loss (for one batch) at step 50: 388.7315, Accuracy: 0.6687\n",
      "Training loss (for one batch) at step 60: 396.8629, Accuracy: 0.6762\n",
      "Training loss (for one batch) at step 70: 418.7170, Accuracy: 0.6739\n",
      "Training loss (for one batch) at step 80: 418.9204, Accuracy: 0.6673\n",
      "Training loss (for one batch) at step 90: 411.4630, Accuracy: 0.6636\n",
      "Training loss (for one batch) at step 100: 394.6168, Accuracy: 0.6613\n",
      "Training loss (for one batch) at step 110: 398.2404, Accuracy: 0.6591\n",
      "---- Training ----\n",
      "Training loss: 129.5435\n",
      "Training acc over epoch: 0.6589\n",
      "---- Validation ----\n",
      "Validation loss: 39.9857\n",
      "Validation acc: 0.6279\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 417.9172, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 412.4153, Accuracy: 0.6300\n",
      "Training loss (for one batch) at step 20: 402.0815, Accuracy: 0.6324\n",
      "Training loss (for one batch) at step 30: 404.7749, Accuracy: 0.6429\n",
      "Training loss (for one batch) at step 40: 389.2791, Accuracy: 0.6482\n",
      "Training loss (for one batch) at step 50: 384.1857, Accuracy: 0.6578\n",
      "Training loss (for one batch) at step 60: 387.8524, Accuracy: 0.6624\n",
      "Training loss (for one batch) at step 70: 405.0863, Accuracy: 0.6640\n",
      "Training loss (for one batch) at step 80: 407.7470, Accuracy: 0.6608\n",
      "Training loss (for one batch) at step 90: 390.1440, Accuracy: 0.6572\n",
      "Training loss (for one batch) at step 100: 392.9096, Accuracy: 0.6563\n",
      "Training loss (for one batch) at step 110: 414.7111, Accuracy: 0.6562\n",
      "---- Training ----\n",
      "Training loss: 120.1698\n",
      "Training acc over epoch: 0.6552\n",
      "---- Validation ----\n",
      "Validation loss: 33.2796\n",
      "Validation acc: 0.5852\n",
      "Time taken: 10.16s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 409.3133, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 422.5466, Accuracy: 0.6449\n",
      "Training loss (for one batch) at step 20: 410.3938, Accuracy: 0.6514\n",
      "Training loss (for one batch) at step 30: 389.5762, Accuracy: 0.6610\n",
      "Training loss (for one batch) at step 40: 381.8219, Accuracy: 0.6627\n",
      "Training loss (for one batch) at step 50: 385.8747, Accuracy: 0.6694\n",
      "Training loss (for one batch) at step 60: 392.6772, Accuracy: 0.6733\n",
      "Training loss (for one batch) at step 70: 420.8378, Accuracy: 0.6700\n",
      "Training loss (for one batch) at step 80: 418.7168, Accuracy: 0.6671\n",
      "Training loss (for one batch) at step 90: 415.3265, Accuracy: 0.6639\n",
      "Training loss (for one batch) at step 100: 402.2493, Accuracy: 0.6626\n",
      "Training loss (for one batch) at step 110: 408.9010, Accuracy: 0.6604\n",
      "---- Training ----\n",
      "Training loss: 123.7222\n",
      "Training acc over epoch: 0.6598\n",
      "---- Validation ----\n",
      "Validation loss: 39.5492\n",
      "Validation acc: 0.6107\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 414.4966, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 410.7089, Accuracy: 0.6342\n",
      "Training loss (for one batch) at step 20: 404.6716, Accuracy: 0.6425\n",
      "Training loss (for one batch) at step 30: 385.8737, Accuracy: 0.6595\n",
      "Training loss (for one batch) at step 40: 389.1552, Accuracy: 0.6700\n",
      "Training loss (for one batch) at step 50: 387.3359, Accuracy: 0.6777\n",
      "Training loss (for one batch) at step 60: 390.3586, Accuracy: 0.6803\n",
      "Training loss (for one batch) at step 70: 404.6852, Accuracy: 0.6788\n",
      "Training loss (for one batch) at step 80: 415.8372, Accuracy: 0.6721\n",
      "Training loss (for one batch) at step 90: 391.6235, Accuracy: 0.6689\n",
      "Training loss (for one batch) at step 100: 396.4851, Accuracy: 0.6696\n",
      "Training loss (for one batch) at step 110: 395.7031, Accuracy: 0.6684\n",
      "---- Training ----\n",
      "Training loss: 128.1189\n",
      "Training acc over epoch: 0.6672\n",
      "---- Validation ----\n",
      "Validation loss: 31.1256\n",
      "Validation acc: 0.6118\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 403.8550, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 403.3275, Accuracy: 0.6342\n",
      "Training loss (for one batch) at step 20: 394.3396, Accuracy: 0.6347\n",
      "Training loss (for one batch) at step 30: 383.0115, Accuracy: 0.6510\n",
      "Training loss (for one batch) at step 40: 377.1818, Accuracy: 0.6637\n",
      "Training loss (for one batch) at step 50: 377.6471, Accuracy: 0.6739\n",
      "Training loss (for one batch) at step 60: 375.3704, Accuracy: 0.6782\n",
      "Training loss (for one batch) at step 70: 415.1719, Accuracy: 0.6773\n",
      "Training loss (for one batch) at step 80: 407.8836, Accuracy: 0.6736\n",
      "Training loss (for one batch) at step 90: 394.4811, Accuracy: 0.6705\n",
      "Training loss (for one batch) at step 100: 383.8157, Accuracy: 0.6677\n",
      "Training loss (for one batch) at step 110: 387.8522, Accuracy: 0.6668\n",
      "---- Training ----\n",
      "Training loss: 132.9658\n",
      "Training acc over epoch: 0.6664\n",
      "---- Validation ----\n",
      "Validation loss: 39.5104\n",
      "Validation acc: 0.6411\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 404.5721, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 411.2682, Accuracy: 0.6420\n",
      "Training loss (for one batch) at step 20: 394.8591, Accuracy: 0.6481\n",
      "Training loss (for one batch) at step 30: 373.8317, Accuracy: 0.6573\n",
      "Training loss (for one batch) at step 40: 374.7644, Accuracy: 0.6665\n",
      "Training loss (for one batch) at step 50: 350.8650, Accuracy: 0.6771\n",
      "Training loss (for one batch) at step 60: 397.4823, Accuracy: 0.6842\n",
      "Training loss (for one batch) at step 70: 392.3469, Accuracy: 0.6778\n",
      "Training loss (for one batch) at step 80: 393.2115, Accuracy: 0.6744\n",
      "Training loss (for one batch) at step 90: 373.8909, Accuracy: 0.6704\n",
      "Training loss (for one batch) at step 100: 368.0524, Accuracy: 0.6679\n",
      "Training loss (for one batch) at step 110: 381.0447, Accuracy: 0.6668\n",
      "---- Training ----\n",
      "Training loss: 128.1023\n",
      "Training acc over epoch: 0.6668\n",
      "---- Validation ----\n",
      "Validation loss: 45.3682\n",
      "Validation acc: 0.6263\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 403.0351, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 388.5545, Accuracy: 0.6456\n",
      "Training loss (for one batch) at step 20: 371.7460, Accuracy: 0.6637\n",
      "Training loss (for one batch) at step 30: 380.7051, Accuracy: 0.6658\n",
      "Training loss (for one batch) at step 40: 362.5352, Accuracy: 0.6744\n",
      "Training loss (for one batch) at step 50: 366.9151, Accuracy: 0.6817\n",
      "Training loss (for one batch) at step 60: 370.3307, Accuracy: 0.6849\n",
      "Training loss (for one batch) at step 70: 378.9742, Accuracy: 0.6839\n",
      "Training loss (for one batch) at step 80: 402.8178, Accuracy: 0.6777\n",
      "Training loss (for one batch) at step 90: 374.2076, Accuracy: 0.6742\n",
      "Training loss (for one batch) at step 100: 387.0943, Accuracy: 0.6724\n",
      "Training loss (for one batch) at step 110: 385.7337, Accuracy: 0.6704\n",
      "---- Training ----\n",
      "Training loss: 121.9374\n",
      "Training acc over epoch: 0.6683\n",
      "---- Validation ----\n",
      "Validation loss: 47.6571\n",
      "Validation acc: 0.5959\n",
      "Time taken: 10.67s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 393.8538, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 380.5498, Accuracy: 0.6271\n",
      "Training loss (for one batch) at step 20: 389.0656, Accuracy: 0.6440\n",
      "Training loss (for one batch) at step 30: 360.3091, Accuracy: 0.6590\n",
      "Training loss (for one batch) at step 40: 364.2736, Accuracy: 0.6723\n",
      "Training loss (for one batch) at step 50: 357.8885, Accuracy: 0.6794\n",
      "Training loss (for one batch) at step 60: 366.1779, Accuracy: 0.6857\n",
      "Training loss (for one batch) at step 70: 395.0525, Accuracy: 0.6853\n",
      "Training loss (for one batch) at step 80: 407.9165, Accuracy: 0.6796\n",
      "Training loss (for one batch) at step 90: 378.8745, Accuracy: 0.6775\n",
      "Training loss (for one batch) at step 100: 384.0199, Accuracy: 0.6747\n",
      "Training loss (for one batch) at step 110: 377.4032, Accuracy: 0.6726\n",
      "---- Training ----\n",
      "Training loss: 116.6145\n",
      "Training acc over epoch: 0.6721\n",
      "---- Validation ----\n",
      "Validation loss: 37.1720\n",
      "Validation acc: 0.5994\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 408.1551, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 385.8975, Accuracy: 0.6243\n",
      "Training loss (for one batch) at step 20: 368.9922, Accuracy: 0.6473\n",
      "Training loss (for one batch) at step 30: 382.5770, Accuracy: 0.6643\n",
      "Training loss (for one batch) at step 40: 353.9431, Accuracy: 0.6732\n",
      "Training loss (for one batch) at step 50: 362.9976, Accuracy: 0.6815\n",
      "Training loss (for one batch) at step 60: 365.0033, Accuracy: 0.6861\n",
      "Training loss (for one batch) at step 70: 391.6216, Accuracy: 0.6831\n",
      "Training loss (for one batch) at step 80: 384.2987, Accuracy: 0.6771\n",
      "Training loss (for one batch) at step 90: 377.0867, Accuracy: 0.6741\n",
      "Training loss (for one batch) at step 100: 366.3763, Accuracy: 0.6745\n",
      "Training loss (for one batch) at step 110: 375.6302, Accuracy: 0.6737\n",
      "---- Training ----\n",
      "Training loss: 127.2606\n",
      "Training acc over epoch: 0.6728\n",
      "---- Validation ----\n",
      "Validation loss: 38.1567\n",
      "Validation acc: 0.6236\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 402.2588, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 383.4862, Accuracy: 0.6470\n",
      "Training loss (for one batch) at step 20: 373.5386, Accuracy: 0.6518\n",
      "Training loss (for one batch) at step 30: 362.0545, Accuracy: 0.6658\n",
      "Training loss (for one batch) at step 40: 333.5674, Accuracy: 0.6764\n",
      "Training loss (for one batch) at step 50: 332.5010, Accuracy: 0.6838\n",
      "Training loss (for one batch) at step 60: 364.8021, Accuracy: 0.6880\n",
      "Training loss (for one batch) at step 70: 382.8613, Accuracy: 0.6852\n",
      "Training loss (for one batch) at step 80: 382.4257, Accuracy: 0.6786\n",
      "Training loss (for one batch) at step 90: 374.1652, Accuracy: 0.6750\n",
      "Training loss (for one batch) at step 100: 361.8262, Accuracy: 0.6754\n",
      "Training loss (for one batch) at step 110: 357.4009, Accuracy: 0.6747\n",
      "---- Training ----\n",
      "Training loss: 117.2282\n",
      "Training acc over epoch: 0.6726\n",
      "---- Validation ----\n",
      "Validation loss: 36.5061\n",
      "Validation acc: 0.6214\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 413.1540, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 382.0131, Accuracy: 0.6222\n",
      "Training loss (for one batch) at step 20: 372.1753, Accuracy: 0.6473\n",
      "Training loss (for one batch) at step 30: 358.5905, Accuracy: 0.6656\n",
      "Training loss (for one batch) at step 40: 349.3932, Accuracy: 0.6715\n",
      "Training loss (for one batch) at step 50: 354.2976, Accuracy: 0.6811\n",
      "Training loss (for one batch) at step 60: 362.9923, Accuracy: 0.6857\n",
      "Training loss (for one batch) at step 70: 357.1995, Accuracy: 0.6810\n",
      "Training loss (for one batch) at step 80: 397.2720, Accuracy: 0.6748\n",
      "Training loss (for one batch) at step 90: 363.9032, Accuracy: 0.6723\n",
      "Training loss (for one batch) at step 100: 375.6714, Accuracy: 0.6716\n",
      "Training loss (for one batch) at step 110: 376.4888, Accuracy: 0.6712\n",
      "---- Training ----\n",
      "Training loss: 118.0882\n",
      "Training acc over epoch: 0.6699\n",
      "---- Validation ----\n",
      "Validation loss: 43.8632\n",
      "Validation acc: 0.6274\n",
      "Time taken: 10.16s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 393.9048, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 379.5867, Accuracy: 0.6385\n",
      "Training loss (for one batch) at step 20: 365.4737, Accuracy: 0.6466\n",
      "Training loss (for one batch) at step 30: 349.9306, Accuracy: 0.6678\n",
      "Training loss (for one batch) at step 40: 350.8889, Accuracy: 0.6753\n",
      "Training loss (for one batch) at step 50: 341.4777, Accuracy: 0.6864\n",
      "Training loss (for one batch) at step 60: 348.0056, Accuracy: 0.6899\n",
      "Training loss (for one batch) at step 70: 354.6617, Accuracy: 0.6857\n",
      "Training loss (for one batch) at step 80: 373.0978, Accuracy: 0.6807\n",
      "Training loss (for one batch) at step 90: 352.9841, Accuracy: 0.6765\n",
      "Training loss (for one batch) at step 100: 364.5681, Accuracy: 0.6771\n",
      "Training loss (for one batch) at step 110: 357.1318, Accuracy: 0.6776\n",
      "---- Training ----\n",
      "Training loss: 117.7528\n",
      "Training acc over epoch: 0.6748\n",
      "---- Validation ----\n",
      "Validation loss: 41.6019\n",
      "Validation acc: 0.6378\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 384.4238, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 365.8990, Accuracy: 0.6371\n",
      "Training loss (for one batch) at step 20: 354.6505, Accuracy: 0.6522\n",
      "Training loss (for one batch) at step 30: 359.0285, Accuracy: 0.6699\n",
      "Training loss (for one batch) at step 40: 339.1395, Accuracy: 0.6778\n",
      "Training loss (for one batch) at step 50: 342.5193, Accuracy: 0.6883\n",
      "Training loss (for one batch) at step 60: 346.5300, Accuracy: 0.6936\n",
      "Training loss (for one batch) at step 70: 360.4304, Accuracy: 0.6878\n",
      "Training loss (for one batch) at step 80: 367.8112, Accuracy: 0.6830\n",
      "Training loss (for one batch) at step 90: 348.8382, Accuracy: 0.6791\n",
      "Training loss (for one batch) at step 100: 368.8869, Accuracy: 0.6779\n",
      "Training loss (for one batch) at step 110: 361.5378, Accuracy: 0.6772\n",
      "---- Training ----\n",
      "Training loss: 111.4951\n",
      "Training acc over epoch: 0.6759\n",
      "---- Validation ----\n",
      "Validation loss: 44.2061\n",
      "Validation acc: 0.6102\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 358.2341, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 366.1884, Accuracy: 0.6456\n",
      "Training loss (for one batch) at step 20: 348.6675, Accuracy: 0.6603\n",
      "Training loss (for one batch) at step 30: 355.1310, Accuracy: 0.6749\n",
      "Training loss (for one batch) at step 40: 341.8820, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 50: 325.0333, Accuracy: 0.6864\n",
      "Training loss (for one batch) at step 60: 348.5453, Accuracy: 0.6916\n",
      "Training loss (for one batch) at step 70: 352.7917, Accuracy: 0.6851\n",
      "Training loss (for one batch) at step 80: 366.1562, Accuracy: 0.6796\n",
      "Training loss (for one batch) at step 90: 359.1447, Accuracy: 0.6771\n",
      "Training loss (for one batch) at step 100: 337.3667, Accuracy: 0.6738\n",
      "Training loss (for one batch) at step 110: 349.8063, Accuracy: 0.6743\n",
      "---- Training ----\n",
      "Training loss: 118.6055\n",
      "Training acc over epoch: 0.6734\n",
      "---- Validation ----\n",
      "Validation loss: 29.7625\n",
      "Validation acc: 0.6314\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 386.9702, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 364.7943, Accuracy: 0.6271\n",
      "Training loss (for one batch) at step 20: 359.3517, Accuracy: 0.6566\n",
      "Training loss (for one batch) at step 30: 353.1123, Accuracy: 0.6678\n",
      "Training loss (for one batch) at step 40: 326.9692, Accuracy: 0.6820\n",
      "Training loss (for one batch) at step 50: 340.4058, Accuracy: 0.6889\n",
      "Training loss (for one batch) at step 60: 345.9465, Accuracy: 0.6919\n",
      "Training loss (for one batch) at step 70: 358.3379, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 80: 370.8000, Accuracy: 0.6807\n",
      "Training loss (for one batch) at step 90: 356.5653, Accuracy: 0.6759\n",
      "Training loss (for one batch) at step 100: 338.0693, Accuracy: 0.6768\n",
      "Training loss (for one batch) at step 110: 342.0468, Accuracy: 0.6756\n",
      "---- Training ----\n",
      "Training loss: 108.8008\n",
      "Training acc over epoch: 0.6738\n",
      "---- Validation ----\n",
      "Validation loss: 33.2380\n",
      "Validation acc: 0.6123\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 375.1866, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 362.4404, Accuracy: 0.6428\n",
      "Training loss (for one batch) at step 20: 353.9966, Accuracy: 0.6536\n",
      "Training loss (for one batch) at step 30: 347.0553, Accuracy: 0.6694\n",
      "Training loss (for one batch) at step 40: 332.1822, Accuracy: 0.6795\n",
      "Training loss (for one batch) at step 50: 335.1402, Accuracy: 0.6886\n",
      "Training loss (for one batch) at step 60: 349.1989, Accuracy: 0.6951\n",
      "Training loss (for one batch) at step 70: 363.3741, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 80: 351.9019, Accuracy: 0.6799\n",
      "Training loss (for one batch) at step 90: 327.8964, Accuracy: 0.6780\n",
      "Training loss (for one batch) at step 100: 339.1620, Accuracy: 0.6795\n",
      "Training loss (for one batch) at step 110: 350.8603, Accuracy: 0.6769\n",
      "---- Training ----\n",
      "Training loss: 108.5767\n",
      "Training acc over epoch: 0.6754\n",
      "---- Validation ----\n",
      "Validation loss: 61.7074\n",
      "Validation acc: 0.6341\n",
      "Time taken: 10.51s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 359.9557, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 366.9574, Accuracy: 0.6186\n",
      "Training loss (for one batch) at step 20: 328.6505, Accuracy: 0.6525\n",
      "Training loss (for one batch) at step 30: 339.0661, Accuracy: 0.6653\n",
      "Training loss (for one batch) at step 40: 331.7753, Accuracy: 0.6795\n",
      "Training loss (for one batch) at step 50: 309.0990, Accuracy: 0.6884\n",
      "Training loss (for one batch) at step 60: 376.5737, Accuracy: 0.6908\n",
      "Training loss (for one batch) at step 70: 340.8806, Accuracy: 0.6879\n",
      "Training loss (for one batch) at step 80: 372.8230, Accuracy: 0.6779\n",
      "Training loss (for one batch) at step 90: 348.1661, Accuracy: 0.6761\n",
      "Training loss (for one batch) at step 100: 317.6948, Accuracy: 0.6761\n",
      "Training loss (for one batch) at step 110: 366.7637, Accuracy: 0.6738\n",
      "---- Training ----\n",
      "Training loss: 106.3367\n",
      "Training acc over epoch: 0.6728\n",
      "---- Validation ----\n",
      "Validation loss: 38.7309\n",
      "Validation acc: 0.6284\n",
      "Time taken: 12.56s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 366.0461, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 369.2593, Accuracy: 0.6435\n",
      "Training loss (for one batch) at step 20: 354.9988, Accuracy: 0.6544\n",
      "Training loss (for one batch) at step 30: 332.2871, Accuracy: 0.6741\n",
      "Training loss (for one batch) at step 40: 315.1551, Accuracy: 0.6806\n",
      "Training loss (for one batch) at step 50: 329.7893, Accuracy: 0.6892\n",
      "Training loss (for one batch) at step 60: 331.9333, Accuracy: 0.6913\n",
      "Training loss (for one batch) at step 70: 368.2532, Accuracy: 0.6888\n",
      "Training loss (for one batch) at step 80: 389.8891, Accuracy: 0.6811\n",
      "Training loss (for one batch) at step 90: 346.2167, Accuracy: 0.6791\n",
      "Training loss (for one batch) at step 100: 334.4036, Accuracy: 0.6795\n",
      "Training loss (for one batch) at step 110: 344.5419, Accuracy: 0.6772\n",
      "---- Training ----\n",
      "Training loss: 111.6013\n",
      "Training acc over epoch: 0.6768\n",
      "---- Validation ----\n",
      "Validation loss: 46.8145\n",
      "Validation acc: 0.6166\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 393.4950, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 349.5958, Accuracy: 0.6129\n",
      "Training loss (for one batch) at step 20: 333.9145, Accuracy: 0.6380\n",
      "Training loss (for one batch) at step 30: 329.5641, Accuracy: 0.6608\n",
      "Training loss (for one batch) at step 40: 326.9389, Accuracy: 0.6723\n",
      "Training loss (for one batch) at step 50: 334.4321, Accuracy: 0.6843\n",
      "Training loss (for one batch) at step 60: 337.2452, Accuracy: 0.6910\n",
      "Training loss (for one batch) at step 70: 347.5963, Accuracy: 0.6847\n",
      "Training loss (for one batch) at step 80: 364.4396, Accuracy: 0.6752\n",
      "Training loss (for one batch) at step 90: 349.3842, Accuracy: 0.6745\n",
      "Training loss (for one batch) at step 100: 348.7194, Accuracy: 0.6739\n",
      "Training loss (for one batch) at step 110: 354.2382, Accuracy: 0.6726\n",
      "---- Training ----\n",
      "Training loss: 100.8751\n",
      "Training acc over epoch: 0.6724\n",
      "---- Validation ----\n",
      "Validation loss: 43.3751\n",
      "Validation acc: 0.6118\n",
      "Time taken: 10.75s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 355.9536, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 338.8976, Accuracy: 0.6342\n",
      "Training loss (for one batch) at step 20: 340.9955, Accuracy: 0.6540\n",
      "Training loss (for one batch) at step 30: 323.9208, Accuracy: 0.6757\n",
      "Training loss (for one batch) at step 40: 322.2977, Accuracy: 0.6831\n",
      "Training loss (for one batch) at step 50: 321.7977, Accuracy: 0.6916\n",
      "Training loss (for one batch) at step 60: 332.1203, Accuracy: 0.6939\n",
      "Training loss (for one batch) at step 70: 348.1182, Accuracy: 0.6894\n",
      "Training loss (for one batch) at step 80: 368.5736, Accuracy: 0.6822\n",
      "Training loss (for one batch) at step 90: 330.2954, Accuracy: 0.6805\n",
      "Training loss (for one batch) at step 100: 334.4213, Accuracy: 0.6801\n",
      "Training loss (for one batch) at step 110: 345.9924, Accuracy: 0.6792\n",
      "---- Training ----\n",
      "Training loss: 119.1663\n",
      "Training acc over epoch: 0.6781\n",
      "---- Validation ----\n",
      "Validation loss: 39.7458\n",
      "Validation acc: 0.6212\n",
      "Time taken: 10.10s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 365.8932, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 334.5517, Accuracy: 0.6342\n",
      "Training loss (for one batch) at step 20: 339.4781, Accuracy: 0.6525\n",
      "Training loss (for one batch) at step 30: 357.3727, Accuracy: 0.6689\n",
      "Training loss (for one batch) at step 40: 305.8597, Accuracy: 0.6846\n",
      "Training loss (for one batch) at step 50: 337.4882, Accuracy: 0.6935\n",
      "Training loss (for one batch) at step 60: 330.5936, Accuracy: 0.6972\n",
      "Training loss (for one batch) at step 70: 347.1264, Accuracy: 0.6918\n",
      "Training loss (for one batch) at step 80: 339.8624, Accuracy: 0.6837\n",
      "Training loss (for one batch) at step 90: 347.3029, Accuracy: 0.6800\n",
      "Training loss (for one batch) at step 100: 322.2349, Accuracy: 0.6814\n",
      "Training loss (for one batch) at step 110: 359.2849, Accuracy: 0.6807\n",
      "---- Training ----\n",
      "Training loss: 117.1123\n",
      "Training acc over epoch: 0.6800\n",
      "---- Validation ----\n",
      "Validation loss: 39.4136\n",
      "Validation acc: 0.6263\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 365.1777, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 344.7493, Accuracy: 0.6165\n",
      "Training loss (for one batch) at step 20: 323.9781, Accuracy: 0.6522\n",
      "Training loss (for one batch) at step 30: 316.0366, Accuracy: 0.6729\n",
      "Training loss (for one batch) at step 40: 325.4291, Accuracy: 0.6867\n",
      "Training loss (for one batch) at step 50: 307.4768, Accuracy: 0.6936\n",
      "Training loss (for one batch) at step 60: 339.7624, Accuracy: 0.6976\n",
      "Training loss (for one batch) at step 70: 330.0567, Accuracy: 0.6903\n",
      "Training loss (for one batch) at step 80: 355.4340, Accuracy: 0.6815\n",
      "Training loss (for one batch) at step 90: 347.5804, Accuracy: 0.6775\n",
      "Training loss (for one batch) at step 100: 322.0109, Accuracy: 0.6784\n",
      "Training loss (for one batch) at step 110: 325.6354, Accuracy: 0.6771\n",
      "---- Training ----\n",
      "Training loss: 113.5990\n",
      "Training acc over epoch: 0.6770\n",
      "---- Validation ----\n",
      "Validation loss: 50.6271\n",
      "Validation acc: 0.6357\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 353.4237, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 361.8890, Accuracy: 0.6349\n",
      "Training loss (for one batch) at step 20: 322.3982, Accuracy: 0.6488\n",
      "Training loss (for one batch) at step 30: 323.4553, Accuracy: 0.6734\n",
      "Training loss (for one batch) at step 40: 303.9241, Accuracy: 0.6881\n",
      "Training loss (for one batch) at step 50: 323.7861, Accuracy: 0.6941\n",
      "Training loss (for one batch) at step 60: 328.9056, Accuracy: 0.6974\n",
      "Training loss (for one batch) at step 70: 339.9518, Accuracy: 0.6909\n",
      "Training loss (for one batch) at step 80: 340.4486, Accuracy: 0.6827\n",
      "Training loss (for one batch) at step 90: 338.7389, Accuracy: 0.6806\n",
      "Training loss (for one batch) at step 100: 335.4193, Accuracy: 0.6798\n",
      "Training loss (for one batch) at step 110: 346.3743, Accuracy: 0.6778\n",
      "---- Training ----\n",
      "Training loss: 113.3899\n",
      "Training acc over epoch: 0.6769\n",
      "---- Validation ----\n",
      "Validation loss: 37.3392\n",
      "Validation acc: 0.6303\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 352.5980, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 359.0381, Accuracy: 0.6300\n",
      "Training loss (for one batch) at step 20: 331.7964, Accuracy: 0.6555\n",
      "Training loss (for one batch) at step 30: 329.0478, Accuracy: 0.6762\n",
      "Training loss (for one batch) at step 40: 321.9294, Accuracy: 0.6879\n",
      "Training loss (for one batch) at step 50: 312.8529, Accuracy: 0.6944\n",
      "Training loss (for one batch) at step 60: 340.6151, Accuracy: 0.6956\n",
      "Training loss (for one batch) at step 70: 333.9182, Accuracy: 0.6903\n",
      "Training loss (for one batch) at step 80: 352.4228, Accuracy: 0.6815\n",
      "Training loss (for one batch) at step 90: 315.2005, Accuracy: 0.6805\n",
      "Training loss (for one batch) at step 100: 319.1514, Accuracy: 0.6808\n",
      "Training loss (for one batch) at step 110: 333.7840, Accuracy: 0.6803\n",
      "---- Training ----\n",
      "Training loss: 101.5762\n",
      "Training acc over epoch: 0.6801\n",
      "---- Validation ----\n",
      "Validation loss: 40.8763\n",
      "Validation acc: 0.6204\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 349.6313, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 357.6857, Accuracy: 0.6364\n",
      "Training loss (for one batch) at step 20: 317.2681, Accuracy: 0.6574\n",
      "Training loss (for one batch) at step 30: 309.9878, Accuracy: 0.6772\n",
      "Training loss (for one batch) at step 40: 301.6497, Accuracy: 0.6877\n",
      "Training loss (for one batch) at step 50: 294.2119, Accuracy: 0.6972\n",
      "Training loss (for one batch) at step 60: 312.6015, Accuracy: 0.7021\n",
      "Training loss (for one batch) at step 70: 351.1087, Accuracy: 0.6950\n",
      "Training loss (for one batch) at step 80: 345.4830, Accuracy: 0.6836\n",
      "Training loss (for one batch) at step 90: 331.6507, Accuracy: 0.6816\n",
      "Training loss (for one batch) at step 100: 328.3957, Accuracy: 0.6805\n",
      "Training loss (for one batch) at step 110: 346.5866, Accuracy: 0.6815\n",
      "---- Training ----\n",
      "Training loss: 98.9845\n",
      "Training acc over epoch: 0.6803\n",
      "---- Validation ----\n",
      "Validation loss: 41.3367\n",
      "Validation acc: 0.6209\n",
      "Time taken: 10.60s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 353.8058, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 330.5385, Accuracy: 0.6349\n",
      "Training loss (for one batch) at step 20: 327.6872, Accuracy: 0.6510\n",
      "Training loss (for one batch) at step 30: 310.4543, Accuracy: 0.6681\n",
      "Training loss (for one batch) at step 40: 294.5372, Accuracy: 0.6810\n",
      "Training loss (for one batch) at step 50: 320.3746, Accuracy: 0.6903\n",
      "Training loss (for one batch) at step 60: 328.8784, Accuracy: 0.6942\n",
      "Training loss (for one batch) at step 70: 339.5271, Accuracy: 0.6879\n",
      "Training loss (for one batch) at step 80: 349.9829, Accuracy: 0.6816\n",
      "Training loss (for one batch) at step 90: 316.8926, Accuracy: 0.6801\n",
      "Training loss (for one batch) at step 100: 335.0565, Accuracy: 0.6795\n",
      "Training loss (for one batch) at step 110: 318.4030, Accuracy: 0.6805\n",
      "---- Training ----\n",
      "Training loss: 108.1616\n",
      "Training acc over epoch: 0.6787\n",
      "---- Validation ----\n",
      "Validation loss: 38.2758\n",
      "Validation acc: 0.6263\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 346.7839, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 344.1860, Accuracy: 0.6470\n",
      "Training loss (for one batch) at step 20: 325.5578, Accuracy: 0.6559\n",
      "Training loss (for one batch) at step 30: 324.9556, Accuracy: 0.6734\n",
      "Training loss (for one batch) at step 40: 310.8436, Accuracy: 0.6818\n",
      "Training loss (for one batch) at step 50: 305.8531, Accuracy: 0.6906\n",
      "Training loss (for one batch) at step 60: 333.8601, Accuracy: 0.6931\n",
      "Training loss (for one batch) at step 70: 318.8893, Accuracy: 0.6885\n",
      "Training loss (for one batch) at step 80: 343.3188, Accuracy: 0.6818\n",
      "Training loss (for one batch) at step 90: 317.6040, Accuracy: 0.6782\n",
      "Training loss (for one batch) at step 100: 311.1420, Accuracy: 0.6778\n",
      "Training loss (for one batch) at step 110: 344.9850, Accuracy: 0.6777\n",
      "---- Training ----\n",
      "Training loss: 102.5132\n",
      "Training acc over epoch: 0.6769\n",
      "---- Validation ----\n",
      "Validation loss: 45.3992\n",
      "Validation acc: 0.6217\n",
      "Time taken: 12.43s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 351.4968, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 328.5511, Accuracy: 0.6321\n",
      "Training loss (for one batch) at step 20: 308.5439, Accuracy: 0.6559\n",
      "Training loss (for one batch) at step 30: 319.6832, Accuracy: 0.6689\n",
      "Training loss (for one batch) at step 40: 305.4760, Accuracy: 0.6835\n",
      "Training loss (for one batch) at step 50: 300.2078, Accuracy: 0.6970\n",
      "Training loss (for one batch) at step 60: 321.6524, Accuracy: 0.6985\n",
      "Training loss (for one batch) at step 70: 341.4945, Accuracy: 0.6930\n",
      "Training loss (for one batch) at step 80: 329.3180, Accuracy: 0.6837\n",
      "Training loss (for one batch) at step 90: 301.6367, Accuracy: 0.6796\n",
      "Training loss (for one batch) at step 100: 313.6977, Accuracy: 0.6785\n",
      "Training loss (for one batch) at step 110: 335.3670, Accuracy: 0.6802\n",
      "---- Training ----\n",
      "Training loss: 123.7412\n",
      "Training acc over epoch: 0.6786\n",
      "---- Validation ----\n",
      "Validation loss: 35.3136\n",
      "Validation acc: 0.6161\n",
      "Time taken: 10.57s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 355.2991, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 322.1851, Accuracy: 0.6229\n",
      "Training loss (for one batch) at step 20: 304.2413, Accuracy: 0.6510\n",
      "Training loss (for one batch) at step 30: 306.8785, Accuracy: 0.6694\n",
      "Training loss (for one batch) at step 40: 297.5158, Accuracy: 0.6810\n",
      "Training loss (for one batch) at step 50: 297.3826, Accuracy: 0.6918\n",
      "Training loss (for one batch) at step 60: 316.4381, Accuracy: 0.6972\n",
      "Training loss (for one batch) at step 70: 339.0857, Accuracy: 0.6922\n",
      "Training loss (for one batch) at step 80: 349.5237, Accuracy: 0.6834\n",
      "Training loss (for one batch) at step 90: 316.5797, Accuracy: 0.6802\n",
      "Training loss (for one batch) at step 100: 320.5277, Accuracy: 0.6810\n",
      "Training loss (for one batch) at step 110: 326.8864, Accuracy: 0.6798\n",
      "---- Training ----\n",
      "Training loss: 97.5530\n",
      "Training acc over epoch: 0.6787\n",
      "---- Validation ----\n",
      "Validation loss: 42.2451\n",
      "Validation acc: 0.6327\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 334.1527, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 343.1759, Accuracy: 0.6307\n",
      "Training loss (for one batch) at step 20: 305.2257, Accuracy: 0.6566\n",
      "Training loss (for one batch) at step 30: 305.8377, Accuracy: 0.6741\n",
      "Training loss (for one batch) at step 40: 305.6006, Accuracy: 0.6854\n",
      "Training loss (for one batch) at step 50: 300.1066, Accuracy: 0.6947\n",
      "Training loss (for one batch) at step 60: 318.2214, Accuracy: 0.6981\n",
      "Training loss (for one batch) at step 70: 344.6857, Accuracy: 0.6897\n",
      "Training loss (for one batch) at step 80: 344.9058, Accuracy: 0.6825\n",
      "Training loss (for one batch) at step 90: 310.1331, Accuracy: 0.6803\n",
      "Training loss (for one batch) at step 100: 318.4448, Accuracy: 0.6814\n",
      "Training loss (for one batch) at step 110: 339.1000, Accuracy: 0.6815\n",
      "---- Training ----\n",
      "Training loss: 99.4002\n",
      "Training acc over epoch: 0.6797\n",
      "---- Validation ----\n",
      "Validation loss: 39.8519\n",
      "Validation acc: 0.6080\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 336.0056, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 321.6489, Accuracy: 0.6293\n",
      "Training loss (for one batch) at step 20: 312.5027, Accuracy: 0.6510\n",
      "Training loss (for one batch) at step 30: 311.2457, Accuracy: 0.6689\n",
      "Training loss (for one batch) at step 40: 316.7833, Accuracy: 0.6845\n",
      "Training loss (for one batch) at step 50: 308.4000, Accuracy: 0.6944\n",
      "Training loss (for one batch) at step 60: 309.1982, Accuracy: 0.6990\n",
      "Training loss (for one batch) at step 70: 341.4893, Accuracy: 0.6904\n",
      "Training loss (for one batch) at step 80: 334.9696, Accuracy: 0.6822\n",
      "Training loss (for one batch) at step 90: 305.5572, Accuracy: 0.6796\n",
      "Training loss (for one batch) at step 100: 306.4432, Accuracy: 0.6800\n",
      "Training loss (for one batch) at step 110: 320.4356, Accuracy: 0.6814\n",
      "---- Training ----\n",
      "Training loss: 117.5574\n",
      "Training acc over epoch: 0.6803\n",
      "---- Validation ----\n",
      "Validation loss: 55.7415\n",
      "Validation acc: 0.6306\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 333.1895, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 336.5203, Accuracy: 0.6364\n",
      "Training loss (for one batch) at step 20: 319.6936, Accuracy: 0.6585\n",
      "Training loss (for one batch) at step 30: 309.8585, Accuracy: 0.6729\n",
      "Training loss (for one batch) at step 40: 298.1377, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 50: 303.3535, Accuracy: 0.6964\n",
      "Training loss (for one batch) at step 60: 323.0786, Accuracy: 0.7015\n",
      "Training loss (for one batch) at step 70: 319.6873, Accuracy: 0.6918\n",
      "Training loss (for one batch) at step 80: 337.8703, Accuracy: 0.6806\n",
      "Training loss (for one batch) at step 90: 312.8989, Accuracy: 0.6815\n",
      "Training loss (for one batch) at step 100: 316.5425, Accuracy: 0.6826\n",
      "Training loss (for one batch) at step 110: 315.9506, Accuracy: 0.6818\n",
      "---- Training ----\n",
      "Training loss: 111.5182\n",
      "Training acc over epoch: 0.6810\n",
      "---- Validation ----\n",
      "Validation loss: 30.8636\n",
      "Validation acc: 0.6174\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 335.8358, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 337.9043, Accuracy: 0.6357\n",
      "Training loss (for one batch) at step 20: 301.8367, Accuracy: 0.6566\n",
      "Training loss (for one batch) at step 30: 308.7950, Accuracy: 0.6807\n",
      "Training loss (for one batch) at step 40: 280.0220, Accuracy: 0.6930\n",
      "Training loss (for one batch) at step 50: 324.9359, Accuracy: 0.6990\n",
      "Training loss (for one batch) at step 60: 319.5693, Accuracy: 0.7013\n",
      "Training loss (for one batch) at step 70: 318.4676, Accuracy: 0.6925\n",
      "Training loss (for one batch) at step 80: 320.6757, Accuracy: 0.6839\n",
      "Training loss (for one batch) at step 90: 329.0983, Accuracy: 0.6819\n",
      "Training loss (for one batch) at step 100: 305.2034, Accuracy: 0.6808\n",
      "Training loss (for one batch) at step 110: 305.1356, Accuracy: 0.6810\n",
      "---- Training ----\n",
      "Training loss: 111.5071\n",
      "Training acc over epoch: 0.6781\n",
      "---- Validation ----\n",
      "Validation loss: 42.9066\n",
      "Validation acc: 0.6145\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 348.7511, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 326.3272, Accuracy: 0.6264\n",
      "Training loss (for one batch) at step 20: 315.4667, Accuracy: 0.6551\n",
      "Training loss (for one batch) at step 30: 326.2637, Accuracy: 0.6792\n",
      "Training loss (for one batch) at step 40: 303.9957, Accuracy: 0.6905\n",
      "Training loss (for one batch) at step 50: 294.4110, Accuracy: 0.6965\n",
      "Training loss (for one batch) at step 60: 309.6275, Accuracy: 0.7026\n",
      "Training loss (for one batch) at step 70: 316.0875, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 80: 327.1689, Accuracy: 0.6880\n",
      "Training loss (for one batch) at step 90: 304.3851, Accuracy: 0.6820\n",
      "Training loss (for one batch) at step 100: 310.5808, Accuracy: 0.6828\n",
      "Training loss (for one batch) at step 110: 328.0817, Accuracy: 0.6812\n",
      "---- Training ----\n",
      "Training loss: 98.5854\n",
      "Training acc over epoch: 0.6814\n",
      "---- Validation ----\n",
      "Validation loss: 45.4580\n",
      "Validation acc: 0.6179\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 329.1945, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 334.8495, Accuracy: 0.5923\n",
      "Training loss (for one batch) at step 20: 319.3616, Accuracy: 0.6447\n",
      "Training loss (for one batch) at step 30: 306.3141, Accuracy: 0.6739\n",
      "Training loss (for one batch) at step 40: 305.7881, Accuracy: 0.6867\n",
      "Training loss (for one batch) at step 50: 293.5240, Accuracy: 0.6970\n",
      "Training loss (for one batch) at step 60: 323.6915, Accuracy: 0.6989\n",
      "Training loss (for one batch) at step 70: 334.4284, Accuracy: 0.6909\n",
      "Training loss (for one batch) at step 80: 333.3142, Accuracy: 0.6839\n",
      "Training loss (for one batch) at step 90: 307.1468, Accuracy: 0.6811\n",
      "Training loss (for one batch) at step 100: 318.5524, Accuracy: 0.6811\n",
      "Training loss (for one batch) at step 110: 300.5419, Accuracy: 0.6822\n",
      "---- Training ----\n",
      "Training loss: 108.9351\n",
      "Training acc over epoch: 0.6801\n",
      "---- Validation ----\n",
      "Validation loss: 73.8979\n",
      "Validation acc: 0.6384\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 330.9982, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 336.0445, Accuracy: 0.6264\n",
      "Training loss (for one batch) at step 20: 294.1523, Accuracy: 0.6559\n",
      "Training loss (for one batch) at step 30: 307.1169, Accuracy: 0.6817\n",
      "Training loss (for one batch) at step 40: 299.0100, Accuracy: 0.6911\n",
      "Training loss (for one batch) at step 50: 307.6062, Accuracy: 0.6975\n",
      "Training loss (for one batch) at step 60: 328.1814, Accuracy: 0.7002\n",
      "Training loss (for one batch) at step 70: 332.2529, Accuracy: 0.6923\n",
      "Training loss (for one batch) at step 80: 330.8818, Accuracy: 0.6854\n",
      "Training loss (for one batch) at step 90: 301.5338, Accuracy: 0.6840\n",
      "Training loss (for one batch) at step 100: 299.1652, Accuracy: 0.6835\n",
      "Training loss (for one batch) at step 110: 321.6599, Accuracy: 0.6822\n",
      "---- Training ----\n",
      "Training loss: 107.1992\n",
      "Training acc over epoch: 0.6820\n",
      "---- Validation ----\n",
      "Validation loss: 67.9691\n",
      "Validation acc: 0.6306\n",
      "Time taken: 10.16s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 321.1202, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 312.2722, Accuracy: 0.6286\n",
      "Training loss (for one batch) at step 20: 311.7704, Accuracy: 0.6592\n",
      "Training loss (for one batch) at step 30: 287.3762, Accuracy: 0.6782\n",
      "Training loss (for one batch) at step 40: 307.0997, Accuracy: 0.6886\n",
      "Training loss (for one batch) at step 50: 302.8690, Accuracy: 0.7013\n",
      "Training loss (for one batch) at step 60: 331.4732, Accuracy: 0.7025\n",
      "Training loss (for one batch) at step 70: 339.0549, Accuracy: 0.6956\n",
      "Training loss (for one batch) at step 80: 318.7848, Accuracy: 0.6871\n",
      "Training loss (for one batch) at step 90: 316.4948, Accuracy: 0.6833\n",
      "Training loss (for one batch) at step 100: 296.5982, Accuracy: 0.6853\n",
      "Training loss (for one batch) at step 110: 321.9819, Accuracy: 0.6837\n",
      "---- Training ----\n",
      "Training loss: 90.8755\n",
      "Training acc over epoch: 0.6819\n",
      "---- Validation ----\n",
      "Validation loss: 46.7552\n",
      "Validation acc: 0.6204\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 318.7761, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 332.4716, Accuracy: 0.6207\n",
      "Training loss (for one batch) at step 20: 303.6323, Accuracy: 0.6503\n",
      "Training loss (for one batch) at step 30: 295.7322, Accuracy: 0.6779\n",
      "Training loss (for one batch) at step 40: 297.9525, Accuracy: 0.6869\n",
      "Training loss (for one batch) at step 50: 288.6077, Accuracy: 0.6956\n",
      "Training loss (for one batch) at step 60: 329.0639, Accuracy: 0.7015\n",
      "Training loss (for one batch) at step 70: 311.1617, Accuracy: 0.6926\n",
      "Training loss (for one batch) at step 80: 317.0843, Accuracy: 0.6836\n",
      "Training loss (for one batch) at step 90: 301.7950, Accuracy: 0.6813\n",
      "Training loss (for one batch) at step 100: 295.6787, Accuracy: 0.6820\n",
      "Training loss (for one batch) at step 110: 319.8713, Accuracy: 0.6819\n",
      "---- Training ----\n",
      "Training loss: 105.5426\n",
      "Training acc over epoch: 0.6809\n",
      "---- Validation ----\n",
      "Validation loss: 50.4158\n",
      "Validation acc: 0.6370\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 328.8325, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 332.0875, Accuracy: 0.6357\n",
      "Training loss (for one batch) at step 20: 319.9874, Accuracy: 0.6589\n",
      "Training loss (for one batch) at step 30: 294.5037, Accuracy: 0.6757\n",
      "Training loss (for one batch) at step 40: 296.2296, Accuracy: 0.6881\n",
      "Training loss (for one batch) at step 50: 299.1155, Accuracy: 0.6972\n",
      "Training loss (for one batch) at step 60: 306.6310, Accuracy: 0.6994\n",
      "Training loss (for one batch) at step 70: 337.1021, Accuracy: 0.6926\n",
      "Training loss (for one batch) at step 80: 332.2242, Accuracy: 0.6851\n",
      "Training loss (for one batch) at step 90: 304.3811, Accuracy: 0.6836\n",
      "Training loss (for one batch) at step 100: 282.1001, Accuracy: 0.6839\n",
      "Training loss (for one batch) at step 110: 310.8659, Accuracy: 0.6837\n",
      "---- Training ----\n",
      "Training loss: 99.7741\n",
      "Training acc over epoch: 0.6819\n",
      "---- Validation ----\n",
      "Validation loss: 53.2769\n",
      "Validation acc: 0.6308\n",
      "Time taken: 10.07s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 364.5230, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 334.5157, Accuracy: 0.6392\n",
      "Training loss (for one batch) at step 20: 306.1326, Accuracy: 0.6600\n",
      "Training loss (for one batch) at step 30: 298.2181, Accuracy: 0.6837\n",
      "Training loss (for one batch) at step 40: 297.9459, Accuracy: 0.6938\n",
      "Training loss (for one batch) at step 50: 284.6844, Accuracy: 0.7019\n",
      "Training loss (for one batch) at step 60: 307.1105, Accuracy: 0.7050\n",
      "Training loss (for one batch) at step 70: 296.3143, Accuracy: 0.6975\n",
      "Training loss (for one batch) at step 80: 322.3768, Accuracy: 0.6863\n",
      "Training loss (for one batch) at step 90: 297.1918, Accuracy: 0.6852\n",
      "Training loss (for one batch) at step 100: 303.4359, Accuracy: 0.6849\n",
      "Training loss (for one batch) at step 110: 301.7412, Accuracy: 0.6841\n",
      "---- Training ----\n",
      "Training loss: 100.9350\n",
      "Training acc over epoch: 0.6832\n",
      "---- Validation ----\n",
      "Validation loss: 50.8604\n",
      "Validation acc: 0.6249\n",
      "Time taken: 10.61s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 320.6839, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 336.9194, Accuracy: 0.6349\n",
      "Training loss (for one batch) at step 20: 299.3871, Accuracy: 0.6574\n",
      "Training loss (for one batch) at step 30: 292.7759, Accuracy: 0.6772\n",
      "Training loss (for one batch) at step 40: 302.9317, Accuracy: 0.6898\n",
      "Training loss (for one batch) at step 50: 287.7714, Accuracy: 0.7008\n",
      "Training loss (for one batch) at step 60: 300.9008, Accuracy: 0.7008\n",
      "Training loss (for one batch) at step 70: 326.1635, Accuracy: 0.6929\n",
      "Training loss (for one batch) at step 80: 325.0372, Accuracy: 0.6844\n",
      "Training loss (for one batch) at step 90: 299.2861, Accuracy: 0.6817\n",
      "Training loss (for one batch) at step 100: 288.9001, Accuracy: 0.6815\n",
      "Training loss (for one batch) at step 110: 301.4907, Accuracy: 0.6819\n",
      "---- Training ----\n",
      "Training loss: 95.9271\n",
      "Training acc over epoch: 0.6798\n",
      "---- Validation ----\n",
      "Validation loss: 44.2552\n",
      "Validation acc: 0.6359\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 326.2274, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 330.1707, Accuracy: 0.6491\n",
      "Training loss (for one batch) at step 20: 281.8310, Accuracy: 0.6700\n",
      "Training loss (for one batch) at step 30: 287.4564, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 40: 289.4641, Accuracy: 0.6905\n",
      "Training loss (for one batch) at step 50: 288.3264, Accuracy: 0.7016\n",
      "Training loss (for one batch) at step 60: 302.7925, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 70: 313.0642, Accuracy: 0.6950\n",
      "Training loss (for one batch) at step 80: 324.4404, Accuracy: 0.6866\n",
      "Training loss (for one batch) at step 90: 305.0361, Accuracy: 0.6829\n",
      "Training loss (for one batch) at step 100: 311.4810, Accuracy: 0.6832\n",
      "Training loss (for one batch) at step 110: 302.9934, Accuracy: 0.6814\n",
      "---- Training ----\n",
      "Training loss: 98.5598\n",
      "Training acc over epoch: 0.6812\n",
      "---- Validation ----\n",
      "Validation loss: 47.0996\n",
      "Validation acc: 0.6416\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 332.7566, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 310.7448, Accuracy: 0.6278\n",
      "Training loss (for one batch) at step 20: 306.2903, Accuracy: 0.6589\n",
      "Training loss (for one batch) at step 30: 297.4214, Accuracy: 0.6812\n",
      "Training loss (for one batch) at step 40: 294.4401, Accuracy: 0.6894\n",
      "Training loss (for one batch) at step 50: 314.5044, Accuracy: 0.6993\n",
      "Training loss (for one batch) at step 60: 297.3896, Accuracy: 0.7036\n",
      "Training loss (for one batch) at step 70: 315.4705, Accuracy: 0.6962\n",
      "Training loss (for one batch) at step 80: 320.4213, Accuracy: 0.6884\n",
      "Training loss (for one batch) at step 90: 307.5477, Accuracy: 0.6852\n",
      "Training loss (for one batch) at step 100: 287.3280, Accuracy: 0.6850\n",
      "Training loss (for one batch) at step 110: 305.8348, Accuracy: 0.6841\n",
      "---- Training ----\n",
      "Training loss: 105.2339\n",
      "Training acc over epoch: 0.6832\n",
      "---- Validation ----\n",
      "Validation loss: 53.6158\n",
      "Validation acc: 0.6131\n",
      "Time taken: 10.54s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 312.6354, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 320.5323, Accuracy: 0.6349\n",
      "Training loss (for one batch) at step 20: 301.4332, Accuracy: 0.6622\n",
      "Training loss (for one batch) at step 30: 295.1562, Accuracy: 0.6835\n",
      "Training loss (for one batch) at step 40: 277.6850, Accuracy: 0.6907\n",
      "Training loss (for one batch) at step 50: 311.5231, Accuracy: 0.6972\n",
      "Training loss (for one batch) at step 60: 306.8305, Accuracy: 0.7021\n",
      "Training loss (for one batch) at step 70: 303.0074, Accuracy: 0.6934\n",
      "Training loss (for one batch) at step 80: 322.8640, Accuracy: 0.6854\n",
      "Training loss (for one batch) at step 90: 285.1029, Accuracy: 0.6839\n",
      "Training loss (for one batch) at step 100: 288.0554, Accuracy: 0.6836\n",
      "Training loss (for one batch) at step 110: 310.1048, Accuracy: 0.6831\n",
      "---- Training ----\n",
      "Training loss: 94.3332\n",
      "Training acc over epoch: 0.6816\n",
      "---- Validation ----\n",
      "Validation loss: 53.8042\n",
      "Validation acc: 0.6150\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 326.3885, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 317.7399, Accuracy: 0.6413\n",
      "Training loss (for one batch) at step 20: 301.4246, Accuracy: 0.6622\n",
      "Training loss (for one batch) at step 30: 295.6884, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 40: 289.4736, Accuracy: 0.6911\n",
      "Training loss (for one batch) at step 50: 289.5033, Accuracy: 0.7001\n",
      "Training loss (for one batch) at step 60: 312.5382, Accuracy: 0.7048\n",
      "Training loss (for one batch) at step 70: 319.3885, Accuracy: 0.6962\n",
      "Training loss (for one batch) at step 80: 333.3995, Accuracy: 0.6864\n",
      "Training loss (for one batch) at step 90: 285.0044, Accuracy: 0.6854\n",
      "Training loss (for one batch) at step 100: 305.7366, Accuracy: 0.6866\n",
      "Training loss (for one batch) at step 110: 311.2047, Accuracy: 0.6866\n",
      "---- Training ----\n",
      "Training loss: 101.0246\n",
      "Training acc over epoch: 0.6846\n",
      "---- Validation ----\n",
      "Validation loss: 63.9994\n",
      "Validation acc: 0.6188\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 317.3983, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 324.9597, Accuracy: 0.6378\n",
      "Training loss (for one batch) at step 20: 319.5259, Accuracy: 0.6536\n",
      "Training loss (for one batch) at step 30: 282.6631, Accuracy: 0.6817\n",
      "Training loss (for one batch) at step 40: 292.4478, Accuracy: 0.6940\n",
      "Training loss (for one batch) at step 50: 281.2031, Accuracy: 0.7034\n",
      "Training loss (for one batch) at step 60: 305.3193, Accuracy: 0.7047\n",
      "Training loss (for one batch) at step 70: 300.0663, Accuracy: 0.6976\n",
      "Training loss (for one batch) at step 80: 309.7321, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 90: 298.8222, Accuracy: 0.6832\n",
      "Training loss (for one batch) at step 100: 287.1148, Accuracy: 0.6834\n",
      "Training loss (for one batch) at step 110: 295.7423, Accuracy: 0.6836\n",
      "---- Training ----\n",
      "Training loss: 110.1723\n",
      "Training acc over epoch: 0.6823\n",
      "---- Validation ----\n",
      "Validation loss: 63.7463\n",
      "Validation acc: 0.6263\n",
      "Time taken: 10.86s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 326.3469, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 349.9149, Accuracy: 0.6385\n",
      "Training loss (for one batch) at step 20: 289.9435, Accuracy: 0.6648\n",
      "Training loss (for one batch) at step 30: 289.9479, Accuracy: 0.6845\n",
      "Training loss (for one batch) at step 40: 272.5525, Accuracy: 0.6917\n",
      "Training loss (for one batch) at step 50: 290.5484, Accuracy: 0.6999\n",
      "Training loss (for one batch) at step 60: 315.5815, Accuracy: 0.7038\n",
      "Training loss (for one batch) at step 70: 326.4864, Accuracy: 0.6959\n",
      "Training loss (for one batch) at step 80: 294.5747, Accuracy: 0.6873\n",
      "Training loss (for one batch) at step 90: 314.8467, Accuracy: 0.6829\n",
      "Training loss (for one batch) at step 100: 301.7255, Accuracy: 0.6823\n",
      "Training loss (for one batch) at step 110: 302.5907, Accuracy: 0.6839\n",
      "---- Training ----\n",
      "Training loss: 98.2207\n",
      "Training acc over epoch: 0.6828\n",
      "---- Validation ----\n",
      "Validation loss: 35.5611\n",
      "Validation acc: 0.6209\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 337.2257, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 324.9200, Accuracy: 0.6257\n",
      "Training loss (for one batch) at step 20: 295.8540, Accuracy: 0.6577\n",
      "Training loss (for one batch) at step 30: 292.6392, Accuracy: 0.6774\n",
      "Training loss (for one batch) at step 40: 286.1363, Accuracy: 0.6877\n",
      "Training loss (for one batch) at step 50: 283.0910, Accuracy: 0.6949\n",
      "Training loss (for one batch) at step 60: 297.3341, Accuracy: 0.6992\n",
      "Training loss (for one batch) at step 70: 307.4580, Accuracy: 0.6925\n",
      "Training loss (for one batch) at step 80: 311.2480, Accuracy: 0.6835\n",
      "Training loss (for one batch) at step 90: 285.4505, Accuracy: 0.6825\n",
      "Training loss (for one batch) at step 100: 297.5596, Accuracy: 0.6845\n",
      "Training loss (for one batch) at step 110: 301.8420, Accuracy: 0.6834\n",
      "---- Training ----\n",
      "Training loss: 91.8293\n",
      "Training acc over epoch: 0.6827\n",
      "---- Validation ----\n",
      "Validation loss: 33.8962\n",
      "Validation acc: 0.6255\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 320.8315, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 315.8448, Accuracy: 0.6342\n",
      "Training loss (for one batch) at step 20: 311.1954, Accuracy: 0.6648\n",
      "Training loss (for one batch) at step 30: 288.6083, Accuracy: 0.6817\n",
      "Training loss (for one batch) at step 40: 290.9878, Accuracy: 0.6957\n",
      "Training loss (for one batch) at step 50: 281.0350, Accuracy: 0.7036\n",
      "Training loss (for one batch) at step 60: 309.2148, Accuracy: 0.7063\n",
      "Training loss (for one batch) at step 70: 309.2194, Accuracy: 0.6985\n",
      "Training loss (for one batch) at step 80: 320.6869, Accuracy: 0.6883\n",
      "Training loss (for one batch) at step 90: 281.8502, Accuracy: 0.6843\n",
      "Training loss (for one batch) at step 100: 295.1038, Accuracy: 0.6845\n",
      "Training loss (for one batch) at step 110: 304.3802, Accuracy: 0.6835\n",
      "---- Training ----\n",
      "Training loss: 94.7260\n",
      "Training acc over epoch: 0.6816\n",
      "---- Validation ----\n",
      "Validation loss: 30.1358\n",
      "Validation acc: 0.6239\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 332.1223, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 316.2882, Accuracy: 0.6278\n",
      "Training loss (for one batch) at step 20: 289.7740, Accuracy: 0.6488\n",
      "Training loss (for one batch) at step 30: 304.7845, Accuracy: 0.6704\n",
      "Training loss (for one batch) at step 40: 281.4359, Accuracy: 0.6850\n",
      "Training loss (for one batch) at step 50: 279.7469, Accuracy: 0.6947\n",
      "Training loss (for one batch) at step 60: 289.3764, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 70: 333.5334, Accuracy: 0.6890\n",
      "Training loss (for one batch) at step 80: 336.3658, Accuracy: 0.6820\n",
      "Training loss (for one batch) at step 90: 299.2111, Accuracy: 0.6802\n",
      "Training loss (for one batch) at step 100: 293.4881, Accuracy: 0.6820\n",
      "Training loss (for one batch) at step 110: 303.4728, Accuracy: 0.6812\n",
      "---- Training ----\n",
      "Training loss: 93.8055\n",
      "Training acc over epoch: 0.6804\n",
      "---- Validation ----\n",
      "Validation loss: 57.9472\n",
      "Validation acc: 0.6134\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 312.7599, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 304.4458, Accuracy: 0.6207\n",
      "Training loss (for one batch) at step 20: 287.6416, Accuracy: 0.6514\n",
      "Training loss (for one batch) at step 30: 282.9494, Accuracy: 0.6774\n",
      "Training loss (for one batch) at step 40: 267.7191, Accuracy: 0.6898\n",
      "Training loss (for one batch) at step 50: 288.6491, Accuracy: 0.6955\n",
      "Training loss (for one batch) at step 60: 286.3922, Accuracy: 0.6981\n",
      "Training loss (for one batch) at step 70: 304.1244, Accuracy: 0.6933\n",
      "Training loss (for one batch) at step 80: 306.5376, Accuracy: 0.6831\n",
      "Training loss (for one batch) at step 90: 310.5886, Accuracy: 0.6805\n",
      "Training loss (for one batch) at step 100: 298.5682, Accuracy: 0.6807\n",
      "Training loss (for one batch) at step 110: 306.8243, Accuracy: 0.6803\n",
      "---- Training ----\n",
      "Training loss: 95.0721\n",
      "Training acc over epoch: 0.6794\n",
      "---- Validation ----\n",
      "Validation loss: 58.6272\n",
      "Validation acc: 0.6206\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 319.4813, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 319.9438, Accuracy: 0.6435\n",
      "Training loss (for one batch) at step 20: 291.5112, Accuracy: 0.6496\n",
      "Training loss (for one batch) at step 30: 301.5965, Accuracy: 0.6744\n",
      "Training loss (for one batch) at step 40: 291.6444, Accuracy: 0.6885\n",
      "Training loss (for one batch) at step 50: 299.5341, Accuracy: 0.6936\n",
      "Training loss (for one batch) at step 60: 309.6927, Accuracy: 0.7008\n",
      "Training loss (for one batch) at step 70: 311.5895, Accuracy: 0.6949\n",
      "Training loss (for one batch) at step 80: 318.3162, Accuracy: 0.6866\n",
      "Training loss (for one batch) at step 90: 299.2234, Accuracy: 0.6822\n",
      "Training loss (for one batch) at step 100: 284.9689, Accuracy: 0.6826\n",
      "Training loss (for one batch) at step 110: 290.9684, Accuracy: 0.6826\n",
      "---- Training ----\n",
      "Training loss: 100.7435\n",
      "Training acc over epoch: 0.6813\n",
      "---- Validation ----\n",
      "Validation loss: 73.8682\n",
      "Validation acc: 0.6198\n",
      "Time taken: 10.54s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 303.7686, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 326.1818, Accuracy: 0.6357\n",
      "Training loss (for one batch) at step 20: 303.0178, Accuracy: 0.6518\n",
      "Training loss (for one batch) at step 30: 288.9569, Accuracy: 0.6762\n",
      "Training loss (for one batch) at step 40: 284.3716, Accuracy: 0.6892\n",
      "Training loss (for one batch) at step 50: 278.0958, Accuracy: 0.6988\n",
      "Training loss (for one batch) at step 60: 288.6658, Accuracy: 0.7008\n",
      "Training loss (for one batch) at step 70: 313.7274, Accuracy: 0.6923\n",
      "Training loss (for one batch) at step 80: 315.4493, Accuracy: 0.6831\n",
      "Training loss (for one batch) at step 90: 300.3589, Accuracy: 0.6819\n",
      "Training loss (for one batch) at step 100: 306.2552, Accuracy: 0.6832\n",
      "Training loss (for one batch) at step 110: 301.0241, Accuracy: 0.6822\n",
      "---- Training ----\n",
      "Training loss: 87.8648\n",
      "Training acc over epoch: 0.6811\n",
      "---- Validation ----\n",
      "Validation loss: 58.0412\n",
      "Validation acc: 0.6300\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 320.2047, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 314.2888, Accuracy: 0.6463\n",
      "Training loss (for one batch) at step 20: 290.6341, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 30: 271.4744, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 40: 291.8919, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 50: 269.4230, Accuracy: 0.6999\n",
      "Training loss (for one batch) at step 60: 287.9583, Accuracy: 0.7044\n",
      "Training loss (for one batch) at step 70: 311.5150, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 80: 306.5903, Accuracy: 0.6877\n",
      "Training loss (for one batch) at step 90: 289.5230, Accuracy: 0.6831\n",
      "Training loss (for one batch) at step 100: 286.8830, Accuracy: 0.6854\n",
      "Training loss (for one batch) at step 110: 315.1038, Accuracy: 0.6822\n",
      "---- Training ----\n",
      "Training loss: 92.5547\n",
      "Training acc over epoch: 0.6815\n",
      "---- Validation ----\n",
      "Validation loss: 63.8960\n",
      "Validation acc: 0.6282\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 323.5062, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 299.5309, Accuracy: 0.6321\n",
      "Training loss (for one batch) at step 20: 277.0781, Accuracy: 0.6540\n",
      "Training loss (for one batch) at step 30: 300.0084, Accuracy: 0.6757\n",
      "Training loss (for one batch) at step 40: 297.9717, Accuracy: 0.6888\n",
      "Training loss (for one batch) at step 50: 282.7402, Accuracy: 0.6994\n",
      "Training loss (for one batch) at step 60: 298.2716, Accuracy: 0.7044\n",
      "Training loss (for one batch) at step 70: 321.7452, Accuracy: 0.6945\n",
      "Training loss (for one batch) at step 80: 303.3329, Accuracy: 0.6854\n",
      "Training loss (for one batch) at step 90: 301.2974, Accuracy: 0.6803\n",
      "Training loss (for one batch) at step 100: 287.4376, Accuracy: 0.6831\n",
      "Training loss (for one batch) at step 110: 291.7878, Accuracy: 0.6832\n",
      "---- Training ----\n",
      "Training loss: 105.2472\n",
      "Training acc over epoch: 0.6817\n",
      "---- Validation ----\n",
      "Validation loss: 42.6159\n",
      "Validation acc: 0.6032\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 311.4824, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 306.8529, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 20: 293.2472, Accuracy: 0.6607\n",
      "Training loss (for one batch) at step 30: 289.7309, Accuracy: 0.6815\n",
      "Training loss (for one batch) at step 40: 268.3845, Accuracy: 0.6925\n",
      "Training loss (for one batch) at step 50: 282.8604, Accuracy: 0.7021\n",
      "Training loss (for one batch) at step 60: 281.3511, Accuracy: 0.7045\n",
      "Training loss (for one batch) at step 70: 293.9267, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 80: 340.3437, Accuracy: 0.6870\n",
      "Training loss (for one batch) at step 90: 275.8537, Accuracy: 0.6858\n",
      "Training loss (for one batch) at step 100: 276.0744, Accuracy: 0.6848\n",
      "Training loss (for one batch) at step 110: 279.3212, Accuracy: 0.6838\n",
      "---- Training ----\n",
      "Training loss: 118.7929\n",
      "Training acc over epoch: 0.6825\n",
      "---- Validation ----\n",
      "Validation loss: 43.7999\n",
      "Validation acc: 0.6236\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 310.4244, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 301.7029, Accuracy: 0.6264\n",
      "Training loss (for one batch) at step 20: 275.8856, Accuracy: 0.6548\n",
      "Training loss (for one batch) at step 30: 278.1015, Accuracy: 0.6759\n",
      "Training loss (for one batch) at step 40: 269.4331, Accuracy: 0.6865\n",
      "Training loss (for one batch) at step 50: 304.3743, Accuracy: 0.6968\n",
      "Training loss (for one batch) at step 60: 309.4575, Accuracy: 0.6998\n",
      "Training loss (for one batch) at step 70: 282.0444, Accuracy: 0.6940\n",
      "Training loss (for one batch) at step 80: 313.6534, Accuracy: 0.6841\n",
      "Training loss (for one batch) at step 90: 298.4989, Accuracy: 0.6821\n",
      "Training loss (for one batch) at step 100: 303.3409, Accuracy: 0.6823\n",
      "Training loss (for one batch) at step 110: 301.3377, Accuracy: 0.6808\n",
      "---- Training ----\n",
      "Training loss: 93.3754\n",
      "Training acc over epoch: 0.6799\n",
      "---- Validation ----\n",
      "Validation loss: 47.7835\n",
      "Validation acc: 0.6255\n",
      "Time taken: 10.11s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 314.4604, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 287.0656, Accuracy: 0.6541\n",
      "Training loss (for one batch) at step 20: 295.2578, Accuracy: 0.6656\n",
      "Training loss (for one batch) at step 30: 269.1754, Accuracy: 0.6837\n",
      "Training loss (for one batch) at step 40: 291.7989, Accuracy: 0.6991\n",
      "Training loss (for one batch) at step 50: 272.2191, Accuracy: 0.7047\n",
      "Training loss (for one batch) at step 60: 283.2831, Accuracy: 0.7080\n",
      "Training loss (for one batch) at step 70: 314.8681, Accuracy: 0.6994\n",
      "Training loss (for one batch) at step 80: 311.0430, Accuracy: 0.6900\n",
      "Training loss (for one batch) at step 90: 304.5134, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 100: 279.9127, Accuracy: 0.6866\n",
      "Training loss (for one batch) at step 110: 302.3596, Accuracy: 0.6860\n",
      "---- Training ----\n",
      "Training loss: 101.1555\n",
      "Training acc over epoch: 0.6841\n",
      "---- Validation ----\n",
      "Validation loss: 44.5203\n",
      "Validation acc: 0.6188\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 302.6724, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 300.2715, Accuracy: 0.6477\n",
      "Training loss (for one batch) at step 20: 283.7179, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 30: 302.7939, Accuracy: 0.6852\n",
      "Training loss (for one batch) at step 40: 284.4599, Accuracy: 0.6949\n",
      "Training loss (for one batch) at step 50: 271.8612, Accuracy: 0.7051\n",
      "Training loss (for one batch) at step 60: 297.1986, Accuracy: 0.7081\n",
      "Training loss (for one batch) at step 70: 313.3075, Accuracy: 0.6994\n",
      "Training loss (for one batch) at step 80: 322.7391, Accuracy: 0.6881\n",
      "Training loss (for one batch) at step 90: 291.3985, Accuracy: 0.6848\n",
      "Training loss (for one batch) at step 100: 308.6472, Accuracy: 0.6854\n",
      "Training loss (for one batch) at step 110: 293.8030, Accuracy: 0.6843\n",
      "---- Training ----\n",
      "Training loss: 92.2494\n",
      "Training acc over epoch: 0.6835\n",
      "---- Validation ----\n",
      "Validation loss: 38.7308\n",
      "Validation acc: 0.6190\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 301.6195, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 318.1612, Accuracy: 0.6278\n",
      "Training loss (for one batch) at step 20: 279.7184, Accuracy: 0.6622\n",
      "Training loss (for one batch) at step 30: 267.8371, Accuracy: 0.6802\n",
      "Training loss (for one batch) at step 40: 283.8264, Accuracy: 0.6915\n",
      "Training loss (for one batch) at step 50: 281.7576, Accuracy: 0.7005\n",
      "Training loss (for one batch) at step 60: 279.0331, Accuracy: 0.7054\n",
      "Training loss (for one batch) at step 70: 314.4703, Accuracy: 0.6956\n",
      "Training loss (for one batch) at step 80: 306.7372, Accuracy: 0.6861\n",
      "Training loss (for one batch) at step 90: 286.3853, Accuracy: 0.6836\n",
      "Training loss (for one batch) at step 100: 280.2841, Accuracy: 0.6857\n",
      "Training loss (for one batch) at step 110: 278.8183, Accuracy: 0.6849\n",
      "---- Training ----\n",
      "Training loss: 98.7787\n",
      "Training acc over epoch: 0.6827\n",
      "---- Validation ----\n",
      "Validation loss: 88.6140\n",
      "Validation acc: 0.6166\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 301.1250, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 305.4917, Accuracy: 0.6442\n",
      "Training loss (for one batch) at step 20: 298.5860, Accuracy: 0.6622\n",
      "Training loss (for one batch) at step 30: 263.0209, Accuracy: 0.6817\n",
      "Training loss (for one batch) at step 40: 277.5243, Accuracy: 0.6925\n",
      "Training loss (for one batch) at step 50: 270.6090, Accuracy: 0.7030\n",
      "Training loss (for one batch) at step 60: 288.2741, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 70: 295.7973, Accuracy: 0.6960\n",
      "Training loss (for one batch) at step 80: 311.0949, Accuracy: 0.6856\n",
      "Training loss (for one batch) at step 90: 281.6939, Accuracy: 0.6811\n",
      "Training loss (for one batch) at step 100: 284.5855, Accuracy: 0.6832\n",
      "Training loss (for one batch) at step 110: 276.8122, Accuracy: 0.6821\n",
      "---- Training ----\n",
      "Training loss: 103.3873\n",
      "Training acc over epoch: 0.6811\n",
      "---- Validation ----\n",
      "Validation loss: 36.8786\n",
      "Validation acc: 0.6188\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 304.6798, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 318.0060, Accuracy: 0.6371\n",
      "Training loss (for one batch) at step 20: 273.0148, Accuracy: 0.6596\n",
      "Training loss (for one batch) at step 30: 272.6118, Accuracy: 0.6827\n",
      "Training loss (for one batch) at step 40: 276.0226, Accuracy: 0.6915\n",
      "Training loss (for one batch) at step 50: 275.7809, Accuracy: 0.7001\n",
      "Training loss (for one batch) at step 60: 291.9559, Accuracy: 0.7025\n",
      "Training loss (for one batch) at step 70: 307.7156, Accuracy: 0.6980\n",
      "Training loss (for one batch) at step 80: 302.1705, Accuracy: 0.6888\n",
      "Training loss (for one batch) at step 90: 291.0757, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 100: 289.0650, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 110: 294.2493, Accuracy: 0.6874\n",
      "---- Training ----\n",
      "Training loss: 94.0849\n",
      "Training acc over epoch: 0.6846\n",
      "---- Validation ----\n",
      "Validation loss: 62.3855\n",
      "Validation acc: 0.6190\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 305.0352, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 290.8889, Accuracy: 0.6364\n",
      "Training loss (for one batch) at step 20: 299.2433, Accuracy: 0.6525\n",
      "Training loss (for one batch) at step 30: 280.9989, Accuracy: 0.6792\n",
      "Training loss (for one batch) at step 40: 264.8036, Accuracy: 0.6892\n",
      "Training loss (for one batch) at step 50: 268.5464, Accuracy: 0.6996\n",
      "Training loss (for one batch) at step 60: 282.6739, Accuracy: 0.7022\n",
      "Training loss (for one batch) at step 70: 309.8490, Accuracy: 0.6966\n",
      "Training loss (for one batch) at step 80: 312.2344, Accuracy: 0.6864\n",
      "Training loss (for one batch) at step 90: 295.1818, Accuracy: 0.6849\n",
      "Training loss (for one batch) at step 100: 268.9200, Accuracy: 0.6858\n",
      "Training loss (for one batch) at step 110: 298.2480, Accuracy: 0.6836\n",
      "---- Training ----\n",
      "Training loss: 98.3122\n",
      "Training acc over epoch: 0.6830\n",
      "---- Validation ----\n",
      "Validation loss: 45.8119\n",
      "Validation acc: 0.6308\n",
      "Time taken: 10.16s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 307.3360, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 294.9019, Accuracy: 0.6207\n",
      "Training loss (for one batch) at step 20: 295.7239, Accuracy: 0.6522\n",
      "Training loss (for one batch) at step 30: 275.7427, Accuracy: 0.6734\n",
      "Training loss (for one batch) at step 40: 276.8415, Accuracy: 0.6909\n",
      "Training loss (for one batch) at step 50: 286.9793, Accuracy: 0.6981\n",
      "Training loss (for one batch) at step 60: 285.8102, Accuracy: 0.7021\n",
      "Training loss (for one batch) at step 70: 300.7988, Accuracy: 0.6930\n",
      "Training loss (for one batch) at step 80: 307.7349, Accuracy: 0.6857\n",
      "Training loss (for one batch) at step 90: 273.6653, Accuracy: 0.6830\n",
      "Training loss (for one batch) at step 100: 283.9588, Accuracy: 0.6853\n",
      "Training loss (for one batch) at step 110: 276.2357, Accuracy: 0.6832\n",
      "---- Training ----\n",
      "Training loss: 129.5939\n",
      "Training acc over epoch: 0.6819\n",
      "---- Validation ----\n",
      "Validation loss: 59.6517\n",
      "Validation acc: 0.6244\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 298.5788, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 303.1109, Accuracy: 0.6442\n",
      "Training loss (for one batch) at step 20: 267.1409, Accuracy: 0.6622\n",
      "Training loss (for one batch) at step 30: 270.4749, Accuracy: 0.6885\n",
      "Training loss (for one batch) at step 40: 284.5334, Accuracy: 0.6980\n",
      "Training loss (for one batch) at step 50: 266.0304, Accuracy: 0.7048\n",
      "Training loss (for one batch) at step 60: 277.8977, Accuracy: 0.7076\n",
      "Training loss (for one batch) at step 70: 299.1295, Accuracy: 0.6992\n",
      "Training loss (for one batch) at step 80: 296.5928, Accuracy: 0.6893\n",
      "Training loss (for one batch) at step 90: 277.2422, Accuracy: 0.6860\n",
      "Training loss (for one batch) at step 100: 275.6501, Accuracy: 0.6881\n",
      "Training loss (for one batch) at step 110: 288.8694, Accuracy: 0.6855\n",
      "---- Training ----\n",
      "Training loss: 88.2812\n",
      "Training acc over epoch: 0.6838\n",
      "---- Validation ----\n",
      "Validation loss: 53.5258\n",
      "Validation acc: 0.6255\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 294.1525, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 317.1557, Accuracy: 0.6321\n",
      "Training loss (for one batch) at step 20: 257.4605, Accuracy: 0.6574\n",
      "Training loss (for one batch) at step 30: 261.9561, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 40: 264.9743, Accuracy: 0.6898\n",
      "Training loss (for one batch) at step 50: 273.1557, Accuracy: 0.6965\n",
      "Training loss (for one batch) at step 60: 306.0325, Accuracy: 0.7011\n",
      "Training loss (for one batch) at step 70: 337.9786, Accuracy: 0.6901\n",
      "Training loss (for one batch) at step 80: 289.7190, Accuracy: 0.6826\n",
      "Training loss (for one batch) at step 90: 292.6239, Accuracy: 0.6812\n",
      "Training loss (for one batch) at step 100: 275.3549, Accuracy: 0.6831\n",
      "Training loss (for one batch) at step 110: 287.3910, Accuracy: 0.6817\n",
      "---- Training ----\n",
      "Training loss: 96.1831\n",
      "Training acc over epoch: 0.6803\n",
      "---- Validation ----\n",
      "Validation loss: 58.5608\n",
      "Validation acc: 0.6346\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 310.1413, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 304.6922, Accuracy: 0.6151\n",
      "Training loss (for one batch) at step 20: 281.8348, Accuracy: 0.6633\n",
      "Training loss (for one batch) at step 30: 284.3163, Accuracy: 0.6789\n",
      "Training loss (for one batch) at step 40: 264.4525, Accuracy: 0.6942\n",
      "Training loss (for one batch) at step 50: 273.0328, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 60: 308.5093, Accuracy: 0.7038\n",
      "Training loss (for one batch) at step 70: 317.3056, Accuracy: 0.6962\n",
      "Training loss (for one batch) at step 80: 307.3925, Accuracy: 0.6874\n",
      "Training loss (for one batch) at step 90: 270.9371, Accuracy: 0.6842\n",
      "Training loss (for one batch) at step 100: 276.4641, Accuracy: 0.6832\n",
      "Training loss (for one batch) at step 110: 288.1773, Accuracy: 0.6807\n",
      "---- Training ----\n",
      "Training loss: 97.9011\n",
      "Training acc over epoch: 0.6809\n",
      "---- Validation ----\n",
      "Validation loss: 46.6716\n",
      "Validation acc: 0.6231\n",
      "Time taken: 10.59s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 304.4856, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 310.0533, Accuracy: 0.6200\n",
      "Training loss (for one batch) at step 20: 274.0302, Accuracy: 0.6529\n",
      "Training loss (for one batch) at step 30: 268.8568, Accuracy: 0.6792\n",
      "Training loss (for one batch) at step 40: 268.9784, Accuracy: 0.6900\n",
      "Training loss (for one batch) at step 50: 262.5295, Accuracy: 0.7011\n",
      "Training loss (for one batch) at step 60: 280.9032, Accuracy: 0.7016\n",
      "Training loss (for one batch) at step 70: 303.4482, Accuracy: 0.6954\n",
      "Training loss (for one batch) at step 80: 307.1367, Accuracy: 0.6881\n",
      "Training loss (for one batch) at step 90: 309.7165, Accuracy: 0.6873\n",
      "Training loss (for one batch) at step 100: 294.0080, Accuracy: 0.6857\n",
      "Training loss (for one batch) at step 110: 292.1194, Accuracy: 0.6858\n",
      "---- Training ----\n",
      "Training loss: 113.0496\n",
      "Training acc over epoch: 0.6834\n",
      "---- Validation ----\n",
      "Validation loss: 65.0704\n",
      "Validation acc: 0.6080\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 307.1841, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 324.9995, Accuracy: 0.6286\n",
      "Training loss (for one batch) at step 20: 282.3929, Accuracy: 0.6644\n",
      "Training loss (for one batch) at step 30: 278.4944, Accuracy: 0.6817\n",
      "Training loss (for one batch) at step 40: 277.6699, Accuracy: 0.6913\n",
      "Training loss (for one batch) at step 50: 272.1800, Accuracy: 0.7007\n",
      "Training loss (for one batch) at step 60: 284.5807, Accuracy: 0.7045\n",
      "Training loss (for one batch) at step 70: 328.4785, Accuracy: 0.6965\n",
      "Training loss (for one batch) at step 80: 324.7670, Accuracy: 0.6870\n",
      "Training loss (for one batch) at step 90: 276.9348, Accuracy: 0.6847\n",
      "Training loss (for one batch) at step 100: 276.4332, Accuracy: 0.6849\n",
      "Training loss (for one batch) at step 110: 271.7779, Accuracy: 0.6857\n",
      "---- Training ----\n",
      "Training loss: 94.7203\n",
      "Training acc over epoch: 0.6836\n",
      "---- Validation ----\n",
      "Validation loss: 63.1436\n",
      "Validation acc: 0.6228\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 299.7808, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 284.6281, Accuracy: 0.6207\n",
      "Training loss (for one batch) at step 20: 278.8188, Accuracy: 0.6622\n",
      "Training loss (for one batch) at step 30: 265.8760, Accuracy: 0.6883\n",
      "Training loss (for one batch) at step 40: 264.1049, Accuracy: 0.6984\n",
      "Training loss (for one batch) at step 50: 252.2038, Accuracy: 0.7056\n",
      "Training loss (for one batch) at step 60: 278.2128, Accuracy: 0.7091\n",
      "Training loss (for one batch) at step 70: 310.9734, Accuracy: 0.7003\n",
      "Training loss (for one batch) at step 80: 292.9376, Accuracy: 0.6914\n",
      "Training loss (for one batch) at step 90: 284.3982, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 100: 276.1649, Accuracy: 0.6878\n",
      "Training loss (for one batch) at step 110: 301.1659, Accuracy: 0.6883\n",
      "---- Training ----\n",
      "Training loss: 93.2847\n",
      "Training acc over epoch: 0.6863\n",
      "---- Validation ----\n",
      "Validation loss: 63.1423\n",
      "Validation acc: 0.6185\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 296.0996, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 294.6078, Accuracy: 0.6286\n",
      "Training loss (for one batch) at step 20: 260.4826, Accuracy: 0.6514\n",
      "Training loss (for one batch) at step 30: 277.1354, Accuracy: 0.6774\n",
      "Training loss (for one batch) at step 40: 272.8544, Accuracy: 0.6896\n",
      "Training loss (for one batch) at step 50: 267.2847, Accuracy: 0.6975\n",
      "Training loss (for one batch) at step 60: 292.8349, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 70: 338.4248, Accuracy: 0.6936\n",
      "Training loss (for one batch) at step 80: 289.2760, Accuracy: 0.6842\n",
      "Training loss (for one batch) at step 90: 284.2531, Accuracy: 0.6830\n",
      "Training loss (for one batch) at step 100: 283.1017, Accuracy: 0.6848\n",
      "Training loss (for one batch) at step 110: 281.4889, Accuracy: 0.6829\n",
      "---- Training ----\n",
      "Training loss: 103.2563\n",
      "Training acc over epoch: 0.6816\n",
      "---- Validation ----\n",
      "Validation loss: 54.0595\n",
      "Validation acc: 0.6359\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 312.9502, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 309.8603, Accuracy: 0.6314\n",
      "Training loss (for one batch) at step 20: 285.0287, Accuracy: 0.6607\n",
      "Training loss (for one batch) at step 30: 271.6901, Accuracy: 0.6850\n",
      "Training loss (for one batch) at step 40: 277.4327, Accuracy: 0.6938\n",
      "Training loss (for one batch) at step 50: 284.7027, Accuracy: 0.7010\n",
      "Training loss (for one batch) at step 60: 274.4364, Accuracy: 0.7071\n",
      "Training loss (for one batch) at step 70: 294.1938, Accuracy: 0.6972\n",
      "Training loss (for one batch) at step 80: 292.9622, Accuracy: 0.6887\n",
      "Training loss (for one batch) at step 90: 285.8193, Accuracy: 0.6848\n",
      "Training loss (for one batch) at step 100: 291.4609, Accuracy: 0.6852\n",
      "Training loss (for one batch) at step 110: 280.0359, Accuracy: 0.6838\n",
      "---- Training ----\n",
      "Training loss: 102.2509\n",
      "Training acc over epoch: 0.6826\n",
      "---- Validation ----\n",
      "Validation loss: 46.2201\n",
      "Validation acc: 0.6131\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 316.8533, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 297.8642, Accuracy: 0.6257\n",
      "Training loss (for one batch) at step 20: 268.5845, Accuracy: 0.6615\n",
      "Training loss (for one batch) at step 30: 276.7141, Accuracy: 0.6878\n",
      "Training loss (for one batch) at step 40: 275.7909, Accuracy: 0.6974\n",
      "Training loss (for one batch) at step 50: 263.7992, Accuracy: 0.7057\n",
      "Training loss (for one batch) at step 60: 275.7881, Accuracy: 0.7108\n",
      "Training loss (for one batch) at step 70: 290.6476, Accuracy: 0.7021\n",
      "Training loss (for one batch) at step 80: 302.0113, Accuracy: 0.6912\n",
      "Training loss (for one batch) at step 90: 272.7007, Accuracy: 0.6896\n",
      "Training loss (for one batch) at step 100: 273.1285, Accuracy: 0.6876\n",
      "Training loss (for one batch) at step 110: 281.5075, Accuracy: 0.6879\n",
      "---- Training ----\n",
      "Training loss: 90.7175\n",
      "Training acc over epoch: 0.6857\n",
      "---- Validation ----\n",
      "Validation loss: 42.2964\n",
      "Validation acc: 0.6239\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 294.4929, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 284.3693, Accuracy: 0.6314\n",
      "Training loss (for one batch) at step 20: 266.9830, Accuracy: 0.6615\n",
      "Training loss (for one batch) at step 30: 283.5763, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 40: 264.1167, Accuracy: 0.6909\n",
      "Training loss (for one batch) at step 50: 261.7216, Accuracy: 0.7021\n",
      "Training loss (for one batch) at step 60: 279.2418, Accuracy: 0.7063\n",
      "Training loss (for one batch) at step 70: 283.4182, Accuracy: 0.6972\n",
      "Training loss (for one batch) at step 80: 294.6420, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 90: 271.9196, Accuracy: 0.6865\n",
      "Training loss (for one batch) at step 100: 277.5367, Accuracy: 0.6860\n",
      "Training loss (for one batch) at step 110: 296.5838, Accuracy: 0.6845\n",
      "---- Training ----\n",
      "Training loss: 87.2391\n",
      "Training acc over epoch: 0.6836\n",
      "---- Validation ----\n",
      "Validation loss: 58.9732\n",
      "Validation acc: 0.6145\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 286.1378, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 282.8430, Accuracy: 0.6314\n",
      "Training loss (for one batch) at step 20: 280.7374, Accuracy: 0.6678\n",
      "Training loss (for one batch) at step 30: 268.2721, Accuracy: 0.6850\n",
      "Training loss (for one batch) at step 40: 274.5352, Accuracy: 0.6944\n",
      "Training loss (for one batch) at step 50: 284.4666, Accuracy: 0.7037\n",
      "Training loss (for one batch) at step 60: 281.5149, Accuracy: 0.7070\n",
      "Training loss (for one batch) at step 70: 301.2953, Accuracy: 0.6987\n",
      "Training loss (for one batch) at step 80: 302.2922, Accuracy: 0.6880\n",
      "Training loss (for one batch) at step 90: 272.5447, Accuracy: 0.6848\n",
      "Training loss (for one batch) at step 100: 261.9996, Accuracy: 0.6847\n",
      "Training loss (for one batch) at step 110: 275.1925, Accuracy: 0.6833\n",
      "---- Training ----\n",
      "Training loss: 86.5236\n",
      "Training acc over epoch: 0.6824\n",
      "---- Validation ----\n",
      "Validation loss: 49.0645\n",
      "Validation acc: 0.6142\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 302.3916, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 306.6487, Accuracy: 0.6413\n",
      "Training loss (for one batch) at step 20: 288.5238, Accuracy: 0.6581\n",
      "Training loss (for one batch) at step 30: 284.9103, Accuracy: 0.6825\n",
      "Training loss (for one batch) at step 40: 277.1621, Accuracy: 0.6900\n",
      "Training loss (for one batch) at step 50: 265.2489, Accuracy: 0.6981\n",
      "Training loss (for one batch) at step 60: 287.6215, Accuracy: 0.7001\n",
      "Training loss (for one batch) at step 70: 288.6730, Accuracy: 0.6930\n",
      "Training loss (for one batch) at step 80: 289.7577, Accuracy: 0.6841\n",
      "Training loss (for one batch) at step 90: 272.5384, Accuracy: 0.6818\n",
      "Training loss (for one batch) at step 100: 288.6196, Accuracy: 0.6807\n",
      "Training loss (for one batch) at step 110: 277.0452, Accuracy: 0.6818\n",
      "---- Training ----\n",
      "Training loss: 95.0908\n",
      "Training acc over epoch: 0.6805\n",
      "---- Validation ----\n",
      "Validation loss: 60.8224\n",
      "Validation acc: 0.6236\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 304.6904, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 290.2104, Accuracy: 0.6349\n",
      "Training loss (for one batch) at step 20: 257.4441, Accuracy: 0.6659\n",
      "Training loss (for one batch) at step 30: 269.2055, Accuracy: 0.6883\n",
      "Training loss (for one batch) at step 40: 258.6139, Accuracy: 0.6894\n",
      "Training loss (for one batch) at step 50: 265.9935, Accuracy: 0.6994\n",
      "Training loss (for one batch) at step 60: 261.7870, Accuracy: 0.7047\n",
      "Training loss (for one batch) at step 70: 293.2414, Accuracy: 0.6992\n",
      "Training loss (for one batch) at step 80: 307.2684, Accuracy: 0.6905\n",
      "Training loss (for one batch) at step 90: 272.4947, Accuracy: 0.6860\n",
      "Training loss (for one batch) at step 100: 297.7078, Accuracy: 0.6863\n",
      "Training loss (for one batch) at step 110: 269.8199, Accuracy: 0.6855\n",
      "---- Training ----\n",
      "Training loss: 107.3933\n",
      "Training acc over epoch: 0.6840\n",
      "---- Validation ----\n",
      "Validation loss: 42.7674\n",
      "Validation acc: 0.6284\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 293.8556, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 300.6942, Accuracy: 0.6385\n",
      "Training loss (for one batch) at step 20: 281.1949, Accuracy: 0.6644\n",
      "Training loss (for one batch) at step 30: 264.4205, Accuracy: 0.6787\n",
      "Training loss (for one batch) at step 40: 254.6023, Accuracy: 0.6928\n",
      "Training loss (for one batch) at step 50: 263.8504, Accuracy: 0.6993\n",
      "Training loss (for one batch) at step 60: 268.8486, Accuracy: 0.7020\n",
      "Training loss (for one batch) at step 70: 299.8535, Accuracy: 0.6947\n",
      "Training loss (for one batch) at step 80: 295.9541, Accuracy: 0.6859\n",
      "Training loss (for one batch) at step 90: 255.0115, Accuracy: 0.6836\n",
      "Training loss (for one batch) at step 100: 264.8496, Accuracy: 0.6843\n",
      "Training loss (for one batch) at step 110: 269.2622, Accuracy: 0.6832\n",
      "---- Training ----\n",
      "Training loss: 91.5125\n",
      "Training acc over epoch: 0.6828\n",
      "---- Validation ----\n",
      "Validation loss: 49.4337\n",
      "Validation acc: 0.6088\n",
      "Time taken: 10.16s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 295.3968, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 310.6199, Accuracy: 0.6193\n",
      "Training loss (for one batch) at step 20: 279.9461, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 30: 280.2090, Accuracy: 0.6862\n",
      "Training loss (for one batch) at step 40: 264.6120, Accuracy: 0.6923\n",
      "Training loss (for one batch) at step 50: 267.0532, Accuracy: 0.7001\n",
      "Training loss (for one batch) at step 60: 283.4441, Accuracy: 0.7077\n",
      "Training loss (for one batch) at step 70: 310.7232, Accuracy: 0.6942\n",
      "Training loss (for one batch) at step 80: 288.4070, Accuracy: 0.6877\n",
      "Training loss (for one batch) at step 90: 265.8306, Accuracy: 0.6852\n",
      "Training loss (for one batch) at step 100: 275.9486, Accuracy: 0.6848\n",
      "Training loss (for one batch) at step 110: 299.5793, Accuracy: 0.6823\n",
      "---- Training ----\n",
      "Training loss: 87.5465\n",
      "Training acc over epoch: 0.6820\n",
      "---- Validation ----\n",
      "Validation loss: 40.4361\n",
      "Validation acc: 0.6201\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 291.3612, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 308.5347, Accuracy: 0.6271\n",
      "Training loss (for one batch) at step 20: 268.9037, Accuracy: 0.6600\n",
      "Training loss (for one batch) at step 30: 265.4155, Accuracy: 0.6845\n",
      "Training loss (for one batch) at step 40: 252.3259, Accuracy: 0.6947\n",
      "Training loss (for one batch) at step 50: 262.7047, Accuracy: 0.7010\n",
      "Training loss (for one batch) at step 60: 284.2544, Accuracy: 0.7059\n",
      "Training loss (for one batch) at step 70: 290.8493, Accuracy: 0.6964\n",
      "Training loss (for one batch) at step 80: 284.4916, Accuracy: 0.6886\n",
      "Training loss (for one batch) at step 90: 263.1494, Accuracy: 0.6847\n",
      "Training loss (for one batch) at step 100: 270.2129, Accuracy: 0.6846\n",
      "Training loss (for one batch) at step 110: 275.2447, Accuracy: 0.6832\n",
      "---- Training ----\n",
      "Training loss: 89.4755\n",
      "Training acc over epoch: 0.6832\n",
      "---- Validation ----\n",
      "Validation loss: 53.0833\n",
      "Validation acc: 0.6196\n",
      "Time taken: 10.22s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.6242\n",
      "Validation AUC: 0.6285\n",
      "Validation Balanced_ACC: 0.4093\n",
      "Validation MI: 0.1009\n",
      "Validation Normalized MI: 0.1479\n",
      "Validation Adjusted MI: 0.1479\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 624.0955, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 600.5129, Accuracy: 0.5227\n",
      "Training loss (for one batch) at step 20: 542.2660, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 30: 526.5534, Accuracy: 0.5139\n",
      "Training loss (for one batch) at step 40: 514.9249, Accuracy: 0.5093\n",
      "Training loss (for one batch) at step 50: 504.6431, Accuracy: 0.5116\n",
      "Training loss (for one batch) at step 60: 492.4019, Accuracy: 0.5120\n",
      "Training loss (for one batch) at step 70: 487.7754, Accuracy: 0.5114\n",
      "Training loss (for one batch) at step 80: 475.8612, Accuracy: 0.5119\n",
      "Training loss (for one batch) at step 90: 476.7937, Accuracy: 0.5084\n",
      "Training loss (for one batch) at step 100: 465.0034, Accuracy: 0.5088\n",
      "Training loss (for one batch) at step 110: 466.7677, Accuracy: 0.5084\n",
      "---- Training ----\n",
      "Training loss: 151.0490\n",
      "Training acc over epoch: 0.5105\n",
      "---- Validation ----\n",
      "Validation loss: 34.5303\n",
      "Validation acc: 0.5118\n",
      "Time taken: 12.41s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 448.4765, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 456.8954, Accuracy: 0.5376\n",
      "Training loss (for one batch) at step 20: 456.0067, Accuracy: 0.5327\n",
      "Training loss (for one batch) at step 30: 455.7407, Accuracy: 0.5343\n",
      "Training loss (for one batch) at step 40: 458.5711, Accuracy: 0.5267\n",
      "Training loss (for one batch) at step 50: 457.6298, Accuracy: 0.5236\n",
      "Training loss (for one batch) at step 60: 452.1611, Accuracy: 0.5252\n",
      "Training loss (for one batch) at step 70: 440.4144, Accuracy: 0.5257\n",
      "Training loss (for one batch) at step 80: 443.3881, Accuracy: 0.5256\n",
      "Training loss (for one batch) at step 90: 448.3925, Accuracy: 0.5272\n",
      "Training loss (for one batch) at step 100: 450.4009, Accuracy: 0.5237\n",
      "Training loss (for one batch) at step 110: 442.4211, Accuracy: 0.5225\n",
      "---- Training ----\n",
      "Training loss: 143.8656\n",
      "Training acc over epoch: 0.5225\n",
      "---- Validation ----\n",
      "Validation loss: 34.5470\n",
      "Validation acc: 0.5132\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 449.6955, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 446.1061, Accuracy: 0.5327\n",
      "Training loss (for one batch) at step 20: 446.7207, Accuracy: 0.5294\n",
      "Training loss (for one batch) at step 30: 451.1729, Accuracy: 0.5270\n",
      "Training loss (for one batch) at step 40: 446.1445, Accuracy: 0.5320\n",
      "Training loss (for one batch) at step 50: 439.8551, Accuracy: 0.5345\n",
      "Training loss (for one batch) at step 60: 450.2710, Accuracy: 0.5339\n",
      "Training loss (for one batch) at step 70: 445.9873, Accuracy: 0.5363\n",
      "Training loss (for one batch) at step 80: 444.6778, Accuracy: 0.5393\n",
      "Training loss (for one batch) at step 90: 445.1166, Accuracy: 0.5385\n",
      "Training loss (for one batch) at step 100: 445.8161, Accuracy: 0.5387\n",
      "Training loss (for one batch) at step 110: 443.7723, Accuracy: 0.5389\n",
      "---- Training ----\n",
      "Training loss: 138.3524\n",
      "Training acc over epoch: 0.5393\n",
      "---- Validation ----\n",
      "Validation loss: 34.5873\n",
      "Validation acc: 0.4922\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 445.8657, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 445.0302, Accuracy: 0.5639\n",
      "Training loss (for one batch) at step 20: 444.8687, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 30: 443.9247, Accuracy: 0.5544\n",
      "Training loss (for one batch) at step 40: 445.9668, Accuracy: 0.5501\n",
      "Training loss (for one batch) at step 50: 444.4943, Accuracy: 0.5487\n",
      "Training loss (for one batch) at step 60: 445.2134, Accuracy: 0.5501\n",
      "Training loss (for one batch) at step 70: 445.6992, Accuracy: 0.5472\n",
      "Training loss (for one batch) at step 80: 442.2214, Accuracy: 0.5508\n",
      "Training loss (for one batch) at step 90: 445.2254, Accuracy: 0.5550\n",
      "Training loss (for one batch) at step 100: 444.7998, Accuracy: 0.5536\n",
      "Training loss (for one batch) at step 110: 444.4576, Accuracy: 0.5512\n",
      "---- Training ----\n",
      "Training loss: 139.2600\n",
      "Training acc over epoch: 0.5508\n",
      "---- Validation ----\n",
      "Validation loss: 34.8694\n",
      "Validation acc: 0.5159\n",
      "Time taken: 10.51s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 442.9711, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 443.6735, Accuracy: 0.5682\n",
      "Training loss (for one batch) at step 20: 439.8965, Accuracy: 0.5670\n",
      "Training loss (for one batch) at step 30: 441.7756, Accuracy: 0.5713\n",
      "Training loss (for one batch) at step 40: 444.4949, Accuracy: 0.5688\n",
      "Training loss (for one batch) at step 50: 442.4793, Accuracy: 0.5692\n",
      "Training loss (for one batch) at step 60: 442.6215, Accuracy: 0.5693\n",
      "Training loss (for one batch) at step 70: 442.5318, Accuracy: 0.5682\n",
      "Training loss (for one batch) at step 80: 444.7344, Accuracy: 0.5676\n",
      "Training loss (for one batch) at step 90: 441.4619, Accuracy: 0.5680\n",
      "Training loss (for one batch) at step 100: 443.2878, Accuracy: 0.5685\n",
      "Training loss (for one batch) at step 110: 440.6222, Accuracy: 0.5659\n",
      "---- Training ----\n",
      "Training loss: 139.2125\n",
      "Training acc over epoch: 0.5654\n",
      "---- Validation ----\n",
      "Validation loss: 34.6256\n",
      "Validation acc: 0.5414\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 442.9681, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 440.9749, Accuracy: 0.5980\n",
      "Training loss (for one batch) at step 20: 444.3171, Accuracy: 0.5889\n",
      "Training loss (for one batch) at step 30: 446.6389, Accuracy: 0.5985\n",
      "Training loss (for one batch) at step 40: 442.6802, Accuracy: 0.5934\n",
      "Training loss (for one batch) at step 50: 440.6254, Accuracy: 0.5944\n",
      "Training loss (for one batch) at step 60: 441.1476, Accuracy: 0.5954\n",
      "Training loss (for one batch) at step 70: 442.0166, Accuracy: 0.5946\n",
      "Training loss (for one batch) at step 80: 443.6016, Accuracy: 0.5944\n",
      "Training loss (for one batch) at step 90: 444.2824, Accuracy: 0.5940\n",
      "Training loss (for one batch) at step 100: 441.1912, Accuracy: 0.5939\n",
      "Training loss (for one batch) at step 110: 441.6308, Accuracy: 0.5930\n",
      "---- Training ----\n",
      "Training loss: 138.0990\n",
      "Training acc over epoch: 0.5939\n",
      "---- Validation ----\n",
      "Validation loss: 35.1245\n",
      "Validation acc: 0.5728\n",
      "Time taken: 10.13s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 441.1132, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 443.9713, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 20: 443.0333, Accuracy: 0.6105\n",
      "Training loss (for one batch) at step 30: 436.4308, Accuracy: 0.6139\n",
      "Training loss (for one batch) at step 40: 440.4103, Accuracy: 0.6178\n",
      "Training loss (for one batch) at step 50: 439.7681, Accuracy: 0.6144\n",
      "Training loss (for one batch) at step 60: 445.4954, Accuracy: 0.6135\n",
      "Training loss (for one batch) at step 70: 441.8175, Accuracy: 0.6117\n",
      "Training loss (for one batch) at step 80: 445.5271, Accuracy: 0.6090\n",
      "Training loss (for one batch) at step 90: 438.2540, Accuracy: 0.6079\n",
      "Training loss (for one batch) at step 100: 439.3474, Accuracy: 0.6062\n",
      "Training loss (for one batch) at step 110: 439.3641, Accuracy: 0.6025\n",
      "---- Training ----\n",
      "Training loss: 133.9209\n",
      "Training acc over epoch: 0.6034\n",
      "---- Validation ----\n",
      "Validation loss: 34.8552\n",
      "Validation acc: 0.5892\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 439.3229, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 441.4126, Accuracy: 0.5945\n",
      "Training loss (for one batch) at step 20: 440.1903, Accuracy: 0.5934\n",
      "Training loss (for one batch) at step 30: 440.8759, Accuracy: 0.6033\n",
      "Training loss (for one batch) at step 40: 443.4301, Accuracy: 0.6136\n",
      "Training loss (for one batch) at step 50: 438.0870, Accuracy: 0.6129\n",
      "Training loss (for one batch) at step 60: 440.5746, Accuracy: 0.6101\n",
      "Training loss (for one batch) at step 70: 442.8022, Accuracy: 0.6123\n",
      "Training loss (for one batch) at step 80: 435.3506, Accuracy: 0.6156\n",
      "Training loss (for one batch) at step 90: 439.4394, Accuracy: 0.6149\n",
      "Training loss (for one batch) at step 100: 440.2212, Accuracy: 0.6156\n",
      "Training loss (for one batch) at step 110: 439.2754, Accuracy: 0.6146\n",
      "---- Training ----\n",
      "Training loss: 135.8534\n",
      "Training acc over epoch: 0.6156\n",
      "---- Validation ----\n",
      "Validation loss: 34.1178\n",
      "Validation acc: 0.6104\n",
      "Time taken: 10.57s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 438.7106, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 444.4812, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 20: 436.4156, Accuracy: 0.6164\n",
      "Training loss (for one batch) at step 30: 435.4788, Accuracy: 0.6215\n",
      "Training loss (for one batch) at step 40: 434.6545, Accuracy: 0.6288\n",
      "Training loss (for one batch) at step 50: 433.5061, Accuracy: 0.6334\n",
      "Training loss (for one batch) at step 60: 437.1944, Accuracy: 0.6335\n",
      "Training loss (for one batch) at step 70: 441.2908, Accuracy: 0.6346\n",
      "Training loss (for one batch) at step 80: 441.0260, Accuracy: 0.6342\n",
      "Training loss (for one batch) at step 90: 440.6167, Accuracy: 0.6321\n",
      "Training loss (for one batch) at step 100: 437.0648, Accuracy: 0.6317\n",
      "Training loss (for one batch) at step 110: 438.0721, Accuracy: 0.6338\n",
      "---- Training ----\n",
      "Training loss: 134.0816\n",
      "Training acc over epoch: 0.6348\n",
      "---- Validation ----\n",
      "Validation loss: 34.4441\n",
      "Validation acc: 0.6145\n",
      "Time taken: 10.16s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 441.5082, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 445.1779, Accuracy: 0.6278\n",
      "Training loss (for one batch) at step 20: 436.6242, Accuracy: 0.6295\n",
      "Training loss (for one batch) at step 30: 438.7602, Accuracy: 0.6343\n",
      "Training loss (for one batch) at step 40: 426.9753, Accuracy: 0.6465\n",
      "Training loss (for one batch) at step 50: 427.7095, Accuracy: 0.6492\n",
      "Training loss (for one batch) at step 60: 439.5099, Accuracy: 0.6502\n",
      "Training loss (for one batch) at step 70: 437.1346, Accuracy: 0.6514\n",
      "Training loss (for one batch) at step 80: 442.6560, Accuracy: 0.6454\n",
      "Training loss (for one batch) at step 90: 435.6678, Accuracy: 0.6433\n",
      "Training loss (for one batch) at step 100: 438.3574, Accuracy: 0.6428\n",
      "Training loss (for one batch) at step 110: 438.8289, Accuracy: 0.6452\n",
      "---- Training ----\n",
      "Training loss: 139.2085\n",
      "Training acc over epoch: 0.6475\n",
      "---- Validation ----\n",
      "Validation loss: 35.2806\n",
      "Validation acc: 0.6177\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 442.0097, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 439.3190, Accuracy: 0.6278\n",
      "Training loss (for one batch) at step 20: 440.2819, Accuracy: 0.6231\n",
      "Training loss (for one batch) at step 30: 434.7815, Accuracy: 0.6379\n",
      "Training loss (for one batch) at step 40: 435.6025, Accuracy: 0.6498\n",
      "Training loss (for one batch) at step 50: 433.9863, Accuracy: 0.6657\n",
      "Training loss (for one batch) at step 60: 440.2446, Accuracy: 0.6671\n",
      "Training loss (for one batch) at step 70: 442.0016, Accuracy: 0.6712\n",
      "Training loss (for one batch) at step 80: 439.7246, Accuracy: 0.6632\n",
      "Training loss (for one batch) at step 90: 439.2764, Accuracy: 0.6572\n",
      "Training loss (for one batch) at step 100: 427.8877, Accuracy: 0.6574\n",
      "Training loss (for one batch) at step 110: 436.3914, Accuracy: 0.6603\n",
      "---- Training ----\n",
      "Training loss: 137.4815\n",
      "Training acc over epoch: 0.6610\n",
      "---- Validation ----\n",
      "Validation loss: 35.7103\n",
      "Validation acc: 0.6521\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 441.1135, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 439.9562, Accuracy: 0.6491\n",
      "Training loss (for one batch) at step 20: 437.7239, Accuracy: 0.6410\n",
      "Training loss (for one batch) at step 30: 432.0096, Accuracy: 0.6527\n",
      "Training loss (for one batch) at step 40: 429.2562, Accuracy: 0.6601\n",
      "Training loss (for one batch) at step 50: 425.6814, Accuracy: 0.6748\n",
      "Training loss (for one batch) at step 60: 437.2828, Accuracy: 0.6788\n",
      "Training loss (for one batch) at step 70: 444.0203, Accuracy: 0.6785\n",
      "Training loss (for one batch) at step 80: 439.3205, Accuracy: 0.6736\n",
      "Training loss (for one batch) at step 90: 434.0751, Accuracy: 0.6682\n",
      "Training loss (for one batch) at step 100: 435.2615, Accuracy: 0.6672\n",
      "Training loss (for one batch) at step 110: 434.3457, Accuracy: 0.6691\n",
      "---- Training ----\n",
      "Training loss: 136.2858\n",
      "Training acc over epoch: 0.6700\n",
      "---- Validation ----\n",
      "Validation loss: 37.1461\n",
      "Validation acc: 0.6566\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 434.0674, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 436.9529, Accuracy: 0.6634\n",
      "Training loss (for one batch) at step 20: 431.4350, Accuracy: 0.6548\n",
      "Training loss (for one batch) at step 30: 436.2471, Accuracy: 0.6578\n",
      "Training loss (for one batch) at step 40: 424.6749, Accuracy: 0.6730\n",
      "Training loss (for one batch) at step 50: 422.2501, Accuracy: 0.6872\n",
      "Training loss (for one batch) at step 60: 425.2731, Accuracy: 0.6906\n",
      "Training loss (for one batch) at step 70: 436.3256, Accuracy: 0.6919\n",
      "Training loss (for one batch) at step 80: 439.8554, Accuracy: 0.6852\n",
      "Training loss (for one batch) at step 90: 430.1603, Accuracy: 0.6788\n",
      "Training loss (for one batch) at step 100: 426.7496, Accuracy: 0.6758\n",
      "Training loss (for one batch) at step 110: 432.1712, Accuracy: 0.6774\n",
      "---- Training ----\n",
      "Training loss: 134.0729\n",
      "Training acc over epoch: 0.6762\n",
      "---- Validation ----\n",
      "Validation loss: 35.1678\n",
      "Validation acc: 0.6827\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 433.1022, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 431.6337, Accuracy: 0.6705\n",
      "Training loss (for one batch) at step 20: 435.9506, Accuracy: 0.6693\n",
      "Training loss (for one batch) at step 30: 427.5189, Accuracy: 0.6842\n",
      "Training loss (for one batch) at step 40: 418.2176, Accuracy: 0.6995\n",
      "Training loss (for one batch) at step 50: 409.0248, Accuracy: 0.7093\n",
      "Training loss (for one batch) at step 60: 434.7909, Accuracy: 0.7176\n",
      "Training loss (for one batch) at step 70: 441.8826, Accuracy: 0.7163\n",
      "Training loss (for one batch) at step 80: 440.4206, Accuracy: 0.7087\n",
      "Training loss (for one batch) at step 90: 439.6255, Accuracy: 0.7010\n",
      "Training loss (for one batch) at step 100: 428.3275, Accuracy: 0.6997\n",
      "Training loss (for one batch) at step 110: 438.2870, Accuracy: 0.7002\n",
      "---- Training ----\n",
      "Training loss: 138.5227\n",
      "Training acc over epoch: 0.7018\n",
      "---- Validation ----\n",
      "Validation loss: 33.5275\n",
      "Validation acc: 0.6948\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 440.9640, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 438.5779, Accuracy: 0.6754\n",
      "Training loss (for one batch) at step 20: 435.3712, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 30: 428.9434, Accuracy: 0.6958\n",
      "Training loss (for one batch) at step 40: 406.2793, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 50: 407.5357, Accuracy: 0.7226\n",
      "Training loss (for one batch) at step 60: 416.2113, Accuracy: 0.7286\n",
      "Training loss (for one batch) at step 70: 440.1339, Accuracy: 0.7324\n",
      "Training loss (for one batch) at step 80: 437.0704, Accuracy: 0.7245\n",
      "Training loss (for one batch) at step 90: 432.7583, Accuracy: 0.7170\n",
      "Training loss (for one batch) at step 100: 426.3503, Accuracy: 0.7149\n",
      "Training loss (for one batch) at step 110: 435.3099, Accuracy: 0.7167\n",
      "---- Training ----\n",
      "Training loss: 133.4652\n",
      "Training acc over epoch: 0.7171\n",
      "---- Validation ----\n",
      "Validation loss: 36.1800\n",
      "Validation acc: 0.7340\n",
      "Time taken: 10.16s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 441.2907, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 440.5565, Accuracy: 0.6932\n",
      "Training loss (for one batch) at step 20: 430.5776, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 30: 414.1844, Accuracy: 0.6963\n",
      "Training loss (for one batch) at step 40: 411.0141, Accuracy: 0.7155\n",
      "Training loss (for one batch) at step 50: 403.9448, Accuracy: 0.7310\n",
      "Training loss (for one batch) at step 60: 412.6985, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 70: 432.3785, Accuracy: 0.7445\n",
      "Training loss (for one batch) at step 80: 428.8220, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 90: 425.9694, Accuracy: 0.7353\n",
      "Training loss (for one batch) at step 100: 426.8066, Accuracy: 0.7345\n",
      "Training loss (for one batch) at step 110: 431.3054, Accuracy: 0.7342\n",
      "---- Training ----\n",
      "Training loss: 136.6379\n",
      "Training acc over epoch: 0.7339\n",
      "---- Validation ----\n",
      "Validation loss: 36.1167\n",
      "Validation acc: 0.7222\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 439.6176, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 437.4940, Accuracy: 0.7060\n",
      "Training loss (for one batch) at step 20: 425.4533, Accuracy: 0.7013\n",
      "Training loss (for one batch) at step 30: 417.6317, Accuracy: 0.7140\n",
      "Training loss (for one batch) at step 40: 411.7840, Accuracy: 0.7315\n",
      "Training loss (for one batch) at step 50: 396.6660, Accuracy: 0.7448\n",
      "Training loss (for one batch) at step 60: 414.2844, Accuracy: 0.7540\n",
      "Training loss (for one batch) at step 70: 434.1134, Accuracy: 0.7547\n",
      "Training loss (for one batch) at step 80: 440.2618, Accuracy: 0.7503\n",
      "Training loss (for one batch) at step 90: 425.6077, Accuracy: 0.7453\n",
      "Training loss (for one batch) at step 100: 423.2579, Accuracy: 0.7447\n",
      "Training loss (for one batch) at step 110: 418.4359, Accuracy: 0.7454\n",
      "---- Training ----\n",
      "Training loss: 133.7577\n",
      "Training acc over epoch: 0.7454\n",
      "---- Validation ----\n",
      "Validation loss: 35.6181\n",
      "Validation acc: 0.7501\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 448.8899, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 429.3916, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 20: 436.6777, Accuracy: 0.7310\n",
      "Training loss (for one batch) at step 30: 408.5742, Accuracy: 0.7440\n",
      "Training loss (for one batch) at step 40: 415.5722, Accuracy: 0.7502\n",
      "Training loss (for one batch) at step 50: 395.0415, Accuracy: 0.7618\n",
      "Training loss (for one batch) at step 60: 406.9221, Accuracy: 0.7699\n",
      "Training loss (for one batch) at step 70: 433.9603, Accuracy: 0.7679\n",
      "Training loss (for one batch) at step 80: 446.6945, Accuracy: 0.7630\n",
      "Training loss (for one batch) at step 90: 424.4312, Accuracy: 0.7564\n",
      "Training loss (for one batch) at step 100: 421.8042, Accuracy: 0.7575\n",
      "Training loss (for one batch) at step 110: 426.3297, Accuracy: 0.7578\n",
      "---- Training ----\n",
      "Training loss: 136.3907\n",
      "Training acc over epoch: 0.7568\n",
      "---- Validation ----\n",
      "Validation loss: 38.4324\n",
      "Validation acc: 0.7571\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 433.7663, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 432.5944, Accuracy: 0.7379\n",
      "Training loss (for one batch) at step 20: 426.1937, Accuracy: 0.7243\n",
      "Training loss (for one batch) at step 30: 417.4494, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 40: 402.4885, Accuracy: 0.7590\n",
      "Training loss (for one batch) at step 50: 400.7171, Accuracy: 0.7688\n",
      "Training loss (for one batch) at step 60: 406.0730, Accuracy: 0.7769\n",
      "Training loss (for one batch) at step 70: 444.4696, Accuracy: 0.7762\n",
      "Training loss (for one batch) at step 80: 433.6313, Accuracy: 0.7707\n",
      "Training loss (for one batch) at step 90: 413.3492, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 100: 397.5133, Accuracy: 0.7657\n",
      "Training loss (for one batch) at step 110: 417.1289, Accuracy: 0.7658\n",
      "---- Training ----\n",
      "Training loss: 132.7067\n",
      "Training acc over epoch: 0.7660\n",
      "---- Validation ----\n",
      "Validation loss: 33.7047\n",
      "Validation acc: 0.7558\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 443.2766, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 427.8988, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 20: 422.9663, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 30: 398.3509, Accuracy: 0.7485\n",
      "Training loss (for one batch) at step 40: 400.1739, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 50: 398.5724, Accuracy: 0.7763\n",
      "Training loss (for one batch) at step 60: 397.7510, Accuracy: 0.7883\n",
      "Training loss (for one batch) at step 70: 410.0936, Accuracy: 0.7879\n",
      "Training loss (for one batch) at step 80: 431.3257, Accuracy: 0.7808\n",
      "Training loss (for one batch) at step 90: 419.3818, Accuracy: 0.7753\n",
      "Training loss (for one batch) at step 100: 406.7434, Accuracy: 0.7750\n",
      "Training loss (for one batch) at step 110: 415.5071, Accuracy: 0.7767\n",
      "---- Training ----\n",
      "Training loss: 135.1534\n",
      "Training acc over epoch: 0.7768\n",
      "---- Validation ----\n",
      "Validation loss: 37.1266\n",
      "Validation acc: 0.7531\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 439.4473, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 431.1557, Accuracy: 0.7493\n",
      "Training loss (for one batch) at step 20: 403.9462, Accuracy: 0.7385\n",
      "Training loss (for one batch) at step 30: 404.4361, Accuracy: 0.7601\n",
      "Training loss (for one batch) at step 40: 399.9239, Accuracy: 0.7689\n",
      "Training loss (for one batch) at step 50: 384.6949, Accuracy: 0.7834\n",
      "Training loss (for one batch) at step 60: 406.5532, Accuracy: 0.7894\n",
      "Training loss (for one batch) at step 70: 421.6696, Accuracy: 0.7886\n",
      "Training loss (for one batch) at step 80: 423.5855, Accuracy: 0.7828\n",
      "Training loss (for one batch) at step 90: 411.0245, Accuracy: 0.7782\n",
      "Training loss (for one batch) at step 100: 402.8192, Accuracy: 0.7784\n",
      "Training loss (for one batch) at step 110: 420.7470, Accuracy: 0.7798\n",
      "---- Training ----\n",
      "Training loss: 133.0898\n",
      "Training acc over epoch: 0.7810\n",
      "---- Validation ----\n",
      "Validation loss: 38.8520\n",
      "Validation acc: 0.7426\n",
      "Time taken: 10.04s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 444.5203, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 421.3832, Accuracy: 0.7472\n",
      "Training loss (for one batch) at step 20: 405.6536, Accuracy: 0.7418\n",
      "Training loss (for one batch) at step 30: 394.7005, Accuracy: 0.7563\n",
      "Training loss (for one batch) at step 40: 392.7586, Accuracy: 0.7698\n",
      "Training loss (for one batch) at step 50: 370.4857, Accuracy: 0.7829\n",
      "Training loss (for one batch) at step 60: 390.9430, Accuracy: 0.7915\n",
      "Training loss (for one batch) at step 70: 420.2645, Accuracy: 0.7883\n",
      "Training loss (for one batch) at step 80: 416.0056, Accuracy: 0.7830\n",
      "Training loss (for one batch) at step 90: 414.5480, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 100: 399.2938, Accuracy: 0.7792\n",
      "Training loss (for one batch) at step 110: 409.6726, Accuracy: 0.7809\n",
      "---- Training ----\n",
      "Training loss: 121.2140\n",
      "Training acc over epoch: 0.7806\n",
      "---- Validation ----\n",
      "Validation loss: 34.1199\n",
      "Validation acc: 0.7558\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 423.0268, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 422.3076, Accuracy: 0.7585\n",
      "Training loss (for one batch) at step 20: 409.9699, Accuracy: 0.7586\n",
      "Training loss (for one batch) at step 30: 385.8325, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 40: 381.4745, Accuracy: 0.7803\n",
      "Training loss (for one batch) at step 50: 368.3097, Accuracy: 0.7961\n",
      "Training loss (for one batch) at step 60: 414.1404, Accuracy: 0.8034\n",
      "Training loss (for one batch) at step 70: 423.3633, Accuracy: 0.8003\n",
      "Training loss (for one batch) at step 80: 415.0766, Accuracy: 0.7929\n",
      "Training loss (for one batch) at step 90: 408.5518, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 100: 392.9613, Accuracy: 0.7902\n",
      "Training loss (for one batch) at step 110: 393.7795, Accuracy: 0.7910\n",
      "---- Training ----\n",
      "Training loss: 123.1592\n",
      "Training acc over epoch: 0.7904\n",
      "---- Validation ----\n",
      "Validation loss: 41.8895\n",
      "Validation acc: 0.7590\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 452.2896, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 422.1721, Accuracy: 0.7479\n",
      "Training loss (for one batch) at step 20: 397.0905, Accuracy: 0.7597\n",
      "Training loss (for one batch) at step 30: 379.8429, Accuracy: 0.7752\n",
      "Training loss (for one batch) at step 40: 377.7816, Accuracy: 0.7906\n",
      "Training loss (for one batch) at step 50: 370.2812, Accuracy: 0.8027\n",
      "Training loss (for one batch) at step 60: 376.3529, Accuracy: 0.8124\n",
      "Training loss (for one batch) at step 70: 411.0226, Accuracy: 0.8081\n",
      "Training loss (for one batch) at step 80: 407.3029, Accuracy: 0.7998\n",
      "Training loss (for one batch) at step 90: 395.8319, Accuracy: 0.7945\n",
      "Training loss (for one batch) at step 100: 390.6935, Accuracy: 0.7946\n",
      "Training loss (for one batch) at step 110: 397.7451, Accuracy: 0.7948\n",
      "---- Training ----\n",
      "Training loss: 126.2509\n",
      "Training acc over epoch: 0.7940\n",
      "---- Validation ----\n",
      "Validation loss: 33.0338\n",
      "Validation acc: 0.7528\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 433.9170, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 436.8884, Accuracy: 0.7472\n",
      "Training loss (for one batch) at step 20: 400.0234, Accuracy: 0.7530\n",
      "Training loss (for one batch) at step 30: 377.5556, Accuracy: 0.7762\n",
      "Training loss (for one batch) at step 40: 387.1223, Accuracy: 0.7885\n",
      "Training loss (for one batch) at step 50: 363.8779, Accuracy: 0.8016\n",
      "Training loss (for one batch) at step 60: 374.8978, Accuracy: 0.8081\n",
      "Training loss (for one batch) at step 70: 404.6855, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 80: 423.7387, Accuracy: 0.7961\n",
      "Training loss (for one batch) at step 90: 397.1634, Accuracy: 0.7916\n",
      "Training loss (for one batch) at step 100: 381.6049, Accuracy: 0.7928\n",
      "Training loss (for one batch) at step 110: 396.6842, Accuracy: 0.7910\n",
      "---- Training ----\n",
      "Training loss: 137.1968\n",
      "Training acc over epoch: 0.7902\n",
      "---- Validation ----\n",
      "Validation loss: 32.9606\n",
      "Validation acc: 0.7281\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 418.7485, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 401.6918, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 20: 397.5683, Accuracy: 0.7608\n",
      "Training loss (for one batch) at step 30: 386.0999, Accuracy: 0.7729\n",
      "Training loss (for one batch) at step 40: 381.2128, Accuracy: 0.7887\n",
      "Training loss (for one batch) at step 50: 360.0104, Accuracy: 0.8007\n",
      "Training loss (for one batch) at step 60: 375.1376, Accuracy: 0.8089\n",
      "Training loss (for one batch) at step 70: 409.0036, Accuracy: 0.8084\n",
      "Training loss (for one batch) at step 80: 422.3734, Accuracy: 0.7985\n",
      "Training loss (for one batch) at step 90: 393.5482, Accuracy: 0.7958\n",
      "Training loss (for one batch) at step 100: 390.7516, Accuracy: 0.7973\n",
      "Training loss (for one batch) at step 110: 404.0267, Accuracy: 0.7984\n",
      "---- Training ----\n",
      "Training loss: 122.3384\n",
      "Training acc over epoch: 0.7972\n",
      "---- Validation ----\n",
      "Validation loss: 40.7349\n",
      "Validation acc: 0.7380\n",
      "Time taken: 10.65s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 422.2205, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 417.2177, Accuracy: 0.7401\n",
      "Training loss (for one batch) at step 20: 386.7548, Accuracy: 0.7567\n",
      "Training loss (for one batch) at step 30: 383.2093, Accuracy: 0.7797\n",
      "Training loss (for one batch) at step 40: 348.1178, Accuracy: 0.7967\n",
      "Training loss (for one batch) at step 50: 350.1728, Accuracy: 0.8116\n",
      "Training loss (for one batch) at step 60: 394.6334, Accuracy: 0.8198\n",
      "Training loss (for one batch) at step 70: 397.4363, Accuracy: 0.8119\n",
      "Training loss (for one batch) at step 80: 414.5620, Accuracy: 0.8037\n",
      "Training loss (for one batch) at step 90: 375.7059, Accuracy: 0.8006\n",
      "Training loss (for one batch) at step 100: 370.7232, Accuracy: 0.7994\n",
      "Training loss (for one batch) at step 110: 390.4373, Accuracy: 0.8005\n",
      "---- Training ----\n",
      "Training loss: 122.8298\n",
      "Training acc over epoch: 0.7994\n",
      "---- Validation ----\n",
      "Validation loss: 37.6857\n",
      "Validation acc: 0.7442\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 444.1297, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 411.1472, Accuracy: 0.7337\n",
      "Training loss (for one batch) at step 20: 398.2412, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 30: 375.4012, Accuracy: 0.7727\n",
      "Training loss (for one batch) at step 40: 386.3522, Accuracy: 0.7875\n",
      "Training loss (for one batch) at step 50: 351.0620, Accuracy: 0.8009\n",
      "Training loss (for one batch) at step 60: 377.7084, Accuracy: 0.8112\n",
      "Training loss (for one batch) at step 70: 397.7284, Accuracy: 0.8070\n",
      "Training loss (for one batch) at step 80: 392.0688, Accuracy: 0.7984\n",
      "Training loss (for one batch) at step 90: 383.3285, Accuracy: 0.7951\n",
      "Training loss (for one batch) at step 100: 387.8445, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 110: 372.4002, Accuracy: 0.7974\n",
      "---- Training ----\n",
      "Training loss: 108.6426\n",
      "Training acc over epoch: 0.7983\n",
      "---- Validation ----\n",
      "Validation loss: 34.6180\n",
      "Validation acc: 0.7544\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 410.6078, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 403.2350, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 20: 377.7940, Accuracy: 0.7533\n",
      "Training loss (for one batch) at step 30: 367.5342, Accuracy: 0.7772\n",
      "Training loss (for one batch) at step 40: 339.7444, Accuracy: 0.7954\n",
      "Training loss (for one batch) at step 50: 347.1340, Accuracy: 0.8091\n",
      "Training loss (for one batch) at step 60: 384.9706, Accuracy: 0.8174\n",
      "Training loss (for one batch) at step 70: 383.5475, Accuracy: 0.8118\n",
      "Training loss (for one batch) at step 80: 410.9435, Accuracy: 0.8019\n",
      "Training loss (for one batch) at step 90: 386.7592, Accuracy: 0.7976\n",
      "Training loss (for one batch) at step 100: 369.5105, Accuracy: 0.7987\n",
      "Training loss (for one batch) at step 110: 394.5638, Accuracy: 0.7996\n",
      "---- Training ----\n",
      "Training loss: 118.7559\n",
      "Training acc over epoch: 0.7982\n",
      "---- Validation ----\n",
      "Validation loss: 35.4819\n",
      "Validation acc: 0.7477\n",
      "Time taken: 10.63s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 417.7224, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 406.2575, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 20: 365.2330, Accuracy: 0.7630\n",
      "Training loss (for one batch) at step 30: 358.7225, Accuracy: 0.7820\n",
      "Training loss (for one batch) at step 40: 342.1831, Accuracy: 0.8022\n",
      "Training loss (for one batch) at step 50: 338.9207, Accuracy: 0.8177\n",
      "Training loss (for one batch) at step 60: 367.2663, Accuracy: 0.8235\n",
      "Training loss (for one batch) at step 70: 376.5023, Accuracy: 0.8150\n",
      "Training loss (for one batch) at step 80: 393.0760, Accuracy: 0.8043\n",
      "Training loss (for one batch) at step 90: 363.5921, Accuracy: 0.8009\n",
      "Training loss (for one batch) at step 100: 369.0167, Accuracy: 0.8044\n",
      "Training loss (for one batch) at step 110: 383.1366, Accuracy: 0.8041\n",
      "---- Training ----\n",
      "Training loss: 120.4468\n",
      "Training acc over epoch: 0.8043\n",
      "---- Validation ----\n",
      "Validation loss: 44.3435\n",
      "Validation acc: 0.7483\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 404.5839, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 392.0368, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 20: 383.4077, Accuracy: 0.7593\n",
      "Training loss (for one batch) at step 30: 362.3706, Accuracy: 0.7843\n",
      "Training loss (for one batch) at step 40: 338.5391, Accuracy: 0.8035\n",
      "Training loss (for one batch) at step 50: 321.5162, Accuracy: 0.8191\n",
      "Training loss (for one batch) at step 60: 354.8499, Accuracy: 0.8262\n",
      "Training loss (for one batch) at step 70: 374.3842, Accuracy: 0.8184\n",
      "Training loss (for one batch) at step 80: 399.9714, Accuracy: 0.8094\n",
      "Training loss (for one batch) at step 90: 362.1908, Accuracy: 0.8034\n",
      "Training loss (for one batch) at step 100: 363.4787, Accuracy: 0.8067\n",
      "Training loss (for one batch) at step 110: 363.6729, Accuracy: 0.8064\n",
      "---- Training ----\n",
      "Training loss: 120.3138\n",
      "Training acc over epoch: 0.8048\n",
      "---- Validation ----\n",
      "Validation loss: 34.9224\n",
      "Validation acc: 0.7251\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 406.0311, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 392.9434, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 20: 349.8612, Accuracy: 0.7489\n",
      "Training loss (for one batch) at step 30: 354.6264, Accuracy: 0.7782\n",
      "Training loss (for one batch) at step 40: 343.6540, Accuracy: 0.7990\n",
      "Training loss (for one batch) at step 50: 333.3031, Accuracy: 0.8174\n",
      "Training loss (for one batch) at step 60: 355.0817, Accuracy: 0.8239\n",
      "Training loss (for one batch) at step 70: 385.1720, Accuracy: 0.8172\n",
      "Training loss (for one batch) at step 80: 419.0933, Accuracy: 0.8083\n",
      "Training loss (for one batch) at step 90: 362.1662, Accuracy: 0.8045\n",
      "Training loss (for one batch) at step 100: 337.6126, Accuracy: 0.8077\n",
      "Training loss (for one batch) at step 110: 377.4767, Accuracy: 0.8070\n",
      "---- Training ----\n",
      "Training loss: 113.8110\n",
      "Training acc over epoch: 0.8055\n",
      "---- Validation ----\n",
      "Validation loss: 44.6737\n",
      "Validation acc: 0.7418\n",
      "Time taken: 10.60s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 404.1017, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 381.9276, Accuracy: 0.7159\n",
      "Training loss (for one batch) at step 20: 377.7605, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 30: 346.6623, Accuracy: 0.7755\n",
      "Training loss (for one batch) at step 40: 349.6513, Accuracy: 0.7934\n",
      "Training loss (for one batch) at step 50: 356.5408, Accuracy: 0.8096\n",
      "Training loss (for one batch) at step 60: 352.5937, Accuracy: 0.8166\n",
      "Training loss (for one batch) at step 70: 382.4911, Accuracy: 0.8104\n",
      "Training loss (for one batch) at step 80: 396.7820, Accuracy: 0.8009\n",
      "Training loss (for one batch) at step 90: 356.8464, Accuracy: 0.7985\n",
      "Training loss (for one batch) at step 100: 367.3281, Accuracy: 0.8023\n",
      "Training loss (for one batch) at step 110: 366.4062, Accuracy: 0.8041\n",
      "---- Training ----\n",
      "Training loss: 120.0121\n",
      "Training acc over epoch: 0.8034\n",
      "---- Validation ----\n",
      "Validation loss: 38.2994\n",
      "Validation acc: 0.7335\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 394.2392, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 386.9621, Accuracy: 0.7230\n",
      "Training loss (for one batch) at step 20: 380.7868, Accuracy: 0.7537\n",
      "Training loss (for one batch) at step 30: 347.3099, Accuracy: 0.7855\n",
      "Training loss (for one batch) at step 40: 335.6801, Accuracy: 0.8062\n",
      "Training loss (for one batch) at step 50: 336.4801, Accuracy: 0.8228\n",
      "Training loss (for one batch) at step 60: 364.1652, Accuracy: 0.8284\n",
      "Training loss (for one batch) at step 70: 369.2271, Accuracy: 0.8223\n",
      "Training loss (for one batch) at step 80: 384.5612, Accuracy: 0.8097\n",
      "Training loss (for one batch) at step 90: 353.7389, Accuracy: 0.8050\n",
      "Training loss (for one batch) at step 100: 344.0750, Accuracy: 0.8065\n",
      "Training loss (for one batch) at step 110: 362.6033, Accuracy: 0.8076\n",
      "---- Training ----\n",
      "Training loss: 117.4437\n",
      "Training acc over epoch: 0.8064\n",
      "---- Validation ----\n",
      "Validation loss: 42.8063\n",
      "Validation acc: 0.7523\n",
      "Time taken: 10.13s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 413.5610, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 387.1244, Accuracy: 0.7202\n",
      "Training loss (for one batch) at step 20: 355.4038, Accuracy: 0.7463\n",
      "Training loss (for one batch) at step 30: 333.9773, Accuracy: 0.7795\n",
      "Training loss (for one batch) at step 40: 322.0596, Accuracy: 0.8003\n",
      "Training loss (for one batch) at step 50: 323.0619, Accuracy: 0.8171\n",
      "Training loss (for one batch) at step 60: 345.6872, Accuracy: 0.8238\n",
      "Training loss (for one batch) at step 70: 371.1433, Accuracy: 0.8150\n",
      "Training loss (for one batch) at step 80: 400.0743, Accuracy: 0.8044\n",
      "Training loss (for one batch) at step 90: 340.0247, Accuracy: 0.8008\n",
      "Training loss (for one batch) at step 100: 335.9541, Accuracy: 0.8034\n",
      "Training loss (for one batch) at step 110: 356.4109, Accuracy: 0.8027\n",
      "---- Training ----\n",
      "Training loss: 115.3457\n",
      "Training acc over epoch: 0.8010\n",
      "---- Validation ----\n",
      "Validation loss: 44.8745\n",
      "Validation acc: 0.7351\n",
      "Time taken: 10.51s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 402.0332, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 380.0237, Accuracy: 0.7145\n",
      "Training loss (for one batch) at step 20: 342.2015, Accuracy: 0.7563\n",
      "Training loss (for one batch) at step 30: 335.2775, Accuracy: 0.7898\n",
      "Training loss (for one batch) at step 40: 325.5019, Accuracy: 0.8102\n",
      "Training loss (for one batch) at step 50: 307.4499, Accuracy: 0.8252\n",
      "Training loss (for one batch) at step 60: 341.1915, Accuracy: 0.8336\n",
      "Training loss (for one batch) at step 70: 388.4189, Accuracy: 0.8231\n",
      "Training loss (for one batch) at step 80: 370.0025, Accuracy: 0.8094\n",
      "Training loss (for one batch) at step 90: 341.2106, Accuracy: 0.8036\n",
      "Training loss (for one batch) at step 100: 337.0657, Accuracy: 0.8069\n",
      "Training loss (for one batch) at step 110: 339.5214, Accuracy: 0.8057\n",
      "---- Training ----\n",
      "Training loss: 121.2193\n",
      "Training acc over epoch: 0.8068\n",
      "---- Validation ----\n",
      "Validation loss: 47.0058\n",
      "Validation acc: 0.7343\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 383.4332, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 378.4260, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 20: 374.2711, Accuracy: 0.7478\n",
      "Training loss (for one batch) at step 30: 348.7433, Accuracy: 0.7815\n",
      "Training loss (for one batch) at step 40: 333.7275, Accuracy: 0.8030\n",
      "Training loss (for one batch) at step 50: 334.6885, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 60: 348.6850, Accuracy: 0.8272\n",
      "Training loss (for one batch) at step 70: 387.2579, Accuracy: 0.8181\n",
      "Training loss (for one batch) at step 80: 368.1575, Accuracy: 0.8059\n",
      "Training loss (for one batch) at step 90: 345.9124, Accuracy: 0.8022\n",
      "Training loss (for one batch) at step 100: 351.7949, Accuracy: 0.8038\n",
      "Training loss (for one batch) at step 110: 352.5655, Accuracy: 0.8060\n",
      "---- Training ----\n",
      "Training loss: 145.1091\n",
      "Training acc over epoch: 0.8053\n",
      "---- Validation ----\n",
      "Validation loss: 53.4678\n",
      "Validation acc: 0.7292\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 396.4751, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 380.1237, Accuracy: 0.7166\n",
      "Training loss (for one batch) at step 20: 340.9070, Accuracy: 0.7504\n",
      "Training loss (for one batch) at step 30: 312.1158, Accuracy: 0.7870\n",
      "Training loss (for one batch) at step 40: 315.8513, Accuracy: 0.8087\n",
      "Training loss (for one batch) at step 50: 356.1932, Accuracy: 0.8243\n",
      "Training loss (for one batch) at step 60: 329.0997, Accuracy: 0.8313\n",
      "Training loss (for one batch) at step 70: 365.1850, Accuracy: 0.8210\n",
      "Training loss (for one batch) at step 80: 385.0425, Accuracy: 0.8090\n",
      "Training loss (for one batch) at step 90: 351.4445, Accuracy: 0.8044\n",
      "Training loss (for one batch) at step 100: 323.9798, Accuracy: 0.8069\n",
      "Training loss (for one batch) at step 110: 353.1060, Accuracy: 0.8086\n",
      "---- Training ----\n",
      "Training loss: 110.9303\n",
      "Training acc over epoch: 0.8074\n",
      "---- Validation ----\n",
      "Validation loss: 35.8356\n",
      "Validation acc: 0.7356\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 388.8681, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 383.0971, Accuracy: 0.7308\n",
      "Training loss (for one batch) at step 20: 370.4604, Accuracy: 0.7478\n",
      "Training loss (for one batch) at step 30: 345.7930, Accuracy: 0.7780\n",
      "Training loss (for one batch) at step 40: 324.5988, Accuracy: 0.8043\n",
      "Training loss (for one batch) at step 50: 311.8338, Accuracy: 0.8222\n",
      "Training loss (for one batch) at step 60: 333.6470, Accuracy: 0.8297\n",
      "Training loss (for one batch) at step 70: 337.5984, Accuracy: 0.8208\n",
      "Training loss (for one batch) at step 80: 372.1962, Accuracy: 0.8088\n",
      "Training loss (for one batch) at step 90: 326.7538, Accuracy: 0.8065\n",
      "Training loss (for one batch) at step 100: 338.8842, Accuracy: 0.8093\n",
      "Training loss (for one batch) at step 110: 355.4703, Accuracy: 0.8112\n",
      "---- Training ----\n",
      "Training loss: 111.3261\n",
      "Training acc over epoch: 0.8095\n",
      "---- Validation ----\n",
      "Validation loss: 37.0267\n",
      "Validation acc: 0.7437\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 405.7015, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 386.3358, Accuracy: 0.7202\n",
      "Training loss (for one batch) at step 20: 343.0031, Accuracy: 0.7448\n",
      "Training loss (for one batch) at step 30: 316.8283, Accuracy: 0.7828\n",
      "Training loss (for one batch) at step 40: 296.5337, Accuracy: 0.8070\n",
      "Training loss (for one batch) at step 50: 307.7340, Accuracy: 0.8244\n",
      "Training loss (for one batch) at step 60: 326.1111, Accuracy: 0.8320\n",
      "Training loss (for one batch) at step 70: 346.9789, Accuracy: 0.8215\n",
      "Training loss (for one batch) at step 80: 372.2260, Accuracy: 0.8090\n",
      "Training loss (for one batch) at step 90: 345.8032, Accuracy: 0.8042\n",
      "Training loss (for one batch) at step 100: 323.1558, Accuracy: 0.8070\n",
      "Training loss (for one batch) at step 110: 350.3453, Accuracy: 0.8082\n",
      "---- Training ----\n",
      "Training loss: 111.8122\n",
      "Training acc over epoch: 0.8079\n",
      "---- Validation ----\n",
      "Validation loss: 52.0039\n",
      "Validation acc: 0.7464\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 379.3522, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 379.5722, Accuracy: 0.7152\n",
      "Training loss (for one batch) at step 20: 344.7471, Accuracy: 0.7459\n",
      "Training loss (for one batch) at step 30: 307.9622, Accuracy: 0.7760\n",
      "Training loss (for one batch) at step 40: 327.7424, Accuracy: 0.8039\n",
      "Training loss (for one batch) at step 50: 308.2704, Accuracy: 0.8186\n",
      "Training loss (for one batch) at step 60: 344.5089, Accuracy: 0.8295\n",
      "Training loss (for one batch) at step 70: 383.8897, Accuracy: 0.8176\n",
      "Training loss (for one batch) at step 80: 376.0097, Accuracy: 0.8060\n",
      "Training loss (for one batch) at step 90: 347.8109, Accuracy: 0.8036\n",
      "Training loss (for one batch) at step 100: 319.8516, Accuracy: 0.8080\n",
      "Training loss (for one batch) at step 110: 339.8072, Accuracy: 0.8076\n",
      "---- Training ----\n",
      "Training loss: 107.1574\n",
      "Training acc over epoch: 0.8068\n",
      "---- Validation ----\n",
      "Validation loss: 40.5005\n",
      "Validation acc: 0.7397\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 387.2183, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 372.3741, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 20: 334.7872, Accuracy: 0.7552\n",
      "Training loss (for one batch) at step 30: 319.5880, Accuracy: 0.7860\n",
      "Training loss (for one batch) at step 40: 321.6752, Accuracy: 0.8087\n",
      "Training loss (for one batch) at step 50: 295.2382, Accuracy: 0.8271\n",
      "Training loss (for one batch) at step 60: 335.8068, Accuracy: 0.8336\n",
      "Training loss (for one batch) at step 70: 346.2059, Accuracy: 0.8238\n",
      "Training loss (for one batch) at step 80: 355.5319, Accuracy: 0.8115\n",
      "Training loss (for one batch) at step 90: 337.2155, Accuracy: 0.8082\n",
      "Training loss (for one batch) at step 100: 319.7064, Accuracy: 0.8099\n",
      "Training loss (for one batch) at step 110: 350.1140, Accuracy: 0.8104\n",
      "---- Training ----\n",
      "Training loss: 114.0128\n",
      "Training acc over epoch: 0.8088\n",
      "---- Validation ----\n",
      "Validation loss: 53.8512\n",
      "Validation acc: 0.7434\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 390.5927, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 365.3200, Accuracy: 0.7010\n",
      "Training loss (for one batch) at step 20: 327.5456, Accuracy: 0.7526\n",
      "Training loss (for one batch) at step 30: 320.6153, Accuracy: 0.7818\n",
      "Training loss (for one batch) at step 40: 302.2533, Accuracy: 0.8070\n",
      "Training loss (for one batch) at step 50: 313.5751, Accuracy: 0.8223\n",
      "Training loss (for one batch) at step 60: 342.1904, Accuracy: 0.8317\n",
      "Training loss (for one batch) at step 70: 365.2432, Accuracy: 0.8178\n",
      "Training loss (for one batch) at step 80: 386.9334, Accuracy: 0.8033\n",
      "Training loss (for one batch) at step 90: 337.5521, Accuracy: 0.7992\n",
      "Training loss (for one batch) at step 100: 326.7871, Accuracy: 0.8029\n",
      "Training loss (for one batch) at step 110: 334.4561, Accuracy: 0.8053\n",
      "---- Training ----\n",
      "Training loss: 118.8879\n",
      "Training acc over epoch: 0.8045\n",
      "---- Validation ----\n",
      "Validation loss: 48.6200\n",
      "Validation acc: 0.7402\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 381.2332, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 366.8440, Accuracy: 0.7166\n",
      "Training loss (for one batch) at step 20: 323.7030, Accuracy: 0.7563\n",
      "Training loss (for one batch) at step 30: 338.5195, Accuracy: 0.7911\n",
      "Training loss (for one batch) at step 40: 313.2613, Accuracy: 0.8108\n",
      "Training loss (for one batch) at step 50: 308.2194, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 60: 325.9366, Accuracy: 0.8350\n",
      "Training loss (for one batch) at step 70: 356.2377, Accuracy: 0.8231\n",
      "Training loss (for one batch) at step 80: 389.4000, Accuracy: 0.8076\n",
      "Training loss (for one batch) at step 90: 325.5136, Accuracy: 0.8034\n",
      "Training loss (for one batch) at step 100: 316.3987, Accuracy: 0.8069\n",
      "Training loss (for one batch) at step 110: 332.1519, Accuracy: 0.8080\n",
      "---- Training ----\n",
      "Training loss: 111.4994\n",
      "Training acc over epoch: 0.8076\n",
      "---- Validation ----\n",
      "Validation loss: 49.6429\n",
      "Validation acc: 0.7372\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 363.5346, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 361.6065, Accuracy: 0.7216\n",
      "Training loss (for one batch) at step 20: 317.4023, Accuracy: 0.7530\n",
      "Training loss (for one batch) at step 30: 307.9390, Accuracy: 0.7876\n",
      "Training loss (for one batch) at step 40: 309.7447, Accuracy: 0.8112\n",
      "Training loss (for one batch) at step 50: 309.7658, Accuracy: 0.8290\n",
      "Training loss (for one batch) at step 60: 347.1107, Accuracy: 0.8347\n",
      "Training loss (for one batch) at step 70: 351.3482, Accuracy: 0.8232\n",
      "Training loss (for one batch) at step 80: 383.5239, Accuracy: 0.8107\n",
      "Training loss (for one batch) at step 90: 361.2736, Accuracy: 0.8077\n",
      "Training loss (for one batch) at step 100: 307.4764, Accuracy: 0.8106\n",
      "Training loss (for one batch) at step 110: 325.7852, Accuracy: 0.8112\n",
      "---- Training ----\n",
      "Training loss: 104.7469\n",
      "Training acc over epoch: 0.8092\n",
      "---- Validation ----\n",
      "Validation loss: 28.9928\n",
      "Validation acc: 0.7421\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 382.6197, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 350.9743, Accuracy: 0.7138\n",
      "Training loss (for one batch) at step 20: 323.3436, Accuracy: 0.7545\n",
      "Training loss (for one batch) at step 30: 321.6968, Accuracy: 0.7881\n",
      "Training loss (for one batch) at step 40: 309.3276, Accuracy: 0.8115\n",
      "Training loss (for one batch) at step 50: 300.7818, Accuracy: 0.8289\n",
      "Training loss (for one batch) at step 60: 324.8014, Accuracy: 0.8366\n",
      "Training loss (for one batch) at step 70: 362.4486, Accuracy: 0.8225\n",
      "Training loss (for one batch) at step 80: 362.8833, Accuracy: 0.8101\n",
      "Training loss (for one batch) at step 90: 319.8872, Accuracy: 0.8061\n",
      "Training loss (for one batch) at step 100: 321.3817, Accuracy: 0.8094\n",
      "Training loss (for one batch) at step 110: 329.8564, Accuracy: 0.8091\n",
      "---- Training ----\n",
      "Training loss: 106.5074\n",
      "Training acc over epoch: 0.8074\n",
      "---- Validation ----\n",
      "Validation loss: 52.2279\n",
      "Validation acc: 0.7356\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 359.6368, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 362.1295, Accuracy: 0.7301\n",
      "Training loss (for one batch) at step 20: 325.9900, Accuracy: 0.7615\n",
      "Training loss (for one batch) at step 30: 335.2856, Accuracy: 0.7896\n",
      "Training loss (for one batch) at step 40: 314.7259, Accuracy: 0.8108\n",
      "Training loss (for one batch) at step 50: 310.3047, Accuracy: 0.8278\n",
      "Training loss (for one batch) at step 60: 342.2597, Accuracy: 0.8354\n",
      "Training loss (for one batch) at step 70: 352.7815, Accuracy: 0.8260\n",
      "Training loss (for one batch) at step 80: 362.1838, Accuracy: 0.8103\n",
      "Training loss (for one batch) at step 90: 293.3798, Accuracy: 0.8063\n",
      "Training loss (for one batch) at step 100: 320.7938, Accuracy: 0.8103\n",
      "Training loss (for one batch) at step 110: 349.5713, Accuracy: 0.8123\n",
      "---- Training ----\n",
      "Training loss: 102.2892\n",
      "Training acc over epoch: 0.8109\n",
      "---- Validation ----\n",
      "Validation loss: 50.4777\n",
      "Validation acc: 0.7359\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 374.4395, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 359.8555, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 20: 328.4129, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 30: 320.1249, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 40: 324.7906, Accuracy: 0.8089\n",
      "Training loss (for one batch) at step 50: 302.7759, Accuracy: 0.8243\n",
      "Training loss (for one batch) at step 60: 325.2843, Accuracy: 0.8334\n",
      "Training loss (for one batch) at step 70: 326.1589, Accuracy: 0.8234\n",
      "Training loss (for one batch) at step 80: 349.8965, Accuracy: 0.8094\n",
      "Training loss (for one batch) at step 90: 329.0778, Accuracy: 0.8049\n",
      "Training loss (for one batch) at step 100: 329.1111, Accuracy: 0.8078\n",
      "Training loss (for one batch) at step 110: 332.3042, Accuracy: 0.8100\n",
      "---- Training ----\n",
      "Training loss: 104.1165\n",
      "Training acc over epoch: 0.8084\n",
      "---- Validation ----\n",
      "Validation loss: 52.9029\n",
      "Validation acc: 0.7413\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 380.4843, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 341.9846, Accuracy: 0.7195\n",
      "Training loss (for one batch) at step 20: 326.6311, Accuracy: 0.7653\n",
      "Training loss (for one batch) at step 30: 301.1766, Accuracy: 0.7964\n",
      "Training loss (for one batch) at step 40: 304.4419, Accuracy: 0.8180\n",
      "Training loss (for one batch) at step 50: 305.3922, Accuracy: 0.8338\n",
      "Training loss (for one batch) at step 60: 300.0414, Accuracy: 0.8375\n",
      "Training loss (for one batch) at step 70: 329.4220, Accuracy: 0.8268\n",
      "Training loss (for one batch) at step 80: 371.0193, Accuracy: 0.8116\n",
      "Training loss (for one batch) at step 90: 312.6512, Accuracy: 0.8094\n",
      "Training loss (for one batch) at step 100: 324.6022, Accuracy: 0.8122\n",
      "Training loss (for one batch) at step 110: 333.5753, Accuracy: 0.8147\n",
      "---- Training ----\n",
      "Training loss: 107.6935\n",
      "Training acc over epoch: 0.8121\n",
      "---- Validation ----\n",
      "Validation loss: 45.1668\n",
      "Validation acc: 0.7483\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 351.2268, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 384.0897, Accuracy: 0.7322\n",
      "Training loss (for one batch) at step 20: 332.0533, Accuracy: 0.7653\n",
      "Training loss (for one batch) at step 30: 305.2467, Accuracy: 0.7959\n",
      "Training loss (for one batch) at step 40: 312.2874, Accuracy: 0.8176\n",
      "Training loss (for one batch) at step 50: 293.9215, Accuracy: 0.8338\n",
      "Training loss (for one batch) at step 60: 324.8165, Accuracy: 0.8407\n",
      "Training loss (for one batch) at step 70: 350.5840, Accuracy: 0.8292\n",
      "Training loss (for one batch) at step 80: 350.1073, Accuracy: 0.8138\n",
      "Training loss (for one batch) at step 90: 312.0627, Accuracy: 0.8084\n",
      "Training loss (for one batch) at step 100: 300.8128, Accuracy: 0.8110\n",
      "Training loss (for one batch) at step 110: 325.3376, Accuracy: 0.8119\n",
      "---- Training ----\n",
      "Training loss: 104.1172\n",
      "Training acc over epoch: 0.8113\n",
      "---- Validation ----\n",
      "Validation loss: 38.2408\n",
      "Validation acc: 0.7356\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 354.4365, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 347.7218, Accuracy: 0.7038\n",
      "Training loss (for one batch) at step 20: 332.3135, Accuracy: 0.7507\n",
      "Training loss (for one batch) at step 30: 325.0878, Accuracy: 0.7853\n",
      "Training loss (for one batch) at step 40: 303.4773, Accuracy: 0.8115\n",
      "Training loss (for one batch) at step 50: 298.0049, Accuracy: 0.8287\n",
      "Training loss (for one batch) at step 60: 324.9701, Accuracy: 0.8352\n",
      "Training loss (for one batch) at step 70: 357.7881, Accuracy: 0.8266\n",
      "Training loss (for one batch) at step 80: 363.3112, Accuracy: 0.8132\n",
      "Training loss (for one batch) at step 90: 321.9506, Accuracy: 0.8067\n",
      "Training loss (for one batch) at step 100: 295.7759, Accuracy: 0.8108\n",
      "Training loss (for one batch) at step 110: 329.1910, Accuracy: 0.8135\n",
      "---- Training ----\n",
      "Training loss: 113.1545\n",
      "Training acc over epoch: 0.8114\n",
      "---- Validation ----\n",
      "Validation loss: 47.1858\n",
      "Validation acc: 0.7397\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 368.3137, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 333.1579, Accuracy: 0.7102\n",
      "Training loss (for one batch) at step 20: 316.3581, Accuracy: 0.7533\n",
      "Training loss (for one batch) at step 30: 298.7930, Accuracy: 0.7908\n",
      "Training loss (for one batch) at step 40: 287.6914, Accuracy: 0.8150\n",
      "Training loss (for one batch) at step 50: 300.1263, Accuracy: 0.8304\n",
      "Training loss (for one batch) at step 60: 295.9933, Accuracy: 0.8372\n",
      "Training loss (for one batch) at step 70: 355.7644, Accuracy: 0.8267\n",
      "Training loss (for one batch) at step 80: 344.7485, Accuracy: 0.8132\n",
      "Training loss (for one batch) at step 90: 313.5464, Accuracy: 0.8095\n",
      "Training loss (for one batch) at step 100: 304.9014, Accuracy: 0.8116\n",
      "Training loss (for one batch) at step 110: 318.2710, Accuracy: 0.8136\n",
      "---- Training ----\n",
      "Training loss: 102.0969\n",
      "Training acc over epoch: 0.8117\n",
      "---- Validation ----\n",
      "Validation loss: 43.8314\n",
      "Validation acc: 0.7429\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 368.0510, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 345.4022, Accuracy: 0.6896\n",
      "Training loss (for one batch) at step 20: 314.0013, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 30: 308.4907, Accuracy: 0.7792\n",
      "Training loss (for one batch) at step 40: 307.4332, Accuracy: 0.8085\n",
      "Training loss (for one batch) at step 50: 294.0030, Accuracy: 0.8257\n",
      "Training loss (for one batch) at step 60: 313.8227, Accuracy: 0.8357\n",
      "Training loss (for one batch) at step 70: 327.4444, Accuracy: 0.8242\n",
      "Training loss (for one batch) at step 80: 335.0212, Accuracy: 0.8096\n",
      "Training loss (for one batch) at step 90: 309.3163, Accuracy: 0.8067\n",
      "Training loss (for one batch) at step 100: 306.1283, Accuracy: 0.8104\n",
      "Training loss (for one batch) at step 110: 331.3814, Accuracy: 0.8113\n",
      "---- Training ----\n",
      "Training loss: 101.8737\n",
      "Training acc over epoch: 0.8109\n",
      "---- Validation ----\n",
      "Validation loss: 58.3570\n",
      "Validation acc: 0.7340\n",
      "Time taken: 10.40s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 383.2305, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 347.2840, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 20: 312.6423, Accuracy: 0.7493\n",
      "Training loss (for one batch) at step 30: 318.9010, Accuracy: 0.7860\n",
      "Training loss (for one batch) at step 40: 306.2910, Accuracy: 0.8096\n",
      "Training loss (for one batch) at step 50: 285.5967, Accuracy: 0.8269\n",
      "Training loss (for one batch) at step 60: 331.5505, Accuracy: 0.8339\n",
      "Training loss (for one batch) at step 70: 330.7120, Accuracy: 0.8237\n",
      "Training loss (for one batch) at step 80: 348.9434, Accuracy: 0.8092\n",
      "Training loss (for one batch) at step 90: 316.0984, Accuracy: 0.8049\n",
      "Training loss (for one batch) at step 100: 294.4012, Accuracy: 0.8095\n",
      "Training loss (for one batch) at step 110: 319.5215, Accuracy: 0.8101\n",
      "---- Training ----\n",
      "Training loss: 105.7337\n",
      "Training acc over epoch: 0.8092\n",
      "---- Validation ----\n",
      "Validation loss: 69.9900\n",
      "Validation acc: 0.7402\n",
      "Time taken: 10.06s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 368.0136, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 336.9509, Accuracy: 0.7166\n",
      "Training loss (for one batch) at step 20: 315.1527, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 30: 298.7769, Accuracy: 0.7883\n",
      "Training loss (for one batch) at step 40: 355.0901, Accuracy: 0.8133\n",
      "Training loss (for one batch) at step 50: 283.6257, Accuracy: 0.8295\n",
      "Training loss (for one batch) at step 60: 303.0811, Accuracy: 0.8368\n",
      "Training loss (for one batch) at step 70: 336.3167, Accuracy: 0.8253\n",
      "Training loss (for one batch) at step 80: 369.4496, Accuracy: 0.8105\n",
      "Training loss (for one batch) at step 90: 314.1618, Accuracy: 0.8051\n",
      "Training loss (for one batch) at step 100: 308.8712, Accuracy: 0.8089\n",
      "Training loss (for one batch) at step 110: 342.5302, Accuracy: 0.8100\n",
      "---- Training ----\n",
      "Training loss: 109.7397\n",
      "Training acc over epoch: 0.8087\n",
      "---- Validation ----\n",
      "Validation loss: 49.7295\n",
      "Validation acc: 0.7413\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 352.1447, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 346.9012, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 20: 312.8574, Accuracy: 0.7400\n",
      "Training loss (for one batch) at step 30: 303.8407, Accuracy: 0.7807\n",
      "Training loss (for one batch) at step 40: 286.8440, Accuracy: 0.8062\n",
      "Training loss (for one batch) at step 50: 298.2801, Accuracy: 0.8252\n",
      "Training loss (for one batch) at step 60: 309.1831, Accuracy: 0.8320\n",
      "Training loss (for one batch) at step 70: 356.2274, Accuracy: 0.8212\n",
      "Training loss (for one batch) at step 80: 362.4754, Accuracy: 0.8057\n",
      "Training loss (for one batch) at step 90: 309.5327, Accuracy: 0.8027\n",
      "Training loss (for one batch) at step 100: 312.9141, Accuracy: 0.8070\n",
      "Training loss (for one batch) at step 110: 343.9102, Accuracy: 0.8081\n",
      "---- Training ----\n",
      "Training loss: 110.5952\n",
      "Training acc over epoch: 0.8080\n",
      "---- Validation ----\n",
      "Validation loss: 50.2821\n",
      "Validation acc: 0.7415\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 331.3307, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 347.4601, Accuracy: 0.7038\n",
      "Training loss (for one batch) at step 20: 322.9069, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 30: 302.1404, Accuracy: 0.7876\n",
      "Training loss (for one batch) at step 40: 279.1895, Accuracy: 0.8136\n",
      "Training loss (for one batch) at step 50: 287.4729, Accuracy: 0.8304\n",
      "Training loss (for one batch) at step 60: 288.0966, Accuracy: 0.8371\n",
      "Training loss (for one batch) at step 70: 326.5270, Accuracy: 0.8261\n",
      "Training loss (for one batch) at step 80: 348.2544, Accuracy: 0.8117\n",
      "Training loss (for one batch) at step 90: 303.3521, Accuracy: 0.8084\n",
      "Training loss (for one batch) at step 100: 301.0223, Accuracy: 0.8120\n",
      "Training loss (for one batch) at step 110: 313.8372, Accuracy: 0.8119\n",
      "---- Training ----\n",
      "Training loss: 115.3684\n",
      "Training acc over epoch: 0.8111\n",
      "---- Validation ----\n",
      "Validation loss: 49.7302\n",
      "Validation acc: 0.7391\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 357.4347, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 354.4715, Accuracy: 0.7159\n",
      "Training loss (for one batch) at step 20: 311.5750, Accuracy: 0.7511\n",
      "Training loss (for one batch) at step 30: 301.8843, Accuracy: 0.7853\n",
      "Training loss (for one batch) at step 40: 292.9437, Accuracy: 0.8110\n",
      "Training loss (for one batch) at step 50: 292.1920, Accuracy: 0.8275\n",
      "Training loss (for one batch) at step 60: 318.7254, Accuracy: 0.8349\n",
      "Training loss (for one batch) at step 70: 307.0832, Accuracy: 0.8246\n",
      "Training loss (for one batch) at step 80: 349.4852, Accuracy: 0.8122\n",
      "Training loss (for one batch) at step 90: 319.8153, Accuracy: 0.8073\n",
      "Training loss (for one batch) at step 100: 315.6696, Accuracy: 0.8113\n",
      "Training loss (for one batch) at step 110: 314.3589, Accuracy: 0.8116\n",
      "---- Training ----\n",
      "Training loss: 108.8755\n",
      "Training acc over epoch: 0.8112\n",
      "---- Validation ----\n",
      "Validation loss: 56.6445\n",
      "Validation acc: 0.7364\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 343.8598, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 349.8201, Accuracy: 0.7159\n",
      "Training loss (for one batch) at step 20: 327.0916, Accuracy: 0.7541\n",
      "Training loss (for one batch) at step 30: 290.1108, Accuracy: 0.7941\n",
      "Training loss (for one batch) at step 40: 306.2886, Accuracy: 0.8171\n",
      "Training loss (for one batch) at step 50: 294.8772, Accuracy: 0.8341\n",
      "Training loss (for one batch) at step 60: 321.9079, Accuracy: 0.8431\n",
      "Training loss (for one batch) at step 70: 363.4414, Accuracy: 0.8301\n",
      "Training loss (for one batch) at step 80: 346.9669, Accuracy: 0.8147\n",
      "Training loss (for one batch) at step 90: 325.5978, Accuracy: 0.8096\n",
      "Training loss (for one batch) at step 100: 300.7592, Accuracy: 0.8134\n",
      "Training loss (for one batch) at step 110: 327.1881, Accuracy: 0.8133\n",
      "---- Training ----\n",
      "Training loss: 113.9470\n",
      "Training acc over epoch: 0.8117\n",
      "---- Validation ----\n",
      "Validation loss: 46.4487\n",
      "Validation acc: 0.7389\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 374.7574, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 318.3058, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 20: 308.4166, Accuracy: 0.7463\n",
      "Training loss (for one batch) at step 30: 293.8486, Accuracy: 0.7870\n",
      "Training loss (for one batch) at step 40: 301.8582, Accuracy: 0.8115\n",
      "Training loss (for one batch) at step 50: 302.2273, Accuracy: 0.8267\n",
      "Training loss (for one batch) at step 60: 301.6056, Accuracy: 0.8366\n",
      "Training loss (for one batch) at step 70: 341.1744, Accuracy: 0.8250\n",
      "Training loss (for one batch) at step 80: 322.1626, Accuracy: 0.8101\n",
      "Training loss (for one batch) at step 90: 300.0810, Accuracy: 0.8060\n",
      "Training loss (for one batch) at step 100: 283.3150, Accuracy: 0.8097\n",
      "Training loss (for one batch) at step 110: 310.4400, Accuracy: 0.8103\n",
      "---- Training ----\n",
      "Training loss: 91.0811\n",
      "Training acc over epoch: 0.8088\n",
      "---- Validation ----\n",
      "Validation loss: 33.7807\n",
      "Validation acc: 0.7389\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 342.9103, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 325.1516, Accuracy: 0.7095\n",
      "Training loss (for one batch) at step 20: 289.9746, Accuracy: 0.7560\n",
      "Training loss (for one batch) at step 30: 299.7892, Accuracy: 0.7936\n",
      "Training loss (for one batch) at step 40: 282.7221, Accuracy: 0.8192\n",
      "Training loss (for one batch) at step 50: 310.3914, Accuracy: 0.8327\n",
      "Training loss (for one batch) at step 60: 309.5851, Accuracy: 0.8381\n",
      "Training loss (for one batch) at step 70: 327.6857, Accuracy: 0.8254\n",
      "Training loss (for one batch) at step 80: 351.1019, Accuracy: 0.8135\n",
      "Training loss (for one batch) at step 90: 301.9167, Accuracy: 0.8086\n",
      "Training loss (for one batch) at step 100: 301.6403, Accuracy: 0.8130\n",
      "Training loss (for one batch) at step 110: 325.5362, Accuracy: 0.8138\n",
      "---- Training ----\n",
      "Training loss: 99.1624\n",
      "Training acc over epoch: 0.8122\n",
      "---- Validation ----\n",
      "Validation loss: 55.5011\n",
      "Validation acc: 0.7472\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 348.4767, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 336.3745, Accuracy: 0.7209\n",
      "Training loss (for one batch) at step 20: 298.2201, Accuracy: 0.7582\n",
      "Training loss (for one batch) at step 30: 285.5414, Accuracy: 0.7954\n",
      "Training loss (for one batch) at step 40: 308.7550, Accuracy: 0.8209\n",
      "Training loss (for one batch) at step 50: 301.0299, Accuracy: 0.8349\n",
      "Training loss (for one batch) at step 60: 302.4289, Accuracy: 0.8425\n",
      "Training loss (for one batch) at step 70: 351.7257, Accuracy: 0.8329\n",
      "Training loss (for one batch) at step 80: 326.3408, Accuracy: 0.8187\n",
      "Training loss (for one batch) at step 90: 293.9657, Accuracy: 0.8133\n",
      "Training loss (for one batch) at step 100: 305.2623, Accuracy: 0.8161\n",
      "Training loss (for one batch) at step 110: 324.4197, Accuracy: 0.8165\n",
      "---- Training ----\n",
      "Training loss: 95.9564\n",
      "Training acc over epoch: 0.8147\n",
      "---- Validation ----\n",
      "Validation loss: 50.8490\n",
      "Validation acc: 0.7407\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 336.0449, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 336.1253, Accuracy: 0.6982\n",
      "Training loss (for one batch) at step 20: 295.2891, Accuracy: 0.7455\n",
      "Training loss (for one batch) at step 30: 278.0349, Accuracy: 0.7848\n",
      "Training loss (for one batch) at step 40: 287.0385, Accuracy: 0.8114\n",
      "Training loss (for one batch) at step 50: 291.6754, Accuracy: 0.8295\n",
      "Training loss (for one batch) at step 60: 308.2127, Accuracy: 0.8364\n",
      "Training loss (for one batch) at step 70: 327.6071, Accuracy: 0.8255\n",
      "Training loss (for one batch) at step 80: 357.2733, Accuracy: 0.8113\n",
      "Training loss (for one batch) at step 90: 302.4507, Accuracy: 0.8055\n",
      "Training loss (for one batch) at step 100: 292.8121, Accuracy: 0.8084\n",
      "Training loss (for one batch) at step 110: 332.0204, Accuracy: 0.8101\n",
      "---- Training ----\n",
      "Training loss: 108.0566\n",
      "Training acc over epoch: 0.8085\n",
      "---- Validation ----\n",
      "Validation loss: 35.1596\n",
      "Validation acc: 0.7461\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 362.8070, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 319.9717, Accuracy: 0.7088\n",
      "Training loss (for one batch) at step 20: 292.6400, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 30: 285.1615, Accuracy: 0.7933\n",
      "Training loss (for one batch) at step 40: 272.9507, Accuracy: 0.8165\n",
      "Training loss (for one batch) at step 50: 294.5457, Accuracy: 0.8335\n",
      "Training loss (for one batch) at step 60: 300.3492, Accuracy: 0.8391\n",
      "Training loss (for one batch) at step 70: 333.0242, Accuracy: 0.8279\n",
      "Training loss (for one batch) at step 80: 339.5190, Accuracy: 0.8121\n",
      "Training loss (for one batch) at step 90: 297.4160, Accuracy: 0.8070\n",
      "Training loss (for one batch) at step 100: 300.4904, Accuracy: 0.8122\n",
      "Training loss (for one batch) at step 110: 319.0786, Accuracy: 0.8119\n",
      "---- Training ----\n",
      "Training loss: 107.2488\n",
      "Training acc over epoch: 0.8111\n",
      "---- Validation ----\n",
      "Validation loss: 61.0296\n",
      "Validation acc: 0.7378\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 345.2504, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 358.0306, Accuracy: 0.7067\n",
      "Training loss (for one batch) at step 20: 302.7557, Accuracy: 0.7519\n",
      "Training loss (for one batch) at step 30: 309.9547, Accuracy: 0.7908\n",
      "Training loss (for one batch) at step 40: 285.2142, Accuracy: 0.8144\n",
      "Training loss (for one batch) at step 50: 285.1191, Accuracy: 0.8309\n",
      "Training loss (for one batch) at step 60: 301.6532, Accuracy: 0.8382\n",
      "Training loss (for one batch) at step 70: 323.4853, Accuracy: 0.8245\n",
      "Training loss (for one batch) at step 80: 330.7801, Accuracy: 0.8098\n",
      "Training loss (for one batch) at step 90: 293.4297, Accuracy: 0.8057\n",
      "Training loss (for one batch) at step 100: 299.9753, Accuracy: 0.8101\n",
      "Training loss (for one batch) at step 110: 286.4827, Accuracy: 0.8117\n",
      "---- Training ----\n",
      "Training loss: 106.5315\n",
      "Training acc over epoch: 0.8102\n",
      "---- Validation ----\n",
      "Validation loss: 43.2617\n",
      "Validation acc: 0.7380\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 355.7488, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 352.0677, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 20: 308.3818, Accuracy: 0.7478\n",
      "Training loss (for one batch) at step 30: 292.1384, Accuracy: 0.7873\n",
      "Training loss (for one batch) at step 40: 279.0649, Accuracy: 0.8150\n",
      "Training loss (for one batch) at step 50: 288.0316, Accuracy: 0.8324\n",
      "Training loss (for one batch) at step 60: 300.5318, Accuracy: 0.8382\n",
      "Training loss (for one batch) at step 70: 322.9882, Accuracy: 0.8296\n",
      "Training loss (for one batch) at step 80: 337.0451, Accuracy: 0.8130\n",
      "Training loss (for one batch) at step 90: 293.1083, Accuracy: 0.8084\n",
      "Training loss (for one batch) at step 100: 304.4056, Accuracy: 0.8120\n",
      "Training loss (for one batch) at step 110: 336.4261, Accuracy: 0.8113\n",
      "---- Training ----\n",
      "Training loss: 103.5593\n",
      "Training acc over epoch: 0.8115\n",
      "---- Validation ----\n",
      "Validation loss: 51.8711\n",
      "Validation acc: 0.7431\n",
      "Time taken: 10.10s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 344.2744, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 330.4889, Accuracy: 0.7116\n",
      "Training loss (for one batch) at step 20: 303.3475, Accuracy: 0.7504\n",
      "Training loss (for one batch) at step 30: 281.4556, Accuracy: 0.7850\n",
      "Training loss (for one batch) at step 40: 286.6522, Accuracy: 0.8131\n",
      "Training loss (for one batch) at step 50: 274.7997, Accuracy: 0.8287\n",
      "Training loss (for one batch) at step 60: 302.8883, Accuracy: 0.8340\n",
      "Training loss (for one batch) at step 70: 340.6838, Accuracy: 0.8232\n",
      "Training loss (for one batch) at step 80: 356.4114, Accuracy: 0.8110\n",
      "Training loss (for one batch) at step 90: 289.4741, Accuracy: 0.8062\n",
      "Training loss (for one batch) at step 100: 285.8183, Accuracy: 0.8100\n",
      "Training loss (for one batch) at step 110: 295.7711, Accuracy: 0.8110\n",
      "---- Training ----\n",
      "Training loss: 97.8426\n",
      "Training acc over epoch: 0.8095\n",
      "---- Validation ----\n",
      "Validation loss: 38.8505\n",
      "Validation acc: 0.7469\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 334.6167, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 336.5873, Accuracy: 0.6882\n",
      "Training loss (for one batch) at step 20: 315.9566, Accuracy: 0.7522\n",
      "Training loss (for one batch) at step 30: 305.8750, Accuracy: 0.7888\n",
      "Training loss (for one batch) at step 40: 282.5826, Accuracy: 0.8155\n",
      "Training loss (for one batch) at step 50: 291.7017, Accuracy: 0.8330\n",
      "Training loss (for one batch) at step 60: 310.0268, Accuracy: 0.8402\n",
      "Training loss (for one batch) at step 70: 318.2947, Accuracy: 0.8301\n",
      "Training loss (for one batch) at step 80: 325.4969, Accuracy: 0.8142\n",
      "Training loss (for one batch) at step 90: 291.8581, Accuracy: 0.8090\n",
      "Training loss (for one batch) at step 100: 299.5858, Accuracy: 0.8137\n",
      "Training loss (for one batch) at step 110: 341.3401, Accuracy: 0.8143\n",
      "---- Training ----\n",
      "Training loss: 102.2297\n",
      "Training acc over epoch: 0.8129\n",
      "---- Validation ----\n",
      "Validation loss: 63.6816\n",
      "Validation acc: 0.7343\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 360.5507, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 326.8993, Accuracy: 0.6982\n",
      "Training loss (for one batch) at step 20: 291.7331, Accuracy: 0.7560\n",
      "Training loss (for one batch) at step 30: 299.9429, Accuracy: 0.7906\n",
      "Training loss (for one batch) at step 40: 283.0786, Accuracy: 0.8186\n",
      "Training loss (for one batch) at step 50: 276.9734, Accuracy: 0.8350\n",
      "Training loss (for one batch) at step 60: 294.2194, Accuracy: 0.8418\n",
      "Training loss (for one batch) at step 70: 333.7838, Accuracy: 0.8302\n",
      "Training loss (for one batch) at step 80: 327.1649, Accuracy: 0.8158\n",
      "Training loss (for one batch) at step 90: 314.8483, Accuracy: 0.8112\n",
      "Training loss (for one batch) at step 100: 290.5513, Accuracy: 0.8151\n",
      "Training loss (for one batch) at step 110: 301.5285, Accuracy: 0.8157\n",
      "---- Training ----\n",
      "Training loss: 117.8704\n",
      "Training acc over epoch: 0.8149\n",
      "---- Validation ----\n",
      "Validation loss: 49.9411\n",
      "Validation acc: 0.7383\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 341.6722, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 332.3567, Accuracy: 0.7010\n",
      "Training loss (for one batch) at step 20: 296.1512, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 30: 269.8087, Accuracy: 0.7830\n",
      "Training loss (for one batch) at step 40: 276.4170, Accuracy: 0.8098\n",
      "Training loss (for one batch) at step 50: 275.1566, Accuracy: 0.8283\n",
      "Training loss (for one batch) at step 60: 307.0096, Accuracy: 0.8364\n",
      "Training loss (for one batch) at step 70: 321.6649, Accuracy: 0.8227\n",
      "Training loss (for one batch) at step 80: 325.1466, Accuracy: 0.8094\n",
      "Training loss (for one batch) at step 90: 329.6965, Accuracy: 0.8056\n",
      "Training loss (for one batch) at step 100: 285.0157, Accuracy: 0.8094\n",
      "Training loss (for one batch) at step 110: 299.0309, Accuracy: 0.8110\n",
      "---- Training ----\n",
      "Training loss: 106.6840\n",
      "Training acc over epoch: 0.8088\n",
      "---- Validation ----\n",
      "Validation loss: 55.2709\n",
      "Validation acc: 0.7297\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 334.0430, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 310.1606, Accuracy: 0.7003\n",
      "Training loss (for one batch) at step 20: 306.7207, Accuracy: 0.7396\n",
      "Training loss (for one batch) at step 30: 286.3096, Accuracy: 0.7843\n",
      "Training loss (for one batch) at step 40: 293.7079, Accuracy: 0.8085\n",
      "Training loss (for one batch) at step 50: 289.8271, Accuracy: 0.8271\n",
      "Training loss (for one batch) at step 60: 303.1725, Accuracy: 0.8354\n",
      "Training loss (for one batch) at step 70: 326.4906, Accuracy: 0.8230\n",
      "Training loss (for one batch) at step 80: 335.1388, Accuracy: 0.8078\n",
      "Training loss (for one batch) at step 90: 294.4038, Accuracy: 0.8062\n",
      "Training loss (for one batch) at step 100: 280.4339, Accuracy: 0.8086\n",
      "Training loss (for one batch) at step 110: 312.2086, Accuracy: 0.8107\n",
      "---- Training ----\n",
      "Training loss: 126.8174\n",
      "Training acc over epoch: 0.8089\n",
      "---- Validation ----\n",
      "Validation loss: 47.5393\n",
      "Validation acc: 0.7477\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 356.1728, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 355.7814, Accuracy: 0.7180\n",
      "Training loss (for one batch) at step 20: 320.2410, Accuracy: 0.7548\n",
      "Training loss (for one batch) at step 30: 265.1929, Accuracy: 0.7961\n",
      "Training loss (for one batch) at step 40: 301.7912, Accuracy: 0.8192\n",
      "Training loss (for one batch) at step 50: 277.1233, Accuracy: 0.8341\n",
      "Training loss (for one batch) at step 60: 300.3901, Accuracy: 0.8402\n",
      "Training loss (for one batch) at step 70: 329.2844, Accuracy: 0.8282\n",
      "Training loss (for one batch) at step 80: 334.9907, Accuracy: 0.8115\n",
      "Training loss (for one batch) at step 90: 298.0625, Accuracy: 0.8089\n",
      "Training loss (for one batch) at step 100: 318.3549, Accuracy: 0.8124\n",
      "Training loss (for one batch) at step 110: 303.6895, Accuracy: 0.8134\n",
      "---- Training ----\n",
      "Training loss: 99.3421\n",
      "Training acc over epoch: 0.8128\n",
      "---- Validation ----\n",
      "Validation loss: 41.7053\n",
      "Validation acc: 0.7418\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 332.8586, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 336.2603, Accuracy: 0.7138\n",
      "Training loss (for one batch) at step 20: 292.2654, Accuracy: 0.7612\n",
      "Training loss (for one batch) at step 30: 285.6550, Accuracy: 0.7939\n",
      "Training loss (for one batch) at step 40: 283.5450, Accuracy: 0.8211\n",
      "Training loss (for one batch) at step 50: 280.8882, Accuracy: 0.8350\n",
      "Training loss (for one batch) at step 60: 305.5959, Accuracy: 0.8402\n",
      "Training loss (for one batch) at step 70: 316.9030, Accuracy: 0.8279\n",
      "Training loss (for one batch) at step 80: 319.6918, Accuracy: 0.8119\n",
      "Training loss (for one batch) at step 90: 294.0158, Accuracy: 0.8073\n",
      "Training loss (for one batch) at step 100: 278.6010, Accuracy: 0.8116\n",
      "Training loss (for one batch) at step 110: 323.5840, Accuracy: 0.8124\n",
      "---- Training ----\n",
      "Training loss: 98.0338\n",
      "Training acc over epoch: 0.8105\n",
      "---- Validation ----\n",
      "Validation loss: 46.0842\n",
      "Validation acc: 0.7356\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 344.0928, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 342.4267, Accuracy: 0.7088\n",
      "Training loss (for one batch) at step 20: 280.4715, Accuracy: 0.7597\n",
      "Training loss (for one batch) at step 30: 289.1445, Accuracy: 0.7966\n",
      "Training loss (for one batch) at step 40: 297.7251, Accuracy: 0.8182\n",
      "Training loss (for one batch) at step 50: 264.9175, Accuracy: 0.8353\n",
      "Training loss (for one batch) at step 60: 309.7399, Accuracy: 0.8417\n",
      "Training loss (for one batch) at step 70: 307.4433, Accuracy: 0.8282\n",
      "Training loss (for one batch) at step 80: 355.2781, Accuracy: 0.8138\n",
      "Training loss (for one batch) at step 90: 276.6169, Accuracy: 0.8101\n",
      "Training loss (for one batch) at step 100: 286.1158, Accuracy: 0.8141\n",
      "Training loss (for one batch) at step 110: 298.8611, Accuracy: 0.8169\n",
      "---- Training ----\n",
      "Training loss: 108.1040\n",
      "Training acc over epoch: 0.8144\n",
      "---- Validation ----\n",
      "Validation loss: 37.9490\n",
      "Validation acc: 0.7227\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 334.1620, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 335.4804, Accuracy: 0.7237\n",
      "Training loss (for one batch) at step 20: 278.9820, Accuracy: 0.7545\n",
      "Training loss (for one batch) at step 30: 291.4545, Accuracy: 0.7954\n",
      "Training loss (for one batch) at step 40: 281.3997, Accuracy: 0.8184\n",
      "Training loss (for one batch) at step 50: 274.0893, Accuracy: 0.8353\n",
      "Training loss (for one batch) at step 60: 291.0446, Accuracy: 0.8420\n",
      "Training loss (for one batch) at step 70: 317.3179, Accuracy: 0.8290\n",
      "Training loss (for one batch) at step 80: 324.7377, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 90: 301.7831, Accuracy: 0.8067\n",
      "Training loss (for one batch) at step 100: 316.4034, Accuracy: 0.8110\n",
      "Training loss (for one batch) at step 110: 306.4564, Accuracy: 0.8129\n",
      "---- Training ----\n",
      "Training loss: 109.0829\n",
      "Training acc over epoch: 0.8109\n",
      "---- Validation ----\n",
      "Validation loss: 55.9766\n",
      "Validation acc: 0.7324\n",
      "Time taken: 10.60s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 360.9068, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 337.0925, Accuracy: 0.7180\n",
      "Training loss (for one batch) at step 20: 291.7113, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 30: 291.9693, Accuracy: 0.7946\n",
      "Training loss (for one batch) at step 40: 285.2838, Accuracy: 0.8159\n",
      "Training loss (for one batch) at step 50: 267.1948, Accuracy: 0.8330\n",
      "Training loss (for one batch) at step 60: 274.0010, Accuracy: 0.8417\n",
      "Training loss (for one batch) at step 70: 313.0842, Accuracy: 0.8272\n",
      "Training loss (for one batch) at step 80: 333.4826, Accuracy: 0.8119\n",
      "Training loss (for one batch) at step 90: 292.1497, Accuracy: 0.8082\n",
      "Training loss (for one batch) at step 100: 294.3339, Accuracy: 0.8120\n",
      "Training loss (for one batch) at step 110: 296.1866, Accuracy: 0.8130\n",
      "---- Training ----\n",
      "Training loss: 97.3089\n",
      "Training acc over epoch: 0.8119\n",
      "---- Validation ----\n",
      "Validation loss: 46.7791\n",
      "Validation acc: 0.7421\n",
      "Time taken: 10.13s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 333.2827, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 324.3687, Accuracy: 0.7095\n",
      "Training loss (for one batch) at step 20: 284.7023, Accuracy: 0.7552\n",
      "Training loss (for one batch) at step 30: 290.4293, Accuracy: 0.7883\n",
      "Training loss (for one batch) at step 40: 270.7225, Accuracy: 0.8133\n",
      "Training loss (for one batch) at step 50: 256.7831, Accuracy: 0.8316\n",
      "Training loss (for one batch) at step 60: 283.6486, Accuracy: 0.8393\n",
      "Training loss (for one batch) at step 70: 337.5741, Accuracy: 0.8268\n",
      "Training loss (for one batch) at step 80: 330.8043, Accuracy: 0.8113\n",
      "Training loss (for one batch) at step 90: 303.5342, Accuracy: 0.8078\n",
      "Training loss (for one batch) at step 100: 290.4768, Accuracy: 0.8113\n",
      "Training loss (for one batch) at step 110: 320.6851, Accuracy: 0.8132\n",
      "---- Training ----\n",
      "Training loss: 97.1343\n",
      "Training acc over epoch: 0.8116\n",
      "---- Validation ----\n",
      "Validation loss: 57.1620\n",
      "Validation acc: 0.7305\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 321.3007, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 322.6998, Accuracy: 0.6960\n",
      "Training loss (for one batch) at step 20: 290.5741, Accuracy: 0.7470\n",
      "Training loss (for one batch) at step 30: 298.7279, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 40: 277.9196, Accuracy: 0.8142\n",
      "Training loss (for one batch) at step 50: 278.5158, Accuracy: 0.8313\n",
      "Training loss (for one batch) at step 60: 309.4896, Accuracy: 0.8399\n",
      "Training loss (for one batch) at step 70: 315.2281, Accuracy: 0.8301\n",
      "Training loss (for one batch) at step 80: 316.1701, Accuracy: 0.8154\n",
      "Training loss (for one batch) at step 90: 306.2714, Accuracy: 0.8098\n",
      "Training loss (for one batch) at step 100: 271.4531, Accuracy: 0.8147\n",
      "Training loss (for one batch) at step 110: 316.8231, Accuracy: 0.8168\n",
      "---- Training ----\n",
      "Training loss: 106.7528\n",
      "Training acc over epoch: 0.8153\n",
      "---- Validation ----\n",
      "Validation loss: 45.8453\n",
      "Validation acc: 0.7260\n",
      "Time taken: 10.05s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 322.8778, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 330.1405, Accuracy: 0.6989\n",
      "Training loss (for one batch) at step 20: 299.1544, Accuracy: 0.7511\n",
      "Training loss (for one batch) at step 30: 284.9767, Accuracy: 0.7928\n",
      "Training loss (for one batch) at step 40: 283.4507, Accuracy: 0.8159\n",
      "Training loss (for one batch) at step 50: 262.6706, Accuracy: 0.8323\n",
      "Training loss (for one batch) at step 60: 285.0228, Accuracy: 0.8399\n",
      "Training loss (for one batch) at step 70: 317.7073, Accuracy: 0.8293\n",
      "Training loss (for one batch) at step 80: 337.5540, Accuracy: 0.8126\n",
      "Training loss (for one batch) at step 90: 293.6400, Accuracy: 0.8079\n",
      "Training loss (for one batch) at step 100: 278.9853, Accuracy: 0.8130\n",
      "Training loss (for one batch) at step 110: 305.0366, Accuracy: 0.8138\n",
      "---- Training ----\n",
      "Training loss: 87.8781\n",
      "Training acc over epoch: 0.8129\n",
      "---- Validation ----\n",
      "Validation loss: 72.7959\n",
      "Validation acc: 0.7380\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 342.4528, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 321.8421, Accuracy: 0.7095\n",
      "Training loss (for one batch) at step 20: 295.3441, Accuracy: 0.7507\n",
      "Training loss (for one batch) at step 30: 272.8219, Accuracy: 0.7878\n",
      "Training loss (for one batch) at step 40: 271.5142, Accuracy: 0.8133\n",
      "Training loss (for one batch) at step 50: 275.5082, Accuracy: 0.8320\n",
      "Training loss (for one batch) at step 60: 290.4478, Accuracy: 0.8398\n",
      "Training loss (for one batch) at step 70: 337.6205, Accuracy: 0.8254\n",
      "Training loss (for one batch) at step 80: 333.1671, Accuracy: 0.8098\n",
      "Training loss (for one batch) at step 90: 286.4673, Accuracy: 0.8070\n",
      "Training loss (for one batch) at step 100: 274.0620, Accuracy: 0.8112\n",
      "Training loss (for one batch) at step 110: 312.8372, Accuracy: 0.8124\n",
      "---- Training ----\n",
      "Training loss: 114.7335\n",
      "Training acc over epoch: 0.8114\n",
      "---- Validation ----\n",
      "Validation loss: 38.0694\n",
      "Validation acc: 0.7251\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 323.4716, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 314.4751, Accuracy: 0.6925\n",
      "Training loss (for one batch) at step 20: 290.1969, Accuracy: 0.7470\n",
      "Training loss (for one batch) at step 30: 274.5923, Accuracy: 0.7911\n",
      "Training loss (for one batch) at step 40: 270.9724, Accuracy: 0.8140\n",
      "Training loss (for one batch) at step 50: 278.9940, Accuracy: 0.8324\n",
      "Training loss (for one batch) at step 60: 301.6945, Accuracy: 0.8382\n",
      "Training loss (for one batch) at step 70: 336.5864, Accuracy: 0.8266\n",
      "Training loss (for one batch) at step 80: 329.7630, Accuracy: 0.8095\n",
      "Training loss (for one batch) at step 90: 291.3830, Accuracy: 0.8068\n",
      "Training loss (for one batch) at step 100: 296.0316, Accuracy: 0.8119\n",
      "Training loss (for one batch) at step 110: 304.6198, Accuracy: 0.8130\n",
      "---- Training ----\n",
      "Training loss: 90.8103\n",
      "Training acc over epoch: 0.8121\n",
      "---- Validation ----\n",
      "Validation loss: 52.5305\n",
      "Validation acc: 0.7294\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 331.2898, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 332.8730, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 20: 283.6126, Accuracy: 0.7452\n",
      "Training loss (for one batch) at step 30: 286.2011, Accuracy: 0.7840\n",
      "Training loss (for one batch) at step 40: 276.0779, Accuracy: 0.8098\n",
      "Training loss (for one batch) at step 50: 286.9113, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 60: 286.3187, Accuracy: 0.8362\n",
      "Training loss (for one batch) at step 70: 326.3448, Accuracy: 0.8263\n",
      "Training loss (for one batch) at step 80: 352.4891, Accuracy: 0.8106\n",
      "Training loss (for one batch) at step 90: 296.5975, Accuracy: 0.8062\n",
      "Training loss (for one batch) at step 100: 286.0016, Accuracy: 0.8110\n",
      "Training loss (for one batch) at step 110: 295.4227, Accuracy: 0.8134\n",
      "---- Training ----\n",
      "Training loss: 100.6530\n",
      "Training acc over epoch: 0.8123\n",
      "---- Validation ----\n",
      "Validation loss: 39.6811\n",
      "Validation acc: 0.7348\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 324.1973, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 314.3806, Accuracy: 0.7145\n",
      "Training loss (for one batch) at step 20: 288.9842, Accuracy: 0.7533\n",
      "Training loss (for one batch) at step 30: 304.9262, Accuracy: 0.7946\n",
      "Training loss (for one batch) at step 40: 279.7925, Accuracy: 0.8186\n",
      "Training loss (for one batch) at step 50: 279.8351, Accuracy: 0.8343\n",
      "Training loss (for one batch) at step 60: 283.4711, Accuracy: 0.8400\n",
      "Training loss (for one batch) at step 70: 306.7119, Accuracy: 0.8279\n",
      "Training loss (for one batch) at step 80: 315.5947, Accuracy: 0.8145\n",
      "Training loss (for one batch) at step 90: 288.7067, Accuracy: 0.8102\n",
      "Training loss (for one batch) at step 100: 273.9502, Accuracy: 0.8144\n",
      "Training loss (for one batch) at step 110: 332.1188, Accuracy: 0.8164\n",
      "---- Training ----\n",
      "Training loss: 96.3717\n",
      "Training acc over epoch: 0.8141\n",
      "---- Validation ----\n",
      "Validation loss: 56.7032\n",
      "Validation acc: 0.7354\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 337.0267, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 324.5178, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 20: 285.0911, Accuracy: 0.7593\n",
      "Training loss (for one batch) at step 30: 287.0834, Accuracy: 0.7956\n",
      "Training loss (for one batch) at step 40: 277.9011, Accuracy: 0.8176\n",
      "Training loss (for one batch) at step 50: 286.1281, Accuracy: 0.8372\n",
      "Training loss (for one batch) at step 60: 276.7310, Accuracy: 0.8429\n",
      "Training loss (for one batch) at step 70: 315.3351, Accuracy: 0.8269\n",
      "Training loss (for one batch) at step 80: 308.4682, Accuracy: 0.8128\n",
      "Training loss (for one batch) at step 90: 303.0484, Accuracy: 0.8086\n",
      "Training loss (for one batch) at step 100: 278.0015, Accuracy: 0.8126\n",
      "Training loss (for one batch) at step 110: 315.6930, Accuracy: 0.8135\n",
      "---- Training ----\n",
      "Training loss: 103.7970\n",
      "Training acc over epoch: 0.8122\n",
      "---- Validation ----\n",
      "Validation loss: 71.1672\n",
      "Validation acc: 0.7332\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 311.7502, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 318.8748, Accuracy: 0.7067\n",
      "Training loss (for one batch) at step 20: 305.1961, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 30: 278.1014, Accuracy: 0.7896\n",
      "Training loss (for one batch) at step 40: 271.1458, Accuracy: 0.8123\n",
      "Training loss (for one batch) at step 50: 290.3147, Accuracy: 0.8307\n",
      "Training loss (for one batch) at step 60: 285.4256, Accuracy: 0.8413\n",
      "Training loss (for one batch) at step 70: 311.8079, Accuracy: 0.8276\n",
      "Training loss (for one batch) at step 80: 320.2910, Accuracy: 0.8111\n",
      "Training loss (for one batch) at step 90: 289.6424, Accuracy: 0.8067\n",
      "Training loss (for one batch) at step 100: 289.7274, Accuracy: 0.8105\n",
      "Training loss (for one batch) at step 110: 297.4130, Accuracy: 0.8125\n",
      "---- Training ----\n",
      "Training loss: 111.6069\n",
      "Training acc over epoch: 0.8116\n",
      "---- Validation ----\n",
      "Validation loss: 43.5466\n",
      "Validation acc: 0.7346\n",
      "Time taken: 10.40s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 330.6480, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 329.3808, Accuracy: 0.7095\n",
      "Training loss (for one batch) at step 20: 286.4460, Accuracy: 0.7548\n",
      "Training loss (for one batch) at step 30: 276.0872, Accuracy: 0.7949\n",
      "Training loss (for one batch) at step 40: 279.2313, Accuracy: 0.8163\n",
      "Training loss (for one batch) at step 50: 276.9521, Accuracy: 0.8352\n",
      "Training loss (for one batch) at step 60: 278.3454, Accuracy: 0.8412\n",
      "Training loss (for one batch) at step 70: 320.2839, Accuracy: 0.8288\n",
      "Training loss (for one batch) at step 80: 323.4365, Accuracy: 0.8133\n",
      "Training loss (for one batch) at step 90: 283.8925, Accuracy: 0.8080\n",
      "Training loss (for one batch) at step 100: 284.9925, Accuracy: 0.8119\n",
      "Training loss (for one batch) at step 110: 287.4625, Accuracy: 0.8129\n",
      "---- Training ----\n",
      "Training loss: 102.6980\n",
      "Training acc over epoch: 0.8109\n",
      "---- Validation ----\n",
      "Validation loss: 46.7965\n",
      "Validation acc: 0.7348\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 340.0699, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 323.1638, Accuracy: 0.7053\n",
      "Training loss (for one batch) at step 20: 273.4350, Accuracy: 0.7507\n",
      "Training loss (for one batch) at step 30: 268.7972, Accuracy: 0.7878\n",
      "Training loss (for one batch) at step 40: 279.5697, Accuracy: 0.8150\n",
      "Training loss (for one batch) at step 50: 271.0089, Accuracy: 0.8333\n",
      "Training loss (for one batch) at step 60: 312.9471, Accuracy: 0.8403\n",
      "Training loss (for one batch) at step 70: 318.3329, Accuracy: 0.8286\n",
      "Training loss (for one batch) at step 80: 306.3289, Accuracy: 0.8159\n",
      "Training loss (for one batch) at step 90: 289.4884, Accuracy: 0.8112\n",
      "Training loss (for one batch) at step 100: 290.3522, Accuracy: 0.8123\n",
      "Training loss (for one batch) at step 110: 294.7041, Accuracy: 0.8136\n",
      "---- Training ----\n",
      "Training loss: 98.8604\n",
      "Training acc over epoch: 0.8133\n",
      "---- Validation ----\n",
      "Validation loss: 39.1539\n",
      "Validation acc: 0.7397\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 328.8198, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 327.2056, Accuracy: 0.7173\n",
      "Training loss (for one batch) at step 20: 283.6812, Accuracy: 0.7463\n",
      "Training loss (for one batch) at step 30: 281.3388, Accuracy: 0.7855\n",
      "Training loss (for one batch) at step 40: 289.9800, Accuracy: 0.8115\n",
      "Training loss (for one batch) at step 50: 263.3805, Accuracy: 0.8294\n",
      "Training loss (for one batch) at step 60: 283.9826, Accuracy: 0.8380\n",
      "Training loss (for one batch) at step 70: 316.2935, Accuracy: 0.8270\n",
      "Training loss (for one batch) at step 80: 319.1744, Accuracy: 0.8111\n",
      "Training loss (for one batch) at step 90: 288.2263, Accuracy: 0.8067\n",
      "Training loss (for one batch) at step 100: 286.1201, Accuracy: 0.8108\n",
      "Training loss (for one batch) at step 110: 292.9517, Accuracy: 0.8127\n",
      "---- Training ----\n",
      "Training loss: 103.2528\n",
      "Training acc over epoch: 0.8118\n",
      "---- Validation ----\n",
      "Validation loss: 60.6050\n",
      "Validation acc: 0.7407\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 333.5400, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 315.1909, Accuracy: 0.7131\n",
      "Training loss (for one batch) at step 20: 268.8440, Accuracy: 0.7589\n",
      "Training loss (for one batch) at step 30: 277.7895, Accuracy: 0.7949\n",
      "Training loss (for one batch) at step 40: 274.0401, Accuracy: 0.8228\n",
      "Training loss (for one batch) at step 50: 285.3965, Accuracy: 0.8407\n",
      "Training loss (for one batch) at step 60: 277.4172, Accuracy: 0.8472\n",
      "Training loss (for one batch) at step 70: 306.0021, Accuracy: 0.8320\n",
      "Training loss (for one batch) at step 80: 322.2424, Accuracy: 0.8195\n",
      "Training loss (for one batch) at step 90: 312.3851, Accuracy: 0.8150\n",
      "Training loss (for one batch) at step 100: 285.8473, Accuracy: 0.8172\n",
      "Training loss (for one batch) at step 110: 291.7792, Accuracy: 0.8174\n",
      "---- Training ----\n",
      "Training loss: 91.2155\n",
      "Training acc over epoch: 0.8165\n",
      "---- Validation ----\n",
      "Validation loss: 56.2640\n",
      "Validation acc: 0.7456\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 327.0637, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 327.4132, Accuracy: 0.6854\n",
      "Training loss (for one batch) at step 20: 287.2816, Accuracy: 0.7388\n",
      "Training loss (for one batch) at step 30: 262.0322, Accuracy: 0.7863\n",
      "Training loss (for one batch) at step 40: 269.2651, Accuracy: 0.8123\n",
      "Training loss (for one batch) at step 50: 256.8417, Accuracy: 0.8326\n",
      "Training loss (for one batch) at step 60: 283.0249, Accuracy: 0.8413\n",
      "Training loss (for one batch) at step 70: 334.7723, Accuracy: 0.8271\n",
      "Training loss (for one batch) at step 80: 323.6377, Accuracy: 0.8131\n",
      "Training loss (for one batch) at step 90: 276.3035, Accuracy: 0.8072\n",
      "Training loss (for one batch) at step 100: 285.2119, Accuracy: 0.8112\n",
      "Training loss (for one batch) at step 110: 285.5855, Accuracy: 0.8138\n",
      "---- Training ----\n",
      "Training loss: 94.3181\n",
      "Training acc over epoch: 0.8129\n",
      "---- Validation ----\n",
      "Validation loss: 43.9612\n",
      "Validation acc: 0.7512\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 341.3782, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 304.5556, Accuracy: 0.6896\n",
      "Training loss (for one batch) at step 20: 274.1534, Accuracy: 0.7381\n",
      "Training loss (for one batch) at step 30: 272.7328, Accuracy: 0.7853\n",
      "Training loss (for one batch) at step 40: 270.4806, Accuracy: 0.8121\n",
      "Training loss (for one batch) at step 50: 269.9277, Accuracy: 0.8312\n",
      "Training loss (for one batch) at step 60: 289.7036, Accuracy: 0.8379\n",
      "Training loss (for one batch) at step 70: 304.1030, Accuracy: 0.8272\n",
      "Training loss (for one batch) at step 80: 338.8232, Accuracy: 0.8114\n",
      "Training loss (for one batch) at step 90: 280.4344, Accuracy: 0.8085\n",
      "Training loss (for one batch) at step 100: 282.5935, Accuracy: 0.8118\n",
      "Training loss (for one batch) at step 110: 303.7359, Accuracy: 0.8136\n",
      "---- Training ----\n",
      "Training loss: 99.6777\n",
      "Training acc over epoch: 0.8123\n",
      "---- Validation ----\n",
      "Validation loss: 52.7958\n",
      "Validation acc: 0.7351\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 317.2465, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 339.1636, Accuracy: 0.7053\n",
      "Training loss (for one batch) at step 20: 264.7878, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 30: 283.6257, Accuracy: 0.7918\n",
      "Training loss (for one batch) at step 40: 271.8583, Accuracy: 0.8175\n",
      "Training loss (for one batch) at step 50: 264.9689, Accuracy: 0.8362\n",
      "Training loss (for one batch) at step 60: 283.6120, Accuracy: 0.8430\n",
      "Training loss (for one batch) at step 70: 304.7243, Accuracy: 0.8340\n",
      "Training loss (for one batch) at step 80: 341.0678, Accuracy: 0.8174\n",
      "Training loss (for one batch) at step 90: 278.4203, Accuracy: 0.8122\n",
      "Training loss (for one batch) at step 100: 272.9934, Accuracy: 0.8170\n",
      "Training loss (for one batch) at step 110: 294.0699, Accuracy: 0.8171\n",
      "---- Training ----\n",
      "Training loss: 107.1293\n",
      "Training acc over epoch: 0.8168\n",
      "---- Validation ----\n",
      "Validation loss: 46.5543\n",
      "Validation acc: 0.7461\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 328.2657, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 334.1817, Accuracy: 0.7102\n",
      "Training loss (for one batch) at step 20: 280.7103, Accuracy: 0.7560\n",
      "Training loss (for one batch) at step 30: 286.6764, Accuracy: 0.7931\n",
      "Training loss (for one batch) at step 40: 275.5371, Accuracy: 0.8157\n",
      "Training loss (for one batch) at step 50: 295.9385, Accuracy: 0.8336\n",
      "Training loss (for one batch) at step 60: 299.6716, Accuracy: 0.8413\n",
      "Training loss (for one batch) at step 70: 324.0405, Accuracy: 0.8292\n",
      "Training loss (for one batch) at step 80: 325.1169, Accuracy: 0.8135\n",
      "Training loss (for one batch) at step 90: 279.5167, Accuracy: 0.8098\n",
      "Training loss (for one batch) at step 100: 282.9596, Accuracy: 0.8127\n",
      "Training loss (for one batch) at step 110: 298.4090, Accuracy: 0.8145\n",
      "---- Training ----\n",
      "Training loss: 105.5264\n",
      "Training acc over epoch: 0.8127\n",
      "---- Validation ----\n",
      "Validation loss: 44.9749\n",
      "Validation acc: 0.7356\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 330.4672, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 324.9814, Accuracy: 0.7294\n",
      "Training loss (for one batch) at step 20: 266.7624, Accuracy: 0.7511\n",
      "Training loss (for one batch) at step 30: 271.0915, Accuracy: 0.7908\n",
      "Training loss (for one batch) at step 40: 267.9537, Accuracy: 0.8176\n",
      "Training loss (for one batch) at step 50: 270.4464, Accuracy: 0.8338\n",
      "Training loss (for one batch) at step 60: 293.0016, Accuracy: 0.8426\n",
      "Training loss (for one batch) at step 70: 311.1082, Accuracy: 0.8298\n",
      "Training loss (for one batch) at step 80: 334.3898, Accuracy: 0.8139\n",
      "Training loss (for one batch) at step 90: 278.0499, Accuracy: 0.8108\n",
      "Training loss (for one batch) at step 100: 273.3524, Accuracy: 0.8153\n",
      "Training loss (for one batch) at step 110: 282.4167, Accuracy: 0.8158\n",
      "---- Training ----\n",
      "Training loss: 93.0726\n",
      "Training acc over epoch: 0.8156\n",
      "---- Validation ----\n",
      "Validation loss: 46.5227\n",
      "Validation acc: 0.7281\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 353.3074, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 317.7314, Accuracy: 0.7180\n",
      "Training loss (for one batch) at step 20: 288.0366, Accuracy: 0.7597\n",
      "Training loss (for one batch) at step 30: 278.0124, Accuracy: 0.7941\n",
      "Training loss (for one batch) at step 40: 284.4807, Accuracy: 0.8205\n",
      "Training loss (for one batch) at step 50: 258.9740, Accuracy: 0.8344\n",
      "Training loss (for one batch) at step 60: 285.3942, Accuracy: 0.8435\n",
      "Training loss (for one batch) at step 70: 313.5044, Accuracy: 0.8319\n",
      "Training loss (for one batch) at step 80: 318.6535, Accuracy: 0.8162\n",
      "Training loss (for one batch) at step 90: 279.0706, Accuracy: 0.8106\n",
      "Training loss (for one batch) at step 100: 289.8087, Accuracy: 0.8151\n",
      "Training loss (for one batch) at step 110: 288.4353, Accuracy: 0.8171\n",
      "---- Training ----\n",
      "Training loss: 105.3137\n",
      "Training acc over epoch: 0.8146\n",
      "---- Validation ----\n",
      "Validation loss: 47.5927\n",
      "Validation acc: 0.7423\n",
      "Time taken: 10.15s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 332.4246, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 315.6790, Accuracy: 0.7045\n",
      "Training loss (for one batch) at step 20: 296.4918, Accuracy: 0.7504\n",
      "Training loss (for one batch) at step 30: 259.7859, Accuracy: 0.7903\n",
      "Training loss (for one batch) at step 40: 272.6342, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 50: 277.4768, Accuracy: 0.8326\n",
      "Training loss (for one batch) at step 60: 297.6924, Accuracy: 0.8399\n",
      "Training loss (for one batch) at step 70: 300.2984, Accuracy: 0.8257\n",
      "Training loss (for one batch) at step 80: 325.6251, Accuracy: 0.8101\n",
      "Training loss (for one batch) at step 90: 280.7449, Accuracy: 0.8064\n",
      "Training loss (for one batch) at step 100: 278.3010, Accuracy: 0.8110\n",
      "Training loss (for one batch) at step 110: 290.0378, Accuracy: 0.8108\n",
      "---- Training ----\n",
      "Training loss: 107.2963\n",
      "Training acc over epoch: 0.8100\n",
      "---- Validation ----\n",
      "Validation loss: 57.7310\n",
      "Validation acc: 0.7480\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 327.3257, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 327.0682, Accuracy: 0.6982\n",
      "Training loss (for one batch) at step 20: 277.0250, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 30: 280.3969, Accuracy: 0.7949\n",
      "Training loss (for one batch) at step 40: 261.9981, Accuracy: 0.8178\n",
      "Training loss (for one batch) at step 50: 270.2369, Accuracy: 0.8355\n",
      "Training loss (for one batch) at step 60: 288.5717, Accuracy: 0.8414\n",
      "Training loss (for one batch) at step 70: 313.6927, Accuracy: 0.8287\n",
      "Training loss (for one batch) at step 80: 315.2009, Accuracy: 0.8139\n",
      "Training loss (for one batch) at step 90: 276.8124, Accuracy: 0.8098\n",
      "Training loss (for one batch) at step 100: 272.8270, Accuracy: 0.8131\n",
      "Training loss (for one batch) at step 110: 305.4809, Accuracy: 0.8137\n",
      "---- Training ----\n",
      "Training loss: 104.2994\n",
      "Training acc over epoch: 0.8135\n",
      "---- Validation ----\n",
      "Validation loss: 53.8923\n",
      "Validation acc: 0.7337\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 334.4004, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 310.0307, Accuracy: 0.6854\n",
      "Training loss (for one batch) at step 20: 277.3546, Accuracy: 0.7426\n",
      "Training loss (for one batch) at step 30: 304.8823, Accuracy: 0.7838\n",
      "Training loss (for one batch) at step 40: 259.3418, Accuracy: 0.8133\n",
      "Training loss (for one batch) at step 50: 274.2912, Accuracy: 0.8310\n",
      "Training loss (for one batch) at step 60: 290.0411, Accuracy: 0.8363\n",
      "Training loss (for one batch) at step 70: 304.2434, Accuracy: 0.8258\n",
      "Training loss (for one batch) at step 80: 315.2090, Accuracy: 0.8102\n",
      "Training loss (for one batch) at step 90: 293.1011, Accuracy: 0.8059\n",
      "Training loss (for one batch) at step 100: 291.0928, Accuracy: 0.8104\n",
      "Training loss (for one batch) at step 110: 306.9367, Accuracy: 0.8120\n",
      "---- Training ----\n",
      "Training loss: 104.6050\n",
      "Training acc over epoch: 0.8117\n",
      "---- Validation ----\n",
      "Validation loss: 56.9612\n",
      "Validation acc: 0.7391\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 352.2415, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 318.1212, Accuracy: 0.7251\n",
      "Training loss (for one batch) at step 20: 286.8358, Accuracy: 0.7545\n",
      "Training loss (for one batch) at step 30: 277.6141, Accuracy: 0.7949\n",
      "Training loss (for one batch) at step 40: 261.7378, Accuracy: 0.8159\n",
      "Training loss (for one batch) at step 50: 264.1681, Accuracy: 0.8346\n",
      "Training loss (for one batch) at step 60: 277.6287, Accuracy: 0.8395\n",
      "Training loss (for one batch) at step 70: 325.7953, Accuracy: 0.8278\n",
      "Training loss (for one batch) at step 80: 294.1077, Accuracy: 0.8149\n",
      "Training loss (for one batch) at step 90: 263.6338, Accuracy: 0.8115\n",
      "Training loss (for one batch) at step 100: 287.8610, Accuracy: 0.8154\n",
      "Training loss (for one batch) at step 110: 287.5037, Accuracy: 0.8162\n",
      "---- Training ----\n",
      "Training loss: 86.9198\n",
      "Training acc over epoch: 0.8147\n",
      "---- Validation ----\n",
      "Validation loss: 49.2117\n",
      "Validation acc: 0.7340\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 313.3822, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 310.3090, Accuracy: 0.6932\n",
      "Training loss (for one batch) at step 20: 275.6890, Accuracy: 0.7522\n",
      "Training loss (for one batch) at step 30: 264.3402, Accuracy: 0.7941\n",
      "Training loss (for one batch) at step 40: 269.5761, Accuracy: 0.8165\n",
      "Training loss (for one batch) at step 50: 264.2752, Accuracy: 0.8350\n",
      "Training loss (for one batch) at step 60: 299.0013, Accuracy: 0.8422\n",
      "Training loss (for one batch) at step 70: 301.0775, Accuracy: 0.8283\n",
      "Training loss (for one batch) at step 80: 323.9571, Accuracy: 0.8119\n",
      "Training loss (for one batch) at step 90: 286.2498, Accuracy: 0.8071\n",
      "Training loss (for one batch) at step 100: 290.1159, Accuracy: 0.8106\n",
      "Training loss (for one batch) at step 110: 275.5993, Accuracy: 0.8121\n",
      "---- Training ----\n",
      "Training loss: 111.9538\n",
      "Training acc over epoch: 0.8123\n",
      "---- Validation ----\n",
      "Validation loss: 67.6706\n",
      "Validation acc: 0.7410\n",
      "Time taken: 10.27s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEjCAYAAADZk82GAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAB/+klEQVR4nO2dd3hb1fn4P0eyZHnvTGfvPUmAMBLCCKPsAIHSBCgFWqClLbTQFsL6/qDQsvdeJUApEDYkJCSQBLKcvR0ndob3tmVZ0vn9ce6VZFuesS2P83kePdLdr67l8953nPcVUko0Go1GownEEmoBNBqNRtPx0MpBo9FoNHXQykGj0Wg0ddDKQaPRaDR10MpBo9FoNHXQykGj0Wg0ddDKQaNpBkKImUKIrFDLodG0NVo5aNoNIUSGEOL0UMuh0WgaRysHjaaLIIQIC7UMmq6DVg6akCOECBdCPC6EOGy8HhdChBvbkoUQnwkhioQQBUKIlUIIi7HtL0KIQ0KIUiHELiHE7HrOf64QYqMQokQIkSmEWBiwbaAQQgoh5gshDgoh8oQQfwvYHiGEeF0IUSiE2A4c18h3ecK4RokQYr0Q4uSAbVYhxF1CiH2GzOuFEP2MbWOEEN8a3zFbCHGXsf51IcQDAeeo4dYyrLG/CCE2A+VCiDAhxF8DrrFdCHFRLRmvF0LsCNg+WQhxuxDiw1r7PSmEeKKh76vpwkgp9Uu/2uUFZACnB1l/H7AG6AGkAKuA+41t/w94HrAZr5MBAYwAMoE+xn4DgSH1XHcmMA71MDQeyAYuDDhOAi8BEcAEoAoYZWx/CFgJJAL9gK1AVgPf8ZdAEhAG/Ak4CjiMbbcDWwzZhXGtJCAGOGLs7zCWpxvHvA48UOu7ZNW6p2mGbBHGurlAH+P7Xg6UA70Dth1CKTkBDAUGAL2N/eKN/cKAHGBKqH83+hWaV8gF0K/u82pAOewDzglYPgvIMD7fB3wCDK11zFBj8DodsDVTjseBx4zPpnJIDdj+M3CF8TkdmBOw7TcNKYcg1yoEJhifdwEXBNlnHrCxnuObohyubUSGNPO6wNfA7+vZ70vgeuPzecD2UP9m9Ct0L+1W0nQE+gAHApYPGOsAHgH2At8IIdKFEH8FkFLuBf4ALARyhBCLhBB9CIIQYroQYpkQIlcIUQzcCCTX2u1owOcKIDpAtsxastWLEOLPhsumWAhRBMQFXKsfShHWpr71TSVQPoQQvxJCpBmuuCJgbBNkAHgDZflgvL91DDJpOjlaOWg6AodRrg2T/sY6pJSlUso/SSkHA+cDfzRjC1LK/0gpTzKOlcDD9Zz/P8BioJ+UMg7lphJNlO0IakANlC0oRnzhDuAyIEFKGQ8UB1wrExgS5NBMYHA9py0HIgOWewXZx1daWQgxAOUiuxlIMmTY2gQZAD4GxgshxqIsh3fq2U/TDdDKQdPe2IQQjoBXGPAu8HchRIoQIhm4G3gbQAhxnhBiqBBCoAZaD+AVQowQQpxmBK6dQCXgreeaMUCBlNIphJgGXNkMed8H7hRCJAghUoFbGtg3BnADuUCYEOJuIDZg+8vA/UKIYUIxXgiRBHwG9BZC/MEIzscIIaYbx6QB5wghEoUQvVDWUkNEoZRFLoAQ4hqU5RAow5+FEFMMGYYaCgUppRP4L0qZ/iylPNjItTRdGK0cNO3NF6iB3HwtBB4A1gGbUQHbDcY6gGHAEqAMWA08K6VcBoSjgsV5KJdQD+DOeq75W+A+IUQpSvG83wx570W5kvYD39Cwq+Vr4Ctgt3GMk5oun38b1/4GKAFeQQWRS4EzgF8Y32UPMMs45i1gEyq28A3wXkPCSim3A/9C3atsVCD+x4DtHwAPohRAKcpaSAw4xRvGMdql1M0RUupmPxqNRiGE6A/sBHpJKUtCLY8mdGjLQaPRAGDMH/kjsEgrBo2eUanRaBBCRKHcUAeAOSEWR9MB0G4ljUaj0dRBu5U0Go1GUwetHDQajUZTB60cNBqNRlMHrRw0Go1GUwetHDQajUZTB60cNBqNRlMHrRw0Go1GUwetHDQajUZTB60cNBqNRlMHrRw0Go1GUwetHDQajUZTB60cNBqNRlMHrRw0Go1GUwetHDQajUZTh07dzyE5OVkOHDiwzvry8nKioqLaX6AgaFmC01FkaUiO9evX50kpU9pZJCD4b7uj3DPQstRHZ5GlSb9tKWWnfU2ZMkUGY9myZUHXhwItS3A6iiwNyQGskx3ot91R7pmUWpb66CyyNOW3rd1KGo1Go6mDVg4ajUajqYNWDhqNRqOpQ6cOSHdEqqurycrKwul0AhAXF8eOHTtCLJVCyxJcjv3795OamorNZgu1OBpNh0Erh1YmKyuLmJgYBg4ciBCC0tJSYmJiQi0WgJYlCCUlJbhcLrKyshg0aFCoxdFoOgzardTKOJ1OkpKSEEKEWhRNExBCkJSU5LP0NBqNQiuHNkArhs6F/ntpNHXpksphfbabl1emh1oMjUajaTE7j5bgrPb4llfty2NPdmm7Xb9LKodNuR5eWKGVg0ajaZxSZzWlzupG90vLLKKsyh10m8cr66zLLKjgmWV7awzwTWXHkRLOfmIlf3p/EwBHiitZ8NpaFry2lvyyKm55dyP//nY3OaV13aGHiipxe7zNvmZtuqRySHIIckurWvRH6ezk5+czceJEJk6cSK9evejbt69v2eVyNXjsunXruPXWWxu9xoknntha4gLw+uuvc/PNN7fqOTWapnAwv4LT/vU9172xrt59pJT8+9vdXPjMjzz4+XbferfHi5SSvLIqTnr4Ox5fstu3zeuV3PZeGo98vYub/7OB6oDBWkrJqz/s59LnVrHjSEmNa60/UMjOoyU8/d1epITPtxzhq61Heeq7vXi8kkNFlZzz5Eo+23yYp77bw8XPrqLK7WHZrhw2Hixk5Z5cTn74O254az3uIAqrOXTJbKXkCOVDPlLsZFByx6hz0l4kJSWRlpYGwMKFC4mOjubPf/4zoDKE3G43YWHB/+xTp05l6tSpjV5j1apVrSavRtMSpJQ8tmQPYRZBZImHiPR8skurSIqyM31QImFWS539S5xuckur2HGkhGmDEgkPs/DLV34it7SK3NIq9uWWMTg5CiEEXq9k+5ESRvWO5cUV6Ty5dA+JUXY+3XSEu88bg80qmPvCaqSEfomRHCl28uTSPdw4Ppxty/ZytNjJugOFnDG6J99uz+ayF1Zz0aS+rEnPJz23nJ1HS7GHWbjkuVVcNb0/Jw1LobzKzS3vbgTAKyXXnzyIlXvyuPHt9QD86oQBHC6qZMmOHP52ziiG9Iji2tfXcf9n23n350yklETaw0iODmfpzhyKCq2ceqoXm7VlNkCXVA6JDnUzDhVWhlQ53PvpNrZkFmK1WlvtnKP7xHLPL8Y065gFCxbgcDhYt24dp5xyCldccQW///3vcTqdRERE8NprrzFixAiWL1/Oo48+ymeffcbChQs5ePAg6enpHDx4kD/84Q8+qyI6OpqysjKWL1/OwoULSU5OZuvWrUyZMoW3334bIQRffPEFf/zjH4mKimLGjBmkp6fz2WefNSprRkYG1157LXl5eaSkpPDaa6/Rv39/PvjgA+69916sVitxcXGsWLGCbdu2cc011+ByufB6vXz44YcMGzasRfdVEzpcbi82q2g0MUBKyV0fbaG4spozR/fiyaV7/Bt/WuP72DM2nIcuHs+skT0AqPZ4ufb1tazck+fbZ3jPaAYnR3O4qJLnfzmF376znqe/28ua9HwSIu2EWQWbs4o5fnAi6w8UcvbYXlx9wgCufOknvt52lBJnNRsPFmERyt00b1p/vtuZzbObqmDTLgBOGJzEi1dP4dPNR7jnk63c/ck2+sZHMDA5knvPH8Ocsb24639beGPVAV5auR+ACalxDO0Rw5r0fG48dQi/OWUIH23MYnd2GbfOHoYAVu3L57zxvQGY2C+et9ccJCUmnOmDElm9L59FvzmeFbtzWbFpN9ZjSLboksrBtBwOF1WGWJKOQ1ZWFkuWLCE+Pp6SkhJWrlxJWFgYS5Ys4a677uLDDz+sc8zOnTtZtmwZpaWljBgxgptuuqnORLGNGzeybds2+vTpw4wZM/jxxx+ZOnUqN9xwAytWrGDQoEHMmzevyXLecsstzJ8/n/nz5/Pqq69y66238vHHH3Pffffx9ddf07dvX4qKigB4/vnn+f3vf89VV12Fy+XC4+l+bsTOzvbDJcx/7WdSEyJ47LKJDEiK5D8/H8RmtXDZ1H44qz38+9vdpGUWcfqoHrz7cyYAX249ypg+sTz/yyl8uGQVUydNpEdsOOm5ZTy+ZA/XvL6W4wYmMGVAIsWV1azck8cNpwxmRC81t+bPH2xid3YZfzpjOHPG9uKU4Sl8tPEQ8ZE2kixQXFnNNTMG8ubqAyRE2njwonHER9hITYjgka93UVJZzYlDkrjh1CG8vy6Tu84Zybxp/Vi0ZC23XXIKEklchA0hBOdP6MOpw1PILa1iSEpUDSX4yoLjqHR5WLM/ny1ZxfzqhAHER9pr3KPfnDKkxvIvJvTxff7zmSO4/s11/N9F4zhjdE88XonVIhicEs0AVwYWi1YONUhwCCwCskKsHO75xZgOM9lr7ty5PgumuLiY+fPns2fPHoQQVFcHD8ade+65hIeHEx4eTo8ePcjOziY1NbXGPtOmTfOtmzhxIhkZGURHRzN48GDfpLJ58+bx4osvNknO1atX87///Q+Aq6++mjvuuAOAGTNmsGDBAi677DIuvvhiAE444QQefPBBsrKyuPjii7XV0IGpdHmwh1mwCPhmezYTUuPJLKzgutfXEmG3si+njDMfX8HE1Hh+zijAZhWMT43jtvc2seNICVF2Kz/vL2Bs31jOHtubZ5bt5cGLxtEvMZKJPcI4aVgyAMN7xjBzRA+eXb6PH/bk8soP6VR7JFcc1487zxnlk8fjlfy0v4CbZqqB99oZg9h5pJRnrprMlAEJvv0umZyKw2YhMUoN2LecNpSXVu6nf2IkD140jkHJUZw6XFW+Hp8aT8FAGykx4XW+f1yEjbiI4DPwI+xWZo3owawRPZp9X08alszGu8/AYVP/29YAZXCsKdpdUjmEWQQ9Yx0cKtSWg0lgXfd//OMfzJo1i48++oiMjAxmzpwZ9JjwcP+P3Gq14nbXzdRoyj6twfPPP89PP/3E559/zpQpU1i/fj1XXnkl06dP5/PPP+ecc87hhRde4LTTTmuT62uaTpXbw9ZDxSzZkUPPmHBOH92TC5/5kTNG9+LCiX244a31xISHUeXxkhofwZvXTcNqETz13V7+tyGLq48fwAfrM7n0udWUu9y89KupjOodw5NL93D9yYMZ1jOG608ejD0suC/dYbPyxzOG88czhlNcWc2Gg4WcOCSpxj5zp/Zj7tR+vuVThqew+s7T6gyoY/vG1Vi+/Lj+XH5c/1a6U62DqRhamy6pHAD6xEdwqKgi1GJ0SIqLi+nbty+gMoVamxEjRpCenk5GRgYDBw7kvffea/KxJ554IosWLeLqq6/mnXfe4eSTTwZg3759TJ8+nenTp/Pll1+SmZlJcXExgwcP5tZbb+XgwYNs3ry5zZSDEGIO8ARgBV6WUj5Ua3t/4A0g3tjnr1LKL4xtdwLXAR7gVinl120iZAfgq61HuPXdNFweL0KAlPDvb3dT4nTz3/WZZBVWEBMexuQBCXil5MkrJpFgPJX/30XjeOCCsVgsghhHGM8u38ets4dxxuieAPzz0gm+69SnGGoTF2Fr8hO5ngxZky6ZygrQNz6Cw0W6JEIw7rjjDu68804mTZrUJk/6ERERPPvss8yZM4cpU6YQExNDXFxc4wcCTz31FK+99hrjx4/nrbfe4oknngDg9ttvZ9y4cYwdO5YTTzyRCRMm8P777zN27FgmTpzI1q1b+dWvftXq3wVACGEFngHOBkYD84QQo2vt9nfgfSnlJOAK4Fnj2NHG8hhgDvCscb4uyTPL9pGaEMFzV00m7R9n8rtZQyh3efjr2SOp9khW7snjosl9eePaabx13XSfYjAxfeS3zh7GS7+ayu9na1dhyGisG1BHfjXUCe6hL3fIoXd9Lj0eb73dkNqC7du311guKSlp8bmcLrfcm1Mq3R7PsYp1zLI0l9LSUimllF6vV950003y3//+d8hkaQhTjtp/Nyn93bKAE4CvpfG7A+4E7pQBv0XgBeAvAfuvCrYv8DVwgmzBb7ujdxnbdbREDvjLZ/KVlek11pdXVUsppVzw6k9ywF8+k9sOFbe5LKGis8hCEzrBdVm3Ut/4CKo9kpzSKnrFOUItTouorPZQXuXG5fYSYe9cRt5LL73EG2+8gcvlYtKkSdxwww2hFulY6AtkBixnAdNr7bMQ+EYIcQsQBZwecOyagP2yjHV1EEL8BvgNQM+ePVm+fHmN7Wb6cEegrKyMJd8t483tLo6UeZnd38b2Ag9WAckVGSxffqDOMWf18DIgzE7O7g3k7A5y0mOQpSPdl64iS5dWDgCHiio6rXLwqidN5LFNdAwJt912G7fddluNda+99prPTeT1erFYLMyYMYNnnnkmFCK2NvOA16WU/xJCnAC8JYQY25wTSClfBF4EmDp1qqydKLB8+fJ6kwfam2XLlvHB4VhWZB2lZ2w4z2+uAuCM0T05/8zGJ1K2Jh3pvnQlWbqschiQFAnA/rwKpgxIDLE0LcOc/X7sVVI6Btdccw3XXHMN0HH6OTSRQ0C/gOVUY10g16FiCkgpVwshHEByE4/tdOwr8vLFlqP88Yzh/G7WUNIyCyksr2ZS//hQi6ZpJTqXr6IZ9EuMxGYV7MstC7UoLcZvOXRC06FrsRYYJoQYJISwowLMi2vtcxCYDSCEGAU4gFxjvyuEEOFCiEHAMODndpO8lXn354OsSc9nbbYbm1WwYMZArBbBlAGJnD66J0nRdXP8NZ2TLms52KwWBiRFsS+n8yoHUydo3RBapJRuIcTNqGCyFXhVSrlNCHEfKrC3GPgT8JIQ4jZAAguMwN82IcT7wHbADfxOStkpp3Kv2J3Lnf/bQmKUHen2cPKwFGIdurVqV6XNlIMQ4lXgPCBHSjm21rY/AY8CKVLKPKESjJ8AzgEqUP9YG45VhiEpUeztxMrBZzmEWA4NSDVn4Yta6+4O+LwdmFHPsQ8CD7apgG1MhcvNXR9toW98BEeKK/FKOGdc71CLpWlD2tKt9DqGDzYQIUQ/4EyUGW5yNsrcHobK1niuNQQYkhLNgfyKGuVyOxN+y0GrB01oWbIjh6zCSh6+ZDwLThxEuBXOGNUz1GJp2pA2Uw5SyhVAQZBNjwF3UPOB+ALgTSMFdw0QL4Q45seSISnRuL2SgwWdc6Z0S7KVZs2axddf15yA+/jjj3PTTTcF3X/mzJmsW6dq2Z9zzjm+onaBLFy4kEcffbTB63788cds3+6vdX/33XezZMmSpgveCLrnQ2hZl1FAlN3K8YMT+du5o3j4lAjiIrVLqSvTrjEHIcQFwCEp5aZaU9WD5ZH3BY4EOUeDueDgz+8tLlau3f98vRqAcKtgQg8r4da60+T3F3voG23BHmRbc4iLi6O01N/Kz+Px1FhuDi6XsngqnU7CvFVNOuaiiy7irbfeqtGQ55133uH+++8PKovH46G8vJzS0lJfmYva+1RVVWGz2Rr8Hh988AFz5syhXz+VmHP77bcHPVfgdZtzX5xOJy6Xq8X3sj5MOZxOZ4fJT++IrM0oZPKABF+fhPjwLpvLojFoN+UghIgE7kK5lFpMY7ng4M/vneKs5r7V3/D69mpfG79Thqfw2oLjalQvLCh3cd2DS7j6+AEsPN/fK8Ht8VJQ7qJHbNPnSezYscOfovnlX3Ef2kiYtWW32eb24PZIwsMs/oYdvcbB2Q/Ve8wvf/lLHnjgAcLDw7Hb7WRkZJCdnc0nn3zCXXfdRVVVFZdeein33nsvoIrlRUVFERMTw8CBA1m3bh3Jyck8+OCDvPHGG/To0YN+/fr5ymC89NJLvPjii7hcLoYOHcpbb71FWloaX375JatWreJf//oXH374Iffffz/nnXcel156KUuXLuXPf/4zbreb4447jueeew6Xy8W4ceOYP38+n376KdXV1XzwwQeMHDky6PdyOBzY7XZiYmJateeDmVLrcDiYNGlSi/5OXZ0SZzU7j5boUhbdjPZU/0OAQcAmIUQGKt97gxCiF22UCx7jsNEzNhyrELx49RT+cd5oVuzO5env9tbYb+fREjxeyX9+PsjRYn89pjdXH2Dmo8spr6dvbFvjizk045jExESmTZvGl19+CcCiRYu47LLLePDBB/n+++/ZvHmz770+1q9fz6JFi0hLS+OLL75g7dq1vm0XX3wxa9euZdOmTYwaNYpXXnmFE088kfPPP59HHnmEtLQ0hgzx1593Op0sWLCA9957jy1btuB2u3nuOX9IKTk5mQ0bNnDTTTc16royMXs+bN68mauuusrXhMjs+bBp0yYWL1aZpmbPh7S0NNatW1en5LimcTYcKERKOG5g55wvpGkZ7WY5SCm3AL7yiIaCmGpkKy0GbhZCLEKVJSiWUtZxKbWE+y4YS1yEjeMHJyGlZE16Pq/+uJ9bZw/1VWHck60ymtweL88t38u9F6jkqrUZBVS4POzJKWNiv/jmX/zsh6g8hsleh3PLKK9y0yvOQY+Yplsv8+bNY9GiRVxwwQUsWrSIV155hffff5/nn38er9fLkSNH2L59O+PHjw96/MqVK7nooouIjFQTCc8//3zftq1bt/L3v/+doqIiysrKOOussxqUZdeuXQwaNIjhw4cDMH/+fJ555hmuu+46AF9vhilTpvj6ODSG7vnQPrjcXhZ+uo2th4qxWkTL/gc0nZY2sxyEEO8Cq4ERQogsIcR1Dez+BZAO7AVeAn7bWnKcNaYXxw9OMmVi1ogeFFdW1whS784uJdYRxrxp/XlrzQHWZqg4+rbDJb7tVW4PFa6mWxAer2T74RIqqlueadTSeQ4XXHABS5cuZcOGDVRUVJCYmMijjz7K4sWL2bx5M+eeey5OZ8sq1i5YsICnn36aLVu2cM8997T4PCZmP4jW6AXx/PPP88ADD5CZmcmUKVPIz8/nyiuvZPHixURERHDOOefw3XffHdM1uhP/+ekA//npIPvzypk1ogdR4V12WpQmCG2ZrTRPStlbSmmTUqZKKV+ptX2glDLP+CyllL+TUg6RUo6TUq5rK7nGp6rS0Zuyin3r9mSXMbxnDHeeM4rUhEj+sCiNQ0WVPgWy+2gpf/1wC5e/oOqn/fubXXy5pWHDptrjxe31Uu1tuXJo6Qzp6OhoZs2axbXXXsu8efMoKSkhKiqKuLg4srOzfS6n+jjllFP4+OOPqayspLS0lE8//dS3rbS0lN69e1NdXc0777zjWx8TExM0WDxixAgyMjLYu1e58t566y1OPfXUZn2f2pg9H4CgPR/uu+8+UlJSyMzMJD093dfz4YILLmjQnabxU+Ks5omlezhxSBKb7zmTl+e3b70kTejpdo8CI3rFYA+zsCWriOU7cxjaM5rdOaWcPbY30eFh/PuyCVz6/Gru+3QbABYBO46WsDmrmFKnm7UZBTy1bC9T+idwdgOTgNyGUjgG3eBTCi05xbx587joootYtGgRI0eOZNKkSUyZMoUBAwYwY0bQuVo+Jk+ezOWXX86ECRPo0aMHxx13nG/b/fffz/Tp00lJSWH69Ok+hXDFFVdw/fXX8+STT/Lf//7Xt7/D4eC1115j7ty5voD0jTfeiMvlasG3Ujz11FNcc801PPLII76ANKgMqT179iClZPbs2UyYMIGHH36Yt956C5vNRq9evbjrrrtafN3uxPtrMymsqOauc0bpJjjdFNGZJ1hNnTpVmjn6gTRWjfDCZ37kUFEluaVV2KyCao/knl+M5poZqufx+U//wGbDsjh1eAor9+T6BvnhPaPZnV2G3Wph/T9O555PtnHtSYN87QR37NjBqFGjKK50cSC/gli7YGCPpjW6qc2OIyVUe7wkR4fTx6gyeyx0pGJ3HUUWUw7z7xaIEGK9lDIkj8zBftvtWfHzxrfWs/NoCctvnxV0e1eqPtqadBZZmvLb7pbJyhNS48gtrSLKbvWluA7v6R+oLj9OJU71jnNwwpAkvBKEUGXAd2eXYbUIXB6v6nm78RCPL6lbnN7tOXbLwXQreTuxAtd0TjZlFTFBB6C7Nd1SOYxPjQfgl8cP4LzxfQAY1jPat/38CX2IsFkZ0yeOEYbSGN83jvMmKDfSFYbyePWH/QAs3ZlDZq1Z2J5WcSvVfO8OvPbaa0ycOLHG63e/+12oxepWZJc4OVLsZILxf6LpnnS7mAPAaSN7cMnkVH598mAsAs4e26tGqmiMw8Yr86fSI9ZBpF21+z1pWDLnje/D4rTDXHfSINak57Mvt5wzRvdk6Y5s3vnpIH89W03gklL6Yw4tlFFK2S0L7wX2fGgvOrNrtS1IyywC0JZDN6dbKoeEKDv/umyCbzlYYPnEocm+z8//cjInDE4mLtLG6jtnA2pC0L7ccm48dQhVbi/Ldubw17NH4nA4yM/Pxy1UjKCllkPgeKUHr7ZDSkl+fj4OR+fsFtgWbMosIswiGNMnNtSiaEJIt1QOzWXO2LrK45fHDyAu0sbk/vH0jXew44iaE5GamkpWVhaZuVlUub1YBcii5geTvV5JtjFbu8RmoSLn2JuoOJ3ODjMIdhRZnE4n8fHxeuZ0AJuyihjVOxaHzRpqUTQhRCuHFjK2b5wvQykuwk5xRTVSSmw2G4MGDeL3n6uMpxgbbLn/3Gaf/0hxJee+pSZsnTwsmbeum3jMMi9fvrzD1A/qKLJ0FDk6CtUeL2kHi7hkilaW3Z1uGZBubRIibbg8Xipc/gZfBeUqj7/C3TK3kLPaH63orP0oNJ2PrYeKKXd5fFUFNN0XrRxagXijrn1hhX9iV0G5CyHAI6HK3fzB3VntVzTVHh1z0LQPa9JV6Zhpg3SRve6OVg6tQHykHYCiimpADewVLg+9jFLfJc7qZp+z0lAOFqEKoGk07cGa9HyG9YgmObpWjMvrhYNruldedTdHK4dWIKGWcjAtiP6JqqppqbP5BeVMyyHGYdNuJU27UO3xsi8jg+ODWQ0r/gmvngW7v2p/wTQhQSuHVqC2W8mMNwxIarlyqDJiDrERYdpy0LQLGeu/4QfL9dxy9E4o2O/fkPkzfP+w+rzx7dAIp2l3tHJoBUzlUFSpLAe/cogCoOwYLIdYhwp2azRtjdz0HpXSTnJhGrzxCyjNVht+eAyiesCkq2H311BRqzW8qwLWPAfulhdT1HQ8tHJoBeIjDLdSeU3Lwe9WannMIdZh67SWQ35ZFcWVzf/uHREhxBwhxC4hxF4hxF+DbH9MCJFmvHYLIYoCtnkCti1uV8GbisdN36NLWW6Zhpj/KVTkw3tXqVjD0a0w8CSYfiN4q2HrhzWP3f4xfPVX2PZR/ecvz1NKRNNp0MqhFbCHWYiyWyk0Yw6t4FYyU1ljHGGdNuZw09sbuNcofd6ZEUJYgWeAs4HRwDwhxOjAfaSUt0kpJ0opJwJPAYFt7SrNbVLK8+mIHPiBKE8xe5NnI/pOgtMXQtZaOLwRig9Cj1HQayzED4ADP9Y8NvMn9b6tgU5+r5wB3z3QZuJ3WTJ/hu2fhOTSWjm0EvGRdooqlVI4WFCJ3WqhX4JSDi3JVvK5lSI6r+WQXeokr6xLuBqmAXullOlSShewCLiggf3nAe+2i2SthHvbp1TIcORgVR6GwTPV+8Y31XvPMeo9LhXKcmsenGn0GN+7FCoL6568uhIK0iF7S6vL3aWREj65GT77Y0gur2dItxLxkTZfttIPe3OZNiiR2AgVi2iR5eD2u5U66zyHCpeHqoD5Gp2YvkBmwHIWqtd5HYQQA4BBQGA/UocQYh3gBh6SUn5cz7G/AX4D0LNnT5YvX15je1lZWZ11rcWQ3espkH3xlOaqa0gvJ1mjIG0RYcCa9BKcR5YzugKiy/ZTlqJksbrLOSlnO/lJx5Gcv5ai58+lMqI3u0bcrOrcAxEVWUwHKo/s5KdWlH/I3pcpTJhEWfiIFt8Xi8eJ19rMMi5SkpL7A3nJxyMtthqbWvNvFFe0lUl5uwBYueQLPGGRdfaxuivol/kRB/tfitdaM/34WGXRyqGVSIi0U1jh4nBRJbuzy5g7pR9Wi8BhbaFycHkQAqIdYbg8XqSUna4jV6XL06IJgJ2cK4D/SikDteIAKeUhIcRg4DshxBYp5b7aB0opXwReBNXsp3ajlrZsJJO3vooCGcO8s06iV5wxWGZNh33fgT2a48+aCxYLVHwBm7YSHR2tZNn3HfwgST77Tlh6L/HZ24kv3kbv+S9DRII6z96l8DNEVOUx8+QZYLXVFeDQeshaB9NvaJrAubth+af0ixEUJE1p2X3J3wfPHg+XvQUDZ8Cur2DcpT6lhpTw32sgvj+ccZ//uMy18P2jcNb/wQk1y8m36t/ov2/6Pp48ug+4nRCZBMnD/Pus/DcceJ+Bx18Ao85qVVm0W6mViIu0UVxRzco9yuQ+ZXgKABFhgrKqFriV3F7CwyyEh6k/UWezHqSUVLjcXUU5HAL6BSynGuuCcQW1XEpSykPGezqwHOhwxZyslXmUhyXQMzbg6TPVaA/bY5RSDADRPaCqGIvHcBdm/gwIte9vvoeLnlfrS4/6z1NsGF3S6/8cSN4eeOti+PIOyN3VNIF3GH747GOIae3+Cjwu2PAmrHgE/vdrpaRMtn6oguw7Pq15XKGR5rvhrbabFFhyBLYvhoGqPzoF++C9q+HLv/j38XphvWqRS14T71sz0MqhlUiItFFY4eL73bn0inUw3GgeFGFr+SQ4h82KzaqeYjpbULrK7cUrocrdJdxKa4FhQohBQgg7SgHUyToSQowEEoDVAesShBDhxudkYAawvV2kbipSEukuIiwmpaZ1mjpNvfcIiL1H9wDAVl2klvctg55jwRGrnrhjjArGgcqhKEAhFGbUuTbv/wosVkDA1v+pp+Fv72l44DWDtAXpWDyqejEZP6pZ3E1ln+H52/MNrH9dfd67VL1XlcLXRr/xgv1QVRbwHQ6o99wdNZVJa7L6aZAeOPufajn9eyjPgUPrlFIw5S86aMhStxvlsaKVQysRH2GnuLKa73flMnOE/58sMkywKbOI37y5rlkprc5qDxE2K3ar+hN1tqB0pVGEsKq6c8kdDCmlG7gZ+BrYAbwvpdwmhLhPCBGYfXQFsEjWrLQ4ClgnhNgELEPFHDqUcqgoLyYcF1EJPWtuSJ0C9hjof4J/XbTax+4qVEHmzDUw9iL/9phe6r30KPz0Aiy5Vw1gYUbZ+trKIW8P5GyHWXepdNm1L8PS++DHx2Hze2qf1c/AM9PBY/z/5O+Do1tgwAxAElV+APL2wjuXwieGm0dK2LtEPWl/dCPk7Kh53WqnUiYDZqj0XGcxOOL9CmP9G1CWDSf9EZA1jy86oPa1RcKGN2qc1lGZDY8OV/K1lIoCWPcajL0Eeo5WCtdUhs5iyN+rPm95X7mZBpwEuTtrnmP/ShIK0o7JstExh1YiPtKGV0K5y8NV0wf41kfZBHtznRwudrLzaCnHDWxaQbPKaq+yHHxupc41yFYYgegu4lZCSvkF8EWtdXfXWl4Y5LhVwLg2Fe4Y2bv/AOOBpJQ+NTdEJMAft4Pd30LXtBzsriLYtAgQMP6KgO2Ggik7qtwi2duUW6rPRPWUXVs57P1WvQ87U50rY6UaDOMHwOd/Usd+/09wFikrZfiZkPYOCAvM+hu8fg4xpfvgf+9CdYUaOIsPwSe/hfTlYItSFs3epXDtV5A0BDxutc1dCTN+rwZja5iS4YfH1fJPzyvFMWUB/PBvlWnVz3CzFR2A5OGQNBS2faye7m1K+SUWrFdKZe9S6NWCP/vWD5XVVF0BJ92m1iUOgQM/+PfJWgspw9W97TsFEgcrF5enGspzIbYPfP8wQ3MyQPyh+TIYtJnlIIR4VQiRI4TYGrDuESHETiHEZiHER0KI+IBtdxoTjHYJIc4KetIOjFl8b9qgRMalxvnWzx1u5/azRgBQ0owJYc5qD+FhFp/l0NkG2UqXcqV1EbdSl+ZApnKT9O4bpIeDI9YfbwDf4B9eVQCb3lUpr3F9/dvDo5W1UZqtfPOeKjiSpgb7+AE1y3IA7PkWkkeooO+Yi9Sge95jcMnLYLXDK2cpxRAWoZ6Uq53KBTTiHGXR2GMYmLFIzcc4+U/qnCv+qQb/U+6Av2TA9d8pF81bF8GB1fDEeHj3cnX+gSfB1f+Dq/4LQ2ar/T6Yr2IjJ9ys5AqPhSOb4as7lduq8IBaP3EeVJXAzs99Xyeu2LAwjqT5v+PGt/2uqIYozYaPblJK+er/+dOHEwep994TITxOKQevR1ldKSPUPasuh8/+AE9MhEMb4MCP5Kac2Pg1G6At3UqvA3NqrfsWGCulHA/sBu4EMCYUXQGMMY551ph41GnobWR4XH/y4BrrU2MsnGO0IW1O7KG8yk2k3Yq9s1oOrq5lOXRljh5WsfWE5D6N7AlEqUSL+KItyl009pK6+8T0Um6OwDkP8f0gYWBNy8FVribUDTtDLUcmws1rYcTZav9LXlYZOgNPVgPxzs9hzbNq9va03yil1XMM9upiGHQqzPq7GljXv64shhm/hzC7GkCv/ADKcuC1OSoIfcZ9cMW7YI9ST9oxvaDfNKX89q+AIafB8DnK6ug5RinCNc/Cqqeg5BAkDFDunLj+kPYfJb+UxBUbAfLDaeq9LEe5un56ofF7u+ZZ5eKa+7q6vkmiMaakTlWuvqx16j56qpRiTVEPn2x8W617fz5I7zErhzZzK0kpVwghBtZa903A4hrgUuPzBShfbRWwXwixFzXxaDWdhBMGJ/HpzSfVsBpMYhzqNpuT4XJLq7jshdXYrILLj+vPdScNIqfESYzDRoRd6cSDBRVMGZCAzYw5dFLl4HJ3zjTcboGU4PVQlHdELUc1ocGP1QYRiSTlr1PLA2fU3Seml3q6BfV07nFBXD+oLFLKYNMiGDdXPd17XDD09ODXGjobrvlCuW/y98G6V2HpvdBzHAw6Re3Tezwy82fEWf+nlMXAk2HHYhh9gbJiTFKnKGXz/UNw4XPBXT5WG/x+k3JZhQVkbfUcCweNoWjPN+B1K8vBYoEJl8PKf0F5PlSX46jKh9hUZTVVFsHRzeq4nICsqq0fwtpXwFWmZPG6Yfc3at3oC5XrKxBzue8UFWNY8YhSEKAUQ8KggH2HQf4eSBpKedQAjoVQBqSvBb40PgebZNS3zhEdGItFBFUM4FcOpuXw/Pf7OJBfTmW1hzdXZwBw0bOruO8z9QNyVns4VFTJwKQon1up2t25UlkrA7riaeuhg7L1Q+Sjw7GVGf96kclNOy66J1ZvlSrGFzgwBWzHZWT3jDGC1fH9YezFEJUMH90APz4BW/4LEYnKtVMfA05UcY7+xnyEy9+BBZ/65yKccjtpEx9QpT3AP7N74ry65xp1Htz4Q8OxAFtETcUA6oldWJS1YqbwxhsD74AZKkU3e4tyWQFM/416P7pZuaPAn3JbUQAf/1YF7Euz4eUz4IVTYdkDSjnPrFO2SynCCVcqS2bIaep6a55R25KHq3sakQgpI+Fiw0IZfYH/HrWQkASkhRB/Q80WfacFxzY4ixTadiZpcykrK2P1DyuxWWDr7nQ+qjrIm6sqObFPGPHhbr7cX81n3yzjUFElH2/IZHZ8PtkVEinBmXuQHUXqPGvWriN/b11P295CD8mRgvjwxvV8e96XtUf9LrSly1cQZav5Q+0of6OOIkdIyNuNqMxnhmUrHosNa3hM046L7qHSOPtPDz4AmRlLoAa7sHDlsrFHwe83K9//mueUv37SL4NPiquNEDA6SFmq6B4Ux4/xL0+6Wrl8zPkBrcG4uUoJWO3w84tqXYKhHHoaSil7G+Ttxm2NImzClfDt3cq1ZGYtlecqF1PaO8pVdvnbYI9UmVR9p8Apt0NEfPDrRyTARc+pz6nHKevhyCaI7uU/5oKn1X3vOwV+tRj6TobVx5Zm2+7KQQixADgPmB2Q8tfkSUaNzSKFtp1J2lxMWeJ/XEJ8Sk8Oh0fglrv4vytPZnV6Hp+lb8HaeySwgUo3WPqMJtkj4Yf1nHvKVMqrPLB+DWPHT+SEITXNfrfHy40Lv+aK4/qz8KwxwQUIIkt7kLsuE9LUU9Nxx59Aj5iaJQo6yt+oo8gREpwlAEwSe/E4emBt6pOmkbFEv+ODbzeVQ3RP5S8//yn/NiHgxFvg7YvV8rjLWiB4A4TZ63dTtRSLVcVAQM35yNmhXEcA0SnKgsreDofWURw3kqToFOVGO7hGTU6LSITKAmVJrH1FKa6extyRa5vZPMliVRZE2jsqY8lk5Ln+z4NPbfl3DbxUq5yliQgh5gB3AOdLKQPr9y4GrhBChAshBgHDgJ/bU7a2JsYRRomzmsNFlSRG2umfFEk/o6T3ij15gGoJ+tmmI+zPKwdgYHIU9jD1Dxss5rA/rxxntZfMgo5TCvm7ndk8sWSPr+Q4dI25Dl2SKqUcwoQXS3QTXUrgT1ftX59yMCbCBXM5gXKN9Byrtveb1vTrdgTGXw6DTlZKyKTnaJWCm7uT4jhj0B97Mez6QsVKxhmh1WX/p7Kgpt94bDKMOFu9J484tvM0Qlumsr6LCiiPEEJkCSGuA54GYoBvjdr2zwNIKbcB76Nmjn4F/K5WbZpOT6zDRkllNfllLpKi1Q/LrNpqltw4Z1xvvtmezfbDJSRH24l12LBblSupOojffufRUgAOFVW2x1doEovTDvPKD+m+gDTomEOHxVns+xgWndL04/pNpyxqEPQaH3y7qTwS61EOQsCV78PVHx2zX7zdOekPML9WOY2eY9XcB/ArhxNvVZPkkEoZRvVQ8zx6T1BpuMfCkNNUfCEwo6kNaMtspSARIV5pYP8HgQfbSp5QExuhlIOz2kNilFIOveMchFkEWYWV9IgJ51cnDOSzzUf4fMsRJvePB8AWVn/5jF2GcjjcgZRDWZWbEqe7xmxwPdehg2JYDoAKajaV0eezLieWmYFPz4E0ZjlAzbkRnR2zvIjVTmnMUPU5KlkFpn98Qs1P6Dka0nNg9j015420BHsU/O6nYztHE9DlM9oJ062kLAeVDRFmtdA3Qc2sHJgUxXEDExjeMxqPVzLQaDFqbyCV1bQcag/GocTMyDpS5PSt05ZDB8VZgsccApqaqdQUEgaoWMKo81rvnB0Zc7Ja3yl4rQEKc9bf4IYVENsbJs+H43/b5k/7rYlWDu1ErMNGqdNNXlkVyVH+H5DZSnRAUiRCCK4+XmVBDEpRysHWQG2lnUdLfFVbjxQ762xvLpkFFfz94y3HNOGurEoph0BXl7Nr9HTocsiqErZ4jaf75lgOjWG1wSUv+QfNrk7KSDXpzkyjNbHa/GmzYy+GOf+vU7nRtHJoJ2IdYRRVuChxun2WA+ALSpstRS+anMqZo3ty2kiVEWIO/rUth7IqN1mFlZxoZDC1Rtxh8abDvL3moM9d1RKCKYemWg6VLg8LF2/znUPTtngri9nqHcj60XfChCsaP0ATHJsDfrsaZvwh1JK0Klo5tBOxEf6ObmZAGgItB2UpRIeH8eKvpjKyVyzgtxxqB6TNAdxUIq0Rd9hxRPmgMwsqyClxkpZZ1OxzlBlupaMBlkxTs5XSMot4fVUG6zIKmn1dTfMRVaWUEknVlF+r9p+alpMwQCmJLoRWDu2EOUsaICnKbzmYfR+G9ww+Aclej+WwL1fNQD1xaDJWi2iScnB7ZYMuIzOGcbCggn99s5vr31zX6DlrU2o89bu9kphw9Z2bGpA2W6M6depr21PtxOJ1USojfQ8mGk0gWjm0E7EO/yzQ5ADLYdaIHnxx68mM6BVcOfgsh1qd4LIKK7EIlQ7bK9bB4aLGYw7Pb6riT+9vCrrNWe0h3VA4Bwsq2HG0pNlB7iq3p0ZsJD7KZqxv2mBvWhg6RtEOGJlKFZYoesV2rSdeTeuglUM7ERsRYDkExByEEIzuE1vvcWYnuNoDbFZhBb1iHdjDLPSNj2hSzCGz1MvenLKg2/Zkl+E19M+B/Ar2ZJfhrPbi9Ta9plN5Vc1BPcEoY95k5eCzHNpHOZQ4qzlQ4qHC1Q1jHMbsaEd0AlZL5wmSatoPrRzaiZgAyyExqp788CAIIbBbLZRXuXl5ZTozH1nGhoOFZBVUkmrEK/rEOzhS3LhyKK6SFFa4gm7bcVQNFmP6xLL+QKFvhnNz0lBrWxpmj4uqJg725rWCKQdntYd/fbOrRkG/Y2X9gULuWeU8pgB8p6VKTYCLiUsIsSCajopWDu2E6VayWQWxjubNPbSHWXjlh/088PkOMvIrWLYzh6zCClKNORJ94iM4UuTE3UA8obzKjdMDBeUuZJDWgTuPlBJhs3LSsOQapS+a8xRfu19FQmQz3Uqmcgiy/49783jqu72sTs9rsjyNXs/4bg5bp2od0iq4K5RySEhsxRRWTZdCK4d2wgxIJ0WFN7u3wfGDkzh5WDJvXzedYT2i2ZRVzNESJ6lG+Y1+iZG4vZLs0qp6z5FrbKtye2sM/qCe+L/fncPwXjEMqhWcrL1vQ5gpqObEvbiI5sYc1LWCWQfmPI6ckvq/Y3MxA9/dUTnk5yslm5yslYMmOLqHdDsRawyUgWmsTeXl+VN9n0f3ieXLrUfxSnyWg/meWVBB3/iIoOfILfMPqgXlLiLt6k/vcnv51as/cyC/gheunkJErYGyWcrBsBxSEyJIzysn0h5GeJilydlKfsuh7v5mamxOAwqwuZhWkTmXpDuRn59LT6B3j56hFkXTQel+/xUhIspuxSJqBqNbwqjesb6MILNwn/meVVh/3CE3YFAtLPfHBn7eX8DGg0X838XjmD2qp29SnhmkbI5bybQczHNE2q1KOQSkppYZsZMqT13Xlmk5BJsXcdiIqeS2gXLojpZDcWE+AP1692pkT013RSuHdkIIQYzDRlIzgtHBGNXbn9lkWgy94x0IQYOlu3NK/KmuBQFB6bRM1ef3rDFqkOgd5yA6PIyxRgZVs2IOPuWg5Iq0Wwm3WX0WQbXHy2/f2cADn+9gR37d8zYUkPZbDsdeJqT29Ry2xv8NhBBzhBC7hBB7hRB12nUJIR4zKg2nCSF2CyGKArbNF0LsMV7zW+0LHAPlJWqiYXxCE1qDarol2q3Ujtx+1ghG1jOfoamMNpSD1SLoHafy08PDrPSKdZBZWFc5lFW5KShz1XArFZb7lcPGg0UMSYnyxQfCrBY+u+UkMvLLWfDaWipd/qd4KSXXvbGOE4ck8euTB9e9luFWMmd9R5iWg+Emem75PlbsVuXJS111LQdTKQRzZbWNW8lQDmENWw5CCCvwDHAGqoXtWiHEYinldnMfKeVtAfvfAkwyPicC9wBTAQmsN44tbLUv0gKqyouoFBFEWLqf1aRpGtpyaEd+efwApg5MPKZzpMSEkxwdrsp9W/1/vn4JkUHdSo9+vYvznlrJ4SIndmMcKDCUg5SStMwiJvarmc44MDnKl27rrJG55OW7nTk88PkO/t+XO+rMgSirqjaUVoDlEGbxPaFvPVRMH0OhlQWZX1ef5SCl9LmVGgtIlzirWX+gbvmNt9YcYNW+mplOTreHMKH6fzfCNGCvlDJdSukCFgEXNLD/POBd4/NZwLdSygJDIXwLzGnsgm2NdBZTZY0OtRiaDoy2HDohpwxLprrWwJyaEMGa9Hwy8soRwl+r6ce9eZQ43azYnUvvKAuZpV7fXIeswkryy11MMnpHBGIGpgOf4osq1XGDkqN44ft0DuRV8MS8iYQbT96lTjcxjjCf6yzCFkZ4mNUXQ8gvdzEwOYrs0irKq5X8UkruWbyNuVP6BSiHmjGH4spqnNVewsMs5JZVIaWsN+Pr7TUH+OdXu/j4dzOY2E99r4P5Fdz9yVam9E/gxJv82TnOag9NDDf0BTIDlrOA6cF2FEIMAAYB3zVwbNBmBo31R2+tftdSSsKqS6mwh5PWwvN1pN7bWpbgHKssWjl0Qv512YQ6g2NqYiRH0g5x1cs/kRITzse/m0FBuYs9xozo/HIXqSlWyjx2n+Ww0SisZw6igTiCKAczkH3HWSPILKzg/77YySdph7lsquqvW+Z0Ex0expg+cZwyPIWJ/eIJt/ndSvllVYxPjSc+wkaZoQAKyl28ufoASVHh9c6QNtNYx/SJZcPBIkoq3cRFBm9KfzBfudYe+nIH715/PEII3lidgZSw4WAhRRUu3+Q8Z7UXW+vPDr4C+G9LOhk21h+9tfpdlzir2fxdBZbIxBafryP13tayBOdYZdFupU5IsKfmfgkRSKlKZW87XEyV28Nao7qpb95BuCAhyu6zHDYeLMRhswSt62Qqh6oglkN8pJ3rTx5Mr1gHy3fl+LaXVinlEBdp481rp9ErzlHDrWS2SI2PtFFmxBzyytQ5K1zuemsrmbO/x6fGAw0HpQ8XOxEC1qQX8OPefMqq3Ly/NpNhPaLxSn+/bvO72ZtmORwC+gUspxrrgnEFfpdSc49tF/JKq4gRleCov2yLRqOVQxfBnBBnD7NQ7ZHsPlrGz/sLsIdZOG+CatsYFy5IjPRbDhsOFjE+Nd5X3C+QCHtdy6G4QlkO8ZE2hBDMGpnCyt15vkqvZYZbKZDwMJWt5Kz2UFrlJjk6nIRIu8+tlGcEystd7nqrspqWw4R+cUDDQenDRZWcPEz1Q958qIi1GQWUVrm5+xejSYi0sWR7NlsPFeNye3G6Pdib9h+wFhgmhBgkhLCjFMDi2jsJIUYCCaje6SZfA2cKIRKEEAnAmca6kJFX5iKRUiwRunSGpn60cugijOodQ//ESO49X3Xf2pRVxM/7C5jUL56Thio/e5xdkBBlo7Bc9bLefriYyf2DDxAOY2JYYLZSUaVSDmZBvZkjelBa5WZdhkq8KTMsh0DUPAePTyElRdmJj7T7AtKmcqhwefyWQ61JcEeLnVgEjO2jlEN9cx2klBwuqmRoSjThYRaKK6opMqykfgmRnDI8hcWbDnPeUz/wSdohqqq9vsKGDSGldAM3owb1HcD7UsptQoj7hBDnB+x6BbBIBtQnkVIWAPejFMxa4D5jXcgoKsynnyUXUkaEUgxNB0fHHLoI8ZF2VtwxCykl//xqJ5+kHWLLoWL+fOZwThqWTGpCBIPjJR6PnQ0Hi9h6qJhqj2RykGA0qJRWm1XUGKhNd1S84e8/aWgyNqtg2a4cThiSRFmVm4HJNctvhNusuNxe8g33UVJ0OPGRNp/lYA70FVWeerOVDhc56RHjoJeR6WS6lTxeWaOiaEmlmwqXhz7xDuIibBRXVlMUYO38+qTBhFksfLghi9yyquZYDkgpvwC+qLXu7lrLC+s59lXg1aZdqe3xHt0CgL3fxNAKounQaMuhiyGEYFxqPGszVDxh3rT+9Ihx8MNfTmNwnJWESDuF5S7WH1BP+5MH1O9acNisNeocFVdUEx5m8cUjosLDmNgvno0H1blKnfVYDm4veeVKCSRF20kIEnMod7l9AenatZWOFFfSJ15NznPYLOSUVFFe5ea4B5fw6abDvv3MsuV94iOIj7RRVKGUgxCqKu641DgenTseq0WoQoTV3qZmK3Up7LlbAYjuPynEkmg6Mlo5dEHG91XulyuO61+nXEdilB23V7J8Vy4DkiJJbqCcR4TNWuMpvqii2mc1mAxJiWZ/Xjmg5jnUjTmobCWf5WC4lVxeZSHUcCvVU5X1cFElveMjEELQK9ZhBN1LamRjgT9w3Sc+gvgIO0WVLoorq4l12HwWhhCCSLuV8iqPSmXthr0MYot2UEAsYfFBM2o1GkArhy7JaaN6MDglit+cUncWc0+j69fq9Px64w0mDpu1ZiprhcsXbzAZlBxFXpmLzIIKnNXeOr0qzHkOBT7LIdx3jsIKVw3lYCoil9vfZEhNgHP6CgqO7RvHpkzlFgN8MQXw99HuE+cgzmc5uGootE8//ZQom8WwHJqcrdSlSCnfTXrYEGhmdWBN96LNlIMQ4lUhRI4QYmvAukQhxLdGjZlvjewNhOJJo27NZiHE5LaSqzswuX8C3/1pJn2CVGg9e2wvnv/lZH590iCuO2lQg+epYzlUVvvKbJgMTlGzbD/drNw7o3vXTI9U8xxUzCE8zEKU3err81BUUR2gHNw1Snv70l/LXbjcXt/M6sn9Ezhc7GTJjmzfOUwOFTmxWQXJ0eH+mENlNfEBMr/33ntsemwBS974NwWHMrB3N8vB7aKPK4MjjqGhlkTTwWlLy+F16pYJ+CuwVEo5DFhqLAOcDQwzXr8BnmtDubo1YVYLc8b25u/njWas4X6qD4fdSmVAWmlxELfS4BQVgF6cppTDmFotT8PDLLg8XnJLq0iOVr0szAlshRUu8kqNmEOVylYy52R8vzuXq1/5iQP5ymVlKropRoxk1T5VVdTMoALlVuodF4HFIog3lENhRTVxAdbO22+/zay/vIojsTd7PniYbx/7Ay+++CKlpd2kG1zeLmy4yY8dGWpJNB2cNlMOUsoVQO2UvQuAN4zPbwAXBqx/UyrWAPFCiN5tJZumaTjCLDhdNSfBxUfUdBv1S4jEahHsPFpKnzhHnRiHWVrjcHGlr5eFz61UXk1+eaDl4PH1vfhuZzYr9+SxdIeaZGcqh1G9Y2v0X6jtVjKLEcZH2qhwecgtcdawHADi4uJIGX8qMaNPpbKkgI8++ojJkyfz1FNPteQ2dS6KVCUPd9zA0Mqh6fC0dyprTynlEePzUcDsNFJf/Zkj1KKx+jPQteqbtCbNlaWy1EmpS/LqJ0uxWQT5ZVWU5h9l+fKaOj/ZAdkV0Cu8us75MzPUk/3uw4WkxlhYvnw5hU5ljSxbt4Vqj8RhVTEHALtUg/3W/coS+WhtOgD7t20gb49yAQ2Igd2FEBEGR/JLfNfcd7SCkYlWli9fTk6Wuu6RYiflhTm+fX788Ue+X/Q5xbmHcYw+jQv+/C9+fUIqTqeTBQsWMG7cuCbfn85IVUk24UB4nO7joGmYkM1zkFJKIUTdus2NH9dg/RnoWvVNWpPmyvJe1nr25pTx4QELzmoPbi+MGzGEmTOH1NhvTMZasnfmMHPCEGbOHFZjW0JmEe/u+pECp+S0Mb2ZOXMCzmoPty3/Cm90D+AQg3rEsuNICQB9kuM5XF5IOQ6ggiPlEofNwnlnzPSVDVlTuZPd3+/j5OE9WZ2ez8yZM1Vp8q++Zsa4IcycOZSSTYd5c/tGJDBm6EBmzlQTvl577TVOvPhajkYOJquwkpQUm++evPPOOx3mb9VWVBYcIRyITNDKQdMw7Z2tlG26i4x3szBPh6s/o/FnKx0uqmRfrvL91445AAw2Jr6N7Vu3Vs+EfvHMP2Eg4G+R6rBZsVthn5GGOsDo/wD+vtNm5hEol1JgPanrThrEE1dMZHSfWEqdbtwer+9cQ3uoAHmgKykw5rBw4UIGj57om7GNp4qMjAwAZs+e3dgt6fQ4i45SIiNIjNd1lTQN097KYTFgdsKaD3wSsP5XRtbS8UBxgPtJEyIcNivFFSqoa1Lbfw9KAUTYrL7CeLW5Y84IZo/swclDU3zrom3CN0dhQFJd5VAd0Ea0T1zNrKuUmHAumNjXJ0uJ083e2sohQIkFyjx37lyiwsN8bixHmJW5c+fWdwu6HPk5h8gnjgn1/K00GpM2cysJId4FZgLJQogsVDesh4D3hRDXAQeAy4zdvwDOAfYCFcA1bSWXpulE2Ky+1p8m8bXmOQCcN743p45IIdYRvIx2pD2MVxYcV2NdvxgLm3I9WC2CQQElNwJLcfeOc3Ck2EmfeEfQ88YHzJfYm1uGzSp8Vkhg4DxQUbjdbuKi/crIEW7D5fIHtbsybo+XysIj2B3JdeajaDS1aTPlIKWcV8+mOra7Uajsd20li6ZlBPZWToi0URgklRXUrOP6FEN9/H5yOP3HHIez2kN+QNvSwEF95ogevPvzwaDzNcA/6BdVVLM3p4yBSVG+7niB8zECZU5JSWHP2uWY/Xb2bFhNcrK/AVBX5oe9efTxFBGTPDrUomg6AbrwnqZeIgIKD112XD9e/WG/b4b1sWIRwucCMvtOAMRF+H+SE1LjmDJgAicPCz54m5ZDUYWLvTllNfpzxzjCEAKkhLgAhfP8889z1gWXknXwECD5oVcPlnzxSe1Td0kWbzrMPywlxPZMDbUomk6AVg6aeokIqC3xu1lDuebEQW3ijogMuE6gWykpOpwzRvcMdgjgjyXklFZxIL+cX4z3T42xWARxEbY69aCGDBnCv97+jFveVC0X/nhCPEOHdv3ZwlJK1uzJJoFSiKn/nmo0Jk1SDkKIKKBSSukVQgwHRgJfSimDtInXdBXCDcshOjyMWIet2a6jphJl9/8MA91KydENKyJzMl3awSK8EoYYloj/XEo51C75sWX1Mko3LkW6q/n2cBhFO1Zx9901qm93OfbnlVNdmgsOIKp7uNE0x0ZTs5VWAA4hRF/gG+BqVHkMTRfGdCv1jK2/cmtrUJ/l0FDFWPC7jpYZrUrH9KlZDiQuwkZ0eFiNTnc33ngjP37zKaXrPwMp2bj6ew4cONAaX6NDszo9nxShihUS3SO0wmg6BU1VDkJKWQFcDDwrpZwLjGk7sTQdAVM5mE122orIgB4QscaAD/55EfVhuo5ySqvoE+dgSErNRkNxkfY6VsOqVat46MkXsDiiiT/pSu7+51Ps3r27db5IB2b1vnyGRhlzR6JSGt5Zo6EZykEIcQJwFfC5sa4bFjvuXpjZSr1ig2cLtRaBge/wMCvhYRYi7VYi7Y17PU3X0inDU2pMlAM4fVQPzh1fs0SXw+EgKjwMERaOuzSfiPAwjhzp2lNqpJSsSS9gWopRJytKWw6axmlqQPoPwJ3AR0bv3MHAsjaTStMh8FsObetWsloEDpsFZ7WXcJuFCJuVaEfTfpqmZXDK8LpPw78yZmYH8otf/AJ3ZSmx0y/myBu/59Z3Ldxyc9fOoj5c7CSvrIqRQ6pUtbJobTloGqdJ/4FSyu+B7wGEEBYgT0p5a1sKpgk9DiMW0KuV0lcbIsoehrPaRXiYFYfN2mi8wSQ+0oZFwIwhjQdZvV4vs2fPpk/PZKJGzCByyDQenmHliot+cazid2h2H1XlyPvaSsFqh3BdOkPTOE1yKwkh/iOEiDWylrYC24UQt7etaJpQ0ycuArvVwphG+j60BpHhShE5bBYi7E1XDqeN7MGV0/vXCGTXh8Vi4Xe/+x2RhkUkwmwkxEQ3clTnZ3e2Ug7J1UeVS0l3gNM0gaa6lUZLKUuEEFcBX6Ka9KwHHmkzyTQhp1ecg233nVUj26etMNNZ7VYLC38xpsnzKYK5jhpi9uzZfPLxR4SHRVDlltiaGDkTQswBnkDF2l6WUj4UZJ/LgIWABDZJKa801nuALcZuB6WU5zdL6GNkV3YpE2OKse35AqYsaM9LazoxTVUONiGEDdWc52kpZXVLym1rOh/toRhATbgLD7MghAgaP2gtXnjhBf7973+DxQpWG+c9CWFhYZSUlNR7jBDCCjwDnIHqNbJWCLFYSrk9YJ9hqLjcDClloRAiMOpbKaWc2CZfqAnszi7lT/bF4LTASX8MlRiaTkZTlcMLQAawCVghhBgA1P/fpNE0kyh7WI0Ob22F2Q70lH8uo7DcxVOzwpvSw2EasFdKmQ4ghFiE6l64PWCf64FnpJSFAFLKnDpnCQEer+RgTgGnhH0DU+ZDXN9Qi6TpJDQ1IP0k8GTAqgNCiFltI5KmOxJpV4HotmbFihUAODO34qqsZtMmKxaLhVNOOaWhw4J1Kpxea5/hAEKIH1Gup4VSyq+MbQ4hxDrADTwkpfz4mL9IE8ksqCCyuhir1QO9xrfXZTVdgKaWz4hDldw2/4O+B+4DittILk03IyUmvF3KSD/yiAqTZWYUUums5G+LdjFt2jS+++67Yz11GDAMVaY+FWVhj5NSFgEDpJSHjBTw74QQW6SU+2qfoLEWuC1pObsh202cUI2atqUfJresecfXR2duf9uWdCVZmupWehWVpWT2X7gaeA01Y1qjOWbuOGsk5S534zseI59++ikAv3r1Zw4XVfKb1Gzee++9xg5rSqfCLOAno97YfiHEbpSyWCulPAQgpUwXQiwHJgF1lENjLXBb0nJ263d7iCMNgDFTToQhzTu+Pjpz+9u2pCvJ0lTlMERKeUnA8r1CiLQWX1WjqUVcpK1J6aitxZwxvcgucZIS5mXHjh2N7b4WGCaEGIRSClcAV9ba52NgHvCaECIZ5WZKF0IkABVSyipj/Qzgn634VRpkx5FShsa4wQVExLfXZTVdgKYqh0ohxElSyh8AhBAzgMpGjtFoOhy33HKLr8yG1+vl1e+/Z/LkyQ0eI6V0CyFuBr5GxRNeNSoF3Aesk1IuNradKYTYDniA26WU+UKIE4EXhBBe1LyihwKznNqaHUdL+FWcG3IBR3x7XVbTBWiqcrgReNOIPQAU4u8FrdF0GqZOner7HBYWxogRI7jlllsaPU5K+QWqnW3gursDPkvgj8YrcJ9VwLhjk7plVLo8ZOSVM3CEoRy05aBpBk3NVtoETBBCxBrLJUKIPwCb21A2jabVufTSS3E4HFitKjNq6dKlVFRUEBkZ2ciRnY/d2aV4JaRGVAECwtt+prum69CsxHIpZYmU0pzfoGfTaDods2fPprLS7xF1uVycfvrpIZSo7dh5VP2r9girBEcsWNpnQqOma3AsvxZdoEXT6XA6nURH++spRUREUFFREUKJ2o4dR0qJtFuJkWU63qBpNseiHHT5DE2nIyoqig0bNviWd+3aRURE2/arCBU7j5YwolcMwlmk4w2aZtNgzEEIUUpwJSCArvkfpenSPP7448ydO5c+ffogpWT//v0sXrw41GK1CfvzyjllWAoUF0FEQqjF0XQyGlQOUsqYtrioEOI24NcoxbMFuAboDSwCklAVX6+WUrra4vqa7stxxx3Hzp072bVrFwBHjx5lypQpIZaq9XG5veSUVtE3IQKOFkGsrqmkaR7tHqESQvQFbgWmSinHovLGrwAeBh6TUg5Fpcpe196yabo+zzzzDOXl5YwdO5axY8dSWVnJs88+G2qxWp2jxU6khD7xEaDdSpoWEKr0hTAgQggRBkSimheeBvzX2P4Gqjy4RtOqvPTSS8THx/uWY2JieOmll0InUBuRVaSC7KlxDqgs0gFpTbNp6iS4VsMoQPYocBA1y/oblBupSEppFtfJQlXCrENjxcmgaxW/ak20LFBSUsKyZct8s6SLi4spLCzsMPeltThc5ASgbzTgrdaWg6bZtLtyMGrNXAAMAoqAD4A5TT2+seJk0LWKX7UmWha45JJLeO6557jhhhsAeOyxx7j00ks7zH1pLQ4VqrkcvcKNOR3actA0k3ZXDsDpwH4pZS6AEOJ/qGJk8UKIMMN6CFb1UqM5Zh5++GFefPFFnn/+eQAGDx5cY1JcV+FwUSU9YsIJr1bNjbTloGkuoYg5HASOF0JECmXbz0Z11FoGXGrsMx/4JASyabo4FouF6dOnM3DgQH7++Wc2btzIqFGjQi1Wq3OoqFIFoyuL1AptOWiaSShiDj8JIf4LbEB1xtqIchN9DiwSQjxgrHulvWXTdF12797Nu+++y7vvvktycjKXX345oNxKXc2lBEo5jO4TC84jaoW2HDTNJBRuJaSU96A6ywWSjurVq9G0OiNHjuTkk0/ms88+Y+jQoYBSDF0RKSWHiio5Y3RPbTloWoyuxKXpFvzvf/+jd+/ezJo1i+uvv56lS5eiqmx3PfLKXLjcXvrGR0DGShAWiEoOtViaTkZILAeNpr258MILufDCCykvL+eTTz7h8ccfJycnh8ceewyXy8WZZ54ZahFbjUNFKsA+sWwFbHoXTv4zhLdJsQNNF0ZbDppuRVRUFFdeeSWffvopWVlZDB06lIcffjjUYrUqu4+WEkMFYzcuhD6TYeZfQy2SphOilYOm25KQkMAvfvELli5dGmpRWpXtR0q4LfwTLJUFcN5jYG2/3tyaroNWDhpNF+No5j6uFl8iJl4FfSaGWhxNJ0UrB03HYMdnsPLfoZai0+P1Sqw5W7DhhqnXhFocTSdGKwdNx2Db/2Ddq6GWotOTWVhBgjtXLcSlhlYYTadGKwdNx8BdpV7B8LjBU92+8nRSth8uoacoRAorRKWEWhxNJ0YrB03HwO0ETz29nZbcA29e0L7y1EIIMUcIsUsIsVcIETT9RwhxmRBiuxBimxDiPwHr5wsh9hiv+W0p5/YjJfQRBRDTCyzWtryUpouj5zl0Z6qd4CrrGBOkqp31Wwd5e6DwQPvKE4AQwgo8A5yBKie/VgixWEq5PWCfYcCdwAwpZaEQooexPhFVDWAqqvPheuPYwraQNS2ziNPCixG685vmGNGWQ3dm5aPw8uxQS6FwO8FTj1upqkRtDx3TgL1SynSjde0iVNn5QK4HnjEHfSlljrH+LOBbKWWBse1bmlGivjm4PV7WHygk1VoIsb3b4hKaboS2HLozBfuh5HCopVC4q8DrBq8XLLWeWZzF9ccj2oe+QGbAchYwvdY+wwGEED+iWt8ulFJ+Vc+xLWpk1ViDpPRiDxUuN3HWHDJLvOxrwwZGunFUcLqSLFo5dGcqC5Sf3+MGa4h/CqZl4K0GS3jNbc6QWw5NIQwYBsxE9SNZIYQY15wTNNbIqrEGSXtWpBPDeuzSSb/R0+l3Yv37Hiu6cVRwupIs2q3UnakoUO/V5aGVA/yWQTALoapEKQ2vp3WvWbCf/gf+C8WN9pU6BPQLWA7WjCoLWCylrJZS7gd2o5RFU45tFX7an8/UBKNxUYx2K2mODa0cujOVhnJwVYRWDgC3MajVDkp7vVBldDNrbddS7k4G738LyrIb23MtMEwIMUgIYQeuABbX2udjlNWAECIZ5WZKB74GzhRCJBgtcs801rUqXq9kbUYhJ/Uw7pEOSGuOEe1W6s5UGAkz1R1BORiDWu10VlcpKskH5VqyR7beNV2GxWSPbnA3KaVbCHEzalC3Aq9KKbcJIe4D1kkpF+NXAtsBD3C7lDIfQAhxP0rBANwnpSxovS+hKK6spriymuGRZWpFbJ/WvoSmm6GVQ3fF7TIGXvyDZCgxYwq1M5acxQH7tLLl4FMOUY3uKqX8Avii1rq7Az5L4I/Gq/axrwJtOv27oEIp1WSZp1Zot5LmGNFupe5KZUCafagtB6/XbzHUdis5S/yfWzso3Qzl0NEpKFf3L96dq2ZGh9lDLJGms6OVQ3elMsCzEWrlEGgt1LYOqkrq33asdCHlkF+mlENM5SGIHxBiaTRdAa0cuisVAcoh1AHp6kr/59oxhza1HMrwCluX6HdQaLiVHKUHIHFQiKXRdAW0cuiutLblICU8fzJs/bD5xwZaBHXcSm0bc/BYHa17zhBRUO7CTjXW0kOQODjU4mi6AFo5dFdqWA6tEJB2O+HoZji0oWXHmtQOSFe1bczBY41o3XOGiPwyF8PsBQjp1cpB0ypo5dBdaW3LwXQNBT7pN5UalkNtt1JbWg5lXcZyKKxwMdphZCpp5aBpBUKiHIQQ8UKI/wohdgohdgghThBCJAohvjXKGn9rTBjStBUVBWAxMplbI+ZgPtW3SDkEWg613EptbjmEN75fJyC/3MVwm9HkRysHTSsQKsvhCeArKeVIYAKwA/grsFRKOQxYaixr2orKAohMBout+eUz9q+AVU/XXNdalkNt66DNYw5dw61UWO5ioCUH7DEQmRRqcTRdgHZXDkKIOOAU4BUAKaVLSlmEKoH8hrHbG8CF7S1bt6KiECIT1Yzj5loOm9+DFY/UXHdMlkNgtlKQeQ7m032g5eB2wRMTYcenzb+eSRcLSKfKIypTSYhQi6PpAoTCchgE5AKvCSE2CiFeFkJEAT2llEeMfY4CPUMgW/ehsgAiEsEW1XzLodpZN4hdfSzKoYGYQ1UJRPcw9gtQDs4iKNwPu79q/vVMulDMIb+8ih7Vh7VLSdNqhKJ8RhgwGbhFSvmTEOIJarmQpJRSCCGDHdxYzXvoWjXVW5NAWY7Ly6Qish9RbijLymB7M2QcczSLFG8133/3LdKi5gjEFW1jElBdmsePTThXoCzJuesZa6zftWMLR4r9x0/KyURIB7HA3l3byapQ2xyV2RwPlO3+kXUtvL8nlhXijBvUYf4+LaXS5cFV7SbedRgSLw21OJouQiiUQxaQJaX8yVj+L0o5ZAshekspjwghegM5wQ5urOY9dK2a6q1JDVnWVhE1YAQcKicyNpoezZEx6ynIg1OPn6JcUwB73ZAGNk8FM089tVHXRg1ZNufANvVxxJBBjJgeIMtWIHEIlO5h6IC+DD3F2Ja9HX6C6MpMZs44HmwtsAB+rMbiiOkwf5+Wkl9eRTxlWKVH11TStBrt7laSUh4FMoUQI4xVs4HtqBLIZvP1+cAn7S1bt0FKqCyCiARVOqK58xxMN5BZShv8biXpacH5AmMJQeY5mD2uA7eZAXCvG7K3NX6N7O1QetS/7PVCddeIORSUu0gQxt8iIjG0wmi6DKGqynoL8I5RGz8duAalqN4XQlwHHAAuC5FsXZ/qStU8xxEHtsjmxwnMgTlQCdSIBxRDeMNlsH37HVzjVywQvHyGIx7CHDWvETg348hGSJ3S8LUWzYM+k2Du68b3d6vLdRHlEIfxt4jUGeCa1iEkykFKmQZMDbKpg3S77+KYcwfCY1W2UumRmtvXvQoDZkDKiLrHgv8JPlA5BNZHchZDXBOazaT9B776K8z4vX9dYLaSx62C5eGxEBZey3IIUA6H0xq+jpSqV7a7CsrzVJbTaX9Xl+gCqaw1LQetHDStg54h3R0xi9mZlkPtQf6z22D9G8GPBX/qqSvArVTbcqgPrwfWvozwVvuVktmmU1hqls/wKbGY+i2H6F6qbEdDVJUoi6T0CKx/XcmduQboGpZDdkkVCcJo8qPdSppWQiuH7og5eIfHKuUQ+BRelKney3P9635+CTa+419u1HIogr1LlV+/Nllr4fM/kViwEcrz1bqSwyCsSpZAy8FlDHjh0UEsB+N6ycOgLGjugp/yPP/nVU+p99zdQNdQDum5ZfQNNxRnpFYOmtZBK4fuSJWhHByxRkA6UDkcVO8VAQPqhjdgdcCMaHNgrirzrwt8qt/zLbx9MexdUvfahtVidxX6r1F6WFkGVlvNmIN5fnt0/ZZDXGrNxkXBCFR0ziL1nr8X6BpupX25ZQyMrFIKNjw21OJoughaOXRmgj2ZN4XabqXqCuWXByg6oN4DB9TqSsjb7X9y91kOZTX3MTmwSr0XZtS9tnGM3VXov0bJYWUZWO01rQOf5RBT13IwFVpsH6U0Aq9fG/M6psvFFulzX3V2y0FKyb7ccvqGV6p4g54drWkltHLoyLirYMt//QN3IAfXwP/rqwbW5lI7II30D66m5RDoijGze3J3GnLVk61klrkw9yvJqntt4xi7q8h/DY/LsBzCg7uVgloOhgxmXn9D1oOpHEaeq4oNjr7Qt6mzK4e8MhfFldWkWCu0S0nTqmjl0JHZvhg+vA5ydtTdlrtTPfEf3lj/8Zvfh7w9ddf7LIdYVT4D/G6aQOVgKiVTCRzdojKIjDTQOpZDeIx6Ksc4Lpji8imHQqjI96+3NeBWCjeVQ61sJatd9UsGpRyyt0NJrcwr8Mc2zrgPrv8Oeo/3bersbqV9ueoexYsynamkaVW0cujIFOxT76afHPyupEpjXW3FcWSTUhhFB+F/18Oa5+qe11msMoPs0YblgF8BmMrBW+0PXJtP6Ue31nx6r2E5VIEtQs1JMDGzkAIxMpwczuyayiXMoQZ7TxUsvgV+fKKW5RBeN+Zgi/QPiJWFsOhK+PYfda9ZnqtcaJGJ0HsCxPrTbJtqOQgh5gghdgkh9goh6lQMFkIsEELkCiHSjNevA7Z5AtYvbtIFm4ipHKI8JTpTSdOqhGoSnKYpmD57cxCuKIDHxsK8d/0Dd+6umsd8/Te1buo1arkkyABdVaKe8oUwnvSpaTmYcYjyPLWfmV56dEvNAbqqFJbcq2YwuyvVAG8JUwHm+q5tfJeo8loup7BwkF7lVtr7nbIAhp2ptvlSWWtZDoHKoaIAirOUNVSb8ly/hQHNVg5CCCvwDHAGqvzLWiHEYinl9lq7vielvDnIKSqllBMbvVAL2JtTRoTNis1VrN1KmlZFWw4dGVM5mGUqirPUpLCc7X5rwvTvm1SVQnkOrHhULQcboJ0lEB6nPtsNt5KrQlkI5TlqJjGobCJfoFdA9paagV9XOWz9L+z6Us1ytjnUEzpAykjlVqodNDeUg0XWKs1tWg7uKqX4nEX+eRRBLYdKZamYyqFgn7J2CjLqxmjKc1XvChNzgp6w4LXY696fukwD9kop06WULmARqsR8yNmXW87glChEZYF2K2laFa0cOjK1LQczkFxR4Hcr5e1WE8tMzAFUelTjl2CunaoS/yBuWg7ZWyDjB/XZVA7luX5lkDJCDdpmNhMot09pthrI3ZUQFuE/75DT1GAdmPUU+F1M4vqpd1+2klMphcoiFXOwhKlttS0HV4VyiZkDYo6hJKuK6wany/P89ZlAWRGWMKV0mpbd0xfIDFjOMtbV5hIhxGajy2G/gPUOIcQ6IcQaIcSFTblgU9l1tIRRyTZlSWnloGlFtFupo1Jd6Z9BbPreTQuiIt/vVnI7lfUQ5oCkIWqQ6DFGDdJ9J6v5CeZTtomzxO9+MWMOnwaUsOhr1Ckqzw2YT9BPXSeweF1xlnI5VRarwLZpOVjDof8JsOZZZbnEBLTmCCzWB5A8HIozlWJB+IPYlYXqe5sDeH0xB3uU6maXGxB7KUiv6WKpyIP+x/uXLVaV5WQG1luHT4F3pZRVQogbUA2rTjO2DZBSHhJCDAa+E0JskVLuq32CxsrR1y7/XlTlJbukikQjsL8rK48j7VR+vKOWog81XUkWrRw6KoW1ntDBn2VUWaCe1h1xSkm8fp4aQO9IV+6dIbPhF49D2rtq/5LDSnGYOIvV5DFQg749Bkaeo57M05erukqgsnxMyyHaGOBNS0BY1SAMSpbIBCXPhMuhxyiI729c+5BSUr7vUstySBkB+5b6Yw7m+Z1FynIIj1HLdWIOlUqxCaGemI0ZzwAU7IdUo3SX16OUaWDMAVTcIXCiX8McAgItgVRjnQ8pZUDqFS8D/wzYdsh4TxdCLAcmAXWUQ2Pl6GuXf/9uZzawjvMmpUIWjJgwnRFjah7TVnTYUvQhpivJopVDRyVwApmZ0hnoVnIWQ+pxahZyZYFa7/XWtBJi+6j3kkM1lUNVMThGq8/RPeCugOCwlGrADY8zLIdy/37gH7wjk1R8wpTLVa4G+KGnq5c5h6F2OqurXCmasmz1xB8/QK0PcyhXmOkS8rrVPnajumuwmIM54Eck+GUBv9ICdT7prascRp4TPO01OGuBYUKIQSilcAVwZeAOZi8SY/F8VF90hBAJQIVhUSQDMwhQHMfClqwShIBhMUb8RmcraVoRrRw6KqZyEFb/07bpSqo0Yg6D+sPoCyA/XcUM3JXqZSoH0zqoHXdwltRfZsH0wUcl14w5mJaDWccoKrnmgFyW43dHgVIe1nDlegrEVa5aWZZlq3OYsYCwcBWjICCYXJzpj2GEOdR2r0e5harL/d/T9LXbY5S7rHC//xymMguMOYC/EmwTzG4ppVsIcTPwNWAFXpVSbhNC3Aesk1IuBm4VQpwPuIECYIFx+CjgBSGEFxXjeyhIllOL2HKomEHJUUS4jYcGna2kaUW0cuioFO5XT80RCUFiDobl4IiD8x5ThfG++LMa9L1uw3+Pf/awMVPZXlWgAshVpcFTPgOJSqmZreSzHAyLIDKp5v5VJWoANxFCWS61s6VcpZAyAq8IwxKZ7B/YbRF1ezkUZ/nTTsOM2dfuKuVOCrSQzHPE9FTfuSBAOZiNgGpbDs1ESvkF8EWtdXcHfL4TuDPIcauAccd08XrYeqiY4wcnQmWGWqED0ppWRGcrdVRyd0HCIKUgXLXcSqVHjWY98WrZHCTNGcfmsj1SuRoM186oHY/Bf+Yq9435RF4fUclKEZgB6Zhe6r08wHKoTWDQGyBhQM2BGpTlEB6Dyx6nzmEqGTNbKZDqCn/TIFPh/fwC7FvmD0iDf1CM7gUJA/1upYL98PmfIGWUPwbRRcgpdXK0xMnYvnE1XX0aTSuhlUNHpPgQ7F8BQ2erbJyqWgFpaaSu1k5H9SmHgCf42L4+t5LDeVTNoIbGq3fWditFGZZDWQMDUVitCWVJQ9X8Aylh20fKx+8qh/BosnvOglG/8LtCzHkOtbGbAWnDcliyEH563p/KCjUth+ThSoHl7ICPblTr5/2nruLq5Ow+qn4To/vEqr9JeGyX+46a0KLdSh2RjW8pBTBlARxJqzvPwSQiXr2bE9nM7BtTWYCa8FV8CKRUxe5MGnMrRSQY6aTGtcOjlRVjWg7mpDIzuAx1B6ekocr9lbsTPlgAJ/9JPfHbo9k/+GoGHDdTDfIWm1J0gSXATXyWQ4DiKTygLKdglsPEq2DFI/DOZVB8EH7xhIpxdDEOFKi/y8CkKPU3OUa3WUegurqarKwsnE5n4zsHEBcXx44dQeqPhYCOJsv+/ftJTU3FZrM1+3itHEKNlMovbwaPPW7Y8KZKR0003EpmELj2HAGf5WC6lYyspcCBNLaPquDqLMbqDfDphzfiVopIUPELM8Zgi1BppebcC9OtlDLSrxyCWQ6gKsuCvwigPQpMUeyRcN3X6ol/5b+MlQJfYDowW8nErDnliznEq/eYnhCVBCfdBkvvhR6jYdLVDX/PTsrB/ArsYRZ6xTqU5WDGhDoxWVlZxMTEMHDgQEQzSo+XlpYSExPThpI1nY4kS0lJCS6Xi6ysLAYNGtTs47VbKRR4PfDN31Udo/0rVL0kc+Dc/ZVSFmZtpMCYg7OkZqzAF3MwLQfTrRRgOcT3V3MGzAEV45+uMcvBPLcZULZF+uccgN+tlDLSv66O5WCkz279UL0HKodA+k5R5zbdSlHJKksLglsOZuA6mOUAcPxNMPGXcP7TKrOpC3Igv4J+CRFYLKLLWA5Op5OkpKRmKQZN/QghSEpKarYlZqKVQyjI36faVe74zOhIJlW9JIC1L0FsKgw/Wy2HR9ec55Aw0H8en1upgZiDWZ4ia516H3iScd7G3ErGuUuPqFITVluAchD+ATlhoH/grm05xPVXx5qppaaCstfzZGU1TF9HnF8J1o45BLqITOVgDoxmzSRbBFz4DKQGpNZ2MQ4UVDAgyVCyZTldwnIAtGJoZY7lfmrlEArKjBIU5Tl+t03BfvVknb4cpi4Aq+Hxs0fVjDkEKofa2UrlQWIO5iSzrLXq/dS/qFfy8IZlNAf/ksN+y8RUDrYIv+UR06uuHCbWMJVxZWI+8de2HHz7GwrAEedXTqblED9AbZ/2G//+5vUGngxXvOuf2d3FkVJyML+c/omR4HYpyzCqayiHUJKfn8/EiROZOHEivXr1om/fvr5ll8vV4LHr1q3j1ltvbfQaJ554YmuJ2+bomEMoKDV89GU5/gGucD9sfk+5UybP9+9rN0pmu10q5hCoHMyn/9pupcAneLOMRebP6r3XOBh0cuMy+txKh/0yBpay6D0R5jwEI86G7x9WCi8wLmCSNBTy96gAthkwt0dRY7KbielWCo/1V1Y1Yw7JQ+FvR5Qr7qu/BpwHsFjUjOduQn65i3KXRykHM401uvO7lUJNUlISaWlpACxcuJDo6Gj+/Oc/+7a73W7CwoIPmVOnTmXq1KmUlpYG3W6yatWqVpO3rdGWQ2uz6mnY913D+5iWQ1mOP9hcmAGHNqi6RIEuAnMALMtWZSAik40mPTEB1oXpVjIC0oGWQ3QPNZgXHcArbI3PbzDxNdEpCFAOhjIKcyhf/vE31WzwExYkldKMOwyfE/CdooNf0+dWivVfPzxgX4vVXxIEum3q5oF8NfdkQFJkwLwTbTm0BQsWLODGG29k+vTp3HHHHfz888+ccMIJTJo0iRNPPJFdu1Q/leXLl3PeeecBSrFce+21zJw5k8GDB/Pkk0/6zhcdHe3bf+bMmVx66aWMHDmSq666Cmk8EH3xxReMHDmSKVOmcOutt/rO296EzHIwGqisAw5JKc8z6tYsApKA9cDVRu38zsXyh9T8hCGn1b9PaYBbyXzqL9iv0jzNBjcm5uBoZgmFxxg1dAKevM1BOVjMQQgVd8jfQ1V4AhFN9UGabh3wKxufW8kRfN/a6wEGnAhp7ygLI+3tgO8U5AnLtDzCY1WsAurGJ8LCVYyhPLfbKoeDRhrrgKRIKDIth66lHO79dBvbD5c0viPg8XiwWhtPPBjdJ5Z7fjGm2bJkZWWxatUqrFYrJSUlrFy5krCwMJYsWcJdd93Fhx9+WOeYnTt3smzZMkpLSxkxYgQ33XRTnXTSjRs3sm3bNvr06cOMGTP48ccfmTp1KjfccAMrVqxg0KBBzJs3r9nythahtBx+j1GczOBh4DEp5VCgELguJFIdC85iVR7CtAaAvlmfw9L7au5XFuBWMp/8ig6oAa/X+Jr7mpaDmTXkiPVXQDWxWJSCCJatBD7XksvejPIK9mj/AB3MrRRIQ5bDyHPhjv01s5rqjTkYbiVHnP+c4UGsDLOkhq2e83RxDuRXIASkJgRaDtqt1FbMnTvXp3yKi4uZO3cuY8eO5bbbbmPbtm1Bjzn33HMJDw8nOTmZHj16kJ2dXWefadOmkZqaisViYeLEiWRkZLBz504GDx7sSz0NpXIIieUghEgFzgUeBP4oVEj9NPyVLt8AFgJBGiB3YMwKpKbbCOiRswIyMuCU2/2DrM9yyDVmDMf6J7j1rq0cjAHZrCAaHgep0+rWIbJHBo85QMuUgxBqgK7I87ut6lMODVkO5rliewfI2gTlYFoRwVxQcalqcmA3tRwy8srpHevAYbP6H0S6mOXQnCf8tp5bEBXl/73+4x//YNasWXz00UdkZGTUWxI7PNwff7NarbjddXuHNGWfUBIqt9LjwB2A+RdNAoqklObdqa/TVqMNUSB0DTcS8zcwHvAUHWalcf3pzlxwO9m8+FkKklRq5XE5+4kCNcmsqoSChIkkVqUBsHJPEZ79ftnjivYwCTi4/Sf6Axu276UkzvBBBnzH4z1WzKF5+Y9rQPiNwv4FbgYD5SKabc24L9NkOJFAXkklW5cvp/fhI4wAisqdpAWcZ+DRIgYCP23YTGVkfvCTATPCorC5y/l+9XrKKpx1/kZJebsYB+zJzEYKC0OFlR/XbcETVrP1wdBSSSrw49qNVNtr1W5qJh2pOUtT2Xa4hFG9jfhPea6yoOpTuJpWpbi4mL591dD0+uuvt/r5R4wYQXp6OhkZGQwcOJD33nuv1a/RVNpdOQghzgNypJTrhRAzm3t8Yw1RoB0abrirlMvFnGD180uq1MTAnrAFrF4nM0+YArZI5HLVn2B8ZA6YMq0urZG9kzh+DnyfBomDOfn0Wlk3RxIgDfrHWSETJp9wqgpa12ZrAlTlQJiDmbNqxTu25MH+t5BRPZt3X/b2gaxDJPdKVcdtyYPdEJ/cq+Z5HDvgwCKmzzjVP9M7GNv6Q0E6p552evC/0R43bIVhY6fA6Ash70pONluWBhKTAbkrmDFrTv3WShPpSM1ZmkKly8O+3DLOHmtM+CvL0ZlK7cgdd9zB/PnzeeCBBzj33HNb/fwRERE8++yzzJkzh6ioKI477rhWv0ZTCYXlMAM4XwhxDuAAYoEngHghRJhhPdTptNVhkBJenAVDZsFZD6q01G/+odwg02/w71eaDfYoBF5AwJ5v4eyHVSG7qmIVsDazmnqNU66a2vEG8LtVfG6leiavma6fYK4WY66Dyx7fvO/qK6dd261U6xojzjHagfahQWJ7++MtwQgLcCvZI/29rGsz6WoYce4xK4bOyM6jJXgljO5jxJzKc3SmUhuwcOHCoOtPOOEEdu/2dx184IEHAJg5cyYzZ86ktLS0zrFbt271fS4rK6uxv8nTTz/t+zxr1ix27tyJlJLf/e53TJ0amorC7R6QllLeKaVMlVIORHXU+k5KeRWwDLjU2G0+8El7y+ZDBsnBN8neBjnbIOMHtbzqSdVgx1nkn0sAKu5gBpGHzFKzgwvS/fGGQEUQ3RPOf0oVpquNqRxKjXhGfWUvzOBssKBw7wlw/G/JT2rmU0jtyW2+VNZa8xkSBsCZD6jAeEP0HKv2rY+koZA0DHo24m+2WLvt0/I2I4NnTB/jb9FF6ipp/Lz00ktMnDiRMWPGUFxczA033ND4QW1AR5rn8BdUcHovKgbxSkik2L8SHuoPh9OCb99l9HvJ2aHmFax9BZJHqHUHfvQHkEsDlIM5qW3vUv+Tc6+A/i/RKTD+srrBaAjIVjoCiPrnCJgDeDDLIcwOc/4frvBmNoPxBZprZSu1NBA8+2645sv6t8f2gVvW1Zzop6nBtkPFzHWsJfXw12pF2VF/lz5Nl+C2224jLS2N7du388477xAZGdn4QW1ASJWDlHK5lPI843O6lHKalHKolHKulLKqseNbHbcLPv+jyhxa9ZR//aZFKq6Qtc5QDkLNWt74lrIaTr9H7edxQZ+J6nNZtr8958CT1YC3d6nfckgeHlBoroGnYHuU/3rD5/jbeNbZrwG3UkvxuZVqlc8INhO6KVht3TbDqDWwuYr45Y4beYTHEJ/fpmJflYX+RkwaTSuiy2eYVBSoMs95uyH1ONj+MRTfp2YIf1TLrBtzMWz7n1IY1nBVXju+vyrt0HOMci+VHgXpxWOxY41MhKGnQ9q7MNCo/xPTW/mKKwsbzjQRQg3KYQ7leqoPcwBvzcG3jlupnpiDpl2otEYjXRYy4qYwsHS96msB2nLQtAkdya3UdhRmwO5vVE/ipffDB9fApvdU6WxQFsFTk2H966qw2yUvq1IVP78I2xertNDfroFZf1ctJ2f9TWUrFWdCv2kqMNrTcBPF9vU3wCk5TFV4shrgh54O1eXww2Mq4BqZpHzFwdpt1mbOQ3DV+w372c0BvPYchGOh3oB0Cy0HzTHhkVa2nPYG3gnGxKjDG9W7Vg6aNqBLWg4D978DO/6u3DyucijJqrlDVIp68t/6IQw7A5bcqwbp+Z9Br7FqnzEXKcsgKllV++wxSr1OvV1tTx6hAtNmCexeY2HX58pvHtNTWQ5uJ1XhSUSCci1ZbKrz2ZXvqeBt3yn+YnQNMemqxvfxuZVa0T9ZO+ZgtcHp9ypFp2l3wsMEN80cAulZ8AN+5RCjlYOm9emSloM7LBri+0HKCFWB9Iz74eqP4fSFcP138KfdcM6jsHcJfPFnSBkO13zhVwwAM+8Ct1OVtRh1ft2LmPsONCqcmmmXiYPqWg6gykDMfV1dZ/Cpat25j6p1rYHPrdQWlkOAG+mkP9S8T5r2xywfcniDeo/WMYfWYNasWXz99dc11j3++OPcdNNNQfefOXMm69apPinnnHMORUVFdfZZuHAhjz76aIPX/fjjj9m+fbtv+e6772bJkiXNlL716ZKWQ1a/CxgabGLTkFn+z9Ouh0GnKOsiMHPIJHkoTPolbHwbRgWpijhktoot9DUaygw7CxZ8Dn0mK+WwfwVUV1AVN91/TLDztBZtYTmkjIC+U+ufb9CNEELMQc3HsQIvSykfqrV9AfAI/vk5T0spXza2zQf+bqx/QEr5xjEJY5YiObIZELquUisxb948Fi1axFlnneVbt2jRIv75z382euwXX6gsxsZKdgfj448/5rzzzmP06NEA3HfffY0c0T50ScuhyaSMCK4YTM5+GH6zrGaZaJMJl8Pv0/xP6haLcjEJoYrMVZWA101FZNAqIK2PqRRaO+Zw/VJ/2e1uilFB+BngbGA0ME8IMTrIru9JKScaL1MxJAL3ANOBacA9Qohm5hTXwh6lkgXclUoxWLvkM167c+mll/L555/7GvtkZGRw+PBh3n33XaZOncqYMWO45557gh47cOBA8vKUi/jBBx9k+PDhnHTSSb6S3qDmLxx33HFMmDCBSy65hIqKClatWsXixYu5/fbbmThxIvv27WPBggX897+q7/rSpUuZNGkS48aN49prr6Wqqsp3vXvuuYfJkyczbtw4du7c2er3Q/+qGsIWoSaQNZdp16uy3VUl5OwsIEixi9bH1gaWg8ZkGrBXSpkOIIRYBFwAbG/wKMVZwLdSygLj2G+BOcC7xyRRbF818bKrxhu+/Csc3dKkXSM87qYpyF7j4OyH6t2cmJjItGnT+PLLL7ngggtYtGgRl112GXfddReJiYl4PB5mz57N5s2bGT8+yJwkVBnuRYsWkZaWhtvtZvLkyUyZorwLF198Mddffz0Af//733nllVe45ZZbOP/88znvvPO49NJLa5zL6XSyYMECli5dyvDhw/nVr37Fc889xx/+8AcAkpOT2bBhA88++yyPPvooL7/8chPuVtPRyqEtEML3tC13L2+fa/rcSt2vpEQ70BfIDFjOQlkCtblECHEKsBu4TUqZWc+xLSoqGVgkcJzbQRKQ77KzJQSFA9uiYGFcXJzPLRNe7cLiaWKVUgnuJuzrrXZR1Yjb58ILL+Stt97itNNO4z//+Q9PP/00b775Jq+//jput5ujR4+yfv16Bg0ahMfjoby8nNLSUqSUlJWV8eOPP3LOOefg8XgQQjBnzhyqqqooLS3l559/5v7776e4uJjy8nJmz55NaWkp1dXVVFZW+r67ubxhwwb69+9P7969KS0tZe7cubz00ktcd911SCk588wzKS0tZeTIkXzwwQd1XFoej4fS0lKczrpFLpuCVg5dBVsbTILTNIdPgXellFVCiBtQZecb6PhUl8aKStYoEljyPyhYT1L/USEpHNgWBQt37NjhL719/r+bfFxzSnbbG9l+xRVXcNddd7Fnzx6cTif9+vXj2muvZe3atSQkJLBgwQKEEMTExGC1WomKiiImJgYhBNHR0QghCA8P98ljt9t9y7/97W/5+OOPmTBhAq+//jrLly8nJiYGm81GRESE7xhzOSoqCqvV6lsfGRlJWFiY73pJSUnExMQQGxuLlLLOPTDvi8PhYNKk5scNu3fMoSvhizlo5dAGHAL6BSzXKQwppcwPmNX/MjClqce2CDNjqau6lUJEdHQ0s2bN4tprr2XevHmUlJQQFRVFXFwc2dnZfPllA+VfgBkzZvDxxx/7LIFPP/3Ut620tJTevXtTXV3NO++841sfExMTNJA9YsQIMjIy2Lt3LwBvvfUWp556ait908bRyqGr0BblMzQma4FhQohBQgg7qmDk4sAdhBAB3Yw4H3+Xw6+BM4UQCUYg+kxj3bFhJknoNNZWZ968eWzatIl58+YxYcIEJk2axMiRI7nyyiuZMWNGg8dOnDiRyy+/nAkTJnD22WfXKLl9//33M336dGbMmMHIkf7OiFdccQWPPPIIkyZNYt8+f+8Sh8PBa6+9xty5cxk3bhwWi4Ubb7yx9b9wPWi3UldBu5XaDCmlWwhxM2pQtwKvSim3CSHuA9ZJKRcDtwohzgfcQAGwwDi2QAhxP0rBANxnBqePiThtObQVF154ITKgMnN9TX0C/fgZGRmAsg7+9re/8be//a3O/jfddFPQORMzZsyoMc8h8HqzZ89m48aNdY4xrwcwderUNmlYpZVDVyF5OJx0Gww9I9SSdEmklF8AX9Rad3fA5zuBO+s59lXg1VYVqP8JcMLNMHhmq55WozHRyqGrYLGqGeCa7oEtQjWb0mjaCB1z0Gg0Gk0dtHLQaDQdBtlQF0ZNszmW+6mVg0aj6RA4HA7y8/O1gmglpJTk5+fjcLRsYqyOOWg0mg5BamoqWVlZ5ObmNus4p9PZ4gGwtelossTHx5Oamtqi47Vy0Gg0HQKbzcagQYOafdzy5ctbNAO4LehKsmi3kkaj0WjqoJWDRqPRaOqglYNGo9Fo6iA6c2aAECIXOBBkUzLQhObM7YKWJTgdRZaG5BggpQxJm7V6ftsd5Z6BlqU+Oossjf62O7VyqA8hxDop5dRQywFalvroKLJ0FDmaQkeSVcsSnK4ki3YraTQajaYOWjloNBqNpg5dVTm8GGoBAtCyBKejyNJR5GgKHUlWLUtwuowsXTLmoNFoNJpjo6taDhqNRqM5BrqUchBCzBFC7BJC7BVC/LWdr91PCLFMCLFdCLFNCPF7Y/1CIcQhIUSa8TqnneTJEEJsMa65zliXKIT4Vgixx3hPaAc5RgR89zQhRIkQ4g/tdV+EEK8KIXKEEFsD1gW9D0LxpPH72SyEmNwWMrUE/duuIY/+bdMOv20pZZd4odo37gMGA3ZgEzC6Ha/fG5hsfI4BdgOjgYXAn0NwPzKA5Frr/gn81fj8V+DhEPyNjgID2uu+AKcAk4Gtjd0H4BzgS0AAxwM/tfffrYH7pn/bfnn0b1u2/W+7K1kO04C9Usp0KaULWARc0F4Xl1IekVJuMD6XohrM922v6zeRC4A3jM9vABe28/VnA/uklMEmLrYJUsoVqJ7OgdR3Hy4A3pSKNUC8EKJ3uwjaMPq33Tj6t61otd92V1IOfYHMgOUsQvQDFkIMBCYBPxmrbjZMuVfbw9w1kMA3Qoj1QojfGOt6SimPGJ+PAu3dnf4K4N2A5VDcF6j/PnSY31AtOoxc+rddL13ut92VlEOHQAgRDXwI/EFKWQI8BwwBJgJHgH+1kygnSSknA2cDvxNCnBK4USpbs91S1YQQduB84ANjVajuSw3a+z50ZvRvOzhd9bfdlZTDIaBfwHKqsa7dEELYUP8870gp/wcgpcyWUnqklF7gJZSLoM2RUh4y3nOAj4zrZpumpPGe0x6yGJwNbJBSZhtyheS+GNR3H0L+G6qHkMulf9sN0iV/211JOawFhgkhBhma/ApgcXtdXAghgFeAHVLKfwesD/TrXQRsrX1sG8gSJYSIMT8DZxrXXQzMN3abD3zS1rIEMI8AszsU9yWA+u7DYuBXRmbH8UBxgIkeSvRv239N/dtumNb7bbdnRL8dovfnoDIp9gF/a+drn4Qy4TYDacbrHOAtYIuxfjHQux1kGYzKaNkEbDPvBZAELAX2AEuAxHa6N1FAPhAXsK5d7gvqn/YIUI3ys15X331AZXI8Y/x+tgBT2/M31Mj30L9tqX/bta7dpr9tPUNao9FoNHXoSm4ljUaj0bQSWjloNBqNpg5aOWg0Go2mDlo5aDQajaYOWjloNBqNpg5aOXRChBCeWtUgW61KpxBiYGCVR42mPdG/7Y5DWKgF0LSISinlxFALodG0Afq33UHQlkMXwqhz/0+j1v3PQoihxvqBQojvjEJgS4UQ/Y31PYUQHwkhNhmvE41TWYUQLwlVu/8bIUREyL6URoP+bYcCrRw6JxG1TO/LA7YVSynHAU8DjxvrngLekFKOB94BnjTWPwl8L6WcgKoLv81YPwx4Rko5BigCLmnTb6PR+NG/7Q6CniHdCRFClEkpo4OszwBOk1KmG4XSjkopk4QQeagp/NXG+iNSymQhRC6QKqWsCjjHQOBbKeUwY/kvgE1K+UA7fDVNN0f/tjsO2nLoesh6PjeHqoDPHnRsStMx0L/tdkQrh67H5QHvq43Pq1CVPAGuAlYan5cCNwEIIaxCiLj2ElKjaQH6t92OaK3ZOYkQQqQFLH8lpTRT/hKEEJtRT0jzjHW3AK8JIW4HcoFrjPW/B14UQlyHeoq6CVXlUaMJFfq33UHQMYcuhOGXnSqlzAu1LBpNa6J/2+2PditpNBqNpg7actBoNBpNHbTloNFoNJo6aOWg0Wg0mjpo5aDRaDSaOmjloNFoNJo6aOWg0Wg0mjpo5aDRaDSaOvx/VOcPMRU3j0AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.7457\n",
      "Validation AUC: 0.7458\n",
      "Validation Balanced_ACC: 0.4304\n",
      "Validation MI: 0.1087\n",
      "Validation Normalized MI: 0.1589\n",
      "Validation Adjusted MI: 0.1589\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 635.7928, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 585.8168, Accuracy: 0.5043\n",
      "Training loss (for one batch) at step 20: 522.1952, Accuracy: 0.5100\n",
      "Training loss (for one batch) at step 30: 527.7510, Accuracy: 0.5081\n",
      "Training loss (for one batch) at step 40: 497.3152, Accuracy: 0.5063\n",
      "Training loss (for one batch) at step 50: 488.3820, Accuracy: 0.5101\n",
      "Training loss (for one batch) at step 60: 480.4414, Accuracy: 0.5102\n",
      "Training loss (for one batch) at step 70: 475.9392, Accuracy: 0.5124\n",
      "Training loss (for one batch) at step 80: 487.0351, Accuracy: 0.5105\n",
      "Training loss (for one batch) at step 90: 468.7535, Accuracy: 0.5121\n",
      "Training loss (for one batch) at step 100: 472.4915, Accuracy: 0.5135\n",
      "Training loss (for one batch) at step 110: 467.6371, Accuracy: 0.5170\n",
      "---- Training ----\n",
      "Training loss: 141.8544\n",
      "Training acc over epoch: 0.5173\n",
      "---- Validation ----\n",
      "Validation loss: 35.5662\n",
      "Validation acc: 0.4866\n",
      "Time taken: 12.27s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 455.0777, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 451.4950, Accuracy: 0.5206\n",
      "Training loss (for one batch) at step 20: 463.8317, Accuracy: 0.5167\n",
      "Training loss (for one batch) at step 30: 456.1898, Accuracy: 0.5136\n",
      "Training loss (for one batch) at step 40: 449.5466, Accuracy: 0.5141\n",
      "Training loss (for one batch) at step 50: 457.6812, Accuracy: 0.5129\n",
      "Training loss (for one batch) at step 60: 452.4429, Accuracy: 0.5145\n",
      "Training loss (for one batch) at step 70: 449.3733, Accuracy: 0.5176\n",
      "Training loss (for one batch) at step 80: 445.5670, Accuracy: 0.5162\n",
      "Training loss (for one batch) at step 90: 447.5962, Accuracy: 0.5191\n",
      "Training loss (for one batch) at step 100: 448.9440, Accuracy: 0.5217\n",
      "Training loss (for one batch) at step 110: 442.0275, Accuracy: 0.5225\n",
      "---- Training ----\n",
      "Training loss: 140.2800\n",
      "Training acc over epoch: 0.5226\n",
      "---- Validation ----\n",
      "Validation loss: 34.4432\n",
      "Validation acc: 0.5134\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 446.7288, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 446.6596, Accuracy: 0.5618\n",
      "Training loss (for one batch) at step 20: 446.2924, Accuracy: 0.5424\n",
      "Training loss (for one batch) at step 30: 447.5376, Accuracy: 0.5335\n",
      "Training loss (for one batch) at step 40: 445.4208, Accuracy: 0.5292\n",
      "Training loss (for one batch) at step 50: 445.1167, Accuracy: 0.5276\n",
      "Training loss (for one batch) at step 60: 444.5085, Accuracy: 0.5298\n",
      "Training loss (for one batch) at step 70: 447.5327, Accuracy: 0.5361\n",
      "Training loss (for one batch) at step 80: 442.3591, Accuracy: 0.5420\n",
      "Training loss (for one batch) at step 90: 444.4308, Accuracy: 0.5454\n",
      "Training loss (for one batch) at step 100: 443.8185, Accuracy: 0.5475\n",
      "Training loss (for one batch) at step 110: 445.2785, Accuracy: 0.5466\n",
      "---- Training ----\n",
      "Training loss: 140.2339\n",
      "Training acc over epoch: 0.5453\n",
      "---- Validation ----\n",
      "Validation loss: 34.3456\n",
      "Validation acc: 0.5760\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 446.0624, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 446.9175, Accuracy: 0.5597\n",
      "Training loss (for one batch) at step 20: 444.3897, Accuracy: 0.5573\n",
      "Training loss (for one batch) at step 30: 443.8141, Accuracy: 0.5615\n",
      "Training loss (for one batch) at step 40: 442.8387, Accuracy: 0.5642\n",
      "Training loss (for one batch) at step 50: 444.8579, Accuracy: 0.5648\n",
      "Training loss (for one batch) at step 60: 443.8791, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 70: 445.6368, Accuracy: 0.5657\n",
      "Training loss (for one batch) at step 80: 441.9160, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 90: 441.8417, Accuracy: 0.5718\n",
      "Training loss (for one batch) at step 100: 444.0899, Accuracy: 0.5736\n",
      "Training loss (for one batch) at step 110: 442.8285, Accuracy: 0.5757\n",
      "---- Training ----\n",
      "Training loss: 138.5573\n",
      "Training acc over epoch: 0.5776\n",
      "---- Validation ----\n",
      "Validation loss: 35.2226\n",
      "Validation acc: 0.5927\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 442.9809, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 443.9221, Accuracy: 0.6108\n",
      "Training loss (for one batch) at step 20: 442.0797, Accuracy: 0.6027\n",
      "Training loss (for one batch) at step 30: 443.7534, Accuracy: 0.5922\n",
      "Training loss (for one batch) at step 40: 442.9697, Accuracy: 0.5896\n",
      "Training loss (for one batch) at step 50: 444.4616, Accuracy: 0.5890\n",
      "Training loss (for one batch) at step 60: 442.9411, Accuracy: 0.5908\n",
      "Training loss (for one batch) at step 70: 442.3723, Accuracy: 0.5945\n",
      "Training loss (for one batch) at step 80: 445.8303, Accuracy: 0.5995\n",
      "Training loss (for one batch) at step 90: 440.8091, Accuracy: 0.6014\n",
      "Training loss (for one batch) at step 100: 441.0261, Accuracy: 0.6026\n",
      "Training loss (for one batch) at step 110: 444.1785, Accuracy: 0.6062\n",
      "---- Training ----\n",
      "Training loss: 138.3015\n",
      "Training acc over epoch: 0.6073\n",
      "---- Validation ----\n",
      "Validation loss: 34.6001\n",
      "Validation acc: 0.6539\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 443.0574, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 444.1224, Accuracy: 0.6037\n",
      "Training loss (for one batch) at step 20: 442.2410, Accuracy: 0.6287\n",
      "Training loss (for one batch) at step 30: 443.0157, Accuracy: 0.6323\n",
      "Training loss (for one batch) at step 40: 442.5685, Accuracy: 0.6303\n",
      "Training loss (for one batch) at step 50: 442.0492, Accuracy: 0.6284\n",
      "Training loss (for one batch) at step 60: 440.8746, Accuracy: 0.6285\n",
      "Training loss (for one batch) at step 70: 445.9352, Accuracy: 0.6359\n",
      "Training loss (for one batch) at step 80: 444.1830, Accuracy: 0.6357\n",
      "Training loss (for one batch) at step 90: 439.0110, Accuracy: 0.6333\n",
      "Training loss (for one batch) at step 100: 445.6045, Accuracy: 0.6314\n",
      "Training loss (for one batch) at step 110: 442.7729, Accuracy: 0.6321\n",
      "---- Training ----\n",
      "Training loss: 137.7210\n",
      "Training acc over epoch: 0.6345\n",
      "---- Validation ----\n",
      "Validation loss: 33.6710\n",
      "Validation acc: 0.6647\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 444.1627, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 441.0764, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 20: 443.3576, Accuracy: 0.6481\n",
      "Training loss (for one batch) at step 30: 441.6221, Accuracy: 0.6570\n",
      "Training loss (for one batch) at step 40: 438.7525, Accuracy: 0.6524\n",
      "Training loss (for one batch) at step 50: 440.3567, Accuracy: 0.6500\n",
      "Training loss (for one batch) at step 60: 439.4816, Accuracy: 0.6536\n",
      "Training loss (for one batch) at step 70: 440.9841, Accuracy: 0.6605\n",
      "Training loss (for one batch) at step 80: 444.2197, Accuracy: 0.6598\n",
      "Training loss (for one batch) at step 90: 441.1298, Accuracy: 0.6609\n",
      "Training loss (for one batch) at step 100: 436.2288, Accuracy: 0.6618\n",
      "Training loss (for one batch) at step 110: 441.7181, Accuracy: 0.6622\n",
      "---- Training ----\n",
      "Training loss: 137.1467\n",
      "Training acc over epoch: 0.6613\n",
      "---- Validation ----\n",
      "Validation loss: 33.9389\n",
      "Validation acc: 0.6617\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 447.7278, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 441.6711, Accuracy: 0.6676\n",
      "Training loss (for one batch) at step 20: 440.5536, Accuracy: 0.6622\n",
      "Training loss (for one batch) at step 30: 432.8046, Accuracy: 0.6683\n",
      "Training loss (for one batch) at step 40: 434.9459, Accuracy: 0.6709\n",
      "Training loss (for one batch) at step 50: 435.2496, Accuracy: 0.6728\n",
      "Training loss (for one batch) at step 60: 439.5119, Accuracy: 0.6755\n",
      "Training loss (for one batch) at step 70: 439.5523, Accuracy: 0.6812\n",
      "Training loss (for one batch) at step 80: 437.4965, Accuracy: 0.6761\n",
      "Training loss (for one batch) at step 90: 435.7797, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 100: 435.5744, Accuracy: 0.6720\n",
      "Training loss (for one batch) at step 110: 438.7475, Accuracy: 0.6741\n",
      "---- Training ----\n",
      "Training loss: 136.6158\n",
      "Training acc over epoch: 0.6746\n",
      "---- Validation ----\n",
      "Validation loss: 35.6848\n",
      "Validation acc: 0.7007\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 441.9976, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 440.5154, Accuracy: 0.6790\n",
      "Training loss (for one batch) at step 20: 440.8870, Accuracy: 0.6808\n",
      "Training loss (for one batch) at step 30: 439.4530, Accuracy: 0.6835\n",
      "Training loss (for one batch) at step 40: 439.5521, Accuracy: 0.6845\n",
      "Training loss (for one batch) at step 50: 428.5536, Accuracy: 0.6850\n",
      "Training loss (for one batch) at step 60: 442.3479, Accuracy: 0.6921\n",
      "Training loss (for one batch) at step 70: 440.3773, Accuracy: 0.6961\n",
      "Training loss (for one batch) at step 80: 434.8852, Accuracy: 0.6950\n",
      "Training loss (for one batch) at step 90: 440.0337, Accuracy: 0.6913\n",
      "Training loss (for one batch) at step 100: 435.4408, Accuracy: 0.6888\n",
      "Training loss (for one batch) at step 110: 438.9344, Accuracy: 0.6881\n",
      "---- Training ----\n",
      "Training loss: 137.7517\n",
      "Training acc over epoch: 0.6880\n",
      "---- Validation ----\n",
      "Validation loss: 34.3493\n",
      "Validation acc: 0.6926\n",
      "Time taken: 10.16s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 441.0570, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 439.5602, Accuracy: 0.6740\n",
      "Training loss (for one batch) at step 20: 435.8581, Accuracy: 0.6734\n",
      "Training loss (for one batch) at step 30: 432.1393, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 40: 427.3012, Accuracy: 0.6942\n",
      "Training loss (for one batch) at step 50: 427.7039, Accuracy: 0.7033\n",
      "Training loss (for one batch) at step 60: 438.5935, Accuracy: 0.7052\n",
      "Training loss (for one batch) at step 70: 434.9874, Accuracy: 0.7064\n",
      "Training loss (for one batch) at step 80: 439.1578, Accuracy: 0.7051\n",
      "Training loss (for one batch) at step 90: 434.0537, Accuracy: 0.7044\n",
      "Training loss (for one batch) at step 100: 435.7787, Accuracy: 0.7065\n",
      "Training loss (for one batch) at step 110: 441.5444, Accuracy: 0.7093\n",
      "---- Training ----\n",
      "Training loss: 143.7122\n",
      "Training acc over epoch: 0.7102\n",
      "---- Validation ----\n",
      "Validation loss: 35.1277\n",
      "Validation acc: 0.7077\n",
      "Time taken: 10.40s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 446.5706, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 438.6440, Accuracy: 0.6861\n",
      "Training loss (for one batch) at step 20: 437.2703, Accuracy: 0.6897\n",
      "Training loss (for one batch) at step 30: 426.4931, Accuracy: 0.6988\n",
      "Training loss (for one batch) at step 40: 424.1382, Accuracy: 0.7026\n",
      "Training loss (for one batch) at step 50: 430.4207, Accuracy: 0.7131\n",
      "Training loss (for one batch) at step 60: 441.1465, Accuracy: 0.7145\n",
      "Training loss (for one batch) at step 70: 444.6618, Accuracy: 0.7153\n",
      "Training loss (for one batch) at step 80: 438.1737, Accuracy: 0.7156\n",
      "Training loss (for one batch) at step 90: 436.1703, Accuracy: 0.7144\n",
      "Training loss (for one batch) at step 100: 429.9070, Accuracy: 0.7160\n",
      "Training loss (for one batch) at step 110: 435.8959, Accuracy: 0.7177\n",
      "---- Training ----\n",
      "Training loss: 134.5539\n",
      "Training acc over epoch: 0.7182\n",
      "---- Validation ----\n",
      "Validation loss: 32.6404\n",
      "Validation acc: 0.7372\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 439.8244, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 433.7819, Accuracy: 0.7138\n",
      "Training loss (for one batch) at step 20: 430.1280, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 30: 426.2435, Accuracy: 0.7170\n",
      "Training loss (for one batch) at step 40: 433.9025, Accuracy: 0.7207\n",
      "Training loss (for one batch) at step 50: 424.4017, Accuracy: 0.7264\n",
      "Training loss (for one batch) at step 60: 421.4102, Accuracy: 0.7296\n",
      "Training loss (for one batch) at step 70: 429.5127, Accuracy: 0.7306\n",
      "Training loss (for one batch) at step 80: 435.2070, Accuracy: 0.7282\n",
      "Training loss (for one batch) at step 90: 435.7943, Accuracy: 0.7282\n",
      "Training loss (for one batch) at step 100: 425.3731, Accuracy: 0.7294\n",
      "Training loss (for one batch) at step 110: 431.9864, Accuracy: 0.7287\n",
      "---- Training ----\n",
      "Training loss: 137.9090\n",
      "Training acc over epoch: 0.7294\n",
      "---- Validation ----\n",
      "Validation loss: 33.3748\n",
      "Validation acc: 0.7246\n",
      "Time taken: 10.14s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 440.3402, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 435.9956, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 20: 436.8952, Accuracy: 0.7180\n",
      "Training loss (for one batch) at step 30: 416.4382, Accuracy: 0.7215\n",
      "Training loss (for one batch) at step 40: 419.7913, Accuracy: 0.7273\n",
      "Training loss (for one batch) at step 50: 420.7310, Accuracy: 0.7327\n",
      "Training loss (for one batch) at step 60: 425.1816, Accuracy: 0.7428\n",
      "Training loss (for one batch) at step 70: 434.6742, Accuracy: 0.7442\n",
      "Training loss (for one batch) at step 80: 444.0099, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 90: 430.2694, Accuracy: 0.7389\n",
      "Training loss (for one batch) at step 100: 434.2975, Accuracy: 0.7393\n",
      "Training loss (for one batch) at step 110: 421.5015, Accuracy: 0.7385\n",
      "---- Training ----\n",
      "Training loss: 131.7588\n",
      "Training acc over epoch: 0.7393\n",
      "---- Validation ----\n",
      "Validation loss: 35.7889\n",
      "Validation acc: 0.6975\n",
      "Time taken: 10.40s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 442.2986, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 431.9381, Accuracy: 0.7479\n",
      "Training loss (for one batch) at step 20: 434.2706, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 30: 424.0824, Accuracy: 0.7427\n",
      "Training loss (for one batch) at step 40: 415.6799, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 50: 417.3200, Accuracy: 0.7589\n",
      "Training loss (for one batch) at step 60: 424.5289, Accuracy: 0.7637\n",
      "Training loss (for one batch) at step 70: 430.5731, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 80: 433.1289, Accuracy: 0.7595\n",
      "Training loss (for one batch) at step 90: 440.6254, Accuracy: 0.7564\n",
      "Training loss (for one batch) at step 100: 426.9375, Accuracy: 0.7518\n",
      "Training loss (for one batch) at step 110: 431.7137, Accuracy: 0.7528\n",
      "---- Training ----\n",
      "Training loss: 138.5481\n",
      "Training acc over epoch: 0.7513\n",
      "---- Validation ----\n",
      "Validation loss: 41.0406\n",
      "Validation acc: 0.7445\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 438.2751, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 437.8179, Accuracy: 0.7415\n",
      "Training loss (for one batch) at step 20: 429.1286, Accuracy: 0.7478\n",
      "Training loss (for one batch) at step 30: 424.7682, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 40: 418.1477, Accuracy: 0.7553\n",
      "Training loss (for one batch) at step 50: 406.2263, Accuracy: 0.7661\n",
      "Training loss (for one batch) at step 60: 434.0585, Accuracy: 0.7746\n",
      "Training loss (for one batch) at step 70: 434.0508, Accuracy: 0.7722\n",
      "Training loss (for one batch) at step 80: 424.4697, Accuracy: 0.7651\n",
      "Training loss (for one batch) at step 90: 428.9868, Accuracy: 0.7617\n",
      "Training loss (for one batch) at step 100: 417.6813, Accuracy: 0.7618\n",
      "Training loss (for one batch) at step 110: 427.2365, Accuracy: 0.7625\n",
      "---- Training ----\n",
      "Training loss: 134.9361\n",
      "Training acc over epoch: 0.7617\n",
      "---- Validation ----\n",
      "Validation loss: 33.3385\n",
      "Validation acc: 0.7609\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 430.5040, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 432.1758, Accuracy: 0.7607\n",
      "Training loss (for one batch) at step 20: 432.2963, Accuracy: 0.7548\n",
      "Training loss (for one batch) at step 30: 407.6024, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 40: 408.2235, Accuracy: 0.7692\n",
      "Training loss (for one batch) at step 50: 408.9431, Accuracy: 0.7767\n",
      "Training loss (for one batch) at step 60: 413.6722, Accuracy: 0.7851\n",
      "Training loss (for one batch) at step 70: 435.2309, Accuracy: 0.7843\n",
      "Training loss (for one batch) at step 80: 429.5368, Accuracy: 0.7815\n",
      "Training loss (for one batch) at step 90: 421.7374, Accuracy: 0.7776\n",
      "Training loss (for one batch) at step 100: 419.3463, Accuracy: 0.7756\n",
      "Training loss (for one batch) at step 110: 423.9083, Accuracy: 0.7752\n",
      "---- Training ----\n",
      "Training loss: 128.9632\n",
      "Training acc over epoch: 0.7738\n",
      "---- Validation ----\n",
      "Validation loss: 39.2855\n",
      "Validation acc: 0.7474\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 437.1813, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 437.6668, Accuracy: 0.7464\n",
      "Training loss (for one batch) at step 20: 435.7702, Accuracy: 0.7548\n",
      "Training loss (for one batch) at step 30: 409.1346, Accuracy: 0.7639\n",
      "Training loss (for one batch) at step 40: 391.7332, Accuracy: 0.7732\n",
      "Training loss (for one batch) at step 50: 392.3923, Accuracy: 0.7883\n",
      "Training loss (for one batch) at step 60: 403.3569, Accuracy: 0.7925\n",
      "Training loss (for one batch) at step 70: 443.4886, Accuracy: 0.7894\n",
      "Training loss (for one batch) at step 80: 431.4314, Accuracy: 0.7838\n",
      "Training loss (for one batch) at step 90: 418.7473, Accuracy: 0.7790\n",
      "Training loss (for one batch) at step 100: 418.0110, Accuracy: 0.7775\n",
      "Training loss (for one batch) at step 110: 430.7151, Accuracy: 0.7776\n",
      "---- Training ----\n",
      "Training loss: 133.9527\n",
      "Training acc over epoch: 0.7780\n",
      "---- Validation ----\n",
      "Validation loss: 35.0607\n",
      "Validation acc: 0.7660\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 433.1447, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 434.1805, Accuracy: 0.7472\n",
      "Training loss (for one batch) at step 20: 419.0303, Accuracy: 0.7560\n",
      "Training loss (for one batch) at step 30: 406.8656, Accuracy: 0.7704\n",
      "Training loss (for one batch) at step 40: 394.2000, Accuracy: 0.7809\n",
      "Training loss (for one batch) at step 50: 385.0880, Accuracy: 0.7886\n",
      "Training loss (for one batch) at step 60: 412.9636, Accuracy: 0.7965\n",
      "Training loss (for one batch) at step 70: 402.0931, Accuracy: 0.7943\n",
      "Training loss (for one batch) at step 80: 413.0158, Accuracy: 0.7912\n",
      "Training loss (for one batch) at step 90: 423.7473, Accuracy: 0.7875\n",
      "Training loss (for one batch) at step 100: 414.7079, Accuracy: 0.7859\n",
      "Training loss (for one batch) at step 110: 420.1835, Accuracy: 0.7833\n",
      "---- Training ----\n",
      "Training loss: 132.1897\n",
      "Training acc over epoch: 0.7832\n",
      "---- Validation ----\n",
      "Validation loss: 35.3002\n",
      "Validation acc: 0.7456\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 429.0508, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 420.9468, Accuracy: 0.7727\n",
      "Training loss (for one batch) at step 20: 417.8040, Accuracy: 0.7719\n",
      "Training loss (for one batch) at step 30: 407.7174, Accuracy: 0.7775\n",
      "Training loss (for one batch) at step 40: 391.8366, Accuracy: 0.7807\n",
      "Training loss (for one batch) at step 50: 401.1711, Accuracy: 0.7935\n",
      "Training loss (for one batch) at step 60: 428.2394, Accuracy: 0.7996\n",
      "Training loss (for one batch) at step 70: 415.8880, Accuracy: 0.7987\n",
      "Training loss (for one batch) at step 80: 436.0559, Accuracy: 0.7931\n",
      "Training loss (for one batch) at step 90: 419.2230, Accuracy: 0.7888\n",
      "Training loss (for one batch) at step 100: 411.4911, Accuracy: 0.7901\n",
      "Training loss (for one batch) at step 110: 417.1674, Accuracy: 0.7893\n",
      "---- Training ----\n",
      "Training loss: 134.6016\n",
      "Training acc over epoch: 0.7892\n",
      "---- Validation ----\n",
      "Validation loss: 33.7127\n",
      "Validation acc: 0.7528\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 427.0463, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 432.6565, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 20: 413.3755, Accuracy: 0.7589\n",
      "Training loss (for one batch) at step 30: 409.2275, Accuracy: 0.7742\n",
      "Training loss (for one batch) at step 40: 394.3049, Accuracy: 0.7860\n",
      "Training loss (for one batch) at step 50: 393.6420, Accuracy: 0.7970\n",
      "Training loss (for one batch) at step 60: 401.2899, Accuracy: 0.8048\n",
      "Training loss (for one batch) at step 70: 417.5934, Accuracy: 0.8003\n",
      "Training loss (for one batch) at step 80: 427.2339, Accuracy: 0.7952\n",
      "Training loss (for one batch) at step 90: 412.7703, Accuracy: 0.7915\n",
      "Training loss (for one batch) at step 100: 401.6871, Accuracy: 0.7930\n",
      "Training loss (for one batch) at step 110: 420.4997, Accuracy: 0.7910\n",
      "---- Training ----\n",
      "Training loss: 129.2766\n",
      "Training acc over epoch: 0.7912\n",
      "---- Validation ----\n",
      "Validation loss: 33.8073\n",
      "Validation acc: 0.7437\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 433.3130, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 431.9894, Accuracy: 0.7408\n",
      "Training loss (for one batch) at step 20: 415.8279, Accuracy: 0.7608\n",
      "Training loss (for one batch) at step 30: 400.5153, Accuracy: 0.7795\n",
      "Training loss (for one batch) at step 40: 389.3654, Accuracy: 0.7925\n",
      "Training loss (for one batch) at step 50: 380.9575, Accuracy: 0.8064\n",
      "Training loss (for one batch) at step 60: 401.5775, Accuracy: 0.8148\n",
      "Training loss (for one batch) at step 70: 406.3439, Accuracy: 0.8153\n",
      "Training loss (for one batch) at step 80: 413.6159, Accuracy: 0.8081\n",
      "Training loss (for one batch) at step 90: 407.3134, Accuracy: 0.8049\n",
      "Training loss (for one batch) at step 100: 417.6322, Accuracy: 0.8031\n",
      "Training loss (for one batch) at step 110: 417.2300, Accuracy: 0.8000\n",
      "---- Training ----\n",
      "Training loss: 132.9160\n",
      "Training acc over epoch: 0.7998\n",
      "---- Validation ----\n",
      "Validation loss: 33.1128\n",
      "Validation acc: 0.7499\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 424.2080, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 411.3723, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 20: 410.0925, Accuracy: 0.7749\n",
      "Training loss (for one batch) at step 30: 401.6975, Accuracy: 0.7843\n",
      "Training loss (for one batch) at step 40: 385.9498, Accuracy: 0.7961\n",
      "Training loss (for one batch) at step 50: 364.1653, Accuracy: 0.8079\n",
      "Training loss (for one batch) at step 60: 381.0113, Accuracy: 0.8140\n",
      "Training loss (for one batch) at step 70: 412.9250, Accuracy: 0.8137\n",
      "Training loss (for one batch) at step 80: 415.7334, Accuracy: 0.8067\n",
      "Training loss (for one batch) at step 90: 404.5013, Accuracy: 0.8043\n",
      "Training loss (for one batch) at step 100: 400.9459, Accuracy: 0.8025\n",
      "Training loss (for one batch) at step 110: 401.4074, Accuracy: 0.8015\n",
      "---- Training ----\n",
      "Training loss: 123.8136\n",
      "Training acc over epoch: 0.8005\n",
      "---- Validation ----\n",
      "Validation loss: 37.0117\n",
      "Validation acc: 0.7262\n",
      "Time taken: 10.60s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 412.3164, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 418.6561, Accuracy: 0.7514\n",
      "Training loss (for one batch) at step 20: 415.7117, Accuracy: 0.7626\n",
      "Training loss (for one batch) at step 30: 393.1837, Accuracy: 0.7825\n",
      "Training loss (for one batch) at step 40: 381.5771, Accuracy: 0.7944\n",
      "Training loss (for one batch) at step 50: 377.5066, Accuracy: 0.8082\n",
      "Training loss (for one batch) at step 60: 378.1239, Accuracy: 0.8170\n",
      "Training loss (for one batch) at step 70: 408.4927, Accuracy: 0.8148\n",
      "Training loss (for one batch) at step 80: 405.9653, Accuracy: 0.8077\n",
      "Training loss (for one batch) at step 90: 395.2435, Accuracy: 0.8031\n",
      "Training loss (for one batch) at step 100: 398.1731, Accuracy: 0.8036\n",
      "Training loss (for one batch) at step 110: 397.3000, Accuracy: 0.8026\n",
      "---- Training ----\n",
      "Training loss: 132.4547\n",
      "Training acc over epoch: 0.8017\n",
      "---- Validation ----\n",
      "Validation loss: 40.1298\n",
      "Validation acc: 0.7550\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 415.8091, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 419.7473, Accuracy: 0.7756\n",
      "Training loss (for one batch) at step 20: 397.9863, Accuracy: 0.7772\n",
      "Training loss (for one batch) at step 30: 377.6790, Accuracy: 0.7863\n",
      "Training loss (for one batch) at step 40: 373.6132, Accuracy: 0.8009\n",
      "Training loss (for one batch) at step 50: 363.3891, Accuracy: 0.8156\n",
      "Training loss (for one batch) at step 60: 373.9603, Accuracy: 0.8217\n",
      "Training loss (for one batch) at step 70: 400.7918, Accuracy: 0.8194\n",
      "Training loss (for one batch) at step 80: 405.4320, Accuracy: 0.8127\n",
      "Training loss (for one batch) at step 90: 402.3691, Accuracy: 0.8090\n",
      "Training loss (for one batch) at step 100: 386.7122, Accuracy: 0.8093\n",
      "Training loss (for one batch) at step 110: 396.5379, Accuracy: 0.8088\n",
      "---- Training ----\n",
      "Training loss: 124.8739\n",
      "Training acc over epoch: 0.8055\n",
      "---- Validation ----\n",
      "Validation loss: 40.2836\n",
      "Validation acc: 0.7410\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 423.4612, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 10: 402.9114, Accuracy: 0.7692\n",
      "Training loss (for one batch) at step 20: 394.9918, Accuracy: 0.7716\n",
      "Training loss (for one batch) at step 30: 370.0300, Accuracy: 0.7989\n",
      "Training loss (for one batch) at step 40: 385.2396, Accuracy: 0.8077\n",
      "Training loss (for one batch) at step 50: 353.4276, Accuracy: 0.8165\n",
      "Training loss (for one batch) at step 60: 383.4084, Accuracy: 0.8247\n",
      "Training loss (for one batch) at step 70: 391.5339, Accuracy: 0.8210\n",
      "Training loss (for one batch) at step 80: 400.2387, Accuracy: 0.8159\n",
      "Training loss (for one batch) at step 90: 375.4707, Accuracy: 0.8112\n",
      "Training loss (for one batch) at step 100: 377.4725, Accuracy: 0.8106\n",
      "Training loss (for one batch) at step 110: 398.7459, Accuracy: 0.8100\n",
      "---- Training ----\n",
      "Training loss: 120.3982\n",
      "Training acc over epoch: 0.8080\n",
      "---- Validation ----\n",
      "Validation loss: 38.9800\n",
      "Validation acc: 0.7488\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 426.5429, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 396.1528, Accuracy: 0.7713\n",
      "Training loss (for one batch) at step 20: 402.5748, Accuracy: 0.7801\n",
      "Training loss (for one batch) at step 30: 370.7320, Accuracy: 0.7933\n",
      "Training loss (for one batch) at step 40: 365.6053, Accuracy: 0.8064\n",
      "Training loss (for one batch) at step 50: 365.2070, Accuracy: 0.8212\n",
      "Training loss (for one batch) at step 60: 378.5204, Accuracy: 0.8259\n",
      "Training loss (for one batch) at step 70: 403.5635, Accuracy: 0.8224\n",
      "Training loss (for one batch) at step 80: 399.5013, Accuracy: 0.8108\n",
      "Training loss (for one batch) at step 90: 379.8230, Accuracy: 0.8070\n",
      "Training loss (for one batch) at step 100: 378.7076, Accuracy: 0.8082\n",
      "Training loss (for one batch) at step 110: 403.8606, Accuracy: 0.8081\n",
      "---- Training ----\n",
      "Training loss: 121.0158\n",
      "Training acc over epoch: 0.8071\n",
      "---- Validation ----\n",
      "Validation loss: 38.2448\n",
      "Validation acc: 0.7569\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 425.2754, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 401.0487, Accuracy: 0.7692\n",
      "Training loss (for one batch) at step 20: 386.7529, Accuracy: 0.7742\n",
      "Training loss (for one batch) at step 30: 373.6789, Accuracy: 0.7966\n",
      "Training loss (for one batch) at step 40: 363.9790, Accuracy: 0.8062\n",
      "Training loss (for one batch) at step 50: 352.4940, Accuracy: 0.8180\n",
      "Training loss (for one batch) at step 60: 360.1940, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 70: 382.0679, Accuracy: 0.8232\n",
      "Training loss (for one batch) at step 80: 407.1209, Accuracy: 0.8118\n",
      "Training loss (for one batch) at step 90: 393.6218, Accuracy: 0.8073\n",
      "Training loss (for one batch) at step 100: 373.1035, Accuracy: 0.8079\n",
      "Training loss (for one batch) at step 110: 395.1993, Accuracy: 0.8090\n",
      "---- Training ----\n",
      "Training loss: 121.1100\n",
      "Training acc over epoch: 0.8092\n",
      "---- Validation ----\n",
      "Validation loss: 33.3879\n",
      "Validation acc: 0.7480\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 423.0408, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 400.6215, Accuracy: 0.7663\n",
      "Training loss (for one batch) at step 20: 376.5115, Accuracy: 0.7842\n",
      "Training loss (for one batch) at step 30: 368.8261, Accuracy: 0.7928\n",
      "Training loss (for one batch) at step 40: 355.7655, Accuracy: 0.8085\n",
      "Training loss (for one batch) at step 50: 348.6048, Accuracy: 0.8189\n",
      "Training loss (for one batch) at step 60: 368.0497, Accuracy: 0.8261\n",
      "Training loss (for one batch) at step 70: 400.9093, Accuracy: 0.8213\n",
      "Training loss (for one batch) at step 80: 388.3510, Accuracy: 0.8150\n",
      "Training loss (for one batch) at step 90: 370.1091, Accuracy: 0.8122\n",
      "Training loss (for one batch) at step 100: 357.7303, Accuracy: 0.8144\n",
      "Training loss (for one batch) at step 110: 362.8833, Accuracy: 0.8142\n",
      "---- Training ----\n",
      "Training loss: 126.9404\n",
      "Training acc over epoch: 0.8128\n",
      "---- Validation ----\n",
      "Validation loss: 40.0913\n",
      "Validation acc: 0.7577\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 400.9099, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 409.9021, Accuracy: 0.7713\n",
      "Training loss (for one batch) at step 20: 363.4868, Accuracy: 0.7842\n",
      "Training loss (for one batch) at step 30: 352.3092, Accuracy: 0.8002\n",
      "Training loss (for one batch) at step 40: 375.2863, Accuracy: 0.8123\n",
      "Training loss (for one batch) at step 50: 344.5745, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 60: 365.7310, Accuracy: 0.8330\n",
      "Training loss (for one batch) at step 70: 388.1696, Accuracy: 0.8296\n",
      "Training loss (for one batch) at step 80: 377.9734, Accuracy: 0.8181\n",
      "Training loss (for one batch) at step 90: 377.4650, Accuracy: 0.8147\n",
      "Training loss (for one batch) at step 100: 363.7761, Accuracy: 0.8156\n",
      "Training loss (for one batch) at step 110: 383.0896, Accuracy: 0.8162\n",
      "---- Training ----\n",
      "Training loss: 126.8817\n",
      "Training acc over epoch: 0.8144\n",
      "---- Validation ----\n",
      "Validation loss: 45.9810\n",
      "Validation acc: 0.7496\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 411.5511, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 395.3434, Accuracy: 0.7607\n",
      "Training loss (for one batch) at step 20: 366.1047, Accuracy: 0.7712\n",
      "Training loss (for one batch) at step 30: 347.2630, Accuracy: 0.7916\n",
      "Training loss (for one batch) at step 40: 372.3893, Accuracy: 0.8058\n",
      "Training loss (for one batch) at step 50: 339.0897, Accuracy: 0.8212\n",
      "Training loss (for one batch) at step 60: 356.2950, Accuracy: 0.8286\n",
      "Training loss (for one batch) at step 70: 363.6870, Accuracy: 0.8232\n",
      "Training loss (for one batch) at step 80: 392.4521, Accuracy: 0.8129\n",
      "Training loss (for one batch) at step 90: 367.1550, Accuracy: 0.8091\n",
      "Training loss (for one batch) at step 100: 354.9890, Accuracy: 0.8113\n",
      "Training loss (for one batch) at step 110: 372.5376, Accuracy: 0.8140\n",
      "---- Training ----\n",
      "Training loss: 121.9245\n",
      "Training acc over epoch: 0.8134\n",
      "---- Validation ----\n",
      "Validation loss: 39.8031\n",
      "Validation acc: 0.7474\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 404.8954, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 383.8372, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 20: 366.4507, Accuracy: 0.7746\n",
      "Training loss (for one batch) at step 30: 348.1836, Accuracy: 0.7964\n",
      "Training loss (for one batch) at step 40: 364.8604, Accuracy: 0.8075\n",
      "Training loss (for one batch) at step 50: 333.9220, Accuracy: 0.8225\n",
      "Training loss (for one batch) at step 60: 359.4563, Accuracy: 0.8298\n",
      "Training loss (for one batch) at step 70: 385.8070, Accuracy: 0.8215\n",
      "Training loss (for one batch) at step 80: 374.4146, Accuracy: 0.8148\n",
      "Training loss (for one batch) at step 90: 354.5587, Accuracy: 0.8108\n",
      "Training loss (for one batch) at step 100: 349.1309, Accuracy: 0.8115\n",
      "Training loss (for one batch) at step 110: 358.6433, Accuracy: 0.8119\n",
      "---- Training ----\n",
      "Training loss: 106.1006\n",
      "Training acc over epoch: 0.8125\n",
      "---- Validation ----\n",
      "Validation loss: 39.4013\n",
      "Validation acc: 0.7550\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 408.8498, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 387.2856, Accuracy: 0.7685\n",
      "Training loss (for one batch) at step 20: 357.4679, Accuracy: 0.7779\n",
      "Training loss (for one batch) at step 30: 363.7223, Accuracy: 0.7933\n",
      "Training loss (for one batch) at step 40: 358.6866, Accuracy: 0.8087\n",
      "Training loss (for one batch) at step 50: 330.9198, Accuracy: 0.8215\n",
      "Training loss (for one batch) at step 60: 369.9553, Accuracy: 0.8288\n",
      "Training loss (for one batch) at step 70: 397.7657, Accuracy: 0.8231\n",
      "Training loss (for one batch) at step 80: 389.2433, Accuracy: 0.8136\n",
      "Training loss (for one batch) at step 90: 365.2653, Accuracy: 0.8098\n",
      "Training loss (for one batch) at step 100: 342.5787, Accuracy: 0.8117\n",
      "Training loss (for one batch) at step 110: 378.7468, Accuracy: 0.8131\n",
      "---- Training ----\n",
      "Training loss: 130.2345\n",
      "Training acc over epoch: 0.8107\n",
      "---- Validation ----\n",
      "Validation loss: 38.3311\n",
      "Validation acc: 0.7501\n",
      "Time taken: 10.58s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 396.3462, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 386.9882, Accuracy: 0.7663\n",
      "Training loss (for one batch) at step 20: 362.4888, Accuracy: 0.7760\n",
      "Training loss (for one batch) at step 30: 348.3361, Accuracy: 0.8009\n",
      "Training loss (for one batch) at step 40: 345.4627, Accuracy: 0.8131\n",
      "Training loss (for one batch) at step 50: 335.6737, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 60: 342.0195, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 70: 385.2340, Accuracy: 0.8260\n",
      "Training loss (for one batch) at step 80: 386.3732, Accuracy: 0.8157\n",
      "Training loss (for one batch) at step 90: 346.4935, Accuracy: 0.8143\n",
      "Training loss (for one batch) at step 100: 345.6888, Accuracy: 0.8179\n",
      "Training loss (for one batch) at step 110: 358.4363, Accuracy: 0.8169\n",
      "---- Training ----\n",
      "Training loss: 112.6034\n",
      "Training acc over epoch: 0.8166\n",
      "---- Validation ----\n",
      "Validation loss: 38.5170\n",
      "Validation acc: 0.7558\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 402.4861, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 379.4382, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 20: 351.8238, Accuracy: 0.7835\n",
      "Training loss (for one batch) at step 30: 325.1730, Accuracy: 0.7996\n",
      "Training loss (for one batch) at step 40: 336.9211, Accuracy: 0.8150\n",
      "Training loss (for one batch) at step 50: 332.1052, Accuracy: 0.8254\n",
      "Training loss (for one batch) at step 60: 335.3340, Accuracy: 0.8343\n",
      "Training loss (for one batch) at step 70: 383.9986, Accuracy: 0.8261\n",
      "Training loss (for one batch) at step 80: 389.9367, Accuracy: 0.8180\n",
      "Training loss (for one batch) at step 90: 355.8196, Accuracy: 0.8129\n",
      "Training loss (for one batch) at step 100: 328.2217, Accuracy: 0.8151\n",
      "Training loss (for one batch) at step 110: 356.0957, Accuracy: 0.8161\n",
      "---- Training ----\n",
      "Training loss: 110.1478\n",
      "Training acc over epoch: 0.8159\n",
      "---- Validation ----\n",
      "Validation loss: 39.6357\n",
      "Validation acc: 0.7464\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 393.7856, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 387.5635, Accuracy: 0.7685\n",
      "Training loss (for one batch) at step 20: 355.6508, Accuracy: 0.7954\n",
      "Training loss (for one batch) at step 30: 341.1657, Accuracy: 0.8105\n",
      "Training loss (for one batch) at step 40: 346.8290, Accuracy: 0.8266\n",
      "Training loss (for one batch) at step 50: 338.2088, Accuracy: 0.8366\n",
      "Training loss (for one batch) at step 60: 337.5877, Accuracy: 0.8418\n",
      "Training loss (for one batch) at step 70: 361.9334, Accuracy: 0.8356\n",
      "Training loss (for one batch) at step 80: 373.5217, Accuracy: 0.8227\n",
      "Training loss (for one batch) at step 90: 343.2656, Accuracy: 0.8189\n",
      "Training loss (for one batch) at step 100: 337.7537, Accuracy: 0.8210\n",
      "Training loss (for one batch) at step 110: 355.3776, Accuracy: 0.8205\n",
      "---- Training ----\n",
      "Training loss: 126.1641\n",
      "Training acc over epoch: 0.8192\n",
      "---- Validation ----\n",
      "Validation loss: 47.0164\n",
      "Validation acc: 0.7509\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 389.5081, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 369.2540, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 20: 351.1701, Accuracy: 0.7809\n",
      "Training loss (for one batch) at step 30: 327.4256, Accuracy: 0.7989\n",
      "Training loss (for one batch) at step 40: 341.7027, Accuracy: 0.8155\n",
      "Training loss (for one batch) at step 50: 321.8643, Accuracy: 0.8286\n",
      "Training loss (for one batch) at step 60: 339.7716, Accuracy: 0.8352\n",
      "Training loss (for one batch) at step 70: 362.1420, Accuracy: 0.8287\n",
      "Training loss (for one batch) at step 80: 369.4574, Accuracy: 0.8190\n",
      "Training loss (for one batch) at step 90: 358.2217, Accuracy: 0.8161\n",
      "Training loss (for one batch) at step 100: 325.8724, Accuracy: 0.8197\n",
      "Training loss (for one batch) at step 110: 356.6661, Accuracy: 0.8193\n",
      "---- Training ----\n",
      "Training loss: 107.6032\n",
      "Training acc over epoch: 0.8181\n",
      "---- Validation ----\n",
      "Validation loss: 37.4450\n",
      "Validation acc: 0.7517\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 370.2912, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 368.2724, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 20: 339.4961, Accuracy: 0.7857\n",
      "Training loss (for one batch) at step 30: 333.5670, Accuracy: 0.8042\n",
      "Training loss (for one batch) at step 40: 318.0128, Accuracy: 0.8230\n",
      "Training loss (for one batch) at step 50: 320.0998, Accuracy: 0.8350\n",
      "Training loss (for one batch) at step 60: 345.2927, Accuracy: 0.8386\n",
      "Training loss (for one batch) at step 70: 354.6570, Accuracy: 0.8303\n",
      "Training loss (for one batch) at step 80: 374.6231, Accuracy: 0.8197\n",
      "Training loss (for one batch) at step 90: 328.3063, Accuracy: 0.8177\n",
      "Training loss (for one batch) at step 100: 353.2996, Accuracy: 0.8202\n",
      "Training loss (for one batch) at step 110: 370.1639, Accuracy: 0.8184\n",
      "---- Training ----\n",
      "Training loss: 106.4004\n",
      "Training acc over epoch: 0.8172\n",
      "---- Validation ----\n",
      "Validation loss: 39.7642\n",
      "Validation acc: 0.7547\n",
      "Time taken: 10.57s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 370.9394, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 366.5783, Accuracy: 0.7493\n",
      "Training loss (for one batch) at step 20: 351.1860, Accuracy: 0.7879\n",
      "Training loss (for one batch) at step 30: 338.9006, Accuracy: 0.8067\n",
      "Training loss (for one batch) at step 40: 324.5674, Accuracy: 0.8220\n",
      "Training loss (for one batch) at step 50: 322.2205, Accuracy: 0.8353\n",
      "Training loss (for one batch) at step 60: 343.9614, Accuracy: 0.8395\n",
      "Training loss (for one batch) at step 70: 358.7789, Accuracy: 0.8316\n",
      "Training loss (for one batch) at step 80: 357.7408, Accuracy: 0.8212\n",
      "Training loss (for one batch) at step 90: 351.9608, Accuracy: 0.8169\n",
      "Training loss (for one batch) at step 100: 326.2143, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 110: 348.9797, Accuracy: 0.8199\n",
      "---- Training ----\n",
      "Training loss: 118.0437\n",
      "Training acc over epoch: 0.8185\n",
      "---- Validation ----\n",
      "Validation loss: 37.2496\n",
      "Validation acc: 0.7466\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 372.0605, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 363.8275, Accuracy: 0.7670\n",
      "Training loss (for one batch) at step 20: 332.8911, Accuracy: 0.7924\n",
      "Training loss (for one batch) at step 30: 337.4209, Accuracy: 0.8100\n",
      "Training loss (for one batch) at step 40: 327.4590, Accuracy: 0.8236\n",
      "Training loss (for one batch) at step 50: 321.7106, Accuracy: 0.8367\n",
      "Training loss (for one batch) at step 60: 325.9947, Accuracy: 0.8411\n",
      "Training loss (for one batch) at step 70: 353.4213, Accuracy: 0.8316\n",
      "Training loss (for one batch) at step 80: 344.5985, Accuracy: 0.8195\n",
      "Training loss (for one batch) at step 90: 353.4649, Accuracy: 0.8162\n",
      "Training loss (for one batch) at step 100: 338.0706, Accuracy: 0.8198\n",
      "Training loss (for one batch) at step 110: 360.6153, Accuracy: 0.8197\n",
      "---- Training ----\n",
      "Training loss: 115.4537\n",
      "Training acc over epoch: 0.8195\n",
      "---- Validation ----\n",
      "Validation loss: 42.2320\n",
      "Validation acc: 0.7491\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 375.5383, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 367.6824, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 20: 360.1146, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 30: 328.5500, Accuracy: 0.8054\n",
      "Training loss (for one batch) at step 40: 317.4390, Accuracy: 0.8249\n",
      "Training loss (for one batch) at step 50: 310.7433, Accuracy: 0.8356\n",
      "Training loss (for one batch) at step 60: 352.9456, Accuracy: 0.8399\n",
      "Training loss (for one batch) at step 70: 362.2357, Accuracy: 0.8321\n",
      "Training loss (for one batch) at step 80: 358.0640, Accuracy: 0.8222\n",
      "Training loss (for one batch) at step 90: 346.6568, Accuracy: 0.8181\n",
      "Training loss (for one batch) at step 100: 332.1241, Accuracy: 0.8193\n",
      "Training loss (for one batch) at step 110: 351.9159, Accuracy: 0.8200\n",
      "---- Training ----\n",
      "Training loss: 117.5843\n",
      "Training acc over epoch: 0.8174\n",
      "---- Validation ----\n",
      "Validation loss: 41.1792\n",
      "Validation acc: 0.7496\n",
      "Time taken: 10.61s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 362.6696, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 360.1385, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 20: 321.7542, Accuracy: 0.7772\n",
      "Training loss (for one batch) at step 30: 333.7192, Accuracy: 0.8034\n",
      "Training loss (for one batch) at step 40: 326.2400, Accuracy: 0.8175\n",
      "Training loss (for one batch) at step 50: 327.3902, Accuracy: 0.8310\n",
      "Training loss (for one batch) at step 60: 340.1484, Accuracy: 0.8382\n",
      "Training loss (for one batch) at step 70: 364.4446, Accuracy: 0.8286\n",
      "Training loss (for one batch) at step 80: 362.4913, Accuracy: 0.8177\n",
      "Training loss (for one batch) at step 90: 325.4615, Accuracy: 0.8146\n",
      "Training loss (for one batch) at step 100: 305.8917, Accuracy: 0.8171\n",
      "Training loss (for one batch) at step 110: 331.5092, Accuracy: 0.8176\n",
      "---- Training ----\n",
      "Training loss: 106.6329\n",
      "Training acc over epoch: 0.8167\n",
      "---- Validation ----\n",
      "Validation loss: 40.4998\n",
      "Validation acc: 0.7456\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 378.9538, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 361.2744, Accuracy: 0.7848\n",
      "Training loss (for one batch) at step 20: 331.5665, Accuracy: 0.7932\n",
      "Training loss (for one batch) at step 30: 335.0820, Accuracy: 0.8059\n",
      "Training loss (for one batch) at step 40: 312.8862, Accuracy: 0.8228\n",
      "Training loss (for one batch) at step 50: 303.5070, Accuracy: 0.8358\n",
      "Training loss (for one batch) at step 60: 324.8376, Accuracy: 0.8418\n",
      "Training loss (for one batch) at step 70: 363.2472, Accuracy: 0.8352\n",
      "Training loss (for one batch) at step 80: 346.3061, Accuracy: 0.8244\n",
      "Training loss (for one batch) at step 90: 338.1003, Accuracy: 0.8216\n",
      "Training loss (for one batch) at step 100: 324.5114, Accuracy: 0.8256\n",
      "Training loss (for one batch) at step 110: 341.2215, Accuracy: 0.8255\n",
      "---- Training ----\n",
      "Training loss: 125.6464\n",
      "Training acc over epoch: 0.8236\n",
      "---- Validation ----\n",
      "Validation loss: 49.6372\n",
      "Validation acc: 0.7316\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 396.3175, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 354.6197, Accuracy: 0.7599\n",
      "Training loss (for one batch) at step 20: 312.4393, Accuracy: 0.7894\n",
      "Training loss (for one batch) at step 30: 323.7689, Accuracy: 0.8080\n",
      "Training loss (for one batch) at step 40: 316.3606, Accuracy: 0.8243\n",
      "Training loss (for one batch) at step 50: 320.5305, Accuracy: 0.8353\n",
      "Training loss (for one batch) at step 60: 319.0961, Accuracy: 0.8386\n",
      "Training loss (for one batch) at step 70: 361.4963, Accuracy: 0.8298\n",
      "Training loss (for one batch) at step 80: 354.7092, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 90: 320.3404, Accuracy: 0.8169\n",
      "Training loss (for one batch) at step 100: 315.3810, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 110: 343.8021, Accuracy: 0.8219\n",
      "---- Training ----\n",
      "Training loss: 103.1750\n",
      "Training acc over epoch: 0.8208\n",
      "---- Validation ----\n",
      "Validation loss: 43.4502\n",
      "Validation acc: 0.7542\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 366.2456, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 338.9629, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 20: 331.3847, Accuracy: 0.7805\n",
      "Training loss (for one batch) at step 30: 323.5828, Accuracy: 0.8049\n",
      "Training loss (for one batch) at step 40: 314.6709, Accuracy: 0.8201\n",
      "Training loss (for one batch) at step 50: 311.3140, Accuracy: 0.8352\n",
      "Training loss (for one batch) at step 60: 344.7178, Accuracy: 0.8408\n",
      "Training loss (for one batch) at step 70: 344.4129, Accuracy: 0.8342\n",
      "Training loss (for one batch) at step 80: 365.1753, Accuracy: 0.8233\n",
      "Training loss (for one batch) at step 90: 331.5117, Accuracy: 0.8211\n",
      "Training loss (for one batch) at step 100: 323.5683, Accuracy: 0.8242\n",
      "Training loss (for one batch) at step 110: 349.5772, Accuracy: 0.8245\n",
      "---- Training ----\n",
      "Training loss: 106.9374\n",
      "Training acc over epoch: 0.8243\n",
      "---- Validation ----\n",
      "Validation loss: 41.4523\n",
      "Validation acc: 0.7523\n",
      "Time taken: 12.44s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 361.6476, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 354.7669, Accuracy: 0.7599\n",
      "Training loss (for one batch) at step 20: 311.6830, Accuracy: 0.7868\n",
      "Training loss (for one batch) at step 30: 323.2184, Accuracy: 0.8130\n",
      "Training loss (for one batch) at step 40: 309.4117, Accuracy: 0.8255\n",
      "Training loss (for one batch) at step 50: 317.0976, Accuracy: 0.8378\n",
      "Training loss (for one batch) at step 60: 326.8837, Accuracy: 0.8443\n",
      "Training loss (for one batch) at step 70: 353.7436, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 80: 358.0288, Accuracy: 0.8241\n",
      "Training loss (for one batch) at step 90: 326.8321, Accuracy: 0.8215\n",
      "Training loss (for one batch) at step 100: 312.5818, Accuracy: 0.8233\n",
      "Training loss (for one batch) at step 110: 332.3758, Accuracy: 0.8239\n",
      "---- Training ----\n",
      "Training loss: 118.3986\n",
      "Training acc over epoch: 0.8222\n",
      "---- Validation ----\n",
      "Validation loss: 43.9435\n",
      "Validation acc: 0.7560\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 346.1644, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 10: 374.5635, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 20: 335.7156, Accuracy: 0.7850\n",
      "Training loss (for one batch) at step 30: 316.2584, Accuracy: 0.8049\n",
      "Training loss (for one batch) at step 40: 311.7979, Accuracy: 0.8230\n",
      "Training loss (for one batch) at step 50: 296.4095, Accuracy: 0.8361\n",
      "Training loss (for one batch) at step 60: 332.5350, Accuracy: 0.8418\n",
      "Training loss (for one batch) at step 70: 364.6221, Accuracy: 0.8304\n",
      "Training loss (for one batch) at step 80: 385.7227, Accuracy: 0.8212\n",
      "Training loss (for one batch) at step 90: 345.2957, Accuracy: 0.8178\n",
      "Training loss (for one batch) at step 100: 323.1324, Accuracy: 0.8211\n",
      "Training loss (for one batch) at step 110: 329.1021, Accuracy: 0.8203\n",
      "---- Training ----\n",
      "Training loss: 114.1933\n",
      "Training acc over epoch: 0.8203\n",
      "---- Validation ----\n",
      "Validation loss: 41.3933\n",
      "Validation acc: 0.7593\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 363.9744, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 348.6427, Accuracy: 0.7663\n",
      "Training loss (for one batch) at step 20: 334.2546, Accuracy: 0.7839\n",
      "Training loss (for one batch) at step 30: 316.5455, Accuracy: 0.8075\n",
      "Training loss (for one batch) at step 40: 304.7553, Accuracy: 0.8201\n",
      "Training loss (for one batch) at step 50: 304.6109, Accuracy: 0.8323\n",
      "Training loss (for one batch) at step 60: 317.3999, Accuracy: 0.8375\n",
      "Training loss (for one batch) at step 70: 342.5754, Accuracy: 0.8285\n",
      "Training loss (for one batch) at step 80: 366.7319, Accuracy: 0.8182\n",
      "Training loss (for one batch) at step 90: 316.3135, Accuracy: 0.8156\n",
      "Training loss (for one batch) at step 100: 312.2424, Accuracy: 0.8169\n",
      "Training loss (for one batch) at step 110: 342.7457, Accuracy: 0.8185\n",
      "---- Training ----\n",
      "Training loss: 102.2947\n",
      "Training acc over epoch: 0.8176\n",
      "---- Validation ----\n",
      "Validation loss: 61.4650\n",
      "Validation acc: 0.7491\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 381.7169, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 347.2331, Accuracy: 0.7720\n",
      "Training loss (for one batch) at step 20: 317.7435, Accuracy: 0.7917\n",
      "Training loss (for one batch) at step 30: 309.9337, Accuracy: 0.8138\n",
      "Training loss (for one batch) at step 40: 315.3914, Accuracy: 0.8268\n",
      "Training loss (for one batch) at step 50: 293.7242, Accuracy: 0.8384\n",
      "Training loss (for one batch) at step 60: 339.0107, Accuracy: 0.8450\n",
      "Training loss (for one batch) at step 70: 325.6132, Accuracy: 0.8358\n",
      "Training loss (for one batch) at step 80: 355.6688, Accuracy: 0.8265\n",
      "Training loss (for one batch) at step 90: 304.5765, Accuracy: 0.8219\n",
      "Training loss (for one batch) at step 100: 318.3205, Accuracy: 0.8240\n",
      "Training loss (for one batch) at step 110: 331.0828, Accuracy: 0.8247\n",
      "---- Training ----\n",
      "Training loss: 99.1767\n",
      "Training acc over epoch: 0.8239\n",
      "---- Validation ----\n",
      "Validation loss: 36.4351\n",
      "Validation acc: 0.7577\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 388.2652, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 347.2099, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 20: 338.0627, Accuracy: 0.7958\n",
      "Training loss (for one batch) at step 30: 312.8520, Accuracy: 0.8077\n",
      "Training loss (for one batch) at step 40: 327.8167, Accuracy: 0.8207\n",
      "Training loss (for one batch) at step 50: 303.1951, Accuracy: 0.8362\n",
      "Training loss (for one batch) at step 60: 333.3931, Accuracy: 0.8404\n",
      "Training loss (for one batch) at step 70: 356.2050, Accuracy: 0.8307\n",
      "Training loss (for one batch) at step 80: 365.4681, Accuracy: 0.8170\n",
      "Training loss (for one batch) at step 90: 315.0529, Accuracy: 0.8149\n",
      "Training loss (for one batch) at step 100: 319.6410, Accuracy: 0.8177\n",
      "Training loss (for one batch) at step 110: 330.7470, Accuracy: 0.8173\n",
      "---- Training ----\n",
      "Training loss: 113.2876\n",
      "Training acc over epoch: 0.8164\n",
      "---- Validation ----\n",
      "Validation loss: 36.2446\n",
      "Validation acc: 0.7456\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 359.3351, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 348.2048, Accuracy: 0.7706\n",
      "Training loss (for one batch) at step 20: 319.3308, Accuracy: 0.7805\n",
      "Training loss (for one batch) at step 30: 314.0356, Accuracy: 0.8059\n",
      "Training loss (for one batch) at step 40: 316.3131, Accuracy: 0.8236\n",
      "Training loss (for one batch) at step 50: 301.7174, Accuracy: 0.8381\n",
      "Training loss (for one batch) at step 60: 319.3673, Accuracy: 0.8435\n",
      "Training loss (for one batch) at step 70: 335.4422, Accuracy: 0.8368\n",
      "Training loss (for one batch) at step 80: 327.9972, Accuracy: 0.8234\n",
      "Training loss (for one batch) at step 90: 322.3620, Accuracy: 0.8227\n",
      "Training loss (for one batch) at step 100: 332.8056, Accuracy: 0.8253\n",
      "Training loss (for one batch) at step 110: 328.3546, Accuracy: 0.8245\n",
      "---- Training ----\n",
      "Training loss: 122.0662\n",
      "Training acc over epoch: 0.8229\n",
      "---- Validation ----\n",
      "Validation loss: 35.4854\n",
      "Validation acc: 0.7601\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 347.0967, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 343.4808, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 20: 328.2220, Accuracy: 0.7842\n",
      "Training loss (for one batch) at step 30: 301.6400, Accuracy: 0.8085\n",
      "Training loss (for one batch) at step 40: 307.3929, Accuracy: 0.8256\n",
      "Training loss (for one batch) at step 50: 304.8093, Accuracy: 0.8390\n",
      "Training loss (for one batch) at step 60: 319.3691, Accuracy: 0.8423\n",
      "Training loss (for one batch) at step 70: 348.5461, Accuracy: 0.8333\n",
      "Training loss (for one batch) at step 80: 344.1904, Accuracy: 0.8239\n",
      "Training loss (for one batch) at step 90: 317.4842, Accuracy: 0.8208\n",
      "Training loss (for one batch) at step 100: 293.2648, Accuracy: 0.8229\n",
      "Training loss (for one batch) at step 110: 340.3965, Accuracy: 0.8223\n",
      "---- Training ----\n",
      "Training loss: 101.0716\n",
      "Training acc over epoch: 0.8222\n",
      "---- Validation ----\n",
      "Validation loss: 44.7827\n",
      "Validation acc: 0.7539\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 330.7657, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 316.3154, Accuracy: 0.7642\n",
      "Training loss (for one batch) at step 20: 312.9126, Accuracy: 0.7909\n",
      "Training loss (for one batch) at step 30: 310.8069, Accuracy: 0.8090\n",
      "Training loss (for one batch) at step 40: 308.2363, Accuracy: 0.8232\n",
      "Training loss (for one batch) at step 50: 307.7131, Accuracy: 0.8344\n",
      "Training loss (for one batch) at step 60: 326.3943, Accuracy: 0.8393\n",
      "Training loss (for one batch) at step 70: 356.6399, Accuracy: 0.8311\n",
      "Training loss (for one batch) at step 80: 348.2914, Accuracy: 0.8189\n",
      "Training loss (for one batch) at step 90: 299.0826, Accuracy: 0.8174\n",
      "Training loss (for one batch) at step 100: 298.4788, Accuracy: 0.8205\n",
      "Training loss (for one batch) at step 110: 322.1389, Accuracy: 0.8202\n",
      "---- Training ----\n",
      "Training loss: 97.5347\n",
      "Training acc over epoch: 0.8195\n",
      "---- Validation ----\n",
      "Validation loss: 46.7697\n",
      "Validation acc: 0.7485\n",
      "Time taken: 10.56s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 352.2326, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 352.1780, Accuracy: 0.7493\n",
      "Training loss (for one batch) at step 20: 326.9251, Accuracy: 0.7827\n",
      "Training loss (for one batch) at step 30: 305.0958, Accuracy: 0.8085\n",
      "Training loss (for one batch) at step 40: 305.3414, Accuracy: 0.8243\n",
      "Training loss (for one batch) at step 50: 306.7359, Accuracy: 0.8376\n",
      "Training loss (for one batch) at step 60: 316.4176, Accuracy: 0.8414\n",
      "Training loss (for one batch) at step 70: 331.5065, Accuracy: 0.8320\n",
      "Training loss (for one batch) at step 80: 366.0824, Accuracy: 0.8166\n",
      "Training loss (for one batch) at step 90: 321.7842, Accuracy: 0.8155\n",
      "Training loss (for one batch) at step 100: 292.0295, Accuracy: 0.8178\n",
      "Training loss (for one batch) at step 110: 328.9049, Accuracy: 0.8194\n",
      "---- Training ----\n",
      "Training loss: 101.5132\n",
      "Training acc over epoch: 0.8170\n",
      "---- Validation ----\n",
      "Validation loss: 36.2330\n",
      "Validation acc: 0.7413\n",
      "Time taken: 14.01s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 353.3466, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 354.4851, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 20: 308.5196, Accuracy: 0.7861\n",
      "Training loss (for one batch) at step 30: 297.2194, Accuracy: 0.8135\n",
      "Training loss (for one batch) at step 40: 298.3929, Accuracy: 0.8285\n",
      "Training loss (for one batch) at step 50: 306.0740, Accuracy: 0.8421\n",
      "Training loss (for one batch) at step 60: 316.4588, Accuracy: 0.8470\n",
      "Training loss (for one batch) at step 70: 339.2971, Accuracy: 0.8357\n",
      "Training loss (for one batch) at step 80: 336.3660, Accuracy: 0.8237\n",
      "Training loss (for one batch) at step 90: 306.0389, Accuracy: 0.8209\n",
      "Training loss (for one batch) at step 100: 319.1997, Accuracy: 0.8233\n",
      "Training loss (for one batch) at step 110: 334.9694, Accuracy: 0.8243\n",
      "---- Training ----\n",
      "Training loss: 106.2958\n",
      "Training acc over epoch: 0.8237\n",
      "---- Validation ----\n",
      "Validation loss: 50.6410\n",
      "Validation acc: 0.7456\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 334.6368, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 328.4236, Accuracy: 0.7699\n",
      "Training loss (for one batch) at step 20: 306.0766, Accuracy: 0.7917\n",
      "Training loss (for one batch) at step 30: 324.7059, Accuracy: 0.8130\n",
      "Training loss (for one batch) at step 40: 299.3834, Accuracy: 0.8277\n",
      "Training loss (for one batch) at step 50: 293.1473, Accuracy: 0.8405\n",
      "Training loss (for one batch) at step 60: 313.2107, Accuracy: 0.8472\n",
      "Training loss (for one batch) at step 70: 334.1437, Accuracy: 0.8381\n",
      "Training loss (for one batch) at step 80: 354.5043, Accuracy: 0.8267\n",
      "Training loss (for one batch) at step 90: 301.9228, Accuracy: 0.8222\n",
      "Training loss (for one batch) at step 100: 324.3535, Accuracy: 0.8242\n",
      "Training loss (for one batch) at step 110: 318.0246, Accuracy: 0.8240\n",
      "---- Training ----\n",
      "Training loss: 105.9398\n",
      "Training acc over epoch: 0.8233\n",
      "---- Validation ----\n",
      "Validation loss: 63.1583\n",
      "Validation acc: 0.7574\n",
      "Time taken: 10.58s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 352.8362, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 344.2844, Accuracy: 0.7663\n",
      "Training loss (for one batch) at step 20: 302.4887, Accuracy: 0.7857\n",
      "Training loss (for one batch) at step 30: 298.4220, Accuracy: 0.8130\n",
      "Training loss (for one batch) at step 40: 292.3301, Accuracy: 0.8264\n",
      "Training loss (for one batch) at step 50: 304.1712, Accuracy: 0.8398\n",
      "Training loss (for one batch) at step 60: 299.4863, Accuracy: 0.8431\n",
      "Training loss (for one batch) at step 70: 352.6017, Accuracy: 0.8338\n",
      "Training loss (for one batch) at step 80: 335.7269, Accuracy: 0.8238\n",
      "Training loss (for one batch) at step 90: 326.3183, Accuracy: 0.8214\n",
      "Training loss (for one batch) at step 100: 308.6988, Accuracy: 0.8242\n",
      "Training loss (for one batch) at step 110: 313.2170, Accuracy: 0.8250\n",
      "---- Training ----\n",
      "Training loss: 115.8582\n",
      "Training acc over epoch: 0.8229\n",
      "---- Validation ----\n",
      "Validation loss: 47.6178\n",
      "Validation acc: 0.7499\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 350.0381, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 336.2528, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 20: 318.5277, Accuracy: 0.7857\n",
      "Training loss (for one batch) at step 30: 294.8686, Accuracy: 0.8032\n",
      "Training loss (for one batch) at step 40: 292.7353, Accuracy: 0.8224\n",
      "Training loss (for one batch) at step 50: 281.5065, Accuracy: 0.8339\n",
      "Training loss (for one batch) at step 60: 306.0988, Accuracy: 0.8399\n",
      "Training loss (for one batch) at step 70: 348.0794, Accuracy: 0.8318\n",
      "Training loss (for one batch) at step 80: 332.7776, Accuracy: 0.8197\n",
      "Training loss (for one batch) at step 90: 320.2444, Accuracy: 0.8175\n",
      "Training loss (for one batch) at step 100: 308.6192, Accuracy: 0.8195\n",
      "Training loss (for one batch) at step 110: 317.9490, Accuracy: 0.8186\n",
      "---- Training ----\n",
      "Training loss: 94.0118\n",
      "Training acc over epoch: 0.8189\n",
      "---- Validation ----\n",
      "Validation loss: 70.6120\n",
      "Validation acc: 0.7501\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 361.6239, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 348.8041, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 20: 313.9098, Accuracy: 0.7842\n",
      "Training loss (for one batch) at step 30: 298.1758, Accuracy: 0.8133\n",
      "Training loss (for one batch) at step 40: 302.2694, Accuracy: 0.8277\n",
      "Training loss (for one batch) at step 50: 290.1799, Accuracy: 0.8416\n",
      "Training loss (for one batch) at step 60: 306.2951, Accuracy: 0.8458\n",
      "Training loss (for one batch) at step 70: 326.2128, Accuracy: 0.8369\n",
      "Training loss (for one batch) at step 80: 327.8862, Accuracy: 0.8246\n",
      "Training loss (for one batch) at step 90: 298.6577, Accuracy: 0.8213\n",
      "Training loss (for one batch) at step 100: 297.8810, Accuracy: 0.8235\n",
      "Training loss (for one batch) at step 110: 307.0214, Accuracy: 0.8233\n",
      "---- Training ----\n",
      "Training loss: 103.2814\n",
      "Training acc over epoch: 0.8221\n",
      "---- Validation ----\n",
      "Validation loss: 37.3838\n",
      "Validation acc: 0.7585\n",
      "Time taken: 12.55s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 344.5638, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 352.3285, Accuracy: 0.7493\n",
      "Training loss (for one batch) at step 20: 321.8863, Accuracy: 0.7779\n",
      "Training loss (for one batch) at step 30: 299.5392, Accuracy: 0.8029\n",
      "Training loss (for one batch) at step 40: 309.0343, Accuracy: 0.8178\n",
      "Training loss (for one batch) at step 50: 293.1132, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 60: 303.0974, Accuracy: 0.8432\n",
      "Training loss (for one batch) at step 70: 344.7081, Accuracy: 0.8318\n",
      "Training loss (for one batch) at step 80: 353.1783, Accuracy: 0.8198\n",
      "Training loss (for one batch) at step 90: 311.0595, Accuracy: 0.8177\n",
      "Training loss (for one batch) at step 100: 334.9231, Accuracy: 0.8207\n",
      "Training loss (for one batch) at step 110: 335.4222, Accuracy: 0.8197\n",
      "---- Training ----\n",
      "Training loss: 104.8392\n",
      "Training acc over epoch: 0.8195\n",
      "---- Validation ----\n",
      "Validation loss: 47.5229\n",
      "Validation acc: 0.7531\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 357.4435, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 335.8205, Accuracy: 0.7472\n",
      "Training loss (for one batch) at step 20: 302.4338, Accuracy: 0.7857\n",
      "Training loss (for one batch) at step 30: 299.3743, Accuracy: 0.8092\n",
      "Training loss (for one batch) at step 40: 299.1157, Accuracy: 0.8245\n",
      "Training loss (for one batch) at step 50: 299.5136, Accuracy: 0.8390\n",
      "Training loss (for one batch) at step 60: 302.1343, Accuracy: 0.8440\n",
      "Training loss (for one batch) at step 70: 318.0751, Accuracy: 0.8344\n",
      "Training loss (for one batch) at step 80: 329.6154, Accuracy: 0.8232\n",
      "Training loss (for one batch) at step 90: 310.2917, Accuracy: 0.8201\n",
      "Training loss (for one batch) at step 100: 297.6711, Accuracy: 0.8245\n",
      "Training loss (for one batch) at step 110: 341.2685, Accuracy: 0.8232\n",
      "---- Training ----\n",
      "Training loss: 104.9760\n",
      "Training acc over epoch: 0.8223\n",
      "---- Validation ----\n",
      "Validation loss: 40.4662\n",
      "Validation acc: 0.7488\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 357.5861, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 329.8867, Accuracy: 0.7464\n",
      "Training loss (for one batch) at step 20: 304.4415, Accuracy: 0.7894\n",
      "Training loss (for one batch) at step 30: 295.8333, Accuracy: 0.8095\n",
      "Training loss (for one batch) at step 40: 308.7498, Accuracy: 0.8251\n",
      "Training loss (for one batch) at step 50: 283.7706, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 60: 320.9246, Accuracy: 0.8425\n",
      "Training loss (for one batch) at step 70: 329.1270, Accuracy: 0.8332\n",
      "Training loss (for one batch) at step 80: 379.7513, Accuracy: 0.8193\n",
      "Training loss (for one batch) at step 90: 298.1628, Accuracy: 0.8175\n",
      "Training loss (for one batch) at step 100: 304.5425, Accuracy: 0.8219\n",
      "Training loss (for one batch) at step 110: 329.7816, Accuracy: 0.8224\n",
      "---- Training ----\n",
      "Training loss: 121.1892\n",
      "Training acc over epoch: 0.8207\n",
      "---- Validation ----\n",
      "Validation loss: 43.2386\n",
      "Validation acc: 0.7485\n",
      "Time taken: 10.82s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 351.8067, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 331.9541, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 20: 337.1339, Accuracy: 0.7909\n",
      "Training loss (for one batch) at step 30: 321.6488, Accuracy: 0.8138\n",
      "Training loss (for one batch) at step 40: 275.3863, Accuracy: 0.8296\n",
      "Training loss (for one batch) at step 50: 287.4164, Accuracy: 0.8433\n",
      "Training loss (for one batch) at step 60: 313.8232, Accuracy: 0.8481\n",
      "Training loss (for one batch) at step 70: 324.7836, Accuracy: 0.8371\n",
      "Training loss (for one batch) at step 80: 331.4514, Accuracy: 0.8258\n",
      "Training loss (for one batch) at step 90: 329.1410, Accuracy: 0.8235\n",
      "Training loss (for one batch) at step 100: 301.5001, Accuracy: 0.8267\n",
      "Training loss (for one batch) at step 110: 314.7163, Accuracy: 0.8262\n",
      "---- Training ----\n",
      "Training loss: 102.8833\n",
      "Training acc over epoch: 0.8243\n",
      "---- Validation ----\n",
      "Validation loss: 35.7658\n",
      "Validation acc: 0.7544\n",
      "Time taken: 10.51s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 342.2186, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 330.0678, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 20: 320.6488, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 30: 295.8510, Accuracy: 0.8077\n",
      "Training loss (for one batch) at step 40: 285.2979, Accuracy: 0.8251\n",
      "Training loss (for one batch) at step 50: 285.1694, Accuracy: 0.8392\n",
      "Training loss (for one batch) at step 60: 308.7406, Accuracy: 0.8449\n",
      "Training loss (for one batch) at step 70: 339.1673, Accuracy: 0.8349\n",
      "Training loss (for one batch) at step 80: 357.8484, Accuracy: 0.8247\n",
      "Training loss (for one batch) at step 90: 315.5677, Accuracy: 0.8221\n",
      "Training loss (for one batch) at step 100: 297.8317, Accuracy: 0.8254\n",
      "Training loss (for one batch) at step 110: 322.1620, Accuracy: 0.8256\n",
      "---- Training ----\n",
      "Training loss: 106.7845\n",
      "Training acc over epoch: 0.8236\n",
      "---- Validation ----\n",
      "Validation loss: 50.0797\n",
      "Validation acc: 0.7555\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 335.3469, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 345.3737, Accuracy: 0.7514\n",
      "Training loss (for one batch) at step 20: 301.8850, Accuracy: 0.7872\n",
      "Training loss (for one batch) at step 30: 304.4385, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 40: 305.2067, Accuracy: 0.8262\n",
      "Training loss (for one batch) at step 50: 284.4881, Accuracy: 0.8399\n",
      "Training loss (for one batch) at step 60: 312.4247, Accuracy: 0.8458\n",
      "Training loss (for one batch) at step 70: 319.6581, Accuracy: 0.8358\n",
      "Training loss (for one batch) at step 80: 333.7627, Accuracy: 0.8244\n",
      "Training loss (for one batch) at step 90: 289.1376, Accuracy: 0.8219\n",
      "Training loss (for one batch) at step 100: 310.7908, Accuracy: 0.8246\n",
      "Training loss (for one batch) at step 110: 290.8887, Accuracy: 0.8247\n",
      "---- Training ----\n",
      "Training loss: 97.7509\n",
      "Training acc over epoch: 0.8233\n",
      "---- Validation ----\n",
      "Validation loss: 51.4505\n",
      "Validation acc: 0.7442\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 370.8832, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 326.8348, Accuracy: 0.7585\n",
      "Training loss (for one batch) at step 20: 296.8986, Accuracy: 0.7835\n",
      "Training loss (for one batch) at step 30: 301.0505, Accuracy: 0.8095\n",
      "Training loss (for one batch) at step 40: 304.5845, Accuracy: 0.8247\n",
      "Training loss (for one batch) at step 50: 278.2129, Accuracy: 0.8361\n",
      "Training loss (for one batch) at step 60: 298.9541, Accuracy: 0.8441\n",
      "Training loss (for one batch) at step 70: 331.0087, Accuracy: 0.8343\n",
      "Training loss (for one batch) at step 80: 328.8635, Accuracy: 0.8219\n",
      "Training loss (for one batch) at step 90: 309.3779, Accuracy: 0.8212\n",
      "Training loss (for one batch) at step 100: 297.9075, Accuracy: 0.8252\n",
      "Training loss (for one batch) at step 110: 313.7061, Accuracy: 0.8262\n",
      "---- Training ----\n",
      "Training loss: 111.1526\n",
      "Training acc over epoch: 0.8238\n",
      "---- Validation ----\n",
      "Validation loss: 48.1433\n",
      "Validation acc: 0.7426\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 350.5192, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 333.2183, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 20: 304.0438, Accuracy: 0.7894\n",
      "Training loss (for one batch) at step 30: 296.1173, Accuracy: 0.8097\n",
      "Training loss (for one batch) at step 40: 291.3715, Accuracy: 0.8247\n",
      "Training loss (for one batch) at step 50: 284.3071, Accuracy: 0.8393\n",
      "Training loss (for one batch) at step 60: 304.5012, Accuracy: 0.8418\n",
      "Training loss (for one batch) at step 70: 343.3571, Accuracy: 0.8338\n",
      "Training loss (for one batch) at step 80: 325.8602, Accuracy: 0.8220\n",
      "Training loss (for one batch) at step 90: 302.4828, Accuracy: 0.8215\n",
      "Training loss (for one batch) at step 100: 296.7640, Accuracy: 0.8223\n",
      "Training loss (for one batch) at step 110: 319.9289, Accuracy: 0.8226\n",
      "---- Training ----\n",
      "Training loss: 104.6583\n",
      "Training acc over epoch: 0.8227\n",
      "---- Validation ----\n",
      "Validation loss: 38.0748\n",
      "Validation acc: 0.7413\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 347.0364, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 335.6385, Accuracy: 0.7379\n",
      "Training loss (for one batch) at step 20: 298.2733, Accuracy: 0.7719\n",
      "Training loss (for one batch) at step 30: 273.2744, Accuracy: 0.7994\n",
      "Training loss (for one batch) at step 40: 277.4852, Accuracy: 0.8182\n",
      "Training loss (for one batch) at step 50: 272.9963, Accuracy: 0.8344\n",
      "Training loss (for one batch) at step 60: 302.5394, Accuracy: 0.8393\n",
      "Training loss (for one batch) at step 70: 326.4399, Accuracy: 0.8331\n",
      "Training loss (for one batch) at step 80: 330.9579, Accuracy: 0.8212\n",
      "Training loss (for one batch) at step 90: 350.2650, Accuracy: 0.8195\n",
      "Training loss (for one batch) at step 100: 286.5828, Accuracy: 0.8231\n",
      "Training loss (for one batch) at step 110: 323.4702, Accuracy: 0.8225\n",
      "---- Training ----\n",
      "Training loss: 98.9962\n",
      "Training acc over epoch: 0.8213\n",
      "---- Validation ----\n",
      "Validation loss: 53.7376\n",
      "Validation acc: 0.7485\n",
      "Time taken: 10.80s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 341.3614, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 354.2206, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 20: 308.9084, Accuracy: 0.7943\n",
      "Training loss (for one batch) at step 30: 313.6077, Accuracy: 0.8130\n",
      "Training loss (for one batch) at step 40: 282.7379, Accuracy: 0.8260\n",
      "Training loss (for one batch) at step 50: 283.1216, Accuracy: 0.8373\n",
      "Training loss (for one batch) at step 60: 293.0047, Accuracy: 0.8436\n",
      "Training loss (for one batch) at step 70: 335.8236, Accuracy: 0.8324\n",
      "Training loss (for one batch) at step 80: 314.3539, Accuracy: 0.8220\n",
      "Training loss (for one batch) at step 90: 302.5245, Accuracy: 0.8206\n",
      "Training loss (for one batch) at step 100: 296.6509, Accuracy: 0.8219\n",
      "Training loss (for one batch) at step 110: 311.6600, Accuracy: 0.8226\n",
      "---- Training ----\n",
      "Training loss: 107.8851\n",
      "Training acc over epoch: 0.8205\n",
      "---- Validation ----\n",
      "Validation loss: 42.9307\n",
      "Validation acc: 0.7458\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 329.7258, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 324.6295, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 20: 311.8577, Accuracy: 0.7853\n",
      "Training loss (for one batch) at step 30: 302.7807, Accuracy: 0.8148\n",
      "Training loss (for one batch) at step 40: 286.9822, Accuracy: 0.8304\n",
      "Training loss (for one batch) at step 50: 297.0301, Accuracy: 0.8421\n",
      "Training loss (for one batch) at step 60: 290.6440, Accuracy: 0.8490\n",
      "Training loss (for one batch) at step 70: 330.4678, Accuracy: 0.8375\n",
      "Training loss (for one batch) at step 80: 343.8084, Accuracy: 0.8261\n",
      "Training loss (for one batch) at step 90: 301.9761, Accuracy: 0.8225\n",
      "Training loss (for one batch) at step 100: 287.5121, Accuracy: 0.8256\n",
      "Training loss (for one batch) at step 110: 301.6307, Accuracy: 0.8256\n",
      "---- Training ----\n",
      "Training loss: 92.4629\n",
      "Training acc over epoch: 0.8244\n",
      "---- Validation ----\n",
      "Validation loss: 40.9914\n",
      "Validation acc: 0.7509\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 335.8146, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 357.0018, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 20: 314.8808, Accuracy: 0.7850\n",
      "Training loss (for one batch) at step 30: 314.3443, Accuracy: 0.8077\n",
      "Training loss (for one batch) at step 40: 293.1180, Accuracy: 0.8247\n",
      "Training loss (for one batch) at step 50: 277.8895, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 60: 291.3833, Accuracy: 0.8432\n",
      "Training loss (for one batch) at step 70: 313.6487, Accuracy: 0.8378\n",
      "Training loss (for one batch) at step 80: 332.6039, Accuracy: 0.8248\n",
      "Training loss (for one batch) at step 90: 293.4030, Accuracy: 0.8218\n",
      "Training loss (for one batch) at step 100: 279.0491, Accuracy: 0.8260\n",
      "Training loss (for one batch) at step 110: 325.4644, Accuracy: 0.8262\n",
      "---- Training ----\n",
      "Training loss: 102.2020\n",
      "Training acc over epoch: 0.8242\n",
      "---- Validation ----\n",
      "Validation loss: 45.8016\n",
      "Validation acc: 0.7356\n",
      "Time taken: 10.68s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 357.4642, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 333.1733, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 20: 297.0415, Accuracy: 0.7742\n",
      "Training loss (for one batch) at step 30: 286.5706, Accuracy: 0.8017\n",
      "Training loss (for one batch) at step 40: 277.6673, Accuracy: 0.8209\n",
      "Training loss (for one batch) at step 50: 282.1468, Accuracy: 0.8362\n",
      "Training loss (for one batch) at step 60: 290.5281, Accuracy: 0.8405\n",
      "Training loss (for one batch) at step 70: 311.9149, Accuracy: 0.8334\n",
      "Training loss (for one batch) at step 80: 342.8015, Accuracy: 0.8210\n",
      "Training loss (for one batch) at step 90: 286.4286, Accuracy: 0.8199\n",
      "Training loss (for one batch) at step 100: 306.3005, Accuracy: 0.8238\n",
      "Training loss (for one batch) at step 110: 295.9534, Accuracy: 0.8232\n",
      "---- Training ----\n",
      "Training loss: 102.0671\n",
      "Training acc over epoch: 0.8229\n",
      "---- Validation ----\n",
      "Validation loss: 53.0810\n",
      "Validation acc: 0.7429\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 355.6204, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 341.9125, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 20: 291.6127, Accuracy: 0.7827\n",
      "Training loss (for one batch) at step 30: 296.5197, Accuracy: 0.8077\n",
      "Training loss (for one batch) at step 40: 282.5999, Accuracy: 0.8243\n",
      "Training loss (for one batch) at step 50: 278.3233, Accuracy: 0.8387\n",
      "Training loss (for one batch) at step 60: 303.8217, Accuracy: 0.8461\n",
      "Training loss (for one batch) at step 70: 333.7458, Accuracy: 0.8369\n",
      "Training loss (for one batch) at step 80: 343.7222, Accuracy: 0.8242\n",
      "Training loss (for one batch) at step 90: 292.9313, Accuracy: 0.8227\n",
      "Training loss (for one batch) at step 100: 270.0095, Accuracy: 0.8250\n",
      "Training loss (for one batch) at step 110: 312.7048, Accuracy: 0.8253\n",
      "---- Training ----\n",
      "Training loss: 100.5083\n",
      "Training acc over epoch: 0.8249\n",
      "---- Validation ----\n",
      "Validation loss: 41.0854\n",
      "Validation acc: 0.7448\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 340.6150, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 338.3034, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 20: 301.2872, Accuracy: 0.7872\n",
      "Training loss (for one batch) at step 30: 278.2448, Accuracy: 0.8133\n",
      "Training loss (for one batch) at step 40: 280.0587, Accuracy: 0.8295\n",
      "Training loss (for one batch) at step 50: 280.8379, Accuracy: 0.8398\n",
      "Training loss (for one batch) at step 60: 293.6216, Accuracy: 0.8454\n",
      "Training loss (for one batch) at step 70: 315.8970, Accuracy: 0.8369\n",
      "Training loss (for one batch) at step 80: 328.5818, Accuracy: 0.8266\n",
      "Training loss (for one batch) at step 90: 310.2616, Accuracy: 0.8251\n",
      "Training loss (for one batch) at step 100: 293.5121, Accuracy: 0.8267\n",
      "Training loss (for one batch) at step 110: 318.4594, Accuracy: 0.8274\n",
      "---- Training ----\n",
      "Training loss: 97.3726\n",
      "Training acc over epoch: 0.8260\n",
      "---- Validation ----\n",
      "Validation loss: 51.4108\n",
      "Validation acc: 0.7574\n",
      "Time taken: 10.60s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 327.1828, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 324.9132, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 20: 294.5283, Accuracy: 0.7872\n",
      "Training loss (for one batch) at step 30: 283.4826, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 40: 281.0723, Accuracy: 0.8287\n",
      "Training loss (for one batch) at step 50: 283.8263, Accuracy: 0.8405\n",
      "Training loss (for one batch) at step 60: 320.2273, Accuracy: 0.8453\n",
      "Training loss (for one batch) at step 70: 331.9500, Accuracy: 0.8347\n",
      "Training loss (for one batch) at step 80: 327.2084, Accuracy: 0.8217\n",
      "Training loss (for one batch) at step 90: 300.4495, Accuracy: 0.8204\n",
      "Training loss (for one batch) at step 100: 278.5600, Accuracy: 0.8250\n",
      "Training loss (for one batch) at step 110: 302.2254, Accuracy: 0.8241\n",
      "---- Training ----\n",
      "Training loss: 105.7899\n",
      "Training acc over epoch: 0.8227\n",
      "---- Validation ----\n",
      "Validation loss: 53.5604\n",
      "Validation acc: 0.7555\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 337.9631, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 333.3672, Accuracy: 0.7607\n",
      "Training loss (for one batch) at step 20: 298.0425, Accuracy: 0.7935\n",
      "Training loss (for one batch) at step 30: 300.6178, Accuracy: 0.8158\n",
      "Training loss (for one batch) at step 40: 280.1114, Accuracy: 0.8293\n",
      "Training loss (for one batch) at step 50: 289.5530, Accuracy: 0.8421\n",
      "Training loss (for one batch) at step 60: 290.4359, Accuracy: 0.8455\n",
      "Training loss (for one batch) at step 70: 310.5044, Accuracy: 0.8357\n",
      "Training loss (for one batch) at step 80: 318.5198, Accuracy: 0.8253\n",
      "Training loss (for one batch) at step 90: 317.4908, Accuracy: 0.8215\n",
      "Training loss (for one batch) at step 100: 303.7722, Accuracy: 0.8259\n",
      "Training loss (for one batch) at step 110: 313.8478, Accuracy: 0.8266\n",
      "---- Training ----\n",
      "Training loss: 93.0429\n",
      "Training acc over epoch: 0.8245\n",
      "---- Validation ----\n",
      "Validation loss: 58.3475\n",
      "Validation acc: 0.7499\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 336.7256, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 334.5052, Accuracy: 0.7564\n",
      "Training loss (for one batch) at step 20: 297.1320, Accuracy: 0.7924\n",
      "Training loss (for one batch) at step 30: 283.9522, Accuracy: 0.8145\n",
      "Training loss (for one batch) at step 40: 279.3545, Accuracy: 0.8279\n",
      "Training loss (for one batch) at step 50: 285.6550, Accuracy: 0.8393\n",
      "Training loss (for one batch) at step 60: 288.6763, Accuracy: 0.8453\n",
      "Training loss (for one batch) at step 70: 324.2521, Accuracy: 0.8355\n",
      "Training loss (for one batch) at step 80: 333.9165, Accuracy: 0.8243\n",
      "Training loss (for one batch) at step 90: 276.6666, Accuracy: 0.8222\n",
      "Training loss (for one batch) at step 100: 295.0539, Accuracy: 0.8253\n",
      "Training loss (for one batch) at step 110: 301.2996, Accuracy: 0.8250\n",
      "---- Training ----\n",
      "Training loss: 114.7238\n",
      "Training acc over epoch: 0.8238\n",
      "---- Validation ----\n",
      "Validation loss: 42.3010\n",
      "Validation acc: 0.7472\n",
      "Time taken: 10.61s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 341.9442, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 314.8531, Accuracy: 0.7628\n",
      "Training loss (for one batch) at step 20: 292.4405, Accuracy: 0.7868\n",
      "Training loss (for one batch) at step 30: 268.1348, Accuracy: 0.8138\n",
      "Training loss (for one batch) at step 40: 278.2174, Accuracy: 0.8295\n",
      "Training loss (for one batch) at step 50: 272.5007, Accuracy: 0.8379\n",
      "Training loss (for one batch) at step 60: 308.2657, Accuracy: 0.8448\n",
      "Training loss (for one batch) at step 70: 316.6707, Accuracy: 0.8360\n",
      "Training loss (for one batch) at step 80: 327.6093, Accuracy: 0.8243\n",
      "Training loss (for one batch) at step 90: 280.2380, Accuracy: 0.8224\n",
      "Training loss (for one batch) at step 100: 265.5879, Accuracy: 0.8248\n",
      "Training loss (for one batch) at step 110: 315.0498, Accuracy: 0.8262\n",
      "---- Training ----\n",
      "Training loss: 93.6879\n",
      "Training acc over epoch: 0.8247\n",
      "---- Validation ----\n",
      "Validation loss: 44.9753\n",
      "Validation acc: 0.7523\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 338.8163, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 327.9915, Accuracy: 0.7628\n",
      "Training loss (for one batch) at step 20: 274.1550, Accuracy: 0.7987\n",
      "Training loss (for one batch) at step 30: 293.5508, Accuracy: 0.8145\n",
      "Training loss (for one batch) at step 40: 278.1570, Accuracy: 0.8333\n",
      "Training loss (for one batch) at step 50: 284.4945, Accuracy: 0.8415\n",
      "Training loss (for one batch) at step 60: 283.6063, Accuracy: 0.8462\n",
      "Training loss (for one batch) at step 70: 319.1181, Accuracy: 0.8368\n",
      "Training loss (for one batch) at step 80: 319.9652, Accuracy: 0.8247\n",
      "Training loss (for one batch) at step 90: 293.0325, Accuracy: 0.8230\n",
      "Training loss (for one batch) at step 100: 268.6782, Accuracy: 0.8279\n",
      "Training loss (for one batch) at step 110: 306.9818, Accuracy: 0.8266\n",
      "---- Training ----\n",
      "Training loss: 110.2092\n",
      "Training acc over epoch: 0.8263\n",
      "---- Validation ----\n",
      "Validation loss: 49.6894\n",
      "Validation acc: 0.7485\n",
      "Time taken: 10.54s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 336.3461, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 329.1062, Accuracy: 0.7713\n",
      "Training loss (for one batch) at step 20: 297.9961, Accuracy: 0.7879\n",
      "Training loss (for one batch) at step 30: 284.8698, Accuracy: 0.8095\n",
      "Training loss (for one batch) at step 40: 282.0287, Accuracy: 0.8264\n",
      "Training loss (for one batch) at step 50: 286.6791, Accuracy: 0.8408\n",
      "Training loss (for one batch) at step 60: 286.6631, Accuracy: 0.8470\n",
      "Training loss (for one batch) at step 70: 311.5095, Accuracy: 0.8364\n",
      "Training loss (for one batch) at step 80: 335.0899, Accuracy: 0.8261\n",
      "Training loss (for one batch) at step 90: 284.2261, Accuracy: 0.8228\n",
      "Training loss (for one batch) at step 100: 287.4443, Accuracy: 0.8245\n",
      "Training loss (for one batch) at step 110: 278.5795, Accuracy: 0.8258\n",
      "---- Training ----\n",
      "Training loss: 114.8626\n",
      "Training acc over epoch: 0.8256\n",
      "---- Validation ----\n",
      "Validation loss: 55.3885\n",
      "Validation acc: 0.7493\n",
      "Time taken: 10.85s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 319.8779, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 328.1945, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 20: 292.2580, Accuracy: 0.7906\n",
      "Training loss (for one batch) at step 30: 281.2789, Accuracy: 0.8163\n",
      "Training loss (for one batch) at step 40: 293.3220, Accuracy: 0.8314\n",
      "Training loss (for one batch) at step 50: 275.1925, Accuracy: 0.8467\n",
      "Training loss (for one batch) at step 60: 308.5487, Accuracy: 0.8494\n",
      "Training loss (for one batch) at step 70: 306.2502, Accuracy: 0.8435\n",
      "Training loss (for one batch) at step 80: 336.5657, Accuracy: 0.8292\n",
      "Training loss (for one batch) at step 90: 300.2685, Accuracy: 0.8250\n",
      "Training loss (for one batch) at step 100: 277.6044, Accuracy: 0.8291\n",
      "Training loss (for one batch) at step 110: 298.1481, Accuracy: 0.8291\n",
      "---- Training ----\n",
      "Training loss: 93.3615\n",
      "Training acc over epoch: 0.8274\n",
      "---- Validation ----\n",
      "Validation loss: 43.1288\n",
      "Validation acc: 0.7491\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 331.9883, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 334.8645, Accuracy: 0.7607\n",
      "Training loss (for one batch) at step 20: 292.5495, Accuracy: 0.7917\n",
      "Training loss (for one batch) at step 30: 286.9328, Accuracy: 0.8178\n",
      "Training loss (for one batch) at step 40: 280.3051, Accuracy: 0.8296\n",
      "Training loss (for one batch) at step 50: 291.7374, Accuracy: 0.8422\n",
      "Training loss (for one batch) at step 60: 291.4149, Accuracy: 0.8470\n",
      "Training loss (for one batch) at step 70: 326.8793, Accuracy: 0.8370\n",
      "Training loss (for one batch) at step 80: 326.1949, Accuracy: 0.8246\n",
      "Training loss (for one batch) at step 90: 282.1956, Accuracy: 0.8222\n",
      "Training loss (for one batch) at step 100: 277.0163, Accuracy: 0.8252\n",
      "Training loss (for one batch) at step 110: 286.1780, Accuracy: 0.8251\n",
      "---- Training ----\n",
      "Training loss: 93.9773\n",
      "Training acc over epoch: 0.8247\n",
      "---- Validation ----\n",
      "Validation loss: 51.1080\n",
      "Validation acc: 0.7493\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 333.3346, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 345.6396, Accuracy: 0.7479\n",
      "Training loss (for one batch) at step 20: 294.7013, Accuracy: 0.7831\n",
      "Training loss (for one batch) at step 30: 287.2111, Accuracy: 0.8107\n",
      "Training loss (for one batch) at step 40: 286.0562, Accuracy: 0.8276\n",
      "Training loss (for one batch) at step 50: 286.1376, Accuracy: 0.8387\n",
      "Training loss (for one batch) at step 60: 283.1918, Accuracy: 0.8436\n",
      "Training loss (for one batch) at step 70: 326.2195, Accuracy: 0.8368\n",
      "Training loss (for one batch) at step 80: 322.2606, Accuracy: 0.8258\n",
      "Training loss (for one batch) at step 90: 296.4906, Accuracy: 0.8230\n",
      "Training loss (for one batch) at step 100: 286.3215, Accuracy: 0.8267\n",
      "Training loss (for one batch) at step 110: 327.2445, Accuracy: 0.8250\n",
      "---- Training ----\n",
      "Training loss: 107.5773\n",
      "Training acc over epoch: 0.8231\n",
      "---- Validation ----\n",
      "Validation loss: 53.9753\n",
      "Validation acc: 0.7531\n",
      "Time taken: 10.78s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 343.3959, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 333.0626, Accuracy: 0.7585\n",
      "Training loss (for one batch) at step 20: 299.8412, Accuracy: 0.7835\n",
      "Training loss (for one batch) at step 30: 269.6631, Accuracy: 0.8100\n",
      "Training loss (for one batch) at step 40: 270.7092, Accuracy: 0.8255\n",
      "Training loss (for one batch) at step 50: 277.6566, Accuracy: 0.8395\n",
      "Training loss (for one batch) at step 60: 304.1457, Accuracy: 0.8461\n",
      "Training loss (for one batch) at step 70: 323.0469, Accuracy: 0.8362\n",
      "Training loss (for one batch) at step 80: 320.0284, Accuracy: 0.8250\n",
      "Training loss (for one batch) at step 90: 277.3277, Accuracy: 0.8231\n",
      "Training loss (for one batch) at step 100: 282.0826, Accuracy: 0.8250\n",
      "Training loss (for one batch) at step 110: 311.6523, Accuracy: 0.8261\n",
      "---- Training ----\n",
      "Training loss: 96.9697\n",
      "Training acc over epoch: 0.8250\n",
      "---- Validation ----\n",
      "Validation loss: 52.1428\n",
      "Validation acc: 0.7517\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 339.9220, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 345.7690, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 20: 288.6008, Accuracy: 0.7857\n",
      "Training loss (for one batch) at step 30: 274.4023, Accuracy: 0.8087\n",
      "Training loss (for one batch) at step 40: 261.3848, Accuracy: 0.8289\n",
      "Training loss (for one batch) at step 50: 267.8737, Accuracy: 0.8439\n",
      "Training loss (for one batch) at step 60: 289.7119, Accuracy: 0.8478\n",
      "Training loss (for one batch) at step 70: 314.3981, Accuracy: 0.8375\n",
      "Training loss (for one batch) at step 80: 321.7172, Accuracy: 0.8243\n",
      "Training loss (for one batch) at step 90: 292.1812, Accuracy: 0.8228\n",
      "Training loss (for one batch) at step 100: 316.5375, Accuracy: 0.8263\n",
      "Training loss (for one batch) at step 110: 315.5963, Accuracy: 0.8257\n",
      "---- Training ----\n",
      "Training loss: 87.2057\n",
      "Training acc over epoch: 0.8246\n",
      "---- Validation ----\n",
      "Validation loss: 43.3023\n",
      "Validation acc: 0.7550\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 364.2144, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 311.2279, Accuracy: 0.7564\n",
      "Training loss (for one batch) at step 20: 282.5936, Accuracy: 0.7879\n",
      "Training loss (for one batch) at step 30: 267.5005, Accuracy: 0.8130\n",
      "Training loss (for one batch) at step 40: 271.3334, Accuracy: 0.8255\n",
      "Training loss (for one batch) at step 50: 276.9850, Accuracy: 0.8422\n",
      "Training loss (for one batch) at step 60: 293.9843, Accuracy: 0.8472\n",
      "Training loss (for one batch) at step 70: 297.5621, Accuracy: 0.8384\n",
      "Training loss (for one batch) at step 80: 334.6586, Accuracy: 0.8280\n",
      "Training loss (for one batch) at step 90: 306.7047, Accuracy: 0.8251\n",
      "Training loss (for one batch) at step 100: 273.6993, Accuracy: 0.8278\n",
      "Training loss (for one batch) at step 110: 313.1399, Accuracy: 0.8284\n",
      "---- Training ----\n",
      "Training loss: 91.0306\n",
      "Training acc over epoch: 0.8258\n",
      "---- Validation ----\n",
      "Validation loss: 38.2304\n",
      "Validation acc: 0.7534\n",
      "Time taken: 10.62s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 325.6406, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 302.8582, Accuracy: 0.7507\n",
      "Training loss (for one batch) at step 20: 276.4190, Accuracy: 0.7842\n",
      "Training loss (for one batch) at step 30: 281.5793, Accuracy: 0.8128\n",
      "Training loss (for one batch) at step 40: 261.0701, Accuracy: 0.8291\n",
      "Training loss (for one batch) at step 50: 267.9298, Accuracy: 0.8427\n",
      "Training loss (for one batch) at step 60: 288.5067, Accuracy: 0.8494\n",
      "Training loss (for one batch) at step 70: 305.1105, Accuracy: 0.8412\n",
      "Training loss (for one batch) at step 80: 323.4298, Accuracy: 0.8286\n",
      "Training loss (for one batch) at step 90: 284.9454, Accuracy: 0.8263\n",
      "Training loss (for one batch) at step 100: 295.8857, Accuracy: 0.8295\n",
      "Training loss (for one batch) at step 110: 309.4976, Accuracy: 0.8288\n",
      "---- Training ----\n",
      "Training loss: 99.6655\n",
      "Training acc over epoch: 0.8274\n",
      "---- Validation ----\n",
      "Validation loss: 54.3602\n",
      "Validation acc: 0.7499\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 328.9178, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 322.3364, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 20: 292.7977, Accuracy: 0.7831\n",
      "Training loss (for one batch) at step 30: 286.3474, Accuracy: 0.8117\n",
      "Training loss (for one batch) at step 40: 282.8050, Accuracy: 0.8283\n",
      "Training loss (for one batch) at step 50: 290.3448, Accuracy: 0.8418\n",
      "Training loss (for one batch) at step 60: 281.3368, Accuracy: 0.8472\n",
      "Training loss (for one batch) at step 70: 308.0099, Accuracy: 0.8373\n",
      "Training loss (for one batch) at step 80: 314.3816, Accuracy: 0.8241\n",
      "Training loss (for one batch) at step 90: 299.1568, Accuracy: 0.8214\n",
      "Training loss (for one batch) at step 100: 277.7958, Accuracy: 0.8247\n",
      "Training loss (for one batch) at step 110: 293.6842, Accuracy: 0.8264\n",
      "---- Training ----\n",
      "Training loss: 90.0084\n",
      "Training acc over epoch: 0.8242\n",
      "---- Validation ----\n",
      "Validation loss: 84.2395\n",
      "Validation acc: 0.7469\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 322.5362, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 321.3947, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 20: 316.6463, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 30: 295.2351, Accuracy: 0.8163\n",
      "Training loss (for one batch) at step 40: 265.8148, Accuracy: 0.8335\n",
      "Training loss (for one batch) at step 50: 287.1177, Accuracy: 0.8465\n",
      "Training loss (for one batch) at step 60: 289.9495, Accuracy: 0.8511\n",
      "Training loss (for one batch) at step 70: 310.4875, Accuracy: 0.8404\n",
      "Training loss (for one batch) at step 80: 311.4228, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 90: 290.4436, Accuracy: 0.8250\n",
      "Training loss (for one batch) at step 100: 289.2813, Accuracy: 0.8272\n",
      "Training loss (for one batch) at step 110: 296.2952, Accuracy: 0.8265\n",
      "---- Training ----\n",
      "Training loss: 109.5992\n",
      "Training acc over epoch: 0.8254\n",
      "---- Validation ----\n",
      "Validation loss: 41.5115\n",
      "Validation acc: 0.7491\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 336.6262, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 317.4194, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 20: 284.4145, Accuracy: 0.7879\n",
      "Training loss (for one batch) at step 30: 272.8373, Accuracy: 0.8140\n",
      "Training loss (for one batch) at step 40: 286.4922, Accuracy: 0.8319\n",
      "Training loss (for one batch) at step 50: 269.3738, Accuracy: 0.8419\n",
      "Training loss (for one batch) at step 60: 281.4669, Accuracy: 0.8472\n",
      "Training loss (for one batch) at step 70: 310.2569, Accuracy: 0.8380\n",
      "Training loss (for one batch) at step 80: 334.4966, Accuracy: 0.8267\n",
      "Training loss (for one batch) at step 90: 275.8823, Accuracy: 0.8227\n",
      "Training loss (for one batch) at step 100: 283.2674, Accuracy: 0.8261\n",
      "Training loss (for one batch) at step 110: 307.8039, Accuracy: 0.8271\n",
      "---- Training ----\n",
      "Training loss: 99.6676\n",
      "Training acc over epoch: 0.8243\n",
      "---- Validation ----\n",
      "Validation loss: 55.4153\n",
      "Validation acc: 0.7509\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 309.0183, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 318.4709, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 20: 303.9278, Accuracy: 0.7961\n",
      "Training loss (for one batch) at step 30: 281.4320, Accuracy: 0.8163\n",
      "Training loss (for one batch) at step 40: 275.7838, Accuracy: 0.8333\n",
      "Training loss (for one batch) at step 50: 278.0871, Accuracy: 0.8450\n",
      "Training loss (for one batch) at step 60: 293.7935, Accuracy: 0.8500\n",
      "Training loss (for one batch) at step 70: 308.3722, Accuracy: 0.8414\n",
      "Training loss (for one batch) at step 80: 310.6047, Accuracy: 0.8294\n",
      "Training loss (for one batch) at step 90: 278.9906, Accuracy: 0.8262\n",
      "Training loss (for one batch) at step 100: 298.4833, Accuracy: 0.8291\n",
      "Training loss (for one batch) at step 110: 295.1602, Accuracy: 0.8282\n",
      "---- Training ----\n",
      "Training loss: 87.3044\n",
      "Training acc over epoch: 0.8260\n",
      "---- Validation ----\n",
      "Validation loss: 52.9377\n",
      "Validation acc: 0.7501\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 327.5539, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 313.4046, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 20: 287.5494, Accuracy: 0.7879\n",
      "Training loss (for one batch) at step 30: 270.9768, Accuracy: 0.8155\n",
      "Training loss (for one batch) at step 40: 265.3942, Accuracy: 0.8287\n",
      "Training loss (for one batch) at step 50: 256.2037, Accuracy: 0.8434\n",
      "Training loss (for one batch) at step 60: 320.8220, Accuracy: 0.8491\n",
      "Training loss (for one batch) at step 70: 301.9061, Accuracy: 0.8397\n",
      "Training loss (for one batch) at step 80: 304.7501, Accuracy: 0.8272\n",
      "Training loss (for one batch) at step 90: 285.6359, Accuracy: 0.8225\n",
      "Training loss (for one batch) at step 100: 301.9880, Accuracy: 0.8260\n",
      "Training loss (for one batch) at step 110: 298.1532, Accuracy: 0.8272\n",
      "---- Training ----\n",
      "Training loss: 94.1409\n",
      "Training acc over epoch: 0.8258\n",
      "---- Validation ----\n",
      "Validation loss: 49.7428\n",
      "Validation acc: 0.7515\n",
      "Time taken: 10.61s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 321.2687, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 299.8056, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 20: 282.5744, Accuracy: 0.7946\n",
      "Training loss (for one batch) at step 30: 285.0598, Accuracy: 0.8173\n",
      "Training loss (for one batch) at step 40: 276.2069, Accuracy: 0.8304\n",
      "Training loss (for one batch) at step 50: 257.2123, Accuracy: 0.8439\n",
      "Training loss (for one batch) at step 60: 301.8768, Accuracy: 0.8494\n",
      "Training loss (for one batch) at step 70: 315.7791, Accuracy: 0.8403\n",
      "Training loss (for one batch) at step 80: 321.5520, Accuracy: 0.8288\n",
      "Training loss (for one batch) at step 90: 284.0899, Accuracy: 0.8249\n",
      "Training loss (for one batch) at step 100: 286.5360, Accuracy: 0.8273\n",
      "Training loss (for one batch) at step 110: 285.3062, Accuracy: 0.8285\n",
      "---- Training ----\n",
      "Training loss: 123.4071\n",
      "Training acc over epoch: 0.8272\n",
      "---- Validation ----\n",
      "Validation loss: 36.8199\n",
      "Validation acc: 0.7397\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 319.9763, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 305.3071, Accuracy: 0.7393\n",
      "Training loss (for one batch) at step 20: 273.5926, Accuracy: 0.7835\n",
      "Training loss (for one batch) at step 30: 279.0801, Accuracy: 0.8052\n",
      "Training loss (for one batch) at step 40: 266.1942, Accuracy: 0.8264\n",
      "Training loss (for one batch) at step 50: 311.8819, Accuracy: 0.8413\n",
      "Training loss (for one batch) at step 60: 302.4853, Accuracy: 0.8472\n",
      "Training loss (for one batch) at step 70: 318.7480, Accuracy: 0.8381\n",
      "Training loss (for one batch) at step 80: 306.0534, Accuracy: 0.8286\n",
      "Training loss (for one batch) at step 90: 296.4367, Accuracy: 0.8224\n",
      "Training loss (for one batch) at step 100: 268.8528, Accuracy: 0.8260\n",
      "Training loss (for one batch) at step 110: 277.0677, Accuracy: 0.8253\n",
      "---- Training ----\n",
      "Training loss: 103.8038\n",
      "Training acc over epoch: 0.8243\n",
      "---- Validation ----\n",
      "Validation loss: 40.1026\n",
      "Validation acc: 0.7509\n",
      "Time taken: 10.14s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 313.9006, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 293.4310, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 20: 285.3782, Accuracy: 0.7935\n",
      "Training loss (for one batch) at step 30: 275.1138, Accuracy: 0.8188\n",
      "Training loss (for one batch) at step 40: 279.1893, Accuracy: 0.8321\n",
      "Training loss (for one batch) at step 50: 258.3985, Accuracy: 0.8441\n",
      "Training loss (for one batch) at step 60: 266.6060, Accuracy: 0.8490\n",
      "Training loss (for one batch) at step 70: 311.2050, Accuracy: 0.8376\n",
      "Training loss (for one batch) at step 80: 318.1309, Accuracy: 0.8268\n",
      "Training loss (for one batch) at step 90: 280.2414, Accuracy: 0.8252\n",
      "Training loss (for one batch) at step 100: 265.6492, Accuracy: 0.8279\n",
      "Training loss (for one batch) at step 110: 297.7332, Accuracy: 0.8284\n",
      "---- Training ----\n",
      "Training loss: 99.5099\n",
      "Training acc over epoch: 0.8271\n",
      "---- Validation ----\n",
      "Validation loss: 37.5041\n",
      "Validation acc: 0.7485\n",
      "Time taken: 10.59s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 325.3466, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 310.1467, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 20: 279.9037, Accuracy: 0.7820\n",
      "Training loss (for one batch) at step 30: 259.2297, Accuracy: 0.8080\n",
      "Training loss (for one batch) at step 40: 264.6064, Accuracy: 0.8249\n",
      "Training loss (for one batch) at step 50: 270.9057, Accuracy: 0.8361\n",
      "Training loss (for one batch) at step 60: 286.5929, Accuracy: 0.8407\n",
      "Training loss (for one batch) at step 70: 339.9876, Accuracy: 0.8334\n",
      "Training loss (for one batch) at step 80: 309.8412, Accuracy: 0.8214\n",
      "Training loss (for one batch) at step 90: 281.4070, Accuracy: 0.8194\n",
      "Training loss (for one batch) at step 100: 264.7493, Accuracy: 0.8236\n",
      "Training loss (for one batch) at step 110: 294.6020, Accuracy: 0.8230\n",
      "---- Training ----\n",
      "Training loss: 99.1840\n",
      "Training acc over epoch: 0.8227\n",
      "---- Validation ----\n",
      "Validation loss: 57.6034\n",
      "Validation acc: 0.7483\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 334.7753, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 334.0305, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 20: 278.7852, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 30: 283.7641, Accuracy: 0.8112\n",
      "Training loss (for one batch) at step 40: 265.5025, Accuracy: 0.8258\n",
      "Training loss (for one batch) at step 50: 266.8241, Accuracy: 0.8421\n",
      "Training loss (for one batch) at step 60: 308.9391, Accuracy: 0.8481\n",
      "Training loss (for one batch) at step 70: 330.1156, Accuracy: 0.8366\n",
      "Training loss (for one batch) at step 80: 313.2195, Accuracy: 0.8264\n",
      "Training loss (for one batch) at step 90: 281.4207, Accuracy: 0.8241\n",
      "Training loss (for one batch) at step 100: 278.9859, Accuracy: 0.8258\n",
      "Training loss (for one batch) at step 110: 309.0072, Accuracy: 0.8271\n",
      "---- Training ----\n",
      "Training loss: 85.2145\n",
      "Training acc over epoch: 0.8250\n",
      "---- Validation ----\n",
      "Validation loss: 50.2688\n",
      "Validation acc: 0.7461\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 331.6957, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 331.6079, Accuracy: 0.7628\n",
      "Training loss (for one batch) at step 20: 290.3737, Accuracy: 0.7946\n",
      "Training loss (for one batch) at step 30: 276.0144, Accuracy: 0.8196\n",
      "Training loss (for one batch) at step 40: 273.4323, Accuracy: 0.8348\n",
      "Training loss (for one batch) at step 50: 269.8542, Accuracy: 0.8482\n",
      "Training loss (for one batch) at step 60: 300.1819, Accuracy: 0.8518\n",
      "Training loss (for one batch) at step 70: 296.6283, Accuracy: 0.8423\n",
      "Training loss (for one batch) at step 80: 331.3415, Accuracy: 0.8312\n",
      "Training loss (for one batch) at step 90: 286.3845, Accuracy: 0.8294\n",
      "Training loss (for one batch) at step 100: 268.4867, Accuracy: 0.8317\n",
      "Training loss (for one batch) at step 110: 307.6702, Accuracy: 0.8318\n",
      "---- Training ----\n",
      "Training loss: 95.5332\n",
      "Training acc over epoch: 0.8302\n",
      "---- Validation ----\n",
      "Validation loss: 49.2726\n",
      "Validation acc: 0.7469\n",
      "Time taken: 10.65s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 329.9849, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 313.7407, Accuracy: 0.7628\n",
      "Training loss (for one batch) at step 20: 295.5076, Accuracy: 0.7954\n",
      "Training loss (for one batch) at step 30: 266.3210, Accuracy: 0.8178\n",
      "Training loss (for one batch) at step 40: 279.9807, Accuracy: 0.8319\n",
      "Training loss (for one batch) at step 50: 272.2853, Accuracy: 0.8448\n",
      "Training loss (for one batch) at step 60: 284.7344, Accuracy: 0.8519\n",
      "Training loss (for one batch) at step 70: 302.7990, Accuracy: 0.8413\n",
      "Training loss (for one batch) at step 80: 302.9807, Accuracy: 0.8293\n",
      "Training loss (for one batch) at step 90: 287.4323, Accuracy: 0.8278\n",
      "Training loss (for one batch) at step 100: 306.4825, Accuracy: 0.8304\n",
      "Training loss (for one batch) at step 110: 307.1962, Accuracy: 0.8309\n",
      "---- Training ----\n",
      "Training loss: 101.5089\n",
      "Training acc over epoch: 0.8291\n",
      "---- Validation ----\n",
      "Validation loss: 51.6182\n",
      "Validation acc: 0.7536\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 342.4745, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 305.5981, Accuracy: 0.7614\n",
      "Training loss (for one batch) at step 20: 297.4586, Accuracy: 0.7850\n",
      "Training loss (for one batch) at step 30: 268.3674, Accuracy: 0.8105\n",
      "Training loss (for one batch) at step 40: 255.3525, Accuracy: 0.8279\n",
      "Training loss (for one batch) at step 50: 265.0093, Accuracy: 0.8418\n",
      "Training loss (for one batch) at step 60: 284.0914, Accuracy: 0.8481\n",
      "Training loss (for one batch) at step 70: 316.1004, Accuracy: 0.8367\n",
      "Training loss (for one batch) at step 80: 319.5721, Accuracy: 0.8245\n",
      "Training loss (for one batch) at step 90: 298.8849, Accuracy: 0.8218\n",
      "Training loss (for one batch) at step 100: 276.6992, Accuracy: 0.8256\n",
      "Training loss (for one batch) at step 110: 293.2969, Accuracy: 0.8260\n",
      "---- Training ----\n",
      "Training loss: 115.5295\n",
      "Training acc over epoch: 0.8244\n",
      "---- Validation ----\n",
      "Validation loss: 37.3336\n",
      "Validation acc: 0.7485\n",
      "Time taken: 10.11s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 324.6035, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 322.8362, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 20: 283.4750, Accuracy: 0.7865\n",
      "Training loss (for one batch) at step 30: 276.7706, Accuracy: 0.8115\n",
      "Training loss (for one batch) at step 40: 265.3967, Accuracy: 0.8302\n",
      "Training loss (for one batch) at step 50: 264.1418, Accuracy: 0.8441\n",
      "Training loss (for one batch) at step 60: 294.2338, Accuracy: 0.8491\n",
      "Training loss (for one batch) at step 70: 311.9726, Accuracy: 0.8395\n",
      "Training loss (for one batch) at step 80: 325.4169, Accuracy: 0.8280\n",
      "Training loss (for one batch) at step 90: 290.7735, Accuracy: 0.8244\n",
      "Training loss (for one batch) at step 100: 275.4218, Accuracy: 0.8289\n",
      "Training loss (for one batch) at step 110: 286.6030, Accuracy: 0.8290\n",
      "---- Training ----\n",
      "Training loss: 99.1633\n",
      "Training acc over epoch: 0.8278\n",
      "---- Validation ----\n",
      "Validation loss: 40.7315\n",
      "Validation acc: 0.7402\n",
      "Time taken: 10.78s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEjCAYAAADZk82GAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAACChklEQVR4nO2dZ3Rc1bmwnz1dvcu994a7DTgGmw4htFDscIMNhJZQcwMBPkJNckNCQkkogdBDMB0MmFCMjQED7r1bFrbc1KVRmb6/H/ucmZE06l3sZ61ZM3PqO0dH+z1v3UJKiUaj0Wg00Vg6WwCNRqPRdD20ctBoNBpNHbRy0Gg0Gk0dtHLQaDQaTR20ctBoNBpNHbRy0Gg0Gk0dtHLQaJqBEGKOECKvs+XQaNobrRw0HYYQIlcIcUpny6HRaBpHKweNpocghLB1tgyanoNWDppORwjhFEI8IoQ4ZLweEUI4jXWZQogPhBClQohiIcSXQgiLse63QoiDQgi3EGKnEOLkeo7/YyHEeiFEuRDigBDi3qh1g4UQUgixQAixXwhRKIT4f1Hr44QQLwghSoQQ24DpjfyWR41zlAsh1gohZketswoh7hRC7DVkXiuEGGCsGyeE+NT4jUeFEHcay18QQvw+6hg13FqGNfZbIcQmoFIIYRNC3B51jm1CiPNryXiVEGJ71PopQohbhRBv1druMSHEow39Xk0PRkqpX/rVIS8gFzglxvL7gW+BbCALWAk8YKz7P+ApwG68ZgMCGAUcAPoa2w0GhtVz3jnABNTD0DHAUeC8qP0k8AwQB0wEvMAYY/2fgC+BdGAAsAXIa+A3/g+QAdiA/wWOAC5j3a3AZkN2YZwrA0gCDhvbu4zvM419XgB+X+u35NW6phsM2eKMZRcBfY3fewlQCfSJWncQpeQEMBwYBPQxtks1trMB+cDUzr5v9KtzXp0ugH79cF4NKIe9wFlR308Hco3P9wPvAcNr7TPcGLxOAezNlOMR4GHjs6kc+ketXwXMMz7nAGdErbu6IeUQ41wlwETj807g3BjbzAfW17N/U5TDFY3IsME8L/AxcFM9230EXGV8PhvY1tn3jH513ku7lTRdgb7A91HfvzeWAfwF2AN8IoTIEULcDiCl3APcDNwL5AshFgkh+hIDIcRMIcQyIUSBEKIMuBbIrLXZkajPVUBilGwHaslWL0KI3xgumzIhRCmQEnWuAShFWJv6ljeVaPkQQlwmhNhguOJKgfFNkAHgRZTlg/H+citk0nRztHLQdAUOoVwbJgONZUgp3VLK/5VSDgXOAX5txhaklP+RUv7I2FcCD9Zz/P8Ai4EBUsoUlJtKNFG2w6gBNVq2mBjxhduAi4E0KWUqUBZ1rgPAsBi7HgCG1nPYSiA+6nvvGNuEWysLIQahXGTXAxmGDFuaIAPAu8AxQojxKMvhlXq20/wA0MpB09HYhRCuqJcNeBW4SwiRJYTIBO4G/g0ghDhbCDFcCCFQA20QCAkhRgkhTjIC1x6gGgjVc84koFhK6RFCzAB+1gx5XwfuEEKkCSH6Azc0sG0SEAAKAJsQ4m4gOWr9v4AHhBAjhOIYIUQG8AHQRwhxsxGcTxJCzDT22QCcJYRIF0L0RllLDZGAUhYFAEKIy1GWQ7QMvxFCTDVkGG4oFKSUHuBNlDJdJaXc38i5ND0YrRw0Hc0S1EBuvu4Ffg+sATahArbrjGUAI4DPgArgG+AJKeUywIkKFheiXELZwB31nPOXwP1CCDdK8bzeDHnvQ7mS9gGf0LCr5WPgv8AuYx8PNV0+fzPO/QlQDjyLCiK7gVOBnxi/ZTcw19jnZWAjKrbwCfBaQ8JKKbcBf0Vdq6OoQPzXUevfAP6AUgBulLWQHnWIF419tEvpB46QUk/2o9FoFEKIgcAOoLeUsryz5dF0Htpy0Gg0ABj1I78GFmnFoNEVlRqNBiFEAsoN9T1wRieLo+kCaLeSRqPRaOqg3UoajUajqYNWDhqNRqOpg1YOGo1Go6mDVg4ajUajqYNWDhqNRqOpg1YOGo1Go6mDVg4ajUajqYNWDhqNRqOpg1YOGo1Go6mDVg4ajUajqYNWDhqNRqOpg1YOGo1Go6mDVg4ajUajqYNWDhqNRqOpQ7eezyEzM1MOHjy4zvLKykoSEhI6XqAYaFli01VkaUiOtWvXFkopszpYJCD2vd1VrhloWeqju8jSpHtbStltX1OnTpWxWLZsWczlnYGWJTZdRZaG5ADWyC50b3eVayallqU+uossTbm3tVtJo9FoNHXQykGj0Wg0ddDKQaPRaDR16NYB6a6I3+8nLy8Pj8cDQEpKCtu3b+9kqRRalthy7Nu3j/79+2O32ztbHI2my6CVQxuTl5dHUlISgwcPRgiB2+0mKSmps8UC0LLEoLy8HJ/PR15eHkOGDOlscTSaLoN2K7UxHo+HjIwMhBCdLYqmCQghyMjICFt6Go1GoZVDO6AVQ/dC/700mrpo5WDwwaZDFFV4O1sMjUajaTbLd+azbEc+oZBss2Nq5QDkl3u4/j/reWNtXmeLotFoNM0i3+3h6pfXcvkLqzn/yZV4/EEOlVZT4gm16rhaOQD7i6sAKHB3f8uhqKiISZMmMWnSJHr37k2/fv3C330+X4P7rlmzhhtvvLHRcxx//PFtJS4AL7zwAtdff32bHlOj6SgCwdiDsDcQZOuhMkqrfNz6xkZ+/ux3VHoDdbaTUvL6mgOc84+v2HigtMa6UEhSVuXnQHEVNy9az7++zKHSG+Cudzez5WAZAM99lUsgGOLXp45k44FSXliZy02L1vOH7zz465GtKfTIbKXqgGTDgVImDUht0vamcugJbqWMjAw2bNgAwL333ktiYiK/+c1vAJUhFAgEsNli/9mnTZvGtGnTGj3HypUr20xejaYhvIEgDqulXeNCvkAIicRpsza4XYU3gNvjJyPBicNmYcWuAh5dupsdh8t57ZrjGJaVSEFVCCkl3+QUcc97W9mdXwGAxRD/l6+s4+nLpnK41MOHmw+TFu/gjbUHWL+/FJtF8MtX1nH5rMFsO1zOCSOyeOqLvew44g7L8O6GQ7y17iDbD5ez7vtSXr5yBq98+z1nTejDjSePYHVuMQ/+dwdSwtXHOLFbW/78327KQQjxHHA2kC+lHF9r3f8CDwFZUspCof7yjwJnAVXAQinlupae++lNXg5+t4YVt83FZW/4Dw5RyqGy4Sfr5nLf+1vZfKAEq7VxGZrK2L7J3POTcc3aZ+HChbhcLtasWcMJJ5zAvHnzuOmmm/B4PMTFxfH8888zatQoli9fzkMPPcQHH3zAvffey/79+8nJyWH//v3cfPPNYasiMTGRiooKli9fzr333ktmZiZbtmxh6tSp/Pvf/0YIwZIlS/j1r39NQkICs2bNIicnhw8++KBRWXNzc7niiisoLCwkKyuL559/noEDB/LGG29w3333YbVaSUlJYcWKFWzdupXLL78cn89HKBTirbfeYsSIES26rpquw3sbDlJc6eMnE/vyk79/xZDMBJ68dCop8aoOZeuhMu55bytPXzYNq0WwozjIcYFgeHCXUnLv4q0UVfoY2SuJMX2SOXl0NhZLXQXjC4T46ZMr2Z3vZuqgNPqlxnG4TGWuXTpzIKeN7Y3FIjhUWs2PH/uSkio/qfF2ThqdzdvrDjIgPY54p43rXllLKAQHS6u599uPqfQF6ZPi4oHzxpNXUsVJo7LJKazkjrc3c/ZjX3GkzIPbsCL6pcbx4E8nMLp3Mhc99Q2//3A78Q4rb687SGaig9+cNpJgCM6b3Jf/fX0ja74v4dSxvfh021FOf+RLvIEQv5wzHIBbTx/Fl7sL+cnEvhzXu7RVf4f2tBxeAP4BvBS9UAgxADgN2B+1+ExghPGaCTxpvLeI0wbZeXC1h1dX7efyWY3nrh8orgagqKJtlUNXIi8vj88++4zU1FTKy8v58ssvsdlsfPbZZ9x555289dZbdfbZsWMHy5Ytw+12M2rUKK677ro6hWLr169n69at9O3bl1mzZvH1118zbdo0rrnmGlasWMGQIUOYP39+k+W84YYbWLBgAQsWLOC5557jxhtv5N133+X+++/n448/pl+/fpSWlgLw1FNPcdNNN3HppZfi8/kIBoOtukaazscfDHH/+9soqvTxwspciip8FFX4OP+Jr3n6smkMz07k2a/2seb7EpZsPszGA6W8sdbDPzZ+xl8vnsjp43qz7XA5L37zPekJDj7YdBiAm04ewcwh6Ty6dDd/+ukxvLMuj8+25zO6TxKbD5ZxzsS+7CusZNfRArISnZR7/Fz773VMHpjK7WeM5tmv9lHtD3LfOeP4dNtR3l53kPMm9eVPPz2GzQfLmPf0t/RLjeOSUQ5I6sWMIemcOaE38Y7IEDtzaAa9kp3c8fZmBmbE8/f5kwHonxaPw6ae8F+9Wg174/ul8F1OMWP7JpOZ6Awf47nLp7Mlr4xjh2Zw3hNfs/OIm6cvm8rYvskAHNM/lU9vOYFBGQms/GpFq/4W7aYcpJQrhBCDY6x6GLgNeC9q2bnAS0a3wG+FEKlCiD5SysMtOfeYDCszh6Tz5PK9ZCQ6OXFkFilx9Ve/HghbDm3rVrrnJ+O6TLHXRRddFLZgysrKWLBgAbt370YIgd/vj7nPj3/8Y5xOJ06nk+zsbI4ePUr//v1rbDNjxozwskmTJpGbm0tiYiJDhw4NF5XNnz+fp59+uklyfvPNN7z99tsA/PznP+e2224DYNasWSxcuJCLL76YCy64AIDjjjuOP/zhD+Tl5XHBBRdoq6GLEwxJSqt8ZEQNdqGQ5NucIoZmJdI7xcXXewopqvQxJDOBfYWV/L+zxjBxQCrX/Xst5z3+NY/Om8SSzWpYeHtdHjuOuJmQacXiSuDGV9fzn6tmsmTzEexWwdJfn4jTbuHWNzfx5PK9vPRNLiVVfs5/4mtKq/zEO6xsO1zOeZP68si8yXVkfXtdHn/6aAeXPP0tALefOZoFxw/msuMGkVdSTf+0OIQQTB+czqe3nECvZBerv/mKOXOOqfcanDS6F1/9NgurEDEtmamD0sOfTxhZt6N2ssvO8cMzAXjh8hm4PX4GZdRsyz2iV9uMNx0acxBCnAsclFJurOVD7AcciPqeZyxrkXIA+M3po7j0X99x46vrmTYojTeuPa5ev2Uk5uBDStkj896j+7r/7ne/Y+7cubzzzjvk5uYyZ86cmPs4nZF/YqvVSiBQN5jWlG3agqeeeorvvvuODz/8kKlTp7J27Vp+9rOfMXPmTD788EPOOuss/vnPf3LSSSe1y/k1ree3b23irXV5nDy6Fz8/bhAFbi+PLd3N/uIqMhMd/PPn01i84RDJLhvvXT+L9ftLmT08E4tF8P4NP+Ky51Zx5YtrAJg9IpMvdxcC8JNhLi48bQYXPPE1C59fjc0iOGl0NmkJDgDu+clYVuwswBsI8fAlE7nrnS2cMiabv10yif9uPsKZE3rXkdVqEVw0bQBnTejDZ9uPkltYxZU/Ug87QggGpMfX2H5oVmKTr0Nr4gDRpCc4SDd+Y3vQYcpBCBEP3IlyKbXmOFcDVwP06tWL5cuX19mmoqICcjfxtxNcLD/g583dJTz9zueMSq/r+/cFJUfLPSTYodIvWfLZchLsLVcOKSkpuN2RAFIwGKzxvSPxer3Y7Xb8fj/V1dVhWYqKikhPT8ftdvPPf/4TKSVut5uqqioCgQButzu8ryl7KBSioqIi/L329gA+nw+Px0Pfvn3Zu3cvW7ZsYdCgQfz73/+usR3UvC4ejwefz4fb7WbGjBk8//zzzJ8/n1deeYXjjjsOt9tNTk4OY8eOZezYsXzwwQfs2LEj3Kbk8ssvZ8+ePaxatYrp06c36xqZcng8npj3kqZ1HC338OB/d5DotPHm2jxOGJnFuv0lfLb9KADH9E/h2hMn8OQXe/jpkysRAi6ZNoBkl50To56c+6bG8dIVM7jgiZVkJDq47fTRfLn7KwZlxDMiVQ2Ur1x1LPOe/oYDxdVcMCVi4WYnuXj5FzMRwMQBqZw0uhdJThsWi+Di6QMalD/BaePcSf3a49J0eTrSchgGDAFMq6E/sE4IMQM4CET/lfoby+ogpXwaeBpg2rRpMtZT7/Lly8NPwyf7gnz+4Od8VZLIgp9MqxOg3ltQgfz0C6YNyeKLXQWMnjSdYcZTQLUvyIGSKkY2w0zbvn17DTdSZ7qVTJeQ3W4nLi4Oq9VKUlISd955JwsWLOCvf/0rP/7xjxFCkJSURHx8PDabjaSkpPC+puwWi4XExMTw99rbAzgcDlwuF9nZ2Tz55JNceOGFJCQkMH36dOx2e73XxeVy4XA4SEpK4sknn+Tyyy/nH//4RzggnZSUxH333cfu3buRUnLyySdz/PHH8+CDD/Lyyy9jt9vp3bs39957b7OvtSmHy+Vi8uTJje+giYmZoeO0WZkyMDVsfT/71T7eXqf+lWcOSef5hdMJhEIs25GP1WIJB4rPHN+bN9fmsSq3mCt+FDtO2Dc1jk9+fQKhkCQlzs7Jo7M5bVwvRGUOoAK7r19zHJ9sPcrJo7Nr7BududiQi1kTRWOzAbXmBQwGttSzLhfIND7/GPgIEMCxwKqmHL+pM8E9uXyPHPTbD+SY330k//H5bhkMhsLrPt9xVA767Qfyb5/slIN++4H8LqdIFlV4pZRSPv3FXjn8zg9lSaW33hmVarNt27Ya38vLy5u8b3vTkbK43W4ppZShUEhed9118m9/+1unydIQphy1/25S6pngGiJaFrfHL695aY0c9NsP5KDffiDPeGSFfOHrfbK00icn3fexvPblNfL7wkpZ5Q20uyydTXeRpSn3drsVwQkhXgW+AUYJIfKEEFc2sPkSIAfYAzwD/LItZbnmhKH8+8qZzB6RyV8+3sllz60i363S1XYcVq6NSQNTAfhk6xGm/v5Tth0qJ6ewEn9Q1UxomsczzzzDpEmTGDduHGVlZVxzzTWdLZKmnfj9B9v4ZNsRbjtjFH88fwI2i+CexVuZ89AySqr8XDpzEAMz4olztF1Kt6b9ac9spQbzF6WUg6M+S+BX7SWLEIIfjchk1vAMFq0+wH3vb+WsR7/k16eO4vFle5g5JJ2xfVQq2HsbDyEl7DhSzuEyleK6fn8pc0ZlN3QKTS1uueUWbrnllhrLnn/+eR599FFAxTAsFguzZs3i8ccf7wwRNW3Asp35LFp9gGtPHBbOtf/ZzIF8sauAX7+2geHZiRw/LKOTpdS0hB5ZIV0fQgjmzxjI1EFp3LRoA3e+s5lEp42HLpoYjvqbLTQOFFdzqFQpB205tA2XX345l19+OdB15nPQNJ/Pth3lyfUetoR2849lexjZK5GbT6mZRnziyCyW3zqHQFDGTNnUdH1+UMrBZGSvJN771Sxe+iaXkb2SwmlpqfF2SqtUzv+BkioOlyrX04YDpYRC+ibXaJZuP8p1r6xFhiRrP9nFpAGpPHNZ3UQPgCSXDvx2Z36QygHAYbPwi9lDayzLSHCElcP2w+W4vQGGZyeyJ7+CfUWV4SwmzQ8PIcQZqBYvVuBfUso/1Vo/EHgRSDW2uV1KucRYdwdwJRAEbpRSftyBorcZh0qrufHV9Yzpk8w1o/wMnzCNwZnxjfYk0nRPdFfWKMzKzWOHprP9cDkAZ03oA6i4QzTeQJC9BRUdKp+mcxBCWIHHUW1exgLzhRBja212F/C6lHIyMA94wth3rPF9HHAG8IRxvG6FlJLfvbuFkITHfzaFBLtgVO8krRh6MFo5RNE/LY6hmQlMH5yOOWfG7BGZWC2CfYVKEZgtd/++dA9nPfol1T7dz+cHwAxgj5QyR0rpAxahWr5EI4Fk43MKcMj4fC6wSErplVLuQ2XkzegAmduESm+AT7cd5bLnVrF0Rz7/e9rIOtXBmp6JVg5R3H32WF65aiYD0iI3/4C0ePqlxrG/uJoNB0o55r5PWJNbzPubDuENhMgtquxEiesyd+5cPv64ptfikUce4brrrou5/Zw5c1izRrUkOOuss8JN7aK59957eeihhxo877vvvsu2bdvC3++++24+++yzZkpfP50850N97V2iuRf4HyFEHio1+4Zm7NslKff4Oemvy7nqpTVsP1zOXT8e06RGlpqewQ825hCL1HgHqSgLAsBmEWQlORmYHs/+4io27C8hGJLc8fZmvi9S/Zj2FVbSK9mFLxCid4qr84Q3mD9/PosWLeL0008PL1u0aBF//vOfG913yZIlLT7vu+++y9lnn83Yscrbcv/997f4WN2U+cALUsq/CiGOA14WQoxvbKdoGmsNY7ZJ7yjWHAlwtNzLFeMdHNfXhj24ny9X7O8UWRpCyxKb1sqilUMMTLO5V7ILq0U12fpk6xH2FigrYXd+BRYBIamUwzvrD1Lg9vLur2bhDYTwmr3lP7qduIPrwdqGl7n3BDjzT/WuvvDCC7nrrrvw+Xw4HA5yc3M5dOgQr776KjfffDNer5cLL7yQ++67r86+gwcPZs2aNWRmZvKHP/yBF198kezsbAYMGMDUqVMBVdz29NNP4/P5GD58OC+//DIbNmxg8eLFfPHFF/z+97/nrbfe4oEHHuDss8/mwgsvZOnSpfzmN78hEAgwffp0nnzyyfD5FixYwPvvv4/f7+eNN95g9OjRjV6CTpjzoSntXa5ExRSQUn4jhHABmU3cF2O/BlvDRLeF6Qg+fnsTic7D3DH/5DrN4jpalobQssSmtbJot1IM+qQopdAvVVkQA9LjKKr0sSmvlMEZ8TisFmYMSadXspO9BRWszi0mr0TVRBRX+sgv77wZ5dLT05kxYwYfffQRoKyGiy++mD/84Q988cUXbNq0KfxeH2vXrmXRokVs2LCBJUuWsHr16vC6Cy64gNWrV7Nx40bGjBnDs88+y/HHH88555zDX/7yFzZs2MCwYcPC23s8HhYuXMhrr73G5s2bCQQCYeUAkJmZybp167juuusadV2ZmHM+bNq0iUsvvTQ8CZE558PGjRtZvHgxEJnzYcOGDaxZs6ZOy/EmshoYIYQYIoRwoALMi2ttsx84GUAIMQZwAQXGdvOEEE4hxBDUnCWrWiJERyKl5IudBcwantFmXUQ13QttOcTAZrUwNDOBYdmqzfVAw5LYdLCMCyb3595z+tA3NY6739vCil2FlFb5sQg1q1QoJCPztp75J6o7odjLdC2de+65LFq0iGeffZbXX3+dp556ilAoxOHDh9m2bRvHHBO77/yXX37J+eefT3y8+t3nnHNOeN2WLVu46667KC0tpaKioob7KhY7d+5kyJAhjBw5EoAFCxbw+OOPc+WVqpuKOTfD1KlTw/M4NEZHz/kgpQwIIa4HPkalqT4npdwqhLgf1aNmMfC/wDNCiFtQwemFRuX/ViHE68A2IAD8SkrZ5bMY9hZUcKjMw/Un6Tkyfqho5VAPr/xiZrgXjKkcpITh2YnhVhpDMhP5NqcYUC6mnMIKJOAPyk6R2eTcc8/llltuYd26dVRVVZGens5DDz3E559/zsCBA1m4cCEej6dFx164cCHvvvsuEydO5IUXXmi1f9WcD6It5oJozzkfjJqFJbWW3R31eRswq559/wD8odkn7SSqfUH++skuAE4YmdnJ0mg6C20v1kN2sitc4RmdvTQsKzJpztDMmjMw7TQmAg+YlkMTCIUkxZVes1Ntm5CYmMjcuXO54oormD9/PuXl5SQkJJCSksLRo0fDLqf6OOGEE3j33Xeprq7G7Xbz/vvvh9e53W769OmD3+/nlVdeCS9PSkqKOW/FqFGjyM3NZc+ePQC8/PLLnHjiia36fccffzyLFi0C4JVXXmH27NkA7N27l5kzZ3L//feTlZXFgQMHyMnJYejQodx4442ce+65DbrTNIpfvLSa/249wm/PGE3/NJ22+kNFK4cmkBpvJ8mpjKxh2ZEq6cGGcnAa879uNzq8BqUkGGraYO/2Bsgrqcbjb1tPw/z589m4cSPz589n4sSJTJ48malTp/Kzn/2MWbNiPuCGmTJlCpdccgkTJ07kzDPPrDGBzgMPPMDMmTOZNWtWjeDxvHnz+Mtf/sLkyZPZu3dveLnL5eL555/noosuYsKECVgsFq699tpW/ba///3vPP/88xxzzDG8/PLL4WZ+t956KxMmTGD8+PEcf/zxTJw4kddff53x48czadIktmzZwmWXXdaqc/d0NuWV8vWeIu44czTXzRnW+A6anktjPb278qup8zm0BWc+skIOv/ND6QsEw8t2H3XLQb/9QF7x/Co56LcfyAXPfSc/+Xqt3HigRFb7VO/6xuYtKK70yo0HSqTb429zmWvTVeZQkLLryKLnc6jJb9/cKEff9ZEsq/Y1uF13mbego+kusjTl3taWQxMZ3TuJMX2Sa2RuDM1M4JoTh/LLuapVselWgqa7lqThTgo10dLQaNqLco+f9zYc4txJfUnWTfN+8OiAdBO579xxdQLNFovgjjPHEAxJLAIOl0WCvP4mDvbmZqE2jDl0Z6LnfDDRcz50DItW7afaH+R/jh3U2aJougBaOTSRhtoPWy2C9AQHhRU+LEJZA/7mWg5aNwA153zoKKRWzHgDQZ79ah/HD8tgfL+UzhZH0wXQbqU2IiNBpWQWeSTBqnL8gbrKocDtJa+4iqKKSJGcthw6FyklRUVFuFyd3/qkM3lvwyGOlnu59kQdhNYotOXQRmQkOuAofHVIEggVUFleQnmiE4/HEx54DpZWg1QVUn1TXViEoKzaj9sTwFNgo8CwTgLBEFaLBdHGcwtFy9LZdBVZPB4PqampLa2c7hFU+4I8+tluxvdLZvYIXdegUWjl0EaYc0Ekxjv5cF8l1T4vb/9yEsuXL2fy5MkcLfdw5otLmT0iky93F/Lur2YxaUAqv/9gG//66gA3nDSc/z1tFIUVXmb+cSl/nz85PJdELIIhyYYDpUwdlNZkGU1ZugJdRZauIkdn8s8VezlYWs3fLp6IaOsnEk23RbuV2ogMYw7qzEQnvZJdHK3VX2l/serietJoVV29J1/ND+E13E+VXlXnUFThIxiS4bms62Pp9qP89MmV7De6w2o0LaHSG+CpL/by42P6MHNoRmeLo+lCaOXQRmQmKuWQleigV7KLfLenRiGc2eL7R8MzsVtFeBY5s/it2q9aR1QYkwlVNTKJUEmVD4DSal8b/grND42th8rx+EP8dEq3mGJC04Fot1IbYbqVspKcOO1W/EEZthZAWQ4WAYMyEhiUkVDHcjCVQZVPKYdqX8N9hswZ6Dz+prfq0Ghqs/lgGYDOUNLUQVsObYQ50U/vlDhG91ZdWHcY81AD7C+qpE9KHA6bheFZiWHLwRtQg7zpVqpsouVQ5TeVQ5dv8NmhVHoDbMor7Wwxug2b80rpnewiO6nzkwM0XQutHNqIE0Zk8fzl05nYP4UR2UkIATuiKqb3F1eFu7sOy07g+6IqfIFQ+MnfdCuZSqKqkUE/Yjlo5RDNa6sP8NMnV+rr0kQ2HyxTVsPBtbDzv50tjqYLoZVDG2G1COaOykYIQZzDyuCMhBrtNPYXVzEoQymH4dmJBEOS/cWVdS2HsFupicohRj3FD5myaj/+oGzU8tKo+FZOYSUT+qXA8gfhw193tkiaLoRWDu3EqF5J7DyqlEOlN0BhhS88/eiwLNXZdU9+RTjmYA72ppJoTDk0x61UWuXjpIeW8315zx8wfUEzhtO6uSF+CGw7VI6UMKF/MpTkgvswBP2dLZami9BuykEI8ZwQIl8IsSVq2V+EEDuEEJuEEO8IIVKj1t0hhNgjhNgphGh4erFuwKjeSeQWVVLll3yztwiITBrUJ0VNP5rv9obdSlVht5IRc2hk0PcYysPbBOWwJ7+CnMJK8twtszJ2HXWzr7CyRft2ND5D2Wq3UuPs27WZa62LGd83CUq/BxmC8kOdLVbzCAawBrrHvdndaE/L4QWMCdej+BQYL6U8BtgF3AEghBiLmpd3nLHPE0IIazvK1u6M7p2ElHDHV9X84qU1AIzspQLVicbcEJXeYNitVGVYDGYqa2PZSlXNyFYqNNp1eFs4Xt7+1iZ+/8G2lu3cwZg9rap92t3WGMk7XuN2+yKySzdDwGgaWXagc4WqD085vH8zFOfUXP7V35j53XXga4KCCIWgeF+7iNcTaTflIKVcARTXWvaJlNIc9b4FzJ4F5wKLpJReKeU+YA8wo71k6whG90kGVM+kP194DEtunM0oI4vJZbdgEcpK8Ppjp7K2ZbZSQYWqhWhp1mtplZ+y6u7hbjAth2ptOTSIlBJbqTHQ7vggsqIsr+kHObIZAj5lbbx+GVQVN75PS5ASFt8Aa5+Hja+pAf6NheCtgLzVOPxlsPWduvsd3girnol83/QaPDZZBd/bCvcRWP4nqC5pu2N2ETqzzuEK4DXjcz+UsjDJM5bVQQhxNXA1QK9evWLOYVxRUdHquY3bghsmO8m2eciu2Ev+LsjfFVnntMLOnFzcVWYRXJDPly1jX556yi8qdTf4G44UVAOwc+8+llsPNijHmt1KObirvS26LsXuKgLe6ja9pu31N9p/UF2/b9eso+r7xm/vrnKvdDT7i6voFzykHg93fBhZUdqA5VB6AFIHAOCqPgL/PB9OvR+EBba9B0NOhOlXqm1LvoeUAVCZD188CKc+AM7EmscLhcDShOfTtc/DtnfBYof9K0EGlTKY+DPI32Fs8yJM/p+a+638O2x+A0adBSn9DCUo1fKLXohsJyUcWAVHNsH0X9CspmbrXoLl/wcb/gP/81bT9zMpy4OELLA5m79vO9MpykEI8f+AAPBKY9vWRkr5NPA0wLRp0+ScOXPqbLN8+XJiLe9o5lC/LCkrl5KamYk8cgR1KWDm8bN5KXcdHCkAm7PB3/DQ5i+hpJzeffszZ87YBuX4tGQz7N2PsDpadF2Cyz7G6mxYnubSXn+jNw6tg0OHGTlmHHPG19+bqrlyCCHOAB4FrMC/pJR/qrX+YWCu8TUeyJZSphrrgsBmY91+KeU5Tfs17cfqfUWcJY6oL8XGtK6ulPrdSgdWw7OnwNXLoe9kMgtXqRjFro/BqSxidv1XKYeS7+HvU2DOHVBVBGueg8E/gvE/VdsFvPCfS8AeB/NfjX2+8sPw6e9gwsXwye9g6BzIHAnr/w1VxlP6919B2X68jnSceasgfztkj4kc4+A69b79fZh2OeQsB1ucUmQluZA2WK1//0Y1yAP0mwL9pjb9Qh7eCAnZ4CmDT++BPlc3vo+vEiw2OLQBXjgL0ofBeU9C/2acNxaVhbD/WxhzduuOY9Dh2UpCiIXA2cClMtJI/yAwIGqz/sayHkuC00qlL4A3EArPT13pC0QVwTWxQjoQcZ8s2Xw4ppvJjDn4gs1vCy6lpNIX6DbZP+3hVjLiX48DZwJjgflGnCyMlPIWKeUkKeUk4O/A21Grq811XUExAOzas4t44UViPCUn9YX0oUo5/PtC5SqJxvT1Gy6ZjKLV6vv+byH3K2U95HyhBr7tiyEUgJWPqcEcYN8K9V5dCu/fBDnLYOdHUFEQW8A9n6mn/v9cpJTQTx6FQceDvwqOGnp285sA5A6eD1YnfPdUZP/q0ojS2/4+fL8SfBVw+h9AWOGrh9W6ku9h3csw2hhQ85rpcjq8CQbPUhbHziXKogJ1zO3v193eVwlPzYZHjlGuuKS+atlL5yiFFQspoXR/47Is+wO8dqlS5G1AhyoH4+nrNuAcKWV0x7jFwDwhhFMIMQQYAazqSNk6mkSnDbcngC8YIs1o2lftC4ZTWaMDzfe8t4X/bjlcY//a7TNyCir45SvreGtdXZ9xoRFz8LYg5uDxhwjJxmMgXYWwcmjbgPQMYI+UMkdK6QMWoeJk9TEfqOeRuGtQvF8lGIhBs9SCtEGQ0h/y1sCeT2GjIX7BTvWkX2kM4ke3gaeMlLKt0HcyhPzgLYdJl0LQC3uXwbbFkNhbPU17y9UT+r4VKl7wl+Hq2GPPAyTs/kQdN+BV25uYWVPjzoezH1HHGHh8ZH3mKChXz4+lqeNg4jzYuEg9PQMc3qDeB8xUrqhvHlcKZOI8mHaFshSOboO1Lyg30pkPKpkProl9wQI+ePEcZXWYVBVD2X7ofYyymCxW+h1cogbyD26GN6+Ewj01j7Psj0ppJRrWxiUvwRUfAQLeuRZCxv/ZwbXwzMnwn3nw7GnwyASliPetgGdPh6K9deXbYjyPfPW32L+hmbSbW0kI8SrKs5IphMgD7kFlJzmBT43WwN9KKa+VUm4VQrwObEP5WH4lpeweo1ELiXfYws3z0hIc7C+uosoXDBfB+YIhY14HwaurDrDpYBlTBqZx/hMreWTepDoB6aJKdayth8rrnMu0HFryMB3Jnuoefw5fuFdVm1o6/YBof0seMDPWhkKIQcAQ4POoxS4hxBrUvf0nKeW79ezbYDytreIj1QGJs2wf2GGXYywj+YojXhf+kGCA17h/SnLZ+PbDHLPpfnaPuBqXJ5+BQOnubzhY9RjjZJANWecz4chWrCEf3zhPYJrtbbwf3Eli5ffsG3wp8fEHcPhKKMyYyYg9/yKw+BaqEgaze8TVuJNGcKzzS6pWPEPw65fIKFpD0Opk9fS/43NmMHLnWjLtKazMWgglgPG7Z8T1wRaoYn/K8Qwv3ElI2CkIJrHKOoMZgRfJff3/kTtkPgO/f5OhwPqM85h0YBViz6fkZ81i28rV2Ow/Yqb1FbwvzcPpLaYsfRpb1u9hnGsQCbu/ZFXUNU4p3YoUFuKr8hi97wuOVFnYka/6UKWWbGISsLFAULJuF2Myj6fvwSWUvLCPFAkhYcP/rzOxBj2AhZDFhstbyKE+p7Nr5HVYRngJ7SoDyug19ArG7HiU71+4iqr4/oza+Th+ewqycB9CSpxAzrJ/E1d9mD5HvsX7z1M5MOBcSlPHUZE0nMyCbxnvKaU0ZSypO5ew+oMXqCCzVfdLuykHKeX8GIufbWD7PwB/aC95uhoJThu5RSr9Li1eTfJT5QuELQdQGUl2iwVfMMSGA6X8c0UOB0ur2XaovI7lUFalsom2H66rHIpMy6EFbiVzkA2EJL5ACIeta9dNmkVwnVjnMA94s9bDzSAp5UEhxFDgcyHEZinl3to7NhZPa6s4zXc5RQxZ/k+C1jhGnvlL2P00vUfPVDGHvMUqkFx2gInfPwuEGJllh+p4OACp3oOkWnLx2ZOZdO71UL0SSnI57oyLYUgy9jcWAjDkxzepGIEQpB3dCnv+hS1YSfIlTzG1/zQlSOW5uNY8p9w806/EsvZFjncvgdOfhYOPgxxU9/dmPwhBP8OT+sDe57FkjyYxKZkZc86BkncZnP8Vgxc8CW/8C9KGMPm8X8IJZ4LVQXZSH7LNAHifAPal90HQQuZP7mXO4FlgXQdL72POjGMgPl09xf/1F+oJPy4dgN62cnqbMq1U7q2Jp/8cEjJh+gSq/jGbtNJNMPl/sAyZg+3jO2HEGSq+EvRB+jD6HnsdfWsH5+WJ8EEZg9a+oL4POQHnRS9CXJqybB6bzFBnCVQchF7jcVaXMnyvce1OvQ/cX0BCFqlXLYaHxzLdvpvlzsGtul90V9ZOItFpDT/tp8crt1KVL0ilN0CCw0qlL0i1L0iVMcZICc99rXK0iyp94cpqs06i1Eg13XHYTTAksVqUL9njD4af/luSyhqtrKp9wS6vHMJ1Dm2rHJoTE5sH/Cp6gZTyoPGeI4RYDkwG6iiHjmLzwTKGiCOE0odiTRsMZzwIo38cccUcdz18/WgkOO0+Al6jFYynDLa/T1HWbPpYrCqQatZIjDoDrvivcolkjYqcMGsMJPdXAVdTMQAcM09lHZ39sHIfxaWpzKYZVyu3Ukr0JTcYa3jzvG5AQPboyLoJF8I710DeKhU7GHScWp4+pO5xxl+gXtGYsh1cCyNOhe+/Vu40ZzJUHIHUQVCwS/0zCqGC0Ul9lWIASMhg48R7Oc6zHE68XWV2HXNR/X+IaISAM/+iXFXxGXDmn8HmiKzvO1nFdKqKYM7tcOJvlWzvXAOf3KViPqf9HhKzVGxm96cw/tSmnbsetHLoJBKctrALxIw5VHgCVPuDDM6Ip7JIuZl8Ub2TzPD9kbLq8DLzCdmsQ6j2B8ktqgy36IieNKihgPT/fbSdeLuNm04ZUWN5ZZR7psofIAV7s39rR9LUmEN+uac57c5XAyOMeNhBlAL4We2NhBCjgTTgm6hlaUCVlNIrhMgEZgF/buqJ24PNB8s4xZaPPXOKGpSOvVatcKXAzGuVX/7wRtj4H5XdU35YKQB7AvgrIeilMHMmfUA9YUfTd5J6RWOxwDVfgCOh5vKBM+G2fZHU0eNvgC/+rLKKyg/BgAZKnZxJcNoDKqaw1whfjlIWAu9cC+5DMOYnzbswfSerJ/Ev/wZZo5Xisscr2b//Rimk//5WKctt76r1Y2uGnryubDjjiead18TmgEterke2KbDFSJXtN01ds8Rs+NnrKiGg1zj1HWDE6fDxHbiqj7ZMDoOu/RjYg0lwRvRyuqEczNhApjE3RJUvQKkRlxjVK4k4u5WMBAeHyzzhfSNupcikP9ui4g7mMYVouEL68+35rNxbWGe5mT2l5Gk/V40vEGqTuEYkW6nhmMP9H2zjhkXrm3RMo3DzeuBjYDvwuhEnu18IEZ19NA9VzBmthccAa4QQG4FlqJhDp5abb84rI9VSHXaVhHElq8BsXKrKvplyGYw8TfVcqiyIPInb4ylJm9i8kyZkKtdKbaJrCpxJKvB8aB1UF0Ny34aPefwNNRWIKwWGnwIl+5TSGHte82R0JqmsqMMb4fEZsOl1GHm6yuKafGnESln3Ivz3dhhxGvy4bYK/jdI3airbflMin612GDY3ohhAyQxkFNUTXG8iWjl0EgmOiHJIM9xK5lN+VpJSDh5/MGwRPHDeeP5782z6psZxqLSu5VBa7SfBYcVuFTXiDmamUq8kV4NupZIqX0w/fW23Ulvxn+1env0q0srgj0u2M/+ZbxvYo2nUbmRYHwVuL0UVDU/FGo2UcomUcqSUcpgRH0NKebeUcnHUNvdKKW+vtd9KKeUEKeVE473euFtH4Pb4ySmsJF5WR+oTYtF/Kpzzd+UOch9RWUCZIyF1IIw4lZC1nYq2eo2DfV+qz0mNKIdYTJynaghO/7/mFbOZTPk5/HKlciv5KlShnUmm4Sr75nFlUf30X0qRdgR9JgICMobXtdZqkzEM0oeRXty6SnDtVuokEpyR1lFmQLqgjuUQUQ69k10MzIgnNd4enijIYbOE6xzKqv1kJjmJs1trKQd1zAHpcezPjz0YhkKSkio/GQl1/+FruJXaUDlsLAgS3FPIlT9S/uA13xfXmXe7JfiaGHOo8AaM3/PDej7anFeGhRCOUDU4EhvfIbmPciWBevpf8L7ywa/a1D4CZo+JtPNozHKIxZhz4NY9Kn7RUtIGw8UvqfiKK2qGvKTe4EhS6bljz63rJmtPnIkqltCniRbb+AsI7PguEh9pAVo5dBLRbqXUWpZDLOWQYiiQ1HhHeJBOj3eE3UqlVX5S4uxkJjrJd0fcToXGMfulxrHncGlMWdyeAMGQrFFQZ1JVw63UdumhvmBkAA+FJHvyK7BbWz9QR9xKDccT3B6z4LBrx1DahIAPnpgJJ9/Nt4fGkSSM+6N2xkwskqKqzBOyIlXF7UV2VG1hcgvmtRaidYohmmjFYB47a6QKWI87v23O0RwuXxIJPDbGSXex3bKcXi1UDPBDe2zqQkQrhwSnFYfNUsetVG0oByEIV1GnxkUGs9R4e42AdEqcnZQ4e40meUWVPhKdNlLi7PhCsW+sokp13phuJV/7uJV8IRk+34GSKjz+ULgJYauOG3YrNazIKryqOj1YzzXpUVTmqwrn3Z/yXU4Rk3sZ91BDbiWTpN6Rz/GZ7SNfNL3GRT4nN97+pMPJHqsC8yNO65zzt2Kwby5aOXQSiVFuJZfdSpLTRl6JiiVkJkZSW8uq/SS77FiM1NTU+IhySIt3hAdUUzkku2zhmgdQg2Cyy4bLYa03IG0W48XK3mmvgLQ/GHm633VUucl8wdYP1k12K3nU7/J0j9q+1mFUDYcObWD9gVJm9jNSJJviVqptObQ36UNVxpEzuWnKq6M56S64/MOOdSl1Elo5dBLRAWmnzcLMoenhugfTcqjyBSir9tdQCKYLClSWkzmgmtulxNlxewOEjEG2whMgyWXHZbMSCBFeHk1xZSQNtjbRCqGxCYiaipQSXyhiqew6GplOtTXFa8GQDCuXhqwcjz8YViItKQzsdlQZWWhGK4ypvY17r7mWQ0IHWA5Wuwr8JnVBqwHU9YjOHOrBaOXQSUS7lZw2K6ePi/wTmjEH062UEu1KquVWgkhWU0qcneQ4O1IqnzqA2+sn0WUjzqEslVhxhWLDreQLhOooD9PyUPJErIhgSPLsV/ta5Gry1pqtra2Ug1kAp45Tv4uqIsoa8nSPfoKto7IIAIsMMMpygLGZhtXaFMvBkQBOw/feEZYDwOxbYNZNHXMuTb1o5dBJRCsHl93CSaOzcRgB2dR4OzaLoNofDAeaTaKtCLM+orjSRzAkSY1zhLc14w4VngCJThtxdkM5xBg0TcsBIgO3SZUvQGZSJEBusvlgGQ98sI1Ptze/0MZUANVh5VARXteayuZo2RsKnldEaQTPD8lyAE5OOUIiRtFYUwLSoJ6Wba6Oc6WM/6mqK9B0Klo5dBLRqaxOm5Ukl51ZwzMA1ZQvzmGlyhekvLblEOVWMj8fLVfZJ2ZAGqDcowZ8tzdAosuGy67+1LEG35KoArraT+4V3iBJLjsOm6WGlWAqn4Ml1TQXU0F5/EGCIcneggp6J7tqrGsJZjA63mFtUMm4o5SD9wdhORQihRW3jOPY+Dw1gxo03aef3EdZDR0YDNV0Plo5dBKJzpoxB4DrTxrOr+YOw2oRxNmtsd1K8XVdTEdM5RBvj2k5JLtsuAzLIZYbqLgySjnUcjtVGb2e4g1lZeI2lM/B0iqaizlwe/whKjwBfIEQgzLijWVNsxxCIcmtb2xky8FIm2czjpASZ1etxusJbru9EUvph2I5BF3pbJODGB7cp4q7QOXsN4WJ82HqgvaTT9Ml0XUOnUSc3YoQYLdYwplIUwelM3WQqn6Md1ip8gcpraUczGpql91CvBFHOFIWZTnE11QObsOt5Aq7lRpWDrWVR6UvSFqCg3h7beWgHrlbZjlEjlNsWC0ZRoZWU5VDSZWPN9bmMSA9nvH9lE/ctBySXXYOl3nwBkLhWEs0FT84y6GISlsqOaE+TPVuiTTRa6pbaeK89pNN02XRlkMnIYQgwWELWw21iXPYKHR7CYZkDeVgBofj7NbwgJ9v1Eek1rIcAsEQ1f4giU57VMyhEcuhllvH7BIb57DW6FcUsRxapxzMFhZmdXZT3Uqm9REtu6kczGtQX9zB/QOMORTKJNzWNKyeIlX5K6wqjqDR1INWDp1IgtOK0x77TxDvsIbdRdGuJJvVQpLLRrzDFt63huUQpRzMvkiJrmjLoe7gW1LlC+9Xx63kC5DgVOerz3KQUiKl5P+WbGfjgdJGf3e0DGbvp9qWQzAkawz8dY8RDMtuElYOxvWqL+7ww8tWKuSgLwFnWm+EDKmZypxJOoagaRCtHDqRBKcNp62u2wNUv6W8EuXPj7Yc1DoHcY6I5WAGpFPjHMTZVfO9smp/2LeeFJWtFGvALK700SfFDAjXDkgr5RBXJ+agRtVKIy5S4Q3wzxU5fLz1SKO/u4ZbyVAAGeZUqca6d9cfZPaDn9f79G+25K5hOQRrWg71uaiilcMPoc5BVhWSWx1HWpbRjqI4p2sWmGm6FFo5dCIJUU//tbnmxGEEjIBqci3lkBqv3ESmS+pQWTUOmwWX3YIQgmSXaqFhDuBJDWQr+QIh3J4A/VJVO+XoFhbBkMTjDxFvBKSrfUHuencz3+UUhbOhAPJKqsOzzVU2wYkfy62UnuCssW5/cRWVviCF7tjWg6k0GnIr1TenQ7nHj8NmwWmzUN3TLYegH+EpoyiUxLAhQ9Wy4n1Nq3HQ/KDRAelOJMFpDSuA2kwfnM6C4wbzwspcshJrdks9bmgGvmAobDkcKK5mXN9kjHm5w/2VzCfkmm6lyMD81to8nvkyB4C+hnKIXm8OwIlOG/EOK2sLK9l8sAyH1YrbE8AiICRV3MF88q9oaNIIg2jXlVkVXtutZMpeUuVjoJHJFI2p5EoasBzqszoqPIFwrypvsOWps92CKlUAV2VPY9QwQzl4y8A5shOF0nQHtHLoRGaPyKrRJK82d5w1mhNHZTE8O7HW8jFAzWDwxAGp4c/JcXbKq/3hrJxEZ1SFdNTg/9GWI+w4ojJXRvZWboZoy8KMWcQ7bMTZbbiNAbuo0ovb42doViJ78iuMuIO5T/2P4k8u38ua3GJOHtMrvKwwHJA2lYMarM2Ad3RMIRrzdxTHiDmkNiHmkOiyEZIST7BnN1cKVhRgBfr27Y8tOXLdteWgaQztVupEfjV3OHcaA30snDYrc0dlhy2C2riiMp0m9o+0FzYtB3MwT6rHctiT7+asCb356rdzOdUYsM3Budzj5+31eYCycOKjUkKLKny4PQEGpcfjsls4WFodHuQrG6hMXpNbzNr9JbFjDok13UqmS6y0KrbyjK6VMC2E2m6l+mIObk+AJJeNBIetx6ey7t6XC8CoYUPAlaqa2oGOOWgaRSuHbow54ENNyyGsHIynb9V4z4g5+CLVyfuLqxiRnUT/tPhwTMIcUG96dT1//u9Okl02RvZKqqEcCiu84QG2f1o8eSVVYeVQ0cBoW1jhpbzaX+OJ3oxVJLts4ZYhEFEO9VkO0cFxU8H4DCsg2dWI5WDUfiQ4bT0+lfXAgQMATBgxXGUnmf2RtHLQNIJWDt0YUznE2a0Mz4q4CcIxhyi3ks1qwSoiA2ZOQSUhCSN6JdY4licQZN3+EpbtLODXp45kw92nMaZPco1issIKH26PnySXnYHp8ewvblpAusDtJSRrBpGLKn04bRZsVgsuuzWGW6keyyFKOZQYvaHCRXDhmEM9loM3QKLTTrzD2uNTWX3lqvdVUobR2NFUDtqtpGkErRy6MVaLwG4VTOiXgi1qFrUUI+bg9gQQgvBTv8MasQx256tYw4hs9QRpZj55/CEe+Ww36QkOrvzRkHD1tnmM9AQHxZURy2Fgejz7iyojbqV6AtJSyvA0qGbRHqiOsGYTQpc9Mu2p6RIrbSTmAJG4g6kcspOcCEG9mU4VXn/ErdTDLYc49z4qiYN41bcrPBF9U6ujNT9YtHLo5gxIi+dHI2r22U+JsxOSqudSotMWjlk4rCI8qO7Jr8BqEQzOVJlAQghcdgsVngBf7i7gomn9a3SOjTfmn5gzMouQhEBIkuSyMzgjnkpfkJ1GYLs+t1JZtR+/MRDnl3uwGmGUkIwoHpfdisdX263UcMwBIi3HvVHtMwalx7PjSHnMfU3FFu9suuUghDhDCLFTCLFHCHF7jPUPCyE2GK9dQojSqHULhBC7jVeHNinKqMrliGNQpOAtwVAO2nLQNILOVurmfHTzbGyWmjreDMgeKK4Kp2wCOCxRlsPRCgZlxNcownPZrRwuU5lH/Y3UVpNTxvQi3+1lWFYCb68/CKhAt1kfkVOoJqGv9Abw+IM89PFOUuPtnDupHwPS48NToIJyLyXYBeU+pSzMiY9cdmvEcjDcSvVZDlW+IEKoKXXNluOm8nHYLIzunRzOxIpGShmOOUDT2mcIIazA48CpQB6wWgixWEq5Leq4t0RtfwMw2ficDtwDTAMksNbYt6TRE7cBfQP72Z86M7IgUcccNE1DWw7dHKfNitVSM5tpcKbqu78pr4wkV6SAzmGNPHHvznczolaKrMtmDafHZtSqreid4uLXp44Mz1IHSjkMqlWDEAhJVu0r5l9f7eOhT3ax4PlVBEOyhnLId3uJs4HNdFk5TcvBgscfwh8MhWMPDaWyZiQ4sFpEuNbBdCs5bBbG9Ekmt6iyTq2DNxAiEJIkGi1ImpitNAPYI6XMkVL6gEXAuQ1sPx941fh8OvCplLLYUAifAmc06awtpXQ/PDQKf85XZFFCdeqIyDrTctDKQdMI7WY5CCGeA84G8qWU441l6cBrwGAgF7hYSlkilN/jUeAsoApYKKVc116y9XQmDkhR1b/+IImuKMvBKqj2h/D4g+QWVXHm+JpTMbrsFg6ZyiHBQSwyE2sqh/5p8eFiuKwkJwVuL/uLVduP6+cO5x/L9vDBpkM1jlHhDZCaKHDZrarmwBlpJljtC9bomlpSWX9A2nR1hWMOwSBWi8BqEYzpk4SUsOOImykD08L7mcHwlDg7waAkIJVScdTTANGgH3Ag6nseMDPWhkKIQcAQ4PMG9u1Xz75XA1cD9OrVi+XLl9dYX1FRUWdZLNKL1nFMxRFK3rmNbCDPmxDeL/toAWOBLbtyKSxp/Fj10VRZOgItS2xaK0t7upVeAP4BvBS17HZgqZTyT4bf9nbgt8CZwAjjNRN4knr++TSN47RZmTY4ja/3FNWYNyLFIcgrqWLHETfBkGR8v+Qa+7nsVnKL1MBuVizXpqZyUJMA9U2NI6+kmkGG++iAoRwuPXYgn20/yqNLdzNv+oAax3FYBS6UcoiOOVR4A+F4Q0qcvV63UrU/GG4hUlwRsRzM2fTG9FG/bfvh8hrKwZRtQFp8uFVIlS+Awxb797aAecCbUspmV9dJKZ8GngaYNm2anDNnTo31y5cvp/aymGwuhM2Q7d4KwNBppzBj6nS1bn8cbP8b4489CQbMaK6IzZelA9CyxKa1srSbW0lKuQIorrX4XOBF4/OLwHlRy1+Sim+BVCFEF51hvHtw7BCVnRJtOQxLtZBTUMnXe9S0keY8CCbRdRNmC+3apMbZMb1YScaxTdfSoAzlzjpgNAzMSHBy1eyh5BRU8sWugrAbCZSLK86hbr8aMQd/KNy3aWC6Cnb7oqb/DATVJD5VviBxDqvKnorKVrIbke7+aXEkuWxsP1wzKH3AmH9iQHp8eDa+ysbnwT4IRGu3/sayWMwj4lJq7r5tgycyAZJX2knrG+VWGjADrvwU+k9vVxE03Z+ODkj3klIeNj4fAcx6/vpM78PUojHTG3qWaddSXOVGj6LigvD5+zl9gOC5L3aRYIfdG75jT1T1tadSDZwWAetXfY2lnsrsRCOYvHX9Go7EWbB7VTwhZOTUb/0+nzgbrPxqBVaPGthX7iki3SVw+8AXAosMEvSrbrKlhUdZvnw55cUeistDfPntGgCcATVj2ZLPlpPqUork/76rZnCKhaNlIawCbHbBocoQy5cvJ/eAF0LB8O/tExfiow0HyPAdZVKWFSEEX+72IYC9m1aRe1Rdo+VffUO/xAafk1YDI4QQQ1AD+zzgZ7U3EkKMBtKAb6IWfwz8UQhhmi+nAXc0dLJWE6UccmQf+qdFzf0sRKssBs0Ph07LVpJSSiFEs5PMGzO9oWeZdi3l+ECIxzd/xuTRg5kzRzVZ83y2DMvWKoo8ktkjMpk7t6bn7oV9q9hRXEB6gpOT5s6t99h9N6yg/IibU+fOJtllZ5dlL8sP7GDOtHG8s2cDpX4r2Smu8O9+bOtycgoq6Z+VwuHSavLdXuIdNmwJiRyqKGPEkIHMmTOGDws2sr+6kGGjx8HqtUwdPYg1R3MYM2k6o4zeTzcs/5jszFQc8T6yk1xkJznZvyOfOXPmsDh/A0kVxZHr3SefO9/ezKPrPLx//Y+Y0D+Fh5c8Sp/UcZxy0lwsO47y5MY1jD1mMpOjXE+1kVIGhBDXowZ6K/CclHKrEOJ+YI2UcrGx6TxgkZRSRu1bLIR4AKVgAO6XUta2qNsWTxlYHeS5RrCxoh9jXPbG99FoatHRyuGoEKKPlPKw4TbKN5Z3vOndw3HYLPz3phNqTBTksglG9U5m++FyxvVNqbOPy0hrzawn3mCSmehECDeJhjvop1P647RZw37+smo/Q7MiT6vHDs0gp6CSrEQnld4A+W4vditYjPOZgeU4hxWPPxiulRiYrtxVZsaSxx/E7QlQXOnD41dupeQ4e9gNVTuwPGdUNv+4dAoXPLGSQqMWYv3yJRTv+zO3lV7KBZf8D7+Z5mRoVuM5/1LKJcCSWsvurvX93nr2fQ54rtGTtBWeMnCl8Kfef2NXfhV6kk9NS+joVNbFgFkEtAB4L2r5ZUJxLFAW5X7StJDeKa4acQSAKQNTAZjQr65yMFtk1BeMNslIdJDosIWrpzMSnSw4fnCNorn0+Mgxjh2q4h9ZSc7wNKcOi8BlnC/BGQlIVxsKACLKwWy+Z7b3Lq70Ue1TAelklw2PP4Q3EMQfjASkTcw6DzMDqve5t3LFQ68zbNgwbv7V1Tx5z0289vLzuN11ayK6LYZyyCsP0is1ofHtNZoYtJtyEEK8ivK9jhJC5AkhrgT+BJwqhNgNnGJ8B/VElgPsAZ4Bftlecv3QmT0iC4fVwmRDSURjNt9LrycYbfKTY/qy4PjBdZabloQ6RpRyGJIOQK9kZ7juwmGNdJUNt8+wqTqHcqON+YA0UzkopVBo1EoUV/qoMrKVzD5K5dWBmCmpZkDe7VHFefluL8P7Z3PhhRcyb948ioqKeOedd5gyZQp///vfG/zd3QZPGdKVQk5BBQPS686FodE0hXZzK0kp59ez6uQY20rgV+0liybC6eN68d2dJ5MWo47BrJaur8bB5JSxvThlbK86y00LACA9yvrITnbx/OXTmdAvhb0FqpLaYRHYHKZbybAcjHezGV+vZDV1qZmNZBbSeQOqUC7OYQ1Xg5d7/PiCdZWDqYwqvH7ySqqo2v0d/1n1d54pOMhll13Gk08+yfnnn09VVRVjx47lhhtuaPC3dws8ZXitSZR7AmFXn0bTXHT7jB8YQoiYigEiqayNxRzqw2ZVU296A6EabiWAuaNUZW7YrWQFh6GMwqmsxvcCt5ckl82YJ9sSroA2m/uBKrpTbiXTcvDXSGU1ibdbEUK5lQ4UV1O162uu/N2vueriHwOEM5vi4+N59tlnW/S7uxyeMsriVZuMMb11JbSmZej2GZowplupduuM5mAW3aXXo4BMN5DdGolxxEfFHMBUDmq7jARnONYQrRwAIyCtzldmKAeHrWaMxWIRJDpslHsC7C+uImXWzzh97qzweq/XS25uLgAnn1zHqO2eeMooDKieV6O0ctC0EK0cNGHi7JG23C3FjB/UF9Q2n/QdFhFWBgnhbCV1OxZUeMMFdmkJ9rDlEN2fyZQ34lYK4A3UDUiDKtar8AbId3sofO9P9EqONBW0WCxcdNFFLfuxXRVPGYc8Dgakx9XoraXRNAetHDRhWutWgohySIuPfYykKLeSaamEs5VquZVABceLw5aDr0ZMIc5Ry60UDIXnpYgm0WXD7fFTXOnHSginM2IZ2e12fL7YLTq6JX4PBL3sr7IzureON2hajlYOmjDmgJyd5GrxMRKdZlA7tmvKdCs5LCJsqUQm+1HfK7yBsEWQHm+PBKQrvDVmvIvOViqr9uPxBXHaY1kOdiq8AUqrfMQlpbF48eLwuq+++orMzMw6+3RbjOro3AqbjjdoWoUOSGvCnDm+Dylx9lalP5oDfXq9biW13m5V/vChmQlhK8MV5db6xeyhxmdnuLFeodvL6D5J7MmvwBcMhRvvOawWSqt8HHV76Z1cV7ElOm2UVvkIBCUzf/5b/vjHP3L99dcjpSQlJYV33323xb+3y2EohzKZwHE6U0nTCrRy0ISJc1g5eUzdFNXmkOC04bBaSHBYY64fmK5afGfGCeaMymaOkcUEMGlAKjeeNJxLZgwMTyKUkeig0hfE4w9SUOFldmIm6QkOjpR7iHOofknJcXb2FlQSDEn61JqkCJRb6UBxFR5/iMGDh7Dk22+pqFB9m9asWcPw4cNb9Zu7FIZyKCdep7FqWkWTlIMQIgGollKGhBAjgdHAR1LK2M32NT9Y+qfFMSQzITw1aW2GZiWy4Z7TWPft13XWxTms/Pq0UTWWmVbF0XIPbk+AzERnDeUAkBxnC09T2i+1ruWQ7LKF56SeHJ/Khx9+yNatW/F4POzbt48VK1Zw991319mvW2IoB68tiUG6AE7TCppqOawAZhudJT9BNRG7BLi0vQTTdE9+fepIfjW34Sfx5GZk0JiZU7uOqif9rCRnOBPKjFkku+xsKCgFoE9KDMvBqQLSwZBk+XN/5FCGg2XLlvGLX/yCL774AoulB4XePKUAZGZmhdubaDQtoan/FUJKWQVcADwhpbwIGNd+Ymm6K06btVmDf2NElIOyDEzLASLKwQxeA/SN4VZKctmN6UcleTs28NJLL5GWlsY999zD448/zq5du9pM3s5GGpZDn169O1kSTXenycpBCHEcylL40FgW26ms0bQhpiL4bp/qcj0wIz7saooPu5WUckhwWMMB72iiZ8NzuZTbKT4+nkOHDmG1Wjl8uIf0eDy6lYrCPAAG99NzZWlaR1PdSjejJih5x+hjPxRY1m5SaTQGZp+nb3OKiHdYGZaVGK7DMHsxmQqhb2pczFhH9Gx4PzrpdEpLS7n11luZMmUKfr+fX/2qB7T1qi6Bf55IggzhlTZG9svqbIk03ZwmKQcp5RfAFwBCCAtQKKW8sT0F02hAuYwsQs3VMGlIOlaL4MKpA4z238piMC2HWJlKEFEeUoY4Ye5cUlNT+elPf8rZZ5/Np59+ytlnn90xP6Y9yVsDIT8WoJxkRulMJU0raZJbSQjxHyFEspG1tAXYJoS4tX1F02hUbyTTjXSMMQdF7xQXl0wfGN7GjDn0TYldvJfoVOuFsPDXe34bXu50OklMbHyin27Bge9AWPk8fR7f2qbrthmaVtPUmMNYKWU5cB7wETAE+Hl7CaXRRGN2kT1mQGrM9aYFESsYDTXdSieddBJvvfUWUTN59gwOfAe9x/Ns/OU8n/G/nS2NpgfQ1JiDXQhhRymHf0gp/S2Z/1mjaQlmUHpi/7qz1wHhzqx96rEczLYgFgEvPPcvHv/7o9hsNlwuF4FAAJvNRnl5eTtI3kEEA5C3FiZfSuFOH4MydH2DpvU01XL4J5ALJAArhBCDgG7836TpTmQmOkiJs4enDa1NltFifFBG7CkxzalCU+MduN1uQqEQPp+P8vJylixZ0r0VA0D+VvBXwoCZFFZ4yUxqect1jcakqQHpx4DHohZ9L4SY2z4iaTQ1+eWc4fx0Sv96q65nDEnnlV/MZPrgtJjrTf97WrydFStW1Fi3ceNGLBYLJ5xwQoMyCCHOAB5FpXD/S0r5pxjbXAzcC0hgo5TyZ8byILDZ2Gy/lPKcBk/WXA6uAyDQZyrFVdvIbMV8HBqNSVPbZ6QA9wDmf9AXwP1AWTvJpdGEGd8vhfH9YruUQM1uN2t4/Z1VXXYLViOw/Ze//F94ucfj4ZtvvmHGjBl8/vnnDR3fCjwOnArkAauFEIullNuithmBSveeJaUsEUJkRx2iWko5qbHf2WKMquhiSwpSqipyjaa1NDXm8BwqS+li4/vPgedRFdMaTZdGCEGi00ZagoNn3n+/xrrXX3+d1157rbFDzAD2SClzjOMtAs4FtkVtcxXwuJSyBEBKmd9W8jeKvxqAgmplWWW1Yj4OjcakqcphmJTyp1Hf7xNCbGgHeTSadmFEdiKjetWd3yArK4vt27c3tns/4EDU9zxgZq1tRgIIIb5GuZ7ulVL+11jnEkKsAQLAn6SU78Y6iRDiauBqgF69eoXntzapqKioswxgaM5O+lmcfL5yLQAHdm9jeeHOxn5Tq6hPls5AyxKb1srSVOVQLYT4kZTyKwAhxCygusVn1WhaQ9Fe2PoOnPCbJu/yxrXHAXDDDTeEYxehUIgvvviCKVOmtIVUNmAEMAfoj0rcmCClLAUGSSkPGp0FPhdCbJZS7q19ACnl08DTANOmTZNz5sypsX758uXUXgZA5ftQlEjfoaNh7UZOnX0sgzNjB+fbinpl6QS0LLFprSxNVQ7XAi8ZsQeAEmBBi8+q0bSG7Yvh8wdgxtXgalolsKkQpk2bFl5ms9kYNWoUN9xwQ2O7HwQGRH3vbyyLJg/4zmhjv08IsQulLFZLKQ8CSClzhBDLgclAHeXQYvzVYI+nsELNsa1jDpq2oKnZShuBiUKIZON7uRDiZmBTO8qm0cQmYMz5HAo0e9cLL7wQl8uF1ar6Mi1dupSqqiri4xusDVgNjBBCDEEphXnAz2pt8y4wH3heCJGJcjPlGG3uq6SUXmP5LODPzRa8IfxVYI+jwO0lzm4Nz8an0bSGZjWyl1KWG5XSAL9uB3k0msYJqifkliiHk08+merqiEfU5/NxyimnNLiPlDIAXA98DGwHXjcaUN4vhDDTUj8GioQQ21BNKW+VUhYBY4A1QoiNxvI/RWc5tQk+pRxUjYMORmvahtY8YuiZRDSdQ8BQDsHmT0To8Xhq9FOKi4ujqqqq0f2klEuAJbWW3R31WaIemH5da5uVwIRmC9oc/FWGW8mnaxw0bUZrpsBqcfsMIcQtQoitQogtQohXhRAuIcQQIcR3Qog9QojXhBD6EUgTm6DpVmq+ckhISGDdunXh7zt37iQuLnZPpm6DvzpiOWjloGkjGrQchBBuYisBAbToP0oI0Q+4EdXMr1oI8TrKh3sW8LCUcpEQ4ingSuDJlpxD08MJK4dgs3d95JFHuOiii+jbty9SSvbt28fixYvbWMAOxl8NSb0prPAyZVDsKnGNprk0qByklHUTw9vuvHFCCD8QDxwGTiIS5HsR1YZAKwdNXcyAdAvcStOnT2fHjh3s3KnqAI4cOcLUqVPbUrqOx19FyB5PUaV2K2najg6fWd1I63sI2I9SCmXAWqDUCPyBSgvs19GyaboJ4YB085XD448/TmVlJePHj2f8+PFUV1fzxBNPtLGAHYy/Gi8O1TpDV0dr2ogOz3kzUvvORc0JUQq8AZzRjP0brCKFnlWl2Jb0FFnGHTlEFrB21be4k4uate/DDz/MuHHjwt+FEDz88MOMHTu2RbJ0CfzVVEllMWjLQdNWdEZC9CnAPillAYAQ4m1U7neqEMJmWA+xioyAxqtIoWdVKbYlPUaWg09AIUydPAkGTG/WrnFxcZx44onhorilS5ficDi6zHVpEf5KKkOq82yGVg6aNqLD3Uood9KxQoh4of5DT0Y1MFsGXGhsswB4rxNk03QHWuFWOuOMM7jkkktYunQpS5cu5YEHHuDMM89sYwE7kKAfQgHchnLI1G4lTRvR4ZaDlPI7IcSbwDpUI7L1KEvgQ2CREOL3xrJnO1o2TTehFQHpBx98kKeffpqnnnoKgKFDh9Yoiut2+FWNRnlAKQU90Y+mreiUOnsp5T2o+SGiyUG1RtZoGibY8vYZFouFmTNnsnfvXl5//XXS09O58sor21jADsRo110asOGwWcKz3mk0rUXfSZruRwvaZ+zatYtXX32VV199lczMTC655BJABai7d7xBWQ7FPitZic56Z8vTaJqLVg6a7kcL3EqjR49m9uzZfPDBBwwfPhxQiqHbY1gORT6bjjdo2pTOCEhrNK2jBe0z3n77bfr06cPcuXO56qqrWLp0KaodUjfHUA6FHotOY9W0Kdpy0HQ/WtA+47zzzuO8886jsrKS9957j0ceeYT8/HwefvhhfD4fp512WjsJ2874KgE4Wq2Vg6Zt0ZaDpvvRiq6sCQkJ/OxnP+P9998nLy+P4cOH8+CDD7axgB2IYTkc9VjI0G4lTRuilYOm+9GKrqzRpKWl8ZOf/ISlS5e2gVCdhBGQrgg5tOWgaVO0ctB0P1qRytrjMCwHD05d46BpU7Ry0HQvpIxyK2nlYFoO1dKhs5U0bYpWDpruRShAeIqRVrqVegSG5VCNkyztVtK0IVo5aLoXpksJWhSQ7nGE3UoO3XRP06Zo5aCJ4PfA3mWdLUXDmC4l6NCYgxDiDCHETmMa29vr2eZiIcQ2Ywrc/0QtXyCE2G28FrSpYP5KAsKBsFhJjbO36aE1P2y0ctBE2L4YXj4Pyg91tiT1E205dJByEEJYgceBM4GxwHwhxNha24wA7gBmSSnHATcby9NRfcRmonqH3WPMadI2+KvxWZykxduxWHTrDE3boZWDJoKnTL0bhVVdkmjLoePcSjOAPVLKHCmlD1iEmrAqmquAx6WUJQBSynxj+enAp1LKYmPdpzRjcqtG8VfhwUV6gg5Ga9oWrRw0EQz/dY2n865GtELoOLdSP+BA1PdY09iOBEYKIb4WQnwrhDijGfu2HH81HhxaOWjaHN0+QxMh4FHvXVo5dE7MoQnYgBHAHNRMhiuEEBOac4DGpsCNNbXq+MMHcAdtBCrLOnQK2J4y5Wxb05Nk0cpBEyGsHLpwFlDnuJUOAgOivseaxjYP+E5K6Qf2CSF2oZTFQZTCiN53eayTNDYFbsypVfc/TF6Ri9FD+jFnTrN0UavoMVPOtjE9SRbtVtJE8HcHyyHardRhymE1MEIIMUQI4QDmAYtrbfMuhhIQQmSi3Ew5wMfAaUKINCMQfZqxrE2QvircQTvpCTqNVdO2aMtBEyHQHWIOHe9WklIGhBDXowZ1K/CclHKrEOJ+YI2UcjERJbANCAK3SimLAIQQD6AUDMD9UsritpIt6K3Eg4sMHXPQtDFaOWgi+LuDWym6CK7jYg5SyiXAklrL7o76LIFfG6/a+z4HPNceconyg+TLqTogrWlztFtJE6HbBaS7sBLrCKqKsXpL2Sd7a8tB0+Zo5aCJ0C2UQ8cXwXVZivYCsE/2Jl033dO0MVo5aCKE6xy68BO56Vayubq2nB1BsVIOubK3ditp2hytHDQRuoXlYLiV7PHacijaSwgL+2Uv0uK1ctC0LVo5aCLtMrqFcjBkcyRqy6F4L6WO3sTHxWG36n9lTdui76gfOuWH4MHB8P033StbyZGgLYeiPRyx9dPBaE27oJXDD53S/eppvDine9U5OH7gbiUpoSiH/aKvjjdo2gWtHHoqphXQGOFOrBXdo0LatBzs8V3bwmlvKgvA5yYn1Is0rRw07UCnKAchRKoQ4k0hxA4hxHYhxHFCiHQhxKfGhCiftmnP+x8a+76EBwdBZWHj25rKwevuHr2Vgj4QVrA5f9iWQ+l+AHb7MknXwWhNO9BZlsOjwH+llKOBicB24HZgqZRyBLDU+K5pCSX71EBfcbTxbaMth24RkPYqxWCx/7CL4HwVABz12klN0DPAadqeDlcOQogU4ATgWQAppU9KWYqaPOVFY7MXgfM6WrYeg1mvEGiCa8lTqt6jLYdAF1YOAR9Y7WC1dWj7jC6H8TcuD9q15aBpFzqjt9IQoAB4XggxEVgL3AT0klIeNrY5AvSKtXNjPe+hZ/VUbwkD9m9hGLB+9beU7XY3KMvQvVsYCOTnbifbWHZw/z52t7PMLb0uIw/kkhkUlBYWk1hRxqpWytmV7pVm4a8CoBqnrnHQtAudoRxswBTgBinld0KIR6nlQpJSSiGEjLVzYz3voWf1VG8Ry1ZCDkyeMAaGRc4dU5byt+AAZCdYlMoG+vXOol87y9zi61L6OlQlkd27H+QdbPW17Ur3SrMwLAePdJAar91KmranM2IOeUCelPI74/ubKGVxVAjRB8B4z69nf01jGE+VNSbGqQ8z5hAdvG7PgHTAB189jCXYBNli7u8Fq0O5lrRbSVkOOltJ0w50uHKQUh4BDgghRhmLTga2oSZPWWAsWwC819Gy9RiaFXMwlUNBZFl7BqRzlsNn95Jaurll+wcN5WCxNT0gbTSo61GE3UoO7VbStAudNZ/DDcArxqxaOcDlKEX1uhDiSuB74OJOkq37E1YOzbAcqooiy9pTORTsAMAWqGrZ/kE/2Ezl0ATL4cBqePYUuOZL6HNMy87ZFTHdSjhI024lTTvQKcpBSrkBmBZj1ckdLErPJOxWaoblQFSIpz3dSgU7AbAGm1ikV5uAF6zOpruVKo6o96LdPUw5VBGwOJFYSInTykHT9ugK6Z5ISyyHaDrAcrAGq1u2f9Bn1Dk00a1kXovyww1v193wV+MTTpJdNmy66Z6mHdB3VU+kqZaDlFBdWnOZsLSfcpCyfsuhqhhKchs/RtCoc7DYlIVTfhgKdtW/vXkt3D1NOVThEw7dV0nTbmjl0BNpquXgqwQZBFdqZJkzuf3cSuWHwKfqLupYDp/cBa80IcwU7VYKBeCze+DFsyEUjL192HI42ArBuyD+aqpxkqqD0Zp2QiuHnkhTs5VMl1JK/8gyZ3L7WQ6GSwliKIf87eA+0vgxgj4jIG0HpNqn4igcWBV7e9NyaKVbSQhxhhBipxBijxCiTmsXIcRCIUSBEGKD8fpF1Lpg1PLFrRLExF9NtdTBaE37oZVDT6SpdQ6mckjuF1nmaoHl4Peo+SAaw3Ap4Uyu61Yq2QfecgiF1PcDq+GNhXV/Q8Cjpgi1WNX36mL1vuODemQzLYdDjctXD0IIK/A4cCYwFpgvhBgbY9PXpJSTjNe/opZXRy0/p8WCROOvojKk01g17YdWDj2RZlsOUcqhJZbDljfh+TPBXavRn6csMsscwJFNEJ8BqQOxBaIsh+oS9UKC15Bp7fOw9R31isZboSb6sRpPzFUl6n37+yqmURvzWrgPRxRP85kB7JFS5kgpfcAiVC+wzsNfTUXIrgvgNO1GZ9U5aNqTpgak67Mcmhu8rSwEJFTmQ1JUS6xX50PaYDjvCWVd7FgCo8+C4hys3ijlULyvpkyuVNj7ufr+3VNwzCUghPruq1RThFoM5VBdDPYEKP0e8rdBr3E1ZTOvRcgPVYWQmE0L6AcciPqeB8yMsd1PhRAnALuAW6SU5j4uIcQaIAD8SUr5bqyTNNY3LLoP1JTifCpCaZQczWP58o5vJtCVelJpWWLTWlm0cuiJNDUgHTPmkNR8t5LRPlo9/UeRvz3yeddHyio45hJY+XesweLIupJayqFgh1JQ/abCwbWQtxoGzFBB50C1Ug6m5eCvgnHnKwtj34oYyiFKCZUfaqlyaArvA69KKb1CiGtQnYVPMtYNklIeFEIMBT4XQmyWUtYp226sb1h0H6jARiseHEwZN4o5xw5qr99UL12pJ5WWJTatlUW7lXoawUDELdRsy0Eol01jbqXqEijcE/nuNZRDVdSA769WT/XmOTYugqS+MOQEcCTUjDlEWw7VpRGr4dwn1HvOF+rdVELOxEjMASBrtLJQcr+qK6s/qhLbdC19/SiUNSt76SAwIOp7f2NZGCllkZTS1Mb/AqZGrTtovOcAy4HJzTl5LKS/imqpO7Jq2g+tHHoa0b78Jgek+6p3e5xKE21IOUgJr/0cnjst4sM30lNrWA5mALi6VDXb2/MZjL9ADerOpJrZSrXdSns/h8yRkD0aXCnKXQWR+IUjIeJWAmXtDPoRfL+yblzBXw1JfQyZDsL+lfDp3bCtWa27VgMjhBBDjJYv81C9wMKYTSMNzkFNYIUQIk0I4TQ+ZwKzUL3EWoe/mmocZCRq5aBpH7Ry6GlEu1EatRxK1VzMccaMrDaX0ZaiAbfSjg8g90vViynfGOPMQbs6ynIw4xaeUrU8FID0IWpZbcuhZB8kG64tT5lyR/UzuqskZEOFoRxMCyXarQQqiD7oeHWeqHRZQF2P1EFqatHyw5EAt9dNU5FSBoDrgY9Rg/7rUsqtQoj7hRBm9tGNQoitQoiNwI3AQmP5GGCNsXwZKubQauUgjDqH3smu1h5Ko4mJjjl0Z6SMBGpNot0ojVkO3nI1sDqT1HebS3U8bchy+PQeNZCX58H+b6D3+NhuJdNy8FVEpiuNz1DvjkRlOZjyF++DflPUMauL1fZJvdW2idmRduK+KOVgfgYlf99J6nPuV9ArKsvUX6WUX0p/Zb2YcnnLG742tZBSLgGW1Fp2d9TnO4A7Yuy3EpjQrJM1RiiELeTBg4PeKVo5aNoHbTl0V/Z/C3/sV7dwrDmWg9etBlarXbmT7IZyCPljp30GvFC8F6YtVPGD71eq5eGAdGlk2+i6AtNtFJeu3h0JWGRQKSFfFbgPQe9jVOuO4hxlZZjKISEzyq0UHXOIeq5xJSvrILE3HFpX93rY4+DU++DwhsixmmE5dDnMv6s9Hpfd2vC2Gk0L0cqhu3JkM/gr4ejWmstNy8EW1wTLwR2xGpyJah/TXROrqZ05oDpTYNBxynKQMmI5VMewHEAN+BCxHMxzeiug0OiLlD1aWTFmn6SwcohyK0XHHGq7lYRQVkbtjCl/lXKdjTsfTrhNKZHkfs22HLoUxgOA3ZXQyYJoejJaOXRXTFdL6f6ay03LIS6t6ZYDKFeNaTlAbNeSOaA6k2DgcSquUPp9PQHpqGSesHKIWA6AsgTMGEHWaIhLjXxPjHIreUpVUDscc0iqFZBOVu+ulLpdZk3LAeCk/wc3boDEXt3bcjAeAOLitHLQtB865tBdMWduK/2+5vJo5RD9JB8LrxsSstRnZ1Ik5mAevyIfMobV3N7cNnu0+nxkc+yYg/twJDZQx62UqN59FSr4bLFD+lA1uJudWc1iuoTMiDzhmENCzVRWU8G5UmpmPpnXwx4f+W6xqO27tXJQf+O4hKROFqRt8fv95OXl4fE0b66PlJQUtm/f3viGHUBXk2Xfvn30798fu735Pbi0cuiK7Fmqsm/MJ95YhJVDbcvBcCvFpSlffkN43ZGn7v7Ta7prPv+DCu7+ZmfN7UENronG4F1VFDtbqfyQsgYOrlGWgz1BWSYQpRwqVb+lzBHqvK6UyP6JUW4l8/dGxxystVJZoR7LoarudXQm1ZwWtZvh91ZiBxISe5ZyyMvLIykpicGDByNqJ1o0gNvtJimpa1yLriRLeXk5Pp+PvLw8hgwZ0uz9tVupq1GSC/++ALa81fB2pluppB7LIT4tdsxh3wp4bIoaRL3lkYH1J4/A6X9QE+mAGtArjtTsjRStHEwroCLfqK0Qyq0kpSrEqzgK2WPUNu5DkXgDqMHdPF7BdsgyphM3W4e7UiKKJDFaORiy2OMjbiVhibipnMk1YwlBvwpu11EOyd3acigtUwowqYcpB4/HQ0ZGRrMUg6Z+hBBkZGQ02xIz0cqhoynOgfWv1L/etAQa62/UFMsh4KnbjO7rx1TGUXFOzZiDielWMrOgogPL5oBqDt5mTyNQhWahgNqm4ijIUM1WFvFpkc/mYF5ZqJRb1pjIcSFiNUDE7VWRr45tj1cuJTNbyZkUSed1pRidXYM1r0W0W8ncpxsHpEvKSgHlNuhpaMXQtrTmemrl0NGs+he898uaT+TRmPMOVBY1fBxTOVTmq3RQk+iYgwzVKGhzegpUrj+oQV+GYigH44ncrE2IDiybLhtzn/iMiOWSanSXqC6Bsjz1OX2oSpE1tzUx3UqH1gEyEr+IS1XvSTGUQ2V+pOkegNVUDlEDpMtwkZlKzLwWsdxKXnfsLq7dgPJy9fvSUnuecuhMioqKmDRpEpMmTaJ3797069cv/N3na7ilzJo1a7jxxhsbPcfxxx/fVuK2Ozrm0NGYAdfS/RG3SzRmnKCqsP5jBHwqgydzpEoFLTsQcc2YT8umiybgUZPjAH0OfwYYA6I5qNdnOUjj6Tu6B1G0WwlU9pFpOaQOhAPfqbhDoRGnyBwRaX9huqEgMsDnrVbvWYZyMC2HaOXgTFRP/hVGzMG0Oky3UrT85v6eMqVoGrIcZKhmwWA3wu1WVk9GSmrnCtLDyMjIYMOGDQDce++9JCYm8pvf/Ca8PhAIYLPFHjKnTZvGtGnTcLsbdleuXLmyzeRtb7Tl0NGYyqF2rMAkbDk0oByqDKui39S6x/JXq4HTHDSj4g5pJeshY7j6Yg7qZkDaxFQOYXlquZUsNpXVBMoaMJVH6kD1Xl2igsw2l6opMK2BWDGHg+vUgJ5uZESZCs0MdpskZEViDua+poXjipI/WjmY1wJiWw7m7+mGVFUqubtK4LMns3DhQq699lpmzpzJbbfdxqpVqzjuuOOYPHkyxx9/PDt3qgeh5cuXc/bZZwNKsVxxxRXMmTOHoUOH8thjj4WPl5iYGN5+zpw5XHjhhYwePZpLL70UaViyS5YsYfTo0UydOpUbb7wxfNyORlsOHYmUNS2HWJhunIYsB9Ol1G8qbHxVDfQVBWouZYfxpG0O4FG1Dg5fKQyYBUV7GrAcaqW8ledFPpsxCtOPGZ8RsTBSDLdSVXEkA8lijQz48VGWg82FxIIgBMNPibiIzG2jLQcwlEO+iiWYVkd0zMHEVHRmPCGsHGpZDqYS6YbKIRSSfH9E3RvC0XPrHO57fyvbDjUtLhQMBrFaG68UH9s3mXt+Mq7R7WqTl5fHypUrsVqtlJeX8+WXX2Kz2fjss8+48847eeutuskjO3bsYNmyZbjdbkaNGsV1111XJ510/fr1bN26lb59+zJr1iy+/vprpk2bxjXXXMOKFSsYMmQI8+fPb7a8bYVWDh1JZYGqaoa69Qkm7ibEHEzl0GucUgIFO2H74poFZGHlELEcHL4yFTh2JkdZDvW4lUzKD8H6f0f89NHbR1sD0TGHgp0w0JgLJ5blIARBaxy2YCWMOD2yPByQrmU5JGYrZWZzRo4TSznUsRxMt1I9loOn+wWll+3MV64LOw2nOmvajIsuuiisfMrKyliwYAG7d+9GCIHfH7tJ5Y9//GOcTidOp5Ps7GyOHj1K//79a2wzY8aM8LJJkyaRm5tLYmIiQ4cODaeezp8/n6effrodf139aOXQkZhWA9SvHEy3UlVh7MZ6EHE5JfaCEafC1rehIKrZnD0ukpJqWg6+Kqwhjyoqi0uLshwSax47WjlYncrC+fhOlVEUn1HTDRU94KcMVGml+dugbD9kXaaWm9ZAXFS2EhC0urAFq5TlYJI9RlkgfWtNd5AyQKXgJvWJuK9MCydaHtPF5KltOdTnVioHuld2zFNf7OUsVxCCRB4AeiDNecJv79qChISIhfa73/2OuXPn8s4775Cbm1vvZDpOpzP82Wq1EggEWrRNZ6JjDh2JqRxSB8WOOYSCKkvIHq/aV0S7Pba8DS/+RC0zLYeETJiyUMUgcr+ENKPQpYZbybAcTDdVQqZy8ZgWTENupewxyhrxlKlAeXRdBNR0FcWlwvBTYd3L6numWbtgPM1HKxLAb09UhXcJtayPW7bUrMoG5aLyVSiF6mzArWQqojqWQ4yANHQ7t1KpN8S+wkqm9Y9Tv0mnfXY4ZWVl9OunJsd64YUX2vz4o0aNIicnh9zcXABee+21Nj9HU+k05SCEsAoh1gshPjC+DxFCfCeE2COEeM2YVKVnYSqHIbNjxxwq8pUP36wPiI477Fyinp4/uEX5360O9dQ8bG7E33/mn9WgGctyMK2NhKyamUMNBaR7R3Wadh+JoRxqpadOuyLSsC+rVnpqtCIBdo38JZzz97rXIBaZI9R70Fc35hAdkK5hEdDjAtKpTgtf/fYkxmTae7TV0JW57bbbuOOOO5g8eXK7POnHxcXxxBNPcMYZZzB16lSSkpI6rZ6lM91KN6EmTjH/ux8EHpZSLhJCPAVcCTzZWcIBkLcW1r0AZz+qevK0lpJc1eo6c6RKRfWU1WwZYWYG9Z6g0jwri1StAKiUVasTNr8B8ZlqkBdCTWJz/I2waZFy0Yw6Sw0ctQPSpnKIz6w5UDcUc+gzEdYblkDQp6wdM9sJIspBWNQAPOJUNddDxdHIxD5ht1JN5VCeMjpS39AYmSMjn03l4EqBkWfC4NlRsttVYV6jloNxy5UfIrWkGPwzu43/3mW3KquvBwejuwL33ntvzOXHHXccu3btCn///e9/D8CcOXOYM2cObre7zr5btmwJf66oqKixvck//vGP8Oe5c+eyY8cOpJT86le/Ytq0aa38NS2jUywHIUR/4MeouXYRqozvJOBNY5MXgfM6Q7YabHkL1r3UcOZQbaqKYccS9apNSa6a6zjVmBC+tvXgjlIOEDlvKASFu2HqQpj8c7U8+ql95tVw1edKgV30AlzwdJTlYLiVol1R5kBtdUS2MzHdSsIaefo35fGUxrYcHEYGk8Wq5k049trIccadDyffrSbbaSlJfSJKIVznYIWfLYKBx9bc1pWslIPfEykOrD3wm8fauYRJG++CQ+tbLltnUFkQaUio6XE888wzTJo0iXHjxlFWVsY111zTKXJ0luXwCHAbYI40GUCpMR0jQB7QL8Z+HUvRbvXuPhzp8dMQUsIzcyPuowUf1FxfkgtDTogEVQ+squm6MYPRvY8xtv8e1jyvXEf+KvWkPfVytd5Vyx1kYnYrrW051I45QF2rASKWQ3y6SpWddCmMOA3eWFB3n/D8DFFB7QkXqpdJSj+Y/b+xZW0qQiiL5fCG2DJH40pRSvehkZEB1FZLOdgc6vocWk9I2LD0ndI6+TqaynxlgWp6JLfccgu33HJLZ4vR8cpBCHE2kC+lXCuEmNOC/a8Grgbo1asXy5cvr7NNRUVFzOXNZUbeZuKBzV9/QsjyOQ5fCUd7n1Tv9nFVh5hZkkvuoIvpdXQ5oTeuo3L071m+fDkiFOCE8kN8Xxpi/44Cprt6E/fhrzmy5n12jL4JgGM2/Ydkq4tvduQzG/B+/iecvhLy+p1Nf2B9XhVlFV8AI6EaaOA3uqqPcCywfcsGjhamM3TvOvoJO1+uXEO/g0WMAKpDdr6rdQybv5wfAZXSxeqVqyD1YpwHCjjOWL/vUBHfG/uIUIATgcqAYHUzr3dz/0Zjgin0ArbnHOBoVf37TfZKkgq/xiID4C0jJGys+PKrOtsdL5w48FAaP4RNX3/bLNk7nYoC5fLTaNqRzrAcZgHnCCHOAlyomMOjQKoQwmZYD/2Bg7F2llI+DTwNMG3aNBkrlcysPmwVAR98oWYgmzA4U01Mf3QLYy65r/4skTXPAzD4J7dB4fmwaD5TDr1Mn7NfV/2GVkgGT/wRg6ecDrPXw/L/o/fKx+g98RQViC5ZD2f+mdkzz4Jv43D61OQ5/fM/B2DyKZdAYlbT5C8/DN/BmOFDGDNtDpQswpOfypy5c2FzEex5mrjUrLrXyeuGryEha2BkXdAP314FSIaMnsiQY6P2+S6ZhNTsZl/vZv+NxGrI/4Ixx0xjzNgG9ssbCOWRNuMWZ0Ls82zKgOIyKtLHtf5e6UhCIeVWql0LotG0MR0ec5BS3iGl7C+lHAzMAz6XUl4KLANMf8QC4L2Olq0GJbmR6l/3EfW9qqjunM3R5H6lCtAyhsOoM+GEW+lz5DP47+2RZnSm790RD6fer4rAPvl/8OndKqA8/Sq1PuwScSmXkiu1eX7mOjGHQvx2wxVl1hzUzlSCmm6l8DJ7zUmBoolPr1sr0R5kGoHwxgKxprvNbMhXOxhtYvyO8uQY/a1iIIQ4Qwix08imuz3G+oVCiAIhxAbj9YuodQuEELuN14ImnbA+qkvUfZnQBDenRtMKulKdw2+BXwsh9qBiEM92qjRFeyKfy/Iig/uRzbG3l1LVGgz+kZFFJOCku8jPmgXb3otSDgMi+wgB5z8FJ90F8/4Dl7wSyYoy/flz7lDvWaOal9deJ1upAJ8j1Th2AzEHs6FdrboEkvvE3mfwbOg/o+lytZRhJ6mAfP/pDW9nZn9NXaCUYH1ZSIZiLEtpPGNKCGEFHgfOBMYC84UQY2Ns+pqUcpLxMpMt0oF7gJnADOAeIURajH2bhtktt6kWpEbTQjpVOUgpl0spzzY+50gpZ0gph0spL5JSxpippgMxlUPqQDWbmWlFHK1HORTtUf+4Q2bXWOxOGq6Wm/sl14qzx6fDCbfC6B9HJrgBVYA27CSYdrkasKPTOZtCg5ZDQ8rBop5KzRRaEzMAWnufc/8BJ/+uebK1BFcK/OTR+gPxJqY1NHi2yuyqXW1tkpgNWaPxmwqzYWYAe4x71AcsAs5touSnA59KKYullCXAp8AZTdy3LpXK1akth7Zn7ty5fPzxxzWWPfLII1x33XUxt58zZw5r1qwB4KyzzqK0tLTONvfeey8PPfRQg+d999132bZtW/j73XffzWeffdZM6dse3T6jPop2q5qArDGwO+qGqc9yOPCdeh9Ys197VbxhKez5XD2NO+pxc9Tm3CeUQrLa4X/eitQNNBWLVSmVQ+ugcA9UFeJLMfKlG7IcAK5bWbP+AqIsh0YG584mdYCqdRgwA0aeVv92ZzyorKoNe5ty1H7AgajveShLoDY/FUKcAOwCbpFSHqhn35iZeI0lW1RUVLBt9VrGAqu25VL1fbApsrcLbZX0EU1KSkqjLa9jEQwGW7Rfbc4//3xefvnlGnMuvPLKKzzwwAMxjx8MBqmsrMTtdocrmWvL4vV6sdvtDcr3xhtvcMYZZzBggBorbr31VoBW/yZTFo/H06K/Vc9UDjKoWkmnNJINW35YDda1B0KAor0qdhDdIbTPRDiype62AIc2qHz/6CIxoDLBUA75W5uXYWKxEDbshp7Y9P2iGXQc7PqvmpM65MdvN36nI1FZD/WlQ8ZyWdRnOXQ1piyA0WdHKrPrI/wbm6QcmsL7wKtSSq8Q4hpUrU79qW0xaCzZYvny5YzNyITtMOOks+v0q+pI2iTpoxbbt29vUY+ktuqt9D//8z/8/ve/x+l04nA4yM3N5ejRo7z33nvcddddVFdXc+GFF3LfffcBqh9SQkJCeN7rNWvW4HQ6eeyxx3jxxRfJzs5mwIAB4UrnZ555hqeffhqfz8fw4cN5+eWX2bBhAx999BErV67kr3/9K2+99RYPPPAAZ599NhdeeCFLly7lN7/5DYFAgOnTp/Pkk0/idDoZPHgwCxYs4P3338fv9/PGG28wenRNF6l5XVwuF5Mn12NBN0CPVA5jtj8MWw7DNStUALOyCPavhFE/jvj0A1545iTlNrrivyo3PmWAWh/wwuFNKl/fzAqx2FXweMVfYMVDMOREGBDl/z60Xg3+tSqpPa4s5f8PeGrGGzqCBe+r3/X6Aji0LqIchFDXpnZcoSH6TlLKL6mLZ8lY7XVbfreeg0D0H69ONp2UMrqN7r+AP0ftO6fWvstbLEmF0TrFrDzvqXx0e/1Wei3igoFI2/eG6D0BzvxTvavT09OZMWMGH330Eeeeey6LFi3i4osv5s477yQ9PZ1gMMjJJ5/Mpk2bOOaYY2IeY/369SxatIgNGzYQCASYMmUKU6eqeVcuuOACrrpKJZzcddddPPvss9xwww2cc845YWUQjcfjYeHChSxdupSRI0dy2WWX8eSTT3LzzTcDkJmZybp163jiiSd46KGH+Ne//tWEq9V0ulJAus043Oc0FQP45HeqQ+fL58Jr/6NeZsfOzW+qiuQD38Kbl8Ojx8A3Rq+fnOXgc6tWFOZAkzoA+k8DJHz+ALxzNQSNmr2gH45uUQNobYQ10huoNVXCLSV1IFz2Hpx4O0UZUWX4qQOa7uIC1Rrj9u9jW1k9n9XACKP/lwOVZbc4egMhRJ+or+egWsMAfAycJoRIMwLRpxnLWkZlQaR1iqbNmT9/PosWLQJg0aJFzJ8/n9dff50pU6YwefJktm7dWiM+UJuVK1dy/vnnEx8fT3JyMuecc0543ZYtW5g9ezYTJkzglVdeYevWrQ3KsnPnToYMGcLIkSreuGDBAlasWBFef8EFFwAwderUcKO+tqRHWg6lacfAcdfDN/9QcxHIoEoRXfu8UhCXvgnfPA7Z41Tgdus7gFDtMmbdBNsWK9/60BNh7zJ10LTBquvopW+pCXnev1FtP/ES1bk04Kk/+Jk5Sj0FdYZyABXEnXsH/tb6iM3q6x8YUsqAEOJ61KBuBZ6TUm4VQtwPrJFSLgZuFEKcAwSAYmChsW+xEOIBlIIBuF9KWdxiYSryI2nFPZkGnvBrU92GLbvPPfdcbrnlFtatW0dVVRXp6ek89NBDrF69mrS0NBYuXIjH42n8QDFYuHAh7777LhMnTuSFF15odczGbPndXu2+e6RyAFQ/n9RBqmHdiNNUcLLvZHjvl/DXUWqu4/OeVKmRW99R+yz7AxTnwM4PVZ2CzRllOQxSLqMRp6hCpFVPw5JbYfkfI5k9fSbFlsWc37mzlIOm1UgplwBLai27O+rzHcAd9ez7HPBcmwhSma8L4NqRxMRE5s6dyxVXXMH8+fMpLy8nISGBlJQUjh49ykcffdRgrGXWrFn86le/4o477iAQCPD++++HeyO53W769OmD3+/nlVdeCbf+TkpKihl8HjVqFLm5uezZsyccozjxxBbGH1tAz1UONqdqSBfN5EtVWmnulzDuApg4X5nnJ96mMnqW/QHeWKgKjcYamYpm6mn0HAMWC5z5IHz5N1VRvPdzZWnUTv80MfsnpQ+LvV6jaSoVBdBrQuPbaVrM/PnzOf/881m0aBGjR49m8uTJjB49mgEDBjBr1qwG9500aRKXXHIJEydOJDs7m+nTI3HJBx54gJkzZ5KVlcXMmTPDCmHevHlcddVVPPbYY7z55pvh7V0uF88//zwXXXRROCB97bXXts+PjoWUstu+pk6dKmOxbNmymMsb5R8zpLwnWcp3rpMyGIws3/O5lJ7y2PsEA1IufUDK5Q/WL0soJOX+VS2TqQ1p8XVpB7qKLA3JgXIZdZl7e9nnS6W8L13KT+9t1W9uC9rj77dt27YW7VdeXs//ZifQFWWJdV2bcm/3XMuhJZz+RzX/8cxra2YdDZtb/z4Wq6pwbgghamY2aTQtwBLyw9jz6o9taTRtiFYO0Qw/Wb00mi5IyOqECzu3q4zmh0OPTGXVaDQaTevQykGj0XQZlDtc01a05npq5aDRaLoELpeLoqIirSDaCCklRUVFuFyuxjeOgY45aDSaLkH//v3Jy8ujoKCgWft5PJ4WD4BtTVeTJTU1lf79W1ZfpZWDRqPpEtjtdoYMaWb3YVQTwJY0lmsPepIs2q2k0Wg0mjpo5aDRaDSaOmjloNFoNJo6iO6cGSCEKAC+j7EqEyjsYHHqQ8sSm64iS0NyDJJSdkoL1Hru7a5yzUDLUh/dRZZG7+1urRzqQwixRko5rfEt2x8tS2y6iixdRY6m0JVk1bLEpifJot1KGo1Go6mDVg4ajUajqUNPVQ5Pd7YAUWhZYtNVZOkqcjSFriSrliU2PUaWHhlz0Gg0Gk3r6KmWg0aj0WhaQY9SDkKIM4QQO4UQe4QQt3fwuQcIIZYJIbYJIbYKIW4ylt8rhDgohNhgvM7qIHlyhRCbjXOuMZalCyE+FULsNt7TOkCOUVG/fYMQolwIcXNHXRchxHNCiHwhxJaoZTGvg1A8Ztw/m4QQU9pDppag7+0a8uh7mw64txubKq67vAArsBcYCjiAjcDYDjx/H2CK8TkJ2AWMBe4FftMJ1yMXyKy17M/A7cbn24EHO+FvdAQY1FHXBTgBmAJsaew6AGcBHwECOBb4rqP/bg1cN31vR+TR97Zs/3u7J1kOM4A9UsocKaUPWASc21Enl1IellKuMz67ge1Av446fxM5F3jR+PwicF4Hn/9kYK+UMlbhYrsgpVwBFNdaXN91OBd4SSq+BVKFEH06RNCG0fd24+h7W9Fm93ZPUg79gANR3/PopBtYCDEYmAx8Zyy63jDlnusIc9dAAp8IIdYKIa42lvWSUh42Ph8BenWQLCbzgFejvnfGdYH6r0OXuYdq0WXk0vd2vfS4e7snKYcugRAiEXgLuFlKWQ48CQwDJgGHgb92kCg/klJOAc4EfiWEOCF6pVS2ZoelqgkhHMA5wBvGos66LjXo6OvQndH3dmx66r3dk5TDQWBA1Pf+xrIOQwhhR/3zvCKlfBtASnlUShmUUoaAZ1AugnZHSnnQeM8H3jHOe9Q0JY33/I6QxeBMYJ2U8qghV6dcF4P6rkOn30P10Oly6Xu7QXrkvd2TlMNqYIQQYoihyecBizvq5EIIATwLbJdS/i1qebRf73xgS+1920GWBCFEkvkZOM0472JggbHZAuC99pYlivlEmd2dcV2iqO86LAYuMzI7jgXKokz0zkTf25Fz6nu7Ydru3u7IiH4HRO/PQmVS7AX+Xwef+0coE24TsMF4nQW8DGw2li8G+nSALENRGS0bga3mtQAygKXAbuAzIL2Drk0CUASkRC3rkOuC+qc9DPhRftYr67sOqEyOx437ZzMwrSPvoUZ+h763pb63a527Xe9tXSGt0Wg0mjr0JLeSRqPRaNoIrRw0Go1GUwetHDQajUZTB60cNBqNRlMHrRw0Go1GUwetHLohQohgrW6QbdalUwgxOLrLo0bTkeh7u+tg62wBNC2iWko5qbOF0GjaAX1vdxG05dCDMPrc/9nodb9KCDHcWD5YCPG50QhsqRBioLG8lxDiHSHERuN1vHEoqxDiGaF6938ihIjrtB+l0aDv7c5AK4fuSVwt0/uSqHVlUsoJwD+AR4xlfwdelFIeA7wCPGYsfwz4Qko5EdUXfquxfATwuJRyHFAK/LRdf41GE0Hf210EXSHdDRFCVEgpE2MszwVOklLmGI3SjkgpM4QQhagSfr+x/LCUMlMIUQD0l1J6o44xGPhUSjnC+P5bwC6l/H0H/DTNDxx9b3cdtOXQ85D1fG4O3qjPQXRsStM10Pd2B6KVQ8/jkqj3b4zPK1GdPAEuBb40Pi8FrgMQQliFECkdJaRG0wL0vd2BaK3ZPYkTQmyI+v5fKaWZ8pcmhNiEekKabyy7AXheCHErUABcbiy/CXhaCHEl6inqOlSXR42ms9D3dhdBxxx6EIZfdpqUsrCzZdFo2hJ9b3c82q2k0Wg0mjpoy0Gj0Wg0ddCWg0aj0WjqoJWDRqPRaOqglYNGo9Fo6qCVg0aj0WjqoJWDRqPRaOqglYNGo9Fo6vD/AZjvhqCXeuwvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.7556\n",
      "Validation AUC: 0.7547\n",
      "Validation Balanced_ACC: 0.4473\n",
      "Validation MI: 0.1161\n",
      "Validation Normalized MI: 0.1696\n",
      "Validation Adjusted MI: 0.1696\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 632.5568, Accuracy: 0.4922\n",
      "Training loss (for one batch) at step 10: 600.4511, Accuracy: 0.4844\n",
      "Training loss (for one batch) at step 20: 591.5323, Accuracy: 0.4874\n",
      "Training loss (for one batch) at step 30: 553.6440, Accuracy: 0.4972\n",
      "Training loss (for one batch) at step 40: 512.5739, Accuracy: 0.5013\n",
      "Training loss (for one batch) at step 50: 489.2502, Accuracy: 0.5104\n",
      "Training loss (for one batch) at step 60: 513.4553, Accuracy: 0.5127\n",
      "Training loss (for one batch) at step 70: 471.7820, Accuracy: 0.5151\n",
      "Training loss (for one batch) at step 80: 477.1508, Accuracy: 0.5149\n",
      "Training loss (for one batch) at step 90: 469.9891, Accuracy: 0.5157\n",
      "Training loss (for one batch) at step 100: 466.3406, Accuracy: 0.5161\n",
      "Training loss (for one batch) at step 110: 472.3595, Accuracy: 0.5151\n",
      "---- Training ----\n",
      "Training loss: 141.0838\n",
      "Training acc over epoch: 0.5151\n",
      "---- Validation ----\n",
      "Validation loss: 35.2953\n",
      "Validation acc: 0.4989\n",
      "Time taken: 11.94s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 454.2089, Accuracy: 0.5469\n",
      "Training loss (for one batch) at step 10: 466.2452, Accuracy: 0.5291\n",
      "Training loss (for one batch) at step 20: 458.2673, Accuracy: 0.5219\n",
      "Training loss (for one batch) at step 30: 453.0400, Accuracy: 0.5111\n",
      "Training loss (for one batch) at step 40: 453.4926, Accuracy: 0.5152\n",
      "Training loss (for one batch) at step 50: 455.6826, Accuracy: 0.5116\n",
      "Training loss (for one batch) at step 60: 452.5033, Accuracy: 0.5136\n",
      "Training loss (for one batch) at step 70: 457.2047, Accuracy: 0.5168\n",
      "Training loss (for one batch) at step 80: 449.0847, Accuracy: 0.5165\n",
      "Training loss (for one batch) at step 90: 448.0839, Accuracy: 0.5173\n",
      "Training loss (for one batch) at step 100: 452.9925, Accuracy: 0.5172\n",
      "Training loss (for one batch) at step 110: 445.7066, Accuracy: 0.5178\n",
      "---- Training ----\n",
      "Training loss: 140.3219\n",
      "Training acc over epoch: 0.5167\n",
      "---- Validation ----\n",
      "Validation loss: 34.5126\n",
      "Validation acc: 0.4944\n",
      "Time taken: 10.79s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 450.1033, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 447.6790, Accuracy: 0.5412\n",
      "Training loss (for one batch) at step 20: 451.3264, Accuracy: 0.5394\n",
      "Training loss (for one batch) at step 30: 450.1969, Accuracy: 0.5381\n",
      "Training loss (for one batch) at step 40: 446.4821, Accuracy: 0.5341\n",
      "Training loss (for one batch) at step 50: 448.0220, Accuracy: 0.5355\n",
      "Training loss (for one batch) at step 60: 446.6179, Accuracy: 0.5346\n",
      "Training loss (for one batch) at step 70: 445.4121, Accuracy: 0.5371\n",
      "Training loss (for one batch) at step 80: 444.7090, Accuracy: 0.5369\n",
      "Training loss (for one batch) at step 90: 444.1035, Accuracy: 0.5376\n",
      "Training loss (for one batch) at step 100: 447.2872, Accuracy: 0.5370\n",
      "Training loss (for one batch) at step 110: 445.0421, Accuracy: 0.5356\n",
      "---- Training ----\n",
      "Training loss: 139.2929\n",
      "Training acc over epoch: 0.5357\n",
      "---- Validation ----\n",
      "Validation loss: 34.7244\n",
      "Validation acc: 0.5172\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 444.3121, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 445.0254, Accuracy: 0.5526\n",
      "Training loss (for one batch) at step 20: 446.3091, Accuracy: 0.5562\n",
      "Training loss (for one batch) at step 30: 443.4709, Accuracy: 0.5499\n",
      "Training loss (for one batch) at step 40: 444.1648, Accuracy: 0.5492\n",
      "Training loss (for one batch) at step 50: 444.0288, Accuracy: 0.5461\n",
      "Training loss (for one batch) at step 60: 443.1780, Accuracy: 0.5444\n",
      "Training loss (for one batch) at step 70: 442.1422, Accuracy: 0.5451\n",
      "Training loss (for one batch) at step 80: 445.1969, Accuracy: 0.5467\n",
      "Training loss (for one batch) at step 90: 444.1288, Accuracy: 0.5476\n",
      "Training loss (for one batch) at step 100: 444.8555, Accuracy: 0.5496\n",
      "Training loss (for one batch) at step 110: 442.8093, Accuracy: 0.5493\n",
      "---- Training ----\n",
      "Training loss: 139.3071\n",
      "Training acc over epoch: 0.5470\n",
      "---- Validation ----\n",
      "Validation loss: 34.6193\n",
      "Validation acc: 0.5341\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 444.7076, Accuracy: 0.4688\n",
      "Training loss (for one batch) at step 10: 443.6782, Accuracy: 0.5455\n",
      "Training loss (for one batch) at step 20: 442.9650, Accuracy: 0.5513\n",
      "Training loss (for one batch) at step 30: 443.9269, Accuracy: 0.5582\n",
      "Training loss (for one batch) at step 40: 443.6317, Accuracy: 0.5560\n",
      "Training loss (for one batch) at step 50: 443.8328, Accuracy: 0.5533\n",
      "Training loss (for one batch) at step 60: 442.2547, Accuracy: 0.5516\n",
      "Training loss (for one batch) at step 70: 442.2780, Accuracy: 0.5551\n",
      "Training loss (for one batch) at step 80: 442.7794, Accuracy: 0.5582\n",
      "Training loss (for one batch) at step 90: 444.3702, Accuracy: 0.5597\n",
      "Training loss (for one batch) at step 100: 444.6699, Accuracy: 0.5590\n",
      "Training loss (for one batch) at step 110: 444.3873, Accuracy: 0.5605\n",
      "---- Training ----\n",
      "Training loss: 139.0325\n",
      "Training acc over epoch: 0.5621\n",
      "---- Validation ----\n",
      "Validation loss: 34.6443\n",
      "Validation acc: 0.6220\n",
      "Time taken: 10.85s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 444.0685, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 443.3152, Accuracy: 0.5682\n",
      "Training loss (for one batch) at step 20: 443.7838, Accuracy: 0.5677\n",
      "Training loss (for one batch) at step 30: 443.9697, Accuracy: 0.5731\n",
      "Training loss (for one batch) at step 40: 442.1074, Accuracy: 0.5736\n",
      "Training loss (for one batch) at step 50: 441.9988, Accuracy: 0.5784\n",
      "Training loss (for one batch) at step 60: 442.7086, Accuracy: 0.5790\n",
      "Training loss (for one batch) at step 70: 443.9681, Accuracy: 0.5836\n",
      "Training loss (for one batch) at step 80: 444.5151, Accuracy: 0.5836\n",
      "Training loss (for one batch) at step 90: 443.9547, Accuracy: 0.5822\n",
      "Training loss (for one batch) at step 100: 442.2390, Accuracy: 0.5834\n",
      "Training loss (for one batch) at step 110: 441.4651, Accuracy: 0.5828\n",
      "---- Training ----\n",
      "Training loss: 137.9494\n",
      "Training acc over epoch: 0.5827\n",
      "---- Validation ----\n",
      "Validation loss: 34.3758\n",
      "Validation acc: 0.6362\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 443.2397, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 444.6597, Accuracy: 0.6023\n",
      "Training loss (for one batch) at step 20: 441.2735, Accuracy: 0.6064\n",
      "Training loss (for one batch) at step 30: 442.0385, Accuracy: 0.6104\n",
      "Training loss (for one batch) at step 40: 441.4708, Accuracy: 0.6069\n",
      "Training loss (for one batch) at step 50: 442.0585, Accuracy: 0.6126\n",
      "Training loss (for one batch) at step 60: 442.4563, Accuracy: 0.6116\n",
      "Training loss (for one batch) at step 70: 443.7378, Accuracy: 0.6116\n",
      "Training loss (for one batch) at step 80: 442.4178, Accuracy: 0.6120\n",
      "Training loss (for one batch) at step 90: 442.3557, Accuracy: 0.6092\n",
      "Training loss (for one batch) at step 100: 441.1619, Accuracy: 0.6095\n",
      "Training loss (for one batch) at step 110: 442.0798, Accuracy: 0.6094\n",
      "---- Training ----\n",
      "Training loss: 136.1809\n",
      "Training acc over epoch: 0.6097\n",
      "---- Validation ----\n",
      "Validation loss: 34.9489\n",
      "Validation acc: 0.6051\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 444.4968, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 443.0817, Accuracy: 0.6378\n",
      "Training loss (for one batch) at step 20: 440.1404, Accuracy: 0.6358\n",
      "Training loss (for one batch) at step 30: 442.5876, Accuracy: 0.6366\n",
      "Training loss (for one batch) at step 40: 443.5406, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 50: 434.1352, Accuracy: 0.6297\n",
      "Training loss (for one batch) at step 60: 442.0986, Accuracy: 0.6319\n",
      "Training loss (for one batch) at step 70: 441.8478, Accuracy: 0.6360\n",
      "Training loss (for one batch) at step 80: 442.3875, Accuracy: 0.6353\n",
      "Training loss (for one batch) at step 90: 440.2823, Accuracy: 0.6290\n",
      "Training loss (for one batch) at step 100: 437.6221, Accuracy: 0.6252\n",
      "Training loss (for one batch) at step 110: 445.3014, Accuracy: 0.6239\n",
      "---- Training ----\n",
      "Training loss: 135.8609\n",
      "Training acc over epoch: 0.6259\n",
      "---- Validation ----\n",
      "Validation loss: 34.1204\n",
      "Validation acc: 0.6397\n",
      "Time taken: 10.73s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 443.8576, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 442.8189, Accuracy: 0.6307\n",
      "Training loss (for one batch) at step 20: 440.0095, Accuracy: 0.6220\n",
      "Training loss (for one batch) at step 30: 438.1045, Accuracy: 0.6326\n",
      "Training loss (for one batch) at step 40: 442.1268, Accuracy: 0.6381\n",
      "Training loss (for one batch) at step 50: 441.6482, Accuracy: 0.6420\n",
      "Training loss (for one batch) at step 60: 437.5958, Accuracy: 0.6401\n",
      "Training loss (for one batch) at step 70: 443.2419, Accuracy: 0.6433\n",
      "Training loss (for one batch) at step 80: 440.8754, Accuracy: 0.6389\n",
      "Training loss (for one batch) at step 90: 438.4635, Accuracy: 0.6337\n",
      "Training loss (for one batch) at step 100: 438.6596, Accuracy: 0.6325\n",
      "Training loss (for one batch) at step 110: 438.4792, Accuracy: 0.6339\n",
      "---- Training ----\n",
      "Training loss: 134.5202\n",
      "Training acc over epoch: 0.6364\n",
      "---- Validation ----\n",
      "Validation loss: 34.7157\n",
      "Validation acc: 0.6650\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 444.5396, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 441.6255, Accuracy: 0.6364\n",
      "Training loss (for one batch) at step 20: 442.7431, Accuracy: 0.6116\n",
      "Training loss (for one batch) at step 30: 440.8879, Accuracy: 0.6222\n",
      "Training loss (for one batch) at step 40: 435.6994, Accuracy: 0.6258\n",
      "Training loss (for one batch) at step 50: 431.2889, Accuracy: 0.6301\n",
      "Training loss (for one batch) at step 60: 437.2320, Accuracy: 0.6360\n",
      "Training loss (for one batch) at step 70: 440.7424, Accuracy: 0.6393\n",
      "Training loss (for one batch) at step 80: 440.4843, Accuracy: 0.6366\n",
      "Training loss (for one batch) at step 90: 438.2487, Accuracy: 0.6324\n",
      "Training loss (for one batch) at step 100: 434.4899, Accuracy: 0.6333\n",
      "Training loss (for one batch) at step 110: 437.5953, Accuracy: 0.6356\n",
      "---- Training ----\n",
      "Training loss: 134.4418\n",
      "Training acc over epoch: 0.6382\n",
      "---- Validation ----\n",
      "Validation loss: 35.1694\n",
      "Validation acc: 0.6679\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 444.0594, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 438.1852, Accuracy: 0.6378\n",
      "Training loss (for one batch) at step 20: 437.5987, Accuracy: 0.6362\n",
      "Training loss (for one batch) at step 30: 435.5689, Accuracy: 0.6384\n",
      "Training loss (for one batch) at step 40: 440.5073, Accuracy: 0.6494\n",
      "Training loss (for one batch) at step 50: 432.0901, Accuracy: 0.6541\n",
      "Training loss (for one batch) at step 60: 441.4713, Accuracy: 0.6575\n",
      "Training loss (for one batch) at step 70: 445.2705, Accuracy: 0.6591\n",
      "Training loss (for one batch) at step 80: 440.3962, Accuracy: 0.6539\n",
      "Training loss (for one batch) at step 90: 441.9473, Accuracy: 0.6431\n",
      "Training loss (for one batch) at step 100: 438.1293, Accuracy: 0.6453\n",
      "Training loss (for one batch) at step 110: 433.1472, Accuracy: 0.6489\n",
      "---- Training ----\n",
      "Training loss: 137.2944\n",
      "Training acc over epoch: 0.6501\n",
      "---- Validation ----\n",
      "Validation loss: 34.9647\n",
      "Validation acc: 0.6518\n",
      "Time taken: 10.78s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 438.6511, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 442.2893, Accuracy: 0.6378\n",
      "Training loss (for one batch) at step 20: 431.3292, Accuracy: 0.6269\n",
      "Training loss (for one batch) at step 30: 439.8095, Accuracy: 0.6220\n",
      "Training loss (for one batch) at step 40: 431.1186, Accuracy: 0.6338\n",
      "Training loss (for one batch) at step 50: 431.8560, Accuracy: 0.6455\n",
      "Training loss (for one batch) at step 60: 430.4269, Accuracy: 0.6547\n",
      "Training loss (for one batch) at step 70: 445.5965, Accuracy: 0.6599\n",
      "Training loss (for one batch) at step 80: 438.6799, Accuracy: 0.6549\n",
      "Training loss (for one batch) at step 90: 436.2669, Accuracy: 0.6508\n",
      "Training loss (for one batch) at step 100: 430.6205, Accuracy: 0.6518\n",
      "Training loss (for one batch) at step 110: 438.5519, Accuracy: 0.6563\n",
      "---- Training ----\n",
      "Training loss: 134.3438\n",
      "Training acc over epoch: 0.6576\n",
      "---- Validation ----\n",
      "Validation loss: 34.0429\n",
      "Validation acc: 0.6617\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 436.5210, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 438.8167, Accuracy: 0.6882\n",
      "Training loss (for one batch) at step 20: 438.0660, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 30: 427.7475, Accuracy: 0.6706\n",
      "Training loss (for one batch) at step 40: 432.8426, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 50: 424.7825, Accuracy: 0.6756\n",
      "Training loss (for one batch) at step 60: 432.7255, Accuracy: 0.6806\n",
      "Training loss (for one batch) at step 70: 432.2421, Accuracy: 0.6811\n",
      "Training loss (for one batch) at step 80: 438.2394, Accuracy: 0.6744\n",
      "Training loss (for one batch) at step 90: 433.1653, Accuracy: 0.6696\n",
      "Training loss (for one batch) at step 100: 432.1519, Accuracy: 0.6668\n",
      "Training loss (for one batch) at step 110: 431.1544, Accuracy: 0.6684\n",
      "---- Training ----\n",
      "Training loss: 131.7373\n",
      "Training acc over epoch: 0.6696\n",
      "---- Validation ----\n",
      "Validation loss: 34.8487\n",
      "Validation acc: 0.6539\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 438.8207, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 440.7071, Accuracy: 0.6548\n",
      "Training loss (for one batch) at step 20: 430.3834, Accuracy: 0.6492\n",
      "Training loss (for one batch) at step 30: 432.8293, Accuracy: 0.6560\n",
      "Training loss (for one batch) at step 40: 431.2598, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 50: 419.8857, Accuracy: 0.6780\n",
      "Training loss (for one batch) at step 60: 436.5872, Accuracy: 0.6810\n",
      "Training loss (for one batch) at step 70: 435.1998, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 80: 435.5757, Accuracy: 0.6767\n",
      "Training loss (for one batch) at step 90: 434.9750, Accuracy: 0.6729\n",
      "Training loss (for one batch) at step 100: 430.1706, Accuracy: 0.6736\n",
      "Training loss (for one batch) at step 110: 426.0430, Accuracy: 0.6765\n",
      "---- Training ----\n",
      "Training loss: 135.6132\n",
      "Training acc over epoch: 0.6756\n",
      "---- Validation ----\n",
      "Validation loss: 32.7441\n",
      "Validation acc: 0.6475\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 441.6281, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 431.4483, Accuracy: 0.6648\n",
      "Training loss (for one batch) at step 20: 432.5637, Accuracy: 0.6689\n",
      "Training loss (for one batch) at step 30: 421.4293, Accuracy: 0.6807\n",
      "Training loss (for one batch) at step 40: 426.0431, Accuracy: 0.6864\n",
      "Training loss (for one batch) at step 50: 418.3413, Accuracy: 0.6896\n",
      "Training loss (for one batch) at step 60: 432.8510, Accuracy: 0.6939\n",
      "Training loss (for one batch) at step 70: 432.7050, Accuracy: 0.6972\n",
      "Training loss (for one batch) at step 80: 440.7621, Accuracy: 0.6906\n",
      "Training loss (for one batch) at step 90: 432.4954, Accuracy: 0.6856\n",
      "Training loss (for one batch) at step 100: 425.3298, Accuracy: 0.6860\n",
      "Training loss (for one batch) at step 110: 418.4067, Accuracy: 0.6857\n",
      "---- Training ----\n",
      "Training loss: 129.4097\n",
      "Training acc over epoch: 0.6864\n",
      "---- Validation ----\n",
      "Validation loss: 36.9028\n",
      "Validation acc: 0.6762\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 439.2135, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 434.4497, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 20: 427.4380, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 30: 430.5087, Accuracy: 0.6767\n",
      "Training loss (for one batch) at step 40: 419.5365, Accuracy: 0.6778\n",
      "Training loss (for one batch) at step 50: 412.9626, Accuracy: 0.6835\n",
      "Training loss (for one batch) at step 60: 420.6411, Accuracy: 0.6895\n",
      "Training loss (for one batch) at step 70: 436.7748, Accuracy: 0.6894\n",
      "Training loss (for one batch) at step 80: 427.7518, Accuracy: 0.6887\n",
      "Training loss (for one batch) at step 90: 430.7769, Accuracy: 0.6863\n",
      "Training loss (for one batch) at step 100: 419.4404, Accuracy: 0.6905\n",
      "Training loss (for one batch) at step 110: 427.5784, Accuracy: 0.6918\n",
      "---- Training ----\n",
      "Training loss: 133.3808\n",
      "Training acc over epoch: 0.6920\n",
      "---- Validation ----\n",
      "Validation loss: 33.5454\n",
      "Validation acc: 0.6609\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 439.0735, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 435.4214, Accuracy: 0.6776\n",
      "Training loss (for one batch) at step 20: 435.3710, Accuracy: 0.6749\n",
      "Training loss (for one batch) at step 30: 424.9389, Accuracy: 0.6923\n",
      "Training loss (for one batch) at step 40: 428.8917, Accuracy: 0.6949\n",
      "Training loss (for one batch) at step 50: 417.1414, Accuracy: 0.7022\n",
      "Training loss (for one batch) at step 60: 436.2179, Accuracy: 0.7108\n",
      "Training loss (for one batch) at step 70: 437.2046, Accuracy: 0.7110\n",
      "Training loss (for one batch) at step 80: 438.7463, Accuracy: 0.7010\n",
      "Training loss (for one batch) at step 90: 427.6080, Accuracy: 0.6973\n",
      "Training loss (for one batch) at step 100: 416.6290, Accuracy: 0.6965\n",
      "Training loss (for one batch) at step 110: 434.3499, Accuracy: 0.6966\n",
      "---- Training ----\n",
      "Training loss: 138.5955\n",
      "Training acc over epoch: 0.6973\n",
      "---- Validation ----\n",
      "Validation loss: 37.8789\n",
      "Validation acc: 0.6900\n",
      "Time taken: 10.59s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 443.4119, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 432.0582, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 20: 419.9282, Accuracy: 0.6961\n",
      "Training loss (for one batch) at step 30: 420.3041, Accuracy: 0.6973\n",
      "Training loss (for one batch) at step 40: 414.0497, Accuracy: 0.7054\n",
      "Training loss (for one batch) at step 50: 406.3615, Accuracy: 0.7105\n",
      "Training loss (for one batch) at step 60: 408.3310, Accuracy: 0.7189\n",
      "Training loss (for one batch) at step 70: 431.4574, Accuracy: 0.7183\n",
      "Training loss (for one batch) at step 80: 429.6558, Accuracy: 0.7118\n",
      "Training loss (for one batch) at step 90: 427.7456, Accuracy: 0.7073\n",
      "Training loss (for one batch) at step 100: 415.2857, Accuracy: 0.7046\n",
      "Training loss (for one batch) at step 110: 429.4629, Accuracy: 0.7053\n",
      "---- Training ----\n",
      "Training loss: 135.3980\n",
      "Training acc over epoch: 0.7056\n",
      "---- Validation ----\n",
      "Validation loss: 33.9469\n",
      "Validation acc: 0.7088\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 432.6029, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 427.6141, Accuracy: 0.7067\n",
      "Training loss (for one batch) at step 20: 424.1855, Accuracy: 0.6961\n",
      "Training loss (for one batch) at step 30: 406.6316, Accuracy: 0.7044\n",
      "Training loss (for one batch) at step 40: 406.5093, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 50: 398.1392, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 60: 420.4564, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 70: 424.8397, Accuracy: 0.7236\n",
      "Training loss (for one batch) at step 80: 430.8918, Accuracy: 0.7173\n",
      "Training loss (for one batch) at step 90: 418.0104, Accuracy: 0.7105\n",
      "Training loss (for one batch) at step 100: 411.9379, Accuracy: 0.7080\n",
      "Training loss (for one batch) at step 110: 412.2752, Accuracy: 0.7088\n",
      "---- Training ----\n",
      "Training loss: 135.8415\n",
      "Training acc over epoch: 0.7093\n",
      "---- Validation ----\n",
      "Validation loss: 34.0748\n",
      "Validation acc: 0.6854\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 433.4865, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 427.4779, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 20: 423.3576, Accuracy: 0.7143\n",
      "Training loss (for one batch) at step 30: 411.2383, Accuracy: 0.7240\n",
      "Training loss (for one batch) at step 40: 402.4315, Accuracy: 0.7302\n",
      "Training loss (for one batch) at step 50: 390.8721, Accuracy: 0.7396\n",
      "Training loss (for one batch) at step 60: 409.1429, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 70: 438.2628, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 80: 416.4688, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 90: 423.3967, Accuracy: 0.7288\n",
      "Training loss (for one batch) at step 100: 406.1855, Accuracy: 0.7273\n",
      "Training loss (for one batch) at step 110: 425.0219, Accuracy: 0.7292\n",
      "---- Training ----\n",
      "Training loss: 125.5675\n",
      "Training acc over epoch: 0.7284\n",
      "---- Validation ----\n",
      "Validation loss: 33.7164\n",
      "Validation acc: 0.6924\n",
      "Time taken: 10.98s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 437.1547, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 434.9083, Accuracy: 0.7067\n",
      "Training loss (for one batch) at step 20: 424.2981, Accuracy: 0.7039\n",
      "Training loss (for one batch) at step 30: 408.4577, Accuracy: 0.7104\n",
      "Training loss (for one batch) at step 40: 390.8970, Accuracy: 0.7199\n",
      "Training loss (for one batch) at step 50: 386.5310, Accuracy: 0.7315\n",
      "Training loss (for one batch) at step 60: 412.5688, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 70: 424.0554, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 80: 425.6194, Accuracy: 0.7311\n",
      "Training loss (for one batch) at step 90: 406.5334, Accuracy: 0.7279\n",
      "Training loss (for one batch) at step 100: 403.6631, Accuracy: 0.7293\n",
      "Training loss (for one batch) at step 110: 407.7941, Accuracy: 0.7282\n",
      "---- Training ----\n",
      "Training loss: 129.7407\n",
      "Training acc over epoch: 0.7283\n",
      "---- Validation ----\n",
      "Validation loss: 37.9289\n",
      "Validation acc: 0.6900\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 425.3295, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 420.9989, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 20: 420.2000, Accuracy: 0.7128\n",
      "Training loss (for one batch) at step 30: 397.4714, Accuracy: 0.7180\n",
      "Training loss (for one batch) at step 40: 386.6199, Accuracy: 0.7290\n",
      "Training loss (for one batch) at step 50: 382.8160, Accuracy: 0.7387\n",
      "Training loss (for one batch) at step 60: 401.8939, Accuracy: 0.7444\n",
      "Training loss (for one batch) at step 70: 419.1417, Accuracy: 0.7424\n",
      "Training loss (for one batch) at step 80: 425.9515, Accuracy: 0.7338\n",
      "Training loss (for one batch) at step 90: 413.0299, Accuracy: 0.7292\n",
      "Training loss (for one batch) at step 100: 406.5713, Accuracy: 0.7284\n",
      "Training loss (for one batch) at step 110: 402.8588, Accuracy: 0.7304\n",
      "---- Training ----\n",
      "Training loss: 119.8380\n",
      "Training acc over epoch: 0.7303\n",
      "---- Validation ----\n",
      "Validation loss: 35.7482\n",
      "Validation acc: 0.7053\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 422.7130, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 432.7082, Accuracy: 0.6974\n",
      "Training loss (for one batch) at step 20: 409.9976, Accuracy: 0.7154\n",
      "Training loss (for one batch) at step 30: 403.5261, Accuracy: 0.7230\n",
      "Training loss (for one batch) at step 40: 387.9483, Accuracy: 0.7321\n",
      "Training loss (for one batch) at step 50: 378.5891, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 60: 408.8114, Accuracy: 0.7524\n",
      "Training loss (for one batch) at step 70: 422.5642, Accuracy: 0.7487\n",
      "Training loss (for one batch) at step 80: 416.6901, Accuracy: 0.7420\n",
      "Training loss (for one batch) at step 90: 402.1320, Accuracy: 0.7369\n",
      "Training loss (for one batch) at step 100: 391.0834, Accuracy: 0.7374\n",
      "Training loss (for one batch) at step 110: 405.0639, Accuracy: 0.7380\n",
      "---- Training ----\n",
      "Training loss: 131.1766\n",
      "Training acc over epoch: 0.7372\n",
      "---- Validation ----\n",
      "Validation loss: 35.3399\n",
      "Validation acc: 0.6771\n",
      "Time taken: 10.80s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 434.8365, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 415.6226, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 20: 392.6986, Accuracy: 0.7057\n",
      "Training loss (for one batch) at step 30: 394.4555, Accuracy: 0.7193\n",
      "Training loss (for one batch) at step 40: 380.6704, Accuracy: 0.7321\n",
      "Training loss (for one batch) at step 50: 378.5244, Accuracy: 0.7434\n",
      "Training loss (for one batch) at step 60: 423.0265, Accuracy: 0.7483\n",
      "Training loss (for one batch) at step 70: 413.4447, Accuracy: 0.7469\n",
      "Training loss (for one batch) at step 80: 420.5215, Accuracy: 0.7419\n",
      "Training loss (for one batch) at step 90: 397.8316, Accuracy: 0.7400\n",
      "Training loss (for one batch) at step 100: 396.7670, Accuracy: 0.7379\n",
      "Training loss (for one batch) at step 110: 387.4711, Accuracy: 0.7401\n",
      "---- Training ----\n",
      "Training loss: 128.6180\n",
      "Training acc over epoch: 0.7393\n",
      "---- Validation ----\n",
      "Validation loss: 37.0093\n",
      "Validation acc: 0.6840\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 435.4511, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 430.0330, Accuracy: 0.6925\n",
      "Training loss (for one batch) at step 20: 392.7581, Accuracy: 0.7128\n",
      "Training loss (for one batch) at step 30: 391.0424, Accuracy: 0.7293\n",
      "Training loss (for one batch) at step 40: 400.7329, Accuracy: 0.7369\n",
      "Training loss (for one batch) at step 50: 380.4849, Accuracy: 0.7462\n",
      "Training loss (for one batch) at step 60: 374.1705, Accuracy: 0.7526\n",
      "Training loss (for one batch) at step 70: 397.5052, Accuracy: 0.7499\n",
      "Training loss (for one batch) at step 80: 409.1114, Accuracy: 0.7445\n",
      "Training loss (for one batch) at step 90: 405.5929, Accuracy: 0.7416\n",
      "Training loss (for one batch) at step 100: 382.5438, Accuracy: 0.7428\n",
      "Training loss (for one batch) at step 110: 389.9005, Accuracy: 0.7430\n",
      "---- Training ----\n",
      "Training loss: 127.0208\n",
      "Training acc over epoch: 0.7423\n",
      "---- Validation ----\n",
      "Validation loss: 36.9856\n",
      "Validation acc: 0.6822\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 415.5325, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 430.5749, Accuracy: 0.7152\n",
      "Training loss (for one batch) at step 20: 397.7225, Accuracy: 0.7180\n",
      "Training loss (for one batch) at step 30: 386.5358, Accuracy: 0.7339\n",
      "Training loss (for one batch) at step 40: 372.8326, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 50: 367.8231, Accuracy: 0.7512\n",
      "Training loss (for one batch) at step 60: 391.1967, Accuracy: 0.7586\n",
      "Training loss (for one batch) at step 70: 404.0876, Accuracy: 0.7551\n",
      "Training loss (for one batch) at step 80: 408.0021, Accuracy: 0.7486\n",
      "Training loss (for one batch) at step 90: 396.4062, Accuracy: 0.7448\n",
      "Training loss (for one batch) at step 100: 380.1024, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 110: 400.6918, Accuracy: 0.7480\n",
      "---- Training ----\n",
      "Training loss: 124.8805\n",
      "Training acc over epoch: 0.7479\n",
      "---- Validation ----\n",
      "Validation loss: 37.5573\n",
      "Validation acc: 0.7085\n",
      "Time taken: 10.70s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 415.3169, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 419.1178, Accuracy: 0.7365\n",
      "Training loss (for one batch) at step 20: 401.6342, Accuracy: 0.7400\n",
      "Training loss (for one batch) at step 30: 379.4559, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 40: 380.8800, Accuracy: 0.7561\n",
      "Training loss (for one batch) at step 50: 359.5850, Accuracy: 0.7636\n",
      "Training loss (for one batch) at step 60: 401.5044, Accuracy: 0.7699\n",
      "Training loss (for one batch) at step 70: 400.4024, Accuracy: 0.7653\n",
      "Training loss (for one batch) at step 80: 413.6832, Accuracy: 0.7541\n",
      "Training loss (for one batch) at step 90: 391.5482, Accuracy: 0.7512\n",
      "Training loss (for one batch) at step 100: 367.0058, Accuracy: 0.7517\n",
      "Training loss (for one batch) at step 110: 386.8361, Accuracy: 0.7543\n",
      "---- Training ----\n",
      "Training loss: 121.5023\n",
      "Training acc over epoch: 0.7537\n",
      "---- Validation ----\n",
      "Validation loss: 55.5288\n",
      "Validation acc: 0.7058\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 423.5710, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 397.9307, Accuracy: 0.7145\n",
      "Training loss (for one batch) at step 20: 398.9849, Accuracy: 0.7251\n",
      "Training loss (for one batch) at step 30: 387.7226, Accuracy: 0.7326\n",
      "Training loss (for one batch) at step 40: 367.4755, Accuracy: 0.7483\n",
      "Training loss (for one batch) at step 50: 354.4388, Accuracy: 0.7598\n",
      "Training loss (for one batch) at step 60: 374.6177, Accuracy: 0.7645\n",
      "Training loss (for one batch) at step 70: 402.5887, Accuracy: 0.7629\n",
      "Training loss (for one batch) at step 80: 411.7585, Accuracy: 0.7545\n",
      "Training loss (for one batch) at step 90: 373.5537, Accuracy: 0.7523\n",
      "Training loss (for one batch) at step 100: 381.8107, Accuracy: 0.7526\n",
      "Training loss (for one batch) at step 110: 387.9028, Accuracy: 0.7525\n",
      "---- Training ----\n",
      "Training loss: 118.3911\n",
      "Training acc over epoch: 0.7513\n",
      "---- Validation ----\n",
      "Validation loss: 36.1651\n",
      "Validation acc: 0.6679\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 400.0226, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 398.2235, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 20: 376.5103, Accuracy: 0.7214\n",
      "Training loss (for one batch) at step 30: 379.8737, Accuracy: 0.7301\n",
      "Training loss (for one batch) at step 40: 379.0782, Accuracy: 0.7439\n",
      "Training loss (for one batch) at step 50: 356.2260, Accuracy: 0.7560\n",
      "Training loss (for one batch) at step 60: 381.8683, Accuracy: 0.7622\n",
      "Training loss (for one batch) at step 70: 410.9045, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 80: 409.2208, Accuracy: 0.7486\n",
      "Training loss (for one batch) at step 90: 371.5522, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 100: 362.0975, Accuracy: 0.7491\n",
      "Training loss (for one batch) at step 110: 384.4050, Accuracy: 0.7515\n",
      "---- Training ----\n",
      "Training loss: 121.3684\n",
      "Training acc over epoch: 0.7510\n",
      "---- Validation ----\n",
      "Validation loss: 40.4095\n",
      "Validation acc: 0.6910\n",
      "Time taken: 10.54s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 403.3965, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 384.8215, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 20: 374.9095, Accuracy: 0.7321\n",
      "Training loss (for one batch) at step 30: 385.1324, Accuracy: 0.7440\n",
      "Training loss (for one batch) at step 40: 364.2473, Accuracy: 0.7534\n",
      "Training loss (for one batch) at step 50: 348.4467, Accuracy: 0.7650\n",
      "Training loss (for one batch) at step 60: 383.3510, Accuracy: 0.7732\n",
      "Training loss (for one batch) at step 70: 392.5620, Accuracy: 0.7665\n",
      "Training loss (for one batch) at step 80: 383.6322, Accuracy: 0.7576\n",
      "Training loss (for one batch) at step 90: 374.1613, Accuracy: 0.7555\n",
      "Training loss (for one batch) at step 100: 369.4930, Accuracy: 0.7576\n",
      "Training loss (for one batch) at step 110: 371.9260, Accuracy: 0.7591\n",
      "---- Training ----\n",
      "Training loss: 121.4052\n",
      "Training acc over epoch: 0.7585\n",
      "---- Validation ----\n",
      "Validation loss: 33.2678\n",
      "Validation acc: 0.6878\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 399.2854, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 381.2167, Accuracy: 0.7294\n",
      "Training loss (for one batch) at step 20: 374.3818, Accuracy: 0.7474\n",
      "Training loss (for one batch) at step 30: 378.1581, Accuracy: 0.7538\n",
      "Training loss (for one batch) at step 40: 354.6547, Accuracy: 0.7658\n",
      "Training loss (for one batch) at step 50: 341.3220, Accuracy: 0.7748\n",
      "Training loss (for one batch) at step 60: 346.4131, Accuracy: 0.7789\n",
      "Training loss (for one batch) at step 70: 374.1504, Accuracy: 0.7721\n",
      "Training loss (for one batch) at step 80: 412.6969, Accuracy: 0.7619\n",
      "Training loss (for one batch) at step 90: 387.1296, Accuracy: 0.7564\n",
      "Training loss (for one batch) at step 100: 350.5153, Accuracy: 0.7587\n",
      "Training loss (for one batch) at step 110: 370.1347, Accuracy: 0.7583\n",
      "---- Training ----\n",
      "Training loss: 122.1882\n",
      "Training acc over epoch: 0.7580\n",
      "---- Validation ----\n",
      "Validation loss: 39.2576\n",
      "Validation acc: 0.7069\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 407.6334, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 408.9599, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 20: 387.5787, Accuracy: 0.7329\n",
      "Training loss (for one batch) at step 30: 358.2800, Accuracy: 0.7490\n",
      "Training loss (for one batch) at step 40: 345.8692, Accuracy: 0.7590\n",
      "Training loss (for one batch) at step 50: 347.2824, Accuracy: 0.7675\n",
      "Training loss (for one batch) at step 60: 397.4738, Accuracy: 0.7741\n",
      "Training loss (for one batch) at step 70: 391.7363, Accuracy: 0.7675\n",
      "Training loss (for one batch) at step 80: 389.0763, Accuracy: 0.7551\n",
      "Training loss (for one batch) at step 90: 378.7963, Accuracy: 0.7502\n",
      "Training loss (for one batch) at step 100: 357.0034, Accuracy: 0.7525\n",
      "Training loss (for one batch) at step 110: 353.9077, Accuracy: 0.7538\n",
      "---- Training ----\n",
      "Training loss: 122.0933\n",
      "Training acc over epoch: 0.7534\n",
      "---- Validation ----\n",
      "Validation loss: 37.4249\n",
      "Validation acc: 0.7069\n",
      "Time taken: 10.74s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 393.1164, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 399.3419, Accuracy: 0.7074\n",
      "Training loss (for one batch) at step 20: 381.6458, Accuracy: 0.7262\n",
      "Training loss (for one batch) at step 30: 348.0302, Accuracy: 0.7472\n",
      "Training loss (for one batch) at step 40: 354.0586, Accuracy: 0.7565\n",
      "Training loss (for one batch) at step 50: 340.3763, Accuracy: 0.7642\n",
      "Training loss (for one batch) at step 60: 343.1635, Accuracy: 0.7701\n",
      "Training loss (for one batch) at step 70: 376.5892, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 80: 386.2776, Accuracy: 0.7545\n",
      "Training loss (for one batch) at step 90: 372.7133, Accuracy: 0.7513\n",
      "Training loss (for one batch) at step 100: 363.6112, Accuracy: 0.7548\n",
      "Training loss (for one batch) at step 110: 359.3773, Accuracy: 0.7552\n",
      "---- Training ----\n",
      "Training loss: 123.1472\n",
      "Training acc over epoch: 0.7554\n",
      "---- Validation ----\n",
      "Validation loss: 40.5556\n",
      "Validation acc: 0.7152\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 406.5165, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 379.1139, Accuracy: 0.7045\n",
      "Training loss (for one batch) at step 20: 373.6223, Accuracy: 0.7359\n",
      "Training loss (for one batch) at step 30: 362.0392, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 40: 347.1375, Accuracy: 0.7599\n",
      "Training loss (for one batch) at step 50: 343.8658, Accuracy: 0.7721\n",
      "Training loss (for one batch) at step 60: 368.4523, Accuracy: 0.7786\n",
      "Training loss (for one batch) at step 70: 369.3037, Accuracy: 0.7722\n",
      "Training loss (for one batch) at step 80: 418.8769, Accuracy: 0.7606\n",
      "Training loss (for one batch) at step 90: 346.4528, Accuracy: 0.7581\n",
      "Training loss (for one batch) at step 100: 347.5439, Accuracy: 0.7607\n",
      "Training loss (for one batch) at step 110: 377.8310, Accuracy: 0.7620\n",
      "---- Training ----\n",
      "Training loss: 127.0191\n",
      "Training acc over epoch: 0.7617\n",
      "---- Validation ----\n",
      "Validation loss: 53.1006\n",
      "Validation acc: 0.7015\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 389.8239, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 377.1566, Accuracy: 0.6982\n",
      "Training loss (for one batch) at step 20: 368.9662, Accuracy: 0.7184\n",
      "Training loss (for one batch) at step 30: 341.7276, Accuracy: 0.7432\n",
      "Training loss (for one batch) at step 40: 356.9193, Accuracy: 0.7597\n",
      "Training loss (for one batch) at step 50: 328.1628, Accuracy: 0.7705\n",
      "Training loss (for one batch) at step 60: 342.7826, Accuracy: 0.7751\n",
      "Training loss (for one batch) at step 70: 384.4401, Accuracy: 0.7647\n",
      "Training loss (for one batch) at step 80: 389.0200, Accuracy: 0.7560\n",
      "Training loss (for one batch) at step 90: 340.8424, Accuracy: 0.7536\n",
      "Training loss (for one batch) at step 100: 353.9123, Accuracy: 0.7568\n",
      "Training loss (for one batch) at step 110: 376.5622, Accuracy: 0.7573\n",
      "---- Training ----\n",
      "Training loss: 111.4132\n",
      "Training acc over epoch: 0.7576\n",
      "---- Validation ----\n",
      "Validation loss: 41.7366\n",
      "Validation acc: 0.6945\n",
      "Time taken: 11.17s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 380.8390, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 372.6978, Accuracy: 0.6896\n",
      "Training loss (for one batch) at step 20: 361.5562, Accuracy: 0.7355\n",
      "Training loss (for one batch) at step 30: 347.8580, Accuracy: 0.7523\n",
      "Training loss (for one batch) at step 40: 326.3823, Accuracy: 0.7612\n",
      "Training loss (for one batch) at step 50: 323.6971, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 60: 346.6170, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 70: 377.1426, Accuracy: 0.7721\n",
      "Training loss (for one batch) at step 80: 371.7734, Accuracy: 0.7612\n",
      "Training loss (for one batch) at step 90: 346.7884, Accuracy: 0.7577\n",
      "Training loss (for one batch) at step 100: 362.4865, Accuracy: 0.7580\n",
      "Training loss (for one batch) at step 110: 376.1933, Accuracy: 0.7589\n",
      "---- Training ----\n",
      "Training loss: 116.6136\n",
      "Training acc over epoch: 0.7583\n",
      "---- Validation ----\n",
      "Validation loss: 33.5375\n",
      "Validation acc: 0.6977\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 383.7204, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 380.0486, Accuracy: 0.6932\n",
      "Training loss (for one batch) at step 20: 349.5277, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 30: 341.7430, Accuracy: 0.7492\n",
      "Training loss (for one batch) at step 40: 357.2824, Accuracy: 0.7624\n",
      "Training loss (for one batch) at step 50: 331.3727, Accuracy: 0.7711\n",
      "Training loss (for one batch) at step 60: 337.6583, Accuracy: 0.7780\n",
      "Training loss (for one batch) at step 70: 366.7197, Accuracy: 0.7688\n",
      "Training loss (for one batch) at step 80: 394.2478, Accuracy: 0.7585\n",
      "Training loss (for one batch) at step 90: 350.8670, Accuracy: 0.7541\n",
      "Training loss (for one batch) at step 100: 365.5669, Accuracy: 0.7564\n",
      "Training loss (for one batch) at step 110: 365.8651, Accuracy: 0.7594\n",
      "---- Training ----\n",
      "Training loss: 132.9662\n",
      "Training acc over epoch: 0.7577\n",
      "---- Validation ----\n",
      "Validation loss: 41.4762\n",
      "Validation acc: 0.6910\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 401.2680, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 374.4900, Accuracy: 0.6996\n",
      "Training loss (for one batch) at step 20: 356.7178, Accuracy: 0.7195\n",
      "Training loss (for one batch) at step 30: 347.6005, Accuracy: 0.7417\n",
      "Training loss (for one batch) at step 40: 321.3900, Accuracy: 0.7591\n",
      "Training loss (for one batch) at step 50: 309.9555, Accuracy: 0.7688\n",
      "Training loss (for one batch) at step 60: 345.2277, Accuracy: 0.7772\n",
      "Training loss (for one batch) at step 70: 374.5186, Accuracy: 0.7684\n",
      "Training loss (for one batch) at step 80: 368.7959, Accuracy: 0.7566\n",
      "Training loss (for one batch) at step 90: 351.4852, Accuracy: 0.7526\n",
      "Training loss (for one batch) at step 100: 329.3428, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 110: 360.4864, Accuracy: 0.7593\n",
      "---- Training ----\n",
      "Training loss: 107.3432\n",
      "Training acc over epoch: 0.7596\n",
      "---- Validation ----\n",
      "Validation loss: 40.8714\n",
      "Validation acc: 0.6924\n",
      "Time taken: 11.02s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 390.0571, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 370.7791, Accuracy: 0.7102\n",
      "Training loss (for one batch) at step 20: 343.5365, Accuracy: 0.7359\n",
      "Training loss (for one batch) at step 30: 353.1559, Accuracy: 0.7558\n",
      "Training loss (for one batch) at step 40: 350.9620, Accuracy: 0.7670\n",
      "Training loss (for one batch) at step 50: 341.6915, Accuracy: 0.7802\n",
      "Training loss (for one batch) at step 60: 331.7932, Accuracy: 0.7856\n",
      "Training loss (for one batch) at step 70: 376.1127, Accuracy: 0.7749\n",
      "Training loss (for one batch) at step 80: 357.3336, Accuracy: 0.7614\n",
      "Training loss (for one batch) at step 90: 330.4050, Accuracy: 0.7600\n",
      "Training loss (for one batch) at step 100: 331.6215, Accuracy: 0.7625\n",
      "Training loss (for one batch) at step 110: 356.7303, Accuracy: 0.7631\n",
      "---- Training ----\n",
      "Training loss: 115.8960\n",
      "Training acc over epoch: 0.7620\n",
      "---- Validation ----\n",
      "Validation loss: 35.7888\n",
      "Validation acc: 0.7028\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 396.3047, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 386.2156, Accuracy: 0.7081\n",
      "Training loss (for one batch) at step 20: 343.2280, Accuracy: 0.7336\n",
      "Training loss (for one batch) at step 30: 339.5314, Accuracy: 0.7568\n",
      "Training loss (for one batch) at step 40: 326.3304, Accuracy: 0.7683\n",
      "Training loss (for one batch) at step 50: 314.0756, Accuracy: 0.7780\n",
      "Training loss (for one batch) at step 60: 323.0387, Accuracy: 0.7842\n",
      "Training loss (for one batch) at step 70: 356.5590, Accuracy: 0.7750\n",
      "Training loss (for one batch) at step 80: 376.3847, Accuracy: 0.7623\n",
      "Training loss (for one batch) at step 90: 345.8645, Accuracy: 0.7592\n",
      "Training loss (for one batch) at step 100: 324.4289, Accuracy: 0.7628\n",
      "Training loss (for one batch) at step 110: 351.0970, Accuracy: 0.7635\n",
      "---- Training ----\n",
      "Training loss: 112.5149\n",
      "Training acc over epoch: 0.7611\n",
      "---- Validation ----\n",
      "Validation loss: 38.3500\n",
      "Validation acc: 0.6843\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 364.0220, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 368.1846, Accuracy: 0.7017\n",
      "Training loss (for one batch) at step 20: 329.0026, Accuracy: 0.7314\n",
      "Training loss (for one batch) at step 30: 335.6046, Accuracy: 0.7495\n",
      "Training loss (for one batch) at step 40: 313.3515, Accuracy: 0.7620\n",
      "Training loss (for one batch) at step 50: 330.1006, Accuracy: 0.7733\n",
      "Training loss (for one batch) at step 60: 330.9319, Accuracy: 0.7815\n",
      "Training loss (for one batch) at step 70: 377.9865, Accuracy: 0.7733\n",
      "Training loss (for one batch) at step 80: 361.0272, Accuracy: 0.7600\n",
      "Training loss (for one batch) at step 90: 339.8409, Accuracy: 0.7571\n",
      "Training loss (for one batch) at step 100: 339.9657, Accuracy: 0.7612\n",
      "Training loss (for one batch) at step 110: 355.4192, Accuracy: 0.7616\n",
      "---- Training ----\n",
      "Training loss: 113.0048\n",
      "Training acc over epoch: 0.7603\n",
      "---- Validation ----\n",
      "Validation loss: 44.9591\n",
      "Validation acc: 0.6977\n",
      "Time taken: 10.96s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 389.9384, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 380.7969, Accuracy: 0.7138\n",
      "Training loss (for one batch) at step 20: 356.7268, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 30: 332.7251, Accuracy: 0.7558\n",
      "Training loss (for one batch) at step 40: 317.7209, Accuracy: 0.7687\n",
      "Training loss (for one batch) at step 50: 324.4375, Accuracy: 0.7797\n",
      "Training loss (for one batch) at step 60: 351.2117, Accuracy: 0.7850\n",
      "Training loss (for one batch) at step 70: 377.3523, Accuracy: 0.7743\n",
      "Training loss (for one batch) at step 80: 367.6596, Accuracy: 0.7623\n",
      "Training loss (for one batch) at step 90: 337.5819, Accuracy: 0.7594\n",
      "Training loss (for one batch) at step 100: 334.9292, Accuracy: 0.7611\n",
      "Training loss (for one batch) at step 110: 334.2370, Accuracy: 0.7624\n",
      "---- Training ----\n",
      "Training loss: 111.7705\n",
      "Training acc over epoch: 0.7631\n",
      "---- Validation ----\n",
      "Validation loss: 43.3637\n",
      "Validation acc: 0.6969\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 390.9391, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 384.7444, Accuracy: 0.7081\n",
      "Training loss (for one batch) at step 20: 341.4911, Accuracy: 0.7273\n",
      "Training loss (for one batch) at step 30: 337.7643, Accuracy: 0.7497\n",
      "Training loss (for one batch) at step 40: 321.5334, Accuracy: 0.7681\n",
      "Training loss (for one batch) at step 50: 329.7899, Accuracy: 0.7765\n",
      "Training loss (for one batch) at step 60: 341.1736, Accuracy: 0.7841\n",
      "Training loss (for one batch) at step 70: 355.2285, Accuracy: 0.7763\n",
      "Training loss (for one batch) at step 80: 368.4700, Accuracy: 0.7626\n",
      "Training loss (for one batch) at step 90: 347.5729, Accuracy: 0.7588\n",
      "Training loss (for one batch) at step 100: 314.1750, Accuracy: 0.7630\n",
      "Training loss (for one batch) at step 110: 348.1497, Accuracy: 0.7628\n",
      "---- Training ----\n",
      "Training loss: 106.9102\n",
      "Training acc over epoch: 0.7620\n",
      "---- Validation ----\n",
      "Validation loss: 43.2645\n",
      "Validation acc: 0.6934\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 365.7357, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 366.1901, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 20: 330.6378, Accuracy: 0.7333\n",
      "Training loss (for one batch) at step 30: 322.5746, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 40: 309.4525, Accuracy: 0.7689\n",
      "Training loss (for one batch) at step 50: 314.3608, Accuracy: 0.7793\n",
      "Training loss (for one batch) at step 60: 326.4175, Accuracy: 0.7842\n",
      "Training loss (for one batch) at step 70: 364.8564, Accuracy: 0.7764\n",
      "Training loss (for one batch) at step 80: 367.3074, Accuracy: 0.7622\n",
      "Training loss (for one batch) at step 90: 331.1802, Accuracy: 0.7594\n",
      "Training loss (for one batch) at step 100: 339.4388, Accuracy: 0.7637\n",
      "Training loss (for one batch) at step 110: 338.7845, Accuracy: 0.7632\n",
      "---- Training ----\n",
      "Training loss: 102.8174\n",
      "Training acc over epoch: 0.7636\n",
      "---- Validation ----\n",
      "Validation loss: 29.4938\n",
      "Validation acc: 0.7109\n",
      "Time taken: 10.69s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 373.1136, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 347.6726, Accuracy: 0.6896\n",
      "Training loss (for one batch) at step 20: 356.5519, Accuracy: 0.7221\n",
      "Training loss (for one batch) at step 30: 320.5415, Accuracy: 0.7462\n",
      "Training loss (for one batch) at step 40: 303.0025, Accuracy: 0.7637\n",
      "Training loss (for one batch) at step 50: 306.3667, Accuracy: 0.7745\n",
      "Training loss (for one batch) at step 60: 342.8476, Accuracy: 0.7792\n",
      "Training loss (for one batch) at step 70: 347.0605, Accuracy: 0.7672\n",
      "Training loss (for one batch) at step 80: 376.5544, Accuracy: 0.7581\n",
      "Training loss (for one batch) at step 90: 329.5735, Accuracy: 0.7547\n",
      "Training loss (for one batch) at step 100: 316.7830, Accuracy: 0.7588\n",
      "Training loss (for one batch) at step 110: 345.6705, Accuracy: 0.7613\n",
      "---- Training ----\n",
      "Training loss: 97.5465\n",
      "Training acc over epoch: 0.7607\n",
      "---- Validation ----\n",
      "Validation loss: 39.0321\n",
      "Validation acc: 0.6969\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 364.8489, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 375.5246, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 20: 371.4173, Accuracy: 0.7329\n",
      "Training loss (for one batch) at step 30: 316.5499, Accuracy: 0.7560\n",
      "Training loss (for one batch) at step 40: 316.0493, Accuracy: 0.7679\n",
      "Training loss (for one batch) at step 50: 323.1525, Accuracy: 0.7794\n",
      "Training loss (for one batch) at step 60: 326.6528, Accuracy: 0.7865\n",
      "Training loss (for one batch) at step 70: 338.8604, Accuracy: 0.7765\n",
      "Training loss (for one batch) at step 80: 366.7478, Accuracy: 0.7630\n",
      "Training loss (for one batch) at step 90: 334.6357, Accuracy: 0.7586\n",
      "Training loss (for one batch) at step 100: 314.7887, Accuracy: 0.7623\n",
      "Training loss (for one batch) at step 110: 350.0253, Accuracy: 0.7650\n",
      "---- Training ----\n",
      "Training loss: 110.9913\n",
      "Training acc over epoch: 0.7644\n",
      "---- Validation ----\n",
      "Validation loss: 44.6210\n",
      "Validation acc: 0.7222\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 353.4335, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 362.0817, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 20: 346.1049, Accuracy: 0.7329\n",
      "Training loss (for one batch) at step 30: 309.7271, Accuracy: 0.7553\n",
      "Training loss (for one batch) at step 40: 325.7273, Accuracy: 0.7725\n",
      "Training loss (for one batch) at step 50: 314.8172, Accuracy: 0.7811\n",
      "Training loss (for one batch) at step 60: 322.5441, Accuracy: 0.7871\n",
      "Training loss (for one batch) at step 70: 346.4390, Accuracy: 0.7761\n",
      "Training loss (for one batch) at step 80: 365.1410, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 90: 325.2153, Accuracy: 0.7636\n",
      "Training loss (for one batch) at step 100: 344.4157, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 110: 342.7052, Accuracy: 0.7642\n",
      "---- Training ----\n",
      "Training loss: 113.7208\n",
      "Training acc over epoch: 0.7641\n",
      "---- Validation ----\n",
      "Validation loss: 37.6357\n",
      "Validation acc: 0.6999\n",
      "Time taken: 10.72s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 383.4393, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 364.1535, Accuracy: 0.6982\n",
      "Training loss (for one batch) at step 20: 342.7527, Accuracy: 0.7236\n",
      "Training loss (for one batch) at step 30: 316.2898, Accuracy: 0.7492\n",
      "Training loss (for one batch) at step 40: 311.6245, Accuracy: 0.7679\n",
      "Training loss (for one batch) at step 50: 308.6455, Accuracy: 0.7794\n",
      "Training loss (for one batch) at step 60: 313.7182, Accuracy: 0.7838\n",
      "Training loss (for one batch) at step 70: 348.3431, Accuracy: 0.7728\n",
      "Training loss (for one batch) at step 80: 353.2490, Accuracy: 0.7589\n",
      "Training loss (for one batch) at step 90: 344.6721, Accuracy: 0.7543\n",
      "Training loss (for one batch) at step 100: 332.5258, Accuracy: 0.7572\n",
      "Training loss (for one batch) at step 110: 329.4527, Accuracy: 0.7591\n",
      "---- Training ----\n",
      "Training loss: 113.8808\n",
      "Training acc over epoch: 0.7573\n",
      "---- Validation ----\n",
      "Validation loss: 55.7312\n",
      "Validation acc: 0.6980\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 350.4240, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 364.7444, Accuracy: 0.7060\n",
      "Training loss (for one batch) at step 20: 339.6500, Accuracy: 0.7418\n",
      "Training loss (for one batch) at step 30: 323.6547, Accuracy: 0.7611\n",
      "Training loss (for one batch) at step 40: 296.0518, Accuracy: 0.7725\n",
      "Training loss (for one batch) at step 50: 326.8417, Accuracy: 0.7819\n",
      "Training loss (for one batch) at step 60: 332.9567, Accuracy: 0.7868\n",
      "Training loss (for one batch) at step 70: 372.1410, Accuracy: 0.7771\n",
      "Training loss (for one batch) at step 80: 376.7278, Accuracy: 0.7622\n",
      "Training loss (for one batch) at step 90: 322.2027, Accuracy: 0.7618\n",
      "Training loss (for one batch) at step 100: 316.9189, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 110: 339.2056, Accuracy: 0.7630\n",
      "---- Training ----\n",
      "Training loss: 105.3110\n",
      "Training acc over epoch: 0.7635\n",
      "---- Validation ----\n",
      "Validation loss: 58.6358\n",
      "Validation acc: 0.7026\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 387.7675, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 347.0593, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 20: 325.0525, Accuracy: 0.7199\n",
      "Training loss (for one batch) at step 30: 313.8192, Accuracy: 0.7490\n",
      "Training loss (for one batch) at step 40: 314.0028, Accuracy: 0.7612\n",
      "Training loss (for one batch) at step 50: 308.7985, Accuracy: 0.7753\n",
      "Training loss (for one batch) at step 60: 326.7628, Accuracy: 0.7833\n",
      "Training loss (for one batch) at step 70: 349.8867, Accuracy: 0.7750\n",
      "Training loss (for one batch) at step 80: 360.8108, Accuracy: 0.7605\n",
      "Training loss (for one batch) at step 90: 311.4408, Accuracy: 0.7588\n",
      "Training loss (for one batch) at step 100: 317.8224, Accuracy: 0.7624\n",
      "Training loss (for one batch) at step 110: 333.5624, Accuracy: 0.7629\n",
      "---- Training ----\n",
      "Training loss: 129.1876\n",
      "Training acc over epoch: 0.7623\n",
      "---- Validation ----\n",
      "Validation loss: 34.9800\n",
      "Validation acc: 0.6953\n",
      "Time taken: 10.68s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 361.9518, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 352.2986, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 20: 327.3821, Accuracy: 0.7269\n",
      "Training loss (for one batch) at step 30: 321.3389, Accuracy: 0.7523\n",
      "Training loss (for one batch) at step 40: 314.2374, Accuracy: 0.7679\n",
      "Training loss (for one batch) at step 50: 305.6398, Accuracy: 0.7783\n",
      "Training loss (for one batch) at step 60: 320.6510, Accuracy: 0.7850\n",
      "Training loss (for one batch) at step 70: 331.2238, Accuracy: 0.7764\n",
      "Training loss (for one batch) at step 80: 352.1084, Accuracy: 0.7647\n",
      "Training loss (for one batch) at step 90: 307.1083, Accuracy: 0.7622\n",
      "Training loss (for one batch) at step 100: 313.9981, Accuracy: 0.7651\n",
      "Training loss (for one batch) at step 110: 326.5313, Accuracy: 0.7651\n",
      "---- Training ----\n",
      "Training loss: 120.9610\n",
      "Training acc over epoch: 0.7635\n",
      "---- Validation ----\n",
      "Validation loss: 53.0917\n",
      "Validation acc: 0.7061\n",
      "Time taken: 10.54s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 356.1076, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 378.2466, Accuracy: 0.7045\n",
      "Training loss (for one batch) at step 20: 328.1434, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 30: 322.5096, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 40: 297.6551, Accuracy: 0.7729\n",
      "Training loss (for one batch) at step 50: 316.1472, Accuracy: 0.7834\n",
      "Training loss (for one batch) at step 60: 320.7516, Accuracy: 0.7875\n",
      "Training loss (for one batch) at step 70: 356.6355, Accuracy: 0.7778\n",
      "Training loss (for one batch) at step 80: 353.2088, Accuracy: 0.7654\n",
      "Training loss (for one batch) at step 90: 332.4067, Accuracy: 0.7623\n",
      "Training loss (for one batch) at step 100: 312.9144, Accuracy: 0.7637\n",
      "Training loss (for one batch) at step 110: 341.3956, Accuracy: 0.7648\n",
      "---- Training ----\n",
      "Training loss: 101.8115\n",
      "Training acc over epoch: 0.7638\n",
      "---- Validation ----\n",
      "Validation loss: 40.5082\n",
      "Validation acc: 0.6900\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 381.9676, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 364.0414, Accuracy: 0.6889\n",
      "Training loss (for one batch) at step 20: 321.3157, Accuracy: 0.7232\n",
      "Training loss (for one batch) at step 30: 302.3930, Accuracy: 0.7518\n",
      "Training loss (for one batch) at step 40: 317.4442, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 50: 313.5934, Accuracy: 0.7774\n",
      "Training loss (for one batch) at step 60: 325.0870, Accuracy: 0.7833\n",
      "Training loss (for one batch) at step 70: 358.7883, Accuracy: 0.7756\n",
      "Training loss (for one batch) at step 80: 347.7571, Accuracy: 0.7628\n",
      "Training loss (for one batch) at step 90: 311.9242, Accuracy: 0.7613\n",
      "Training loss (for one batch) at step 100: 337.6344, Accuracy: 0.7641\n",
      "Training loss (for one batch) at step 110: 334.6969, Accuracy: 0.7651\n",
      "---- Training ----\n",
      "Training loss: 102.5773\n",
      "Training acc over epoch: 0.7632\n",
      "---- Validation ----\n",
      "Validation loss: 66.4628\n",
      "Validation acc: 0.6959\n",
      "Time taken: 10.75s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 367.3501, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 338.5424, Accuracy: 0.6861\n",
      "Training loss (for one batch) at step 20: 314.6224, Accuracy: 0.7277\n",
      "Training loss (for one batch) at step 30: 328.4892, Accuracy: 0.7533\n",
      "Training loss (for one batch) at step 40: 296.8507, Accuracy: 0.7689\n",
      "Training loss (for one batch) at step 50: 305.3664, Accuracy: 0.7782\n",
      "Training loss (for one batch) at step 60: 322.6519, Accuracy: 0.7824\n",
      "Training loss (for one batch) at step 70: 338.7422, Accuracy: 0.7720\n",
      "Training loss (for one batch) at step 80: 322.8167, Accuracy: 0.7603\n",
      "Training loss (for one batch) at step 90: 325.4786, Accuracy: 0.7566\n",
      "Training loss (for one batch) at step 100: 316.9132, Accuracy: 0.7595\n",
      "Training loss (for one batch) at step 110: 310.3647, Accuracy: 0.7606\n",
      "---- Training ----\n",
      "Training loss: 113.9742\n",
      "Training acc over epoch: 0.7601\n",
      "---- Validation ----\n",
      "Validation loss: 37.4633\n",
      "Validation acc: 0.7004\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 367.1650, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 337.5194, Accuracy: 0.6996\n",
      "Training loss (for one batch) at step 20: 311.0026, Accuracy: 0.7333\n",
      "Training loss (for one batch) at step 30: 308.4507, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 40: 304.1753, Accuracy: 0.7731\n",
      "Training loss (for one batch) at step 50: 298.4025, Accuracy: 0.7814\n",
      "Training loss (for one batch) at step 60: 308.8897, Accuracy: 0.7884\n",
      "Training loss (for one batch) at step 70: 347.8136, Accuracy: 0.7809\n",
      "Training loss (for one batch) at step 80: 342.9895, Accuracy: 0.7677\n",
      "Training loss (for one batch) at step 90: 307.5012, Accuracy: 0.7644\n",
      "Training loss (for one batch) at step 100: 310.8231, Accuracy: 0.7657\n",
      "Training loss (for one batch) at step 110: 328.2504, Accuracy: 0.7671\n",
      "---- Training ----\n",
      "Training loss: 111.0366\n",
      "Training acc over epoch: 0.7669\n",
      "---- Validation ----\n",
      "Validation loss: 38.7526\n",
      "Validation acc: 0.6964\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 351.6218, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 337.5157, Accuracy: 0.6982\n",
      "Training loss (for one batch) at step 20: 299.4717, Accuracy: 0.7303\n",
      "Training loss (for one batch) at step 30: 319.0719, Accuracy: 0.7548\n",
      "Training loss (for one batch) at step 40: 298.9515, Accuracy: 0.7675\n",
      "Training loss (for one batch) at step 50: 304.0462, Accuracy: 0.7820\n",
      "Training loss (for one batch) at step 60: 322.1447, Accuracy: 0.7862\n",
      "Training loss (for one batch) at step 70: 345.2758, Accuracy: 0.7753\n",
      "Training loss (for one batch) at step 80: 344.0139, Accuracy: 0.7604\n",
      "Training loss (for one batch) at step 90: 307.9023, Accuracy: 0.7579\n",
      "Training loss (for one batch) at step 100: 320.0399, Accuracy: 0.7625\n",
      "Training loss (for one batch) at step 110: 314.5527, Accuracy: 0.7629\n",
      "---- Training ----\n",
      "Training loss: 101.6847\n",
      "Training acc over epoch: 0.7619\n",
      "---- Validation ----\n",
      "Validation loss: 57.1899\n",
      "Validation acc: 0.7015\n",
      "Time taken: 10.95s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 368.1949, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 352.7878, Accuracy: 0.7010\n",
      "Training loss (for one batch) at step 20: 332.2708, Accuracy: 0.7329\n",
      "Training loss (for one batch) at step 30: 303.1507, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 40: 312.5478, Accuracy: 0.7700\n",
      "Training loss (for one batch) at step 50: 315.8077, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 60: 320.9707, Accuracy: 0.7869\n",
      "Training loss (for one batch) at step 70: 331.4837, Accuracy: 0.7777\n",
      "Training loss (for one batch) at step 80: 349.0063, Accuracy: 0.7641\n",
      "Training loss (for one batch) at step 90: 301.7617, Accuracy: 0.7619\n",
      "Training loss (for one batch) at step 100: 313.8196, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 110: 326.3239, Accuracy: 0.7649\n",
      "---- Training ----\n",
      "Training loss: 123.4769\n",
      "Training acc over epoch: 0.7632\n",
      "---- Validation ----\n",
      "Validation loss: 32.5899\n",
      "Validation acc: 0.6934\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 346.9919, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 364.3517, Accuracy: 0.7017\n",
      "Training loss (for one batch) at step 20: 314.4262, Accuracy: 0.7340\n",
      "Training loss (for one batch) at step 30: 296.9153, Accuracy: 0.7583\n",
      "Training loss (for one batch) at step 40: 285.5870, Accuracy: 0.7692\n",
      "Training loss (for one batch) at step 50: 284.1601, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 60: 306.4890, Accuracy: 0.7896\n",
      "Training loss (for one batch) at step 70: 342.5799, Accuracy: 0.7793\n",
      "Training loss (for one batch) at step 80: 344.3896, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 90: 311.1054, Accuracy: 0.7630\n",
      "Training loss (for one batch) at step 100: 339.1476, Accuracy: 0.7642\n",
      "Training loss (for one batch) at step 110: 331.2851, Accuracy: 0.7664\n",
      "---- Training ----\n",
      "Training loss: 112.9577\n",
      "Training acc over epoch: 0.7654\n",
      "---- Validation ----\n",
      "Validation loss: 39.0118\n",
      "Validation acc: 0.6913\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 383.2248, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 360.7247, Accuracy: 0.6776\n",
      "Training loss (for one batch) at step 20: 308.0160, Accuracy: 0.7236\n",
      "Training loss (for one batch) at step 30: 305.8055, Accuracy: 0.7545\n",
      "Training loss (for one batch) at step 40: 307.0991, Accuracy: 0.7681\n",
      "Training loss (for one batch) at step 50: 291.5547, Accuracy: 0.7797\n",
      "Training loss (for one batch) at step 60: 330.4848, Accuracy: 0.7851\n",
      "Training loss (for one batch) at step 70: 339.9817, Accuracy: 0.7750\n",
      "Training loss (for one batch) at step 80: 340.3553, Accuracy: 0.7642\n",
      "Training loss (for one batch) at step 90: 296.8001, Accuracy: 0.7618\n",
      "Training loss (for one batch) at step 100: 295.3904, Accuracy: 0.7661\n",
      "Training loss (for one batch) at step 110: 342.4802, Accuracy: 0.7679\n",
      "---- Training ----\n",
      "Training loss: 102.1330\n",
      "Training acc over epoch: 0.7663\n",
      "---- Validation ----\n",
      "Validation loss: 34.7767\n",
      "Validation acc: 0.7106\n",
      "Time taken: 11.05s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 351.3531, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 361.2271, Accuracy: 0.6946\n",
      "Training loss (for one batch) at step 20: 312.6579, Accuracy: 0.7307\n",
      "Training loss (for one batch) at step 30: 310.7720, Accuracy: 0.7553\n",
      "Training loss (for one batch) at step 40: 305.3814, Accuracy: 0.7689\n",
      "Training loss (for one batch) at step 50: 290.1313, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 60: 306.9537, Accuracy: 0.7870\n",
      "Training loss (for one batch) at step 70: 337.6635, Accuracy: 0.7767\n",
      "Training loss (for one batch) at step 80: 324.6616, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 90: 303.3109, Accuracy: 0.7616\n",
      "Training loss (for one batch) at step 100: 329.8202, Accuracy: 0.7625\n",
      "Training loss (for one batch) at step 110: 338.0208, Accuracy: 0.7634\n",
      "---- Training ----\n",
      "Training loss: 113.7726\n",
      "Training acc over epoch: 0.7627\n",
      "---- Validation ----\n",
      "Validation loss: 53.7168\n",
      "Validation acc: 0.7080\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 352.1746, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 337.8774, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 20: 300.6355, Accuracy: 0.7232\n",
      "Training loss (for one batch) at step 30: 290.1898, Accuracy: 0.7568\n",
      "Training loss (for one batch) at step 40: 294.8340, Accuracy: 0.7702\n",
      "Training loss (for one batch) at step 50: 291.1512, Accuracy: 0.7814\n",
      "Training loss (for one batch) at step 60: 332.3856, Accuracy: 0.7896\n",
      "Training loss (for one batch) at step 70: 343.6013, Accuracy: 0.7796\n",
      "Training loss (for one batch) at step 80: 336.0391, Accuracy: 0.7648\n",
      "Training loss (for one batch) at step 90: 291.9439, Accuracy: 0.7612\n",
      "Training loss (for one batch) at step 100: 309.1788, Accuracy: 0.7645\n",
      "Training loss (for one batch) at step 110: 333.8080, Accuracy: 0.7648\n",
      "---- Training ----\n",
      "Training loss: 108.5321\n",
      "Training acc over epoch: 0.7628\n",
      "---- Validation ----\n",
      "Validation loss: 45.4461\n",
      "Validation acc: 0.6900\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 335.0905, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 342.0726, Accuracy: 0.7017\n",
      "Training loss (for one batch) at step 20: 295.2661, Accuracy: 0.7381\n",
      "Training loss (for one batch) at step 30: 275.7780, Accuracy: 0.7636\n",
      "Training loss (for one batch) at step 40: 290.0962, Accuracy: 0.7729\n",
      "Training loss (for one batch) at step 50: 292.8880, Accuracy: 0.7869\n",
      "Training loss (for one batch) at step 60: 299.8151, Accuracy: 0.7914\n",
      "Training loss (for one batch) at step 70: 345.6193, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 80: 335.4486, Accuracy: 0.7677\n",
      "Training loss (for one batch) at step 90: 305.2122, Accuracy: 0.7643\n",
      "Training loss (for one batch) at step 100: 302.7905, Accuracy: 0.7655\n",
      "Training loss (for one batch) at step 110: 343.0186, Accuracy: 0.7661\n",
      "---- Training ----\n",
      "Training loss: 97.1524\n",
      "Training acc over epoch: 0.7645\n",
      "---- Validation ----\n",
      "Validation loss: 49.3824\n",
      "Validation acc: 0.7012\n",
      "Time taken: 10.87s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 354.0941, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 343.7848, Accuracy: 0.6939\n",
      "Training loss (for one batch) at step 20: 304.6797, Accuracy: 0.7277\n",
      "Training loss (for one batch) at step 30: 298.0335, Accuracy: 0.7538\n",
      "Training loss (for one batch) at step 40: 301.8488, Accuracy: 0.7715\n",
      "Training loss (for one batch) at step 50: 302.6838, Accuracy: 0.7851\n",
      "Training loss (for one batch) at step 60: 321.1812, Accuracy: 0.7902\n",
      "Training loss (for one batch) at step 70: 342.3898, Accuracy: 0.7784\n",
      "Training loss (for one batch) at step 80: 347.5353, Accuracy: 0.7639\n",
      "Training loss (for one batch) at step 90: 289.7379, Accuracy: 0.7606\n",
      "Training loss (for one batch) at step 100: 285.1746, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 110: 316.6297, Accuracy: 0.7650\n",
      "---- Training ----\n",
      "Training loss: 94.5139\n",
      "Training acc over epoch: 0.7632\n",
      "---- Validation ----\n",
      "Validation loss: 42.3420\n",
      "Validation acc: 0.6959\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 349.2777, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 332.6663, Accuracy: 0.6818\n",
      "Training loss (for one batch) at step 20: 290.1306, Accuracy: 0.7247\n",
      "Training loss (for one batch) at step 30: 288.3546, Accuracy: 0.7558\n",
      "Training loss (for one batch) at step 40: 288.6547, Accuracy: 0.7681\n",
      "Training loss (for one batch) at step 50: 290.8159, Accuracy: 0.7805\n",
      "Training loss (for one batch) at step 60: 297.2135, Accuracy: 0.7874\n",
      "Training loss (for one batch) at step 70: 338.4529, Accuracy: 0.7755\n",
      "Training loss (for one batch) at step 80: 324.1100, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 90: 306.4334, Accuracy: 0.7589\n",
      "Training loss (for one batch) at step 100: 300.2827, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 110: 313.9228, Accuracy: 0.7659\n",
      "---- Training ----\n",
      "Training loss: 108.3092\n",
      "Training acc over epoch: 0.7648\n",
      "---- Validation ----\n",
      "Validation loss: 44.7414\n",
      "Validation acc: 0.6902\n",
      "Time taken: 10.40s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 337.6526, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 320.3719, Accuracy: 0.7102\n",
      "Training loss (for one batch) at step 20: 292.4034, Accuracy: 0.7281\n",
      "Training loss (for one batch) at step 30: 316.3640, Accuracy: 0.7548\n",
      "Training loss (for one batch) at step 40: 301.2018, Accuracy: 0.7696\n",
      "Training loss (for one batch) at step 50: 303.5446, Accuracy: 0.7846\n",
      "Training loss (for one batch) at step 60: 320.9054, Accuracy: 0.7887\n",
      "Training loss (for one batch) at step 70: 332.5641, Accuracy: 0.7763\n",
      "Training loss (for one batch) at step 80: 339.0811, Accuracy: 0.7629\n",
      "Training loss (for one batch) at step 90: 296.5569, Accuracy: 0.7613\n",
      "Training loss (for one batch) at step 100: 294.4800, Accuracy: 0.7644\n",
      "Training loss (for one batch) at step 110: 328.0468, Accuracy: 0.7650\n",
      "---- Training ----\n",
      "Training loss: 117.1086\n",
      "Training acc over epoch: 0.7649\n",
      "---- Validation ----\n",
      "Validation loss: 46.8138\n",
      "Validation acc: 0.6945\n",
      "Time taken: 10.78s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 323.3329, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 357.6342, Accuracy: 0.6946\n",
      "Training loss (for one batch) at step 20: 310.8368, Accuracy: 0.7221\n",
      "Training loss (for one batch) at step 30: 305.6177, Accuracy: 0.7518\n",
      "Training loss (for one batch) at step 40: 281.4587, Accuracy: 0.7668\n",
      "Training loss (for one batch) at step 50: 300.1552, Accuracy: 0.7790\n",
      "Training loss (for one batch) at step 60: 299.6392, Accuracy: 0.7842\n",
      "Training loss (for one batch) at step 70: 325.6481, Accuracy: 0.7752\n",
      "Training loss (for one batch) at step 80: 317.7703, Accuracy: 0.7631\n",
      "Training loss (for one batch) at step 90: 301.4075, Accuracy: 0.7594\n",
      "Training loss (for one batch) at step 100: 293.8348, Accuracy: 0.7623\n",
      "Training loss (for one batch) at step 110: 316.0506, Accuracy: 0.7625\n",
      "---- Training ----\n",
      "Training loss: 110.0917\n",
      "Training acc over epoch: 0.7625\n",
      "---- Validation ----\n",
      "Validation loss: 44.5133\n",
      "Validation acc: 0.6988\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 344.9298, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 357.9220, Accuracy: 0.6754\n",
      "Training loss (for one batch) at step 20: 304.4538, Accuracy: 0.7191\n",
      "Training loss (for one batch) at step 30: 281.0064, Accuracy: 0.7510\n",
      "Training loss (for one batch) at step 40: 282.4782, Accuracy: 0.7670\n",
      "Training loss (for one batch) at step 50: 283.3886, Accuracy: 0.7774\n",
      "Training loss (for one batch) at step 60: 291.6326, Accuracy: 0.7851\n",
      "Training loss (for one batch) at step 70: 331.8596, Accuracy: 0.7767\n",
      "Training loss (for one batch) at step 80: 337.8071, Accuracy: 0.7653\n",
      "Training loss (for one batch) at step 90: 291.1986, Accuracy: 0.7620\n",
      "Training loss (for one batch) at step 100: 306.2097, Accuracy: 0.7648\n",
      "Training loss (for one batch) at step 110: 322.8787, Accuracy: 0.7655\n",
      "---- Training ----\n",
      "Training loss: 99.1235\n",
      "Training acc over epoch: 0.7650\n",
      "---- Validation ----\n",
      "Validation loss: 50.4472\n",
      "Validation acc: 0.7098\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 333.0904, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 344.0698, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 20: 294.7523, Accuracy: 0.7150\n",
      "Training loss (for one batch) at step 30: 295.3371, Accuracy: 0.7518\n",
      "Training loss (for one batch) at step 40: 291.5251, Accuracy: 0.7671\n",
      "Training loss (for one batch) at step 50: 310.8903, Accuracy: 0.7779\n",
      "Training loss (for one batch) at step 60: 304.2687, Accuracy: 0.7846\n",
      "Training loss (for one batch) at step 70: 316.7069, Accuracy: 0.7744\n",
      "Training loss (for one batch) at step 80: 338.2525, Accuracy: 0.7595\n",
      "Training loss (for one batch) at step 90: 298.4219, Accuracy: 0.7568\n",
      "Training loss (for one batch) at step 100: 300.9566, Accuracy: 0.7599\n",
      "Training loss (for one batch) at step 110: 319.8438, Accuracy: 0.7620\n",
      "---- Training ----\n",
      "Training loss: 98.4479\n",
      "Training acc over epoch: 0.7616\n",
      "---- Validation ----\n",
      "Validation loss: 33.8733\n",
      "Validation acc: 0.6983\n",
      "Time taken: 10.80s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 343.1425, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 326.0148, Accuracy: 0.7003\n",
      "Training loss (for one batch) at step 20: 309.9929, Accuracy: 0.7295\n",
      "Training loss (for one batch) at step 30: 288.8393, Accuracy: 0.7611\n",
      "Training loss (for one batch) at step 40: 279.1503, Accuracy: 0.7681\n",
      "Training loss (for one batch) at step 50: 273.9372, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 60: 301.1910, Accuracy: 0.7886\n",
      "Training loss (for one batch) at step 70: 310.5298, Accuracy: 0.7795\n",
      "Training loss (for one batch) at step 80: 321.5019, Accuracy: 0.7644\n",
      "Training loss (for one batch) at step 90: 273.8980, Accuracy: 0.7632\n",
      "Training loss (for one batch) at step 100: 318.2876, Accuracy: 0.7666\n",
      "Training loss (for one batch) at step 110: 319.1854, Accuracy: 0.7679\n",
      "---- Training ----\n",
      "Training loss: 105.2150\n",
      "Training acc over epoch: 0.7664\n",
      "---- Validation ----\n",
      "Validation loss: 59.4044\n",
      "Validation acc: 0.6921\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 334.5876, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 319.6856, Accuracy: 0.6882\n",
      "Training loss (for one batch) at step 20: 295.9443, Accuracy: 0.7228\n",
      "Training loss (for one batch) at step 30: 289.1169, Accuracy: 0.7419\n",
      "Training loss (for one batch) at step 40: 284.4908, Accuracy: 0.7662\n",
      "Training loss (for one batch) at step 50: 300.6303, Accuracy: 0.7765\n",
      "Training loss (for one batch) at step 60: 320.4124, Accuracy: 0.7797\n",
      "Training loss (for one batch) at step 70: 324.6968, Accuracy: 0.7720\n",
      "Training loss (for one batch) at step 80: 347.3834, Accuracy: 0.7634\n",
      "Training loss (for one batch) at step 90: 298.4141, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 100: 288.2415, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 110: 315.7174, Accuracy: 0.7621\n",
      "---- Training ----\n",
      "Training loss: 111.8910\n",
      "Training acc over epoch: 0.7620\n",
      "---- Validation ----\n",
      "Validation loss: 71.4202\n",
      "Validation acc: 0.7053\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 330.6337, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 354.3130, Accuracy: 0.6818\n",
      "Training loss (for one batch) at step 20: 283.7225, Accuracy: 0.7288\n",
      "Training loss (for one batch) at step 30: 299.8397, Accuracy: 0.7568\n",
      "Training loss (for one batch) at step 40: 281.7817, Accuracy: 0.7694\n",
      "Training loss (for one batch) at step 50: 290.2513, Accuracy: 0.7829\n",
      "Training loss (for one batch) at step 60: 319.2336, Accuracy: 0.7880\n",
      "Training loss (for one batch) at step 70: 316.1374, Accuracy: 0.7790\n",
      "Training loss (for one batch) at step 80: 335.5356, Accuracy: 0.7670\n",
      "Training loss (for one batch) at step 90: 299.3350, Accuracy: 0.7637\n",
      "Training loss (for one batch) at step 100: 290.3403, Accuracy: 0.7659\n",
      "Training loss (for one batch) at step 110: 327.1488, Accuracy: 0.7670\n",
      "---- Training ----\n",
      "Training loss: 99.9234\n",
      "Training acc over epoch: 0.7653\n",
      "---- Validation ----\n",
      "Validation loss: 44.4037\n",
      "Validation acc: 0.7034\n",
      "Time taken: 10.65s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 350.5981, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 330.9652, Accuracy: 0.6839\n",
      "Training loss (for one batch) at step 20: 305.3500, Accuracy: 0.7165\n",
      "Training loss (for one batch) at step 30: 280.4260, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 40: 285.3094, Accuracy: 0.7666\n",
      "Training loss (for one batch) at step 50: 276.2776, Accuracy: 0.7786\n",
      "Training loss (for one batch) at step 60: 319.7540, Accuracy: 0.7864\n",
      "Training loss (for one batch) at step 70: 350.7213, Accuracy: 0.7759\n",
      "Training loss (for one batch) at step 80: 347.4999, Accuracy: 0.7618\n",
      "Training loss (for one batch) at step 90: 297.8713, Accuracy: 0.7594\n",
      "Training loss (for one batch) at step 100: 297.8387, Accuracy: 0.7631\n",
      "Training loss (for one batch) at step 110: 323.3411, Accuracy: 0.7651\n",
      "---- Training ----\n",
      "Training loss: 99.0146\n",
      "Training acc over epoch: 0.7638\n",
      "---- Validation ----\n",
      "Validation loss: 42.1202\n",
      "Validation acc: 0.7018\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 335.2713, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 342.8183, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 20: 309.8462, Accuracy: 0.7347\n",
      "Training loss (for one batch) at step 30: 294.0559, Accuracy: 0.7629\n",
      "Training loss (for one batch) at step 40: 281.5014, Accuracy: 0.7772\n",
      "Training loss (for one batch) at step 50: 285.9753, Accuracy: 0.7881\n",
      "Training loss (for one batch) at step 60: 294.7236, Accuracy: 0.7916\n",
      "Training loss (for one batch) at step 70: 351.0924, Accuracy: 0.7814\n",
      "Training loss (for one batch) at step 80: 345.5304, Accuracy: 0.7673\n",
      "Training loss (for one batch) at step 90: 289.6221, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 100: 300.7191, Accuracy: 0.7695\n",
      "Training loss (for one batch) at step 110: 336.9610, Accuracy: 0.7695\n",
      "---- Training ----\n",
      "Training loss: 123.0327\n",
      "Training acc over epoch: 0.7683\n",
      "---- Validation ----\n",
      "Validation loss: 54.7617\n",
      "Validation acc: 0.6832\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 344.1669, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 327.2833, Accuracy: 0.6605\n",
      "Training loss (for one batch) at step 20: 295.5341, Accuracy: 0.7139\n",
      "Training loss (for one batch) at step 30: 295.4958, Accuracy: 0.7513\n",
      "Training loss (for one batch) at step 40: 285.7863, Accuracy: 0.7666\n",
      "Training loss (for one batch) at step 50: 280.1174, Accuracy: 0.7808\n",
      "Training loss (for one batch) at step 60: 285.3333, Accuracy: 0.7848\n",
      "Training loss (for one batch) at step 70: 327.0461, Accuracy: 0.7765\n",
      "Training loss (for one batch) at step 80: 294.7920, Accuracy: 0.7632\n",
      "Training loss (for one batch) at step 90: 306.0026, Accuracy: 0.7576\n",
      "Training loss (for one batch) at step 100: 289.9403, Accuracy: 0.7600\n",
      "Training loss (for one batch) at step 110: 324.5936, Accuracy: 0.7635\n",
      "---- Training ----\n",
      "Training loss: 105.8600\n",
      "Training acc over epoch: 0.7630\n",
      "---- Validation ----\n",
      "Validation loss: 51.0270\n",
      "Validation acc: 0.6934\n",
      "Time taken: 10.69s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 327.1949, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 323.7215, Accuracy: 0.6882\n",
      "Training loss (for one batch) at step 20: 304.0290, Accuracy: 0.7202\n",
      "Training loss (for one batch) at step 30: 297.4446, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 40: 292.4550, Accuracy: 0.7704\n",
      "Training loss (for one batch) at step 50: 280.0477, Accuracy: 0.7780\n",
      "Training loss (for one batch) at step 60: 290.8459, Accuracy: 0.7865\n",
      "Training loss (for one batch) at step 70: 320.7694, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 80: 332.4041, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 90: 314.3116, Accuracy: 0.7618\n",
      "Training loss (for one batch) at step 100: 285.6592, Accuracy: 0.7658\n",
      "Training loss (for one batch) at step 110: 297.6776, Accuracy: 0.7671\n",
      "---- Training ----\n",
      "Training loss: 93.6695\n",
      "Training acc over epoch: 0.7660\n",
      "---- Validation ----\n",
      "Validation loss: 55.5087\n",
      "Validation acc: 0.7020\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 346.8784, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 342.8858, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 20: 286.3535, Accuracy: 0.7217\n",
      "Training loss (for one batch) at step 30: 277.3073, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 40: 287.6815, Accuracy: 0.7713\n",
      "Training loss (for one batch) at step 50: 298.5163, Accuracy: 0.7829\n",
      "Training loss (for one batch) at step 60: 299.8184, Accuracy: 0.7874\n",
      "Training loss (for one batch) at step 70: 335.2734, Accuracy: 0.7765\n",
      "Training loss (for one batch) at step 80: 329.3038, Accuracy: 0.7634\n",
      "Training loss (for one batch) at step 90: 274.4940, Accuracy: 0.7616\n",
      "Training loss (for one batch) at step 100: 297.9357, Accuracy: 0.7636\n",
      "Training loss (for one batch) at step 110: 306.4863, Accuracy: 0.7663\n",
      "---- Training ----\n",
      "Training loss: 100.4672\n",
      "Training acc over epoch: 0.7647\n",
      "---- Validation ----\n",
      "Validation loss: 60.8386\n",
      "Validation acc: 0.7015\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 332.2505, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 344.1088, Accuracy: 0.7038\n",
      "Training loss (for one batch) at step 20: 300.1074, Accuracy: 0.7299\n",
      "Training loss (for one batch) at step 30: 278.8325, Accuracy: 0.7593\n",
      "Training loss (for one batch) at step 40: 290.7423, Accuracy: 0.7704\n",
      "Training loss (for one batch) at step 50: 291.5472, Accuracy: 0.7814\n",
      "Training loss (for one batch) at step 60: 292.2855, Accuracy: 0.7871\n",
      "Training loss (for one batch) at step 70: 305.5864, Accuracy: 0.7774\n",
      "Training loss (for one batch) at step 80: 316.6982, Accuracy: 0.7641\n",
      "Training loss (for one batch) at step 90: 305.0677, Accuracy: 0.7597\n",
      "Training loss (for one batch) at step 100: 280.7621, Accuracy: 0.7643\n",
      "Training loss (for one batch) at step 110: 297.3831, Accuracy: 0.7656\n",
      "---- Training ----\n",
      "Training loss: 104.6236\n",
      "Training acc over epoch: 0.7642\n",
      "---- Validation ----\n",
      "Validation loss: 60.1499\n",
      "Validation acc: 0.6991\n",
      "Time taken: 10.89s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 331.6424, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 385.3733, Accuracy: 0.6996\n",
      "Training loss (for one batch) at step 20: 315.8031, Accuracy: 0.7333\n",
      "Training loss (for one batch) at step 30: 296.9943, Accuracy: 0.7560\n",
      "Training loss (for one batch) at step 40: 286.5558, Accuracy: 0.7717\n",
      "Training loss (for one batch) at step 50: 281.1249, Accuracy: 0.7834\n",
      "Training loss (for one batch) at step 60: 298.7949, Accuracy: 0.7887\n",
      "Training loss (for one batch) at step 70: 331.5745, Accuracy: 0.7803\n",
      "Training loss (for one batch) at step 80: 333.8470, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 90: 289.0143, Accuracy: 0.7608\n",
      "Training loss (for one batch) at step 100: 299.3895, Accuracy: 0.7659\n",
      "Training loss (for one batch) at step 110: 310.7488, Accuracy: 0.7666\n",
      "---- Training ----\n",
      "Training loss: 116.7710\n",
      "Training acc over epoch: 0.7652\n",
      "---- Validation ----\n",
      "Validation loss: 47.1460\n",
      "Validation acc: 0.6959\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 341.2585, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 315.1396, Accuracy: 0.6839\n",
      "Training loss (for one batch) at step 20: 298.7841, Accuracy: 0.7292\n",
      "Training loss (for one batch) at step 30: 280.3024, Accuracy: 0.7533\n",
      "Training loss (for one batch) at step 40: 289.3053, Accuracy: 0.7677\n",
      "Training loss (for one batch) at step 50: 296.4094, Accuracy: 0.7809\n",
      "Training loss (for one batch) at step 60: 287.4610, Accuracy: 0.7862\n",
      "Training loss (for one batch) at step 70: 331.8737, Accuracy: 0.7744\n",
      "Training loss (for one batch) at step 80: 322.0659, Accuracy: 0.7631\n",
      "Training loss (for one batch) at step 90: 282.0317, Accuracy: 0.7615\n",
      "Training loss (for one batch) at step 100: 301.3000, Accuracy: 0.7622\n",
      "Training loss (for one batch) at step 110: 308.3422, Accuracy: 0.7634\n",
      "---- Training ----\n",
      "Training loss: 91.7809\n",
      "Training acc over epoch: 0.7645\n",
      "---- Validation ----\n",
      "Validation loss: 33.8906\n",
      "Validation acc: 0.6969\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 341.1778, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 323.8178, Accuracy: 0.6832\n",
      "Training loss (for one batch) at step 20: 306.9846, Accuracy: 0.7288\n",
      "Training loss (for one batch) at step 30: 284.0145, Accuracy: 0.7571\n",
      "Training loss (for one batch) at step 40: 283.1234, Accuracy: 0.7702\n",
      "Training loss (for one batch) at step 50: 279.2806, Accuracy: 0.7817\n",
      "Training loss (for one batch) at step 60: 299.4085, Accuracy: 0.7879\n",
      "Training loss (for one batch) at step 70: 308.7812, Accuracy: 0.7778\n",
      "Training loss (for one batch) at step 80: 351.1313, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 90: 292.1046, Accuracy: 0.7611\n",
      "Training loss (for one batch) at step 100: 296.0282, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 110: 311.9500, Accuracy: 0.7656\n",
      "---- Training ----\n",
      "Training loss: 101.8389\n",
      "Training acc over epoch: 0.7644\n",
      "---- Validation ----\n",
      "Validation loss: 48.1516\n",
      "Validation acc: 0.6999\n",
      "Time taken: 10.87s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 343.0085, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 331.5440, Accuracy: 0.6911\n",
      "Training loss (for one batch) at step 20: 296.6502, Accuracy: 0.7284\n",
      "Training loss (for one batch) at step 30: 278.8259, Accuracy: 0.7545\n",
      "Training loss (for one batch) at step 40: 285.2067, Accuracy: 0.7715\n",
      "Training loss (for one batch) at step 50: 287.1692, Accuracy: 0.7862\n",
      "Training loss (for one batch) at step 60: 299.6904, Accuracy: 0.7868\n",
      "Training loss (for one batch) at step 70: 311.8521, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 80: 318.1597, Accuracy: 0.7616\n",
      "Training loss (for one batch) at step 90: 293.7959, Accuracy: 0.7592\n",
      "Training loss (for one batch) at step 100: 296.3843, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 110: 305.0841, Accuracy: 0.7672\n",
      "---- Training ----\n",
      "Training loss: 104.0537\n",
      "Training acc over epoch: 0.7661\n",
      "---- Validation ----\n",
      "Validation loss: 115.8543\n",
      "Validation acc: 0.6983\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 329.2892, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 326.1174, Accuracy: 0.6839\n",
      "Training loss (for one batch) at step 20: 294.1537, Accuracy: 0.7281\n",
      "Training loss (for one batch) at step 30: 289.4173, Accuracy: 0.7535\n",
      "Training loss (for one batch) at step 40: 281.2661, Accuracy: 0.7712\n",
      "Training loss (for one batch) at step 50: 280.5570, Accuracy: 0.7797\n",
      "Training loss (for one batch) at step 60: 293.2395, Accuracy: 0.7861\n",
      "Training loss (for one batch) at step 70: 312.5040, Accuracy: 0.7764\n",
      "Training loss (for one batch) at step 80: 329.4814, Accuracy: 0.7623\n",
      "Training loss (for one batch) at step 90: 318.5605, Accuracy: 0.7593\n",
      "Training loss (for one batch) at step 100: 297.6269, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 110: 293.6655, Accuracy: 0.7630\n",
      "---- Training ----\n",
      "Training loss: 97.1305\n",
      "Training acc over epoch: 0.7630\n",
      "---- Validation ----\n",
      "Validation loss: 32.9169\n",
      "Validation acc: 0.7045\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 327.0674, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 341.6647, Accuracy: 0.6932\n",
      "Training loss (for one batch) at step 20: 289.2913, Accuracy: 0.7269\n",
      "Training loss (for one batch) at step 30: 281.6296, Accuracy: 0.7553\n",
      "Training loss (for one batch) at step 40: 290.9101, Accuracy: 0.7683\n",
      "Training loss (for one batch) at step 50: 272.0365, Accuracy: 0.7797\n",
      "Training loss (for one batch) at step 60: 294.7829, Accuracy: 0.7860\n",
      "Training loss (for one batch) at step 70: 310.4021, Accuracy: 0.7761\n",
      "Training loss (for one batch) at step 80: 315.6376, Accuracy: 0.7638\n",
      "Training loss (for one batch) at step 90: 286.9896, Accuracy: 0.7618\n",
      "Training loss (for one batch) at step 100: 292.6049, Accuracy: 0.7655\n",
      "Training loss (for one batch) at step 110: 288.4972, Accuracy: 0.7669\n",
      "---- Training ----\n",
      "Training loss: 97.3591\n",
      "Training acc over epoch: 0.7654\n",
      "---- Validation ----\n",
      "Validation loss: 67.4549\n",
      "Validation acc: 0.7004\n",
      "Time taken: 10.73s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 324.9895, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 318.5981, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 20: 290.3573, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 30: 280.9381, Accuracy: 0.7523\n",
      "Training loss (for one batch) at step 40: 265.2214, Accuracy: 0.7677\n",
      "Training loss (for one batch) at step 50: 276.0771, Accuracy: 0.7803\n",
      "Training loss (for one batch) at step 60: 302.4132, Accuracy: 0.7873\n",
      "Training loss (for one batch) at step 70: 322.3237, Accuracy: 0.7754\n",
      "Training loss (for one batch) at step 80: 347.8200, Accuracy: 0.7638\n",
      "Training loss (for one batch) at step 90: 300.9660, Accuracy: 0.7596\n",
      "Training loss (for one batch) at step 100: 289.8240, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 110: 318.6085, Accuracy: 0.7671\n",
      "---- Training ----\n",
      "Training loss: 104.1750\n",
      "Training acc over epoch: 0.7657\n",
      "---- Validation ----\n",
      "Validation loss: 39.3979\n",
      "Validation acc: 0.6977\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 340.0624, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 325.4008, Accuracy: 0.6783\n",
      "Training loss (for one batch) at step 20: 295.2112, Accuracy: 0.7262\n",
      "Training loss (for one batch) at step 30: 278.4196, Accuracy: 0.7485\n",
      "Training loss (for one batch) at step 40: 272.2160, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 50: 319.9489, Accuracy: 0.7800\n",
      "Training loss (for one batch) at step 60: 291.9004, Accuracy: 0.7856\n",
      "Training loss (for one batch) at step 70: 322.3740, Accuracy: 0.7737\n",
      "Training loss (for one batch) at step 80: 325.3904, Accuracy: 0.7626\n",
      "Training loss (for one batch) at step 90: 301.7106, Accuracy: 0.7602\n",
      "Training loss (for one batch) at step 100: 296.1621, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 110: 302.8558, Accuracy: 0.7665\n",
      "---- Training ----\n",
      "Training loss: 93.7386\n",
      "Training acc over epoch: 0.7654\n",
      "---- Validation ----\n",
      "Validation loss: 57.2103\n",
      "Validation acc: 0.6964\n",
      "Time taken: 11.03s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 330.7822, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 323.2962, Accuracy: 0.6676\n",
      "Training loss (for one batch) at step 20: 298.6220, Accuracy: 0.7147\n",
      "Training loss (for one batch) at step 30: 293.2052, Accuracy: 0.7457\n",
      "Training loss (for one batch) at step 40: 279.2302, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 50: 281.3084, Accuracy: 0.7774\n",
      "Training loss (for one batch) at step 60: 311.1496, Accuracy: 0.7828\n",
      "Training loss (for one batch) at step 70: 312.2889, Accuracy: 0.7719\n",
      "Training loss (for one batch) at step 80: 316.9911, Accuracy: 0.7586\n",
      "Training loss (for one batch) at step 90: 292.1815, Accuracy: 0.7561\n",
      "Training loss (for one batch) at step 100: 286.7270, Accuracy: 0.7607\n",
      "Training loss (for one batch) at step 110: 293.3873, Accuracy: 0.7639\n",
      "---- Training ----\n",
      "Training loss: 104.3175\n",
      "Training acc over epoch: 0.7624\n",
      "---- Validation ----\n",
      "Validation loss: 47.2981\n",
      "Validation acc: 0.6840\n",
      "Time taken: 10.59s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 334.3431, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 328.4337, Accuracy: 0.6847\n",
      "Training loss (for one batch) at step 20: 306.6021, Accuracy: 0.7232\n",
      "Training loss (for one batch) at step 30: 280.5504, Accuracy: 0.7545\n",
      "Training loss (for one batch) at step 40: 268.9534, Accuracy: 0.7715\n",
      "Training loss (for one batch) at step 50: 270.6968, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 60: 285.5430, Accuracy: 0.7896\n",
      "Training loss (for one batch) at step 70: 295.6685, Accuracy: 0.7796\n",
      "Training loss (for one batch) at step 80: 333.9459, Accuracy: 0.7663\n",
      "Training loss (for one batch) at step 90: 282.0323, Accuracy: 0.7644\n",
      "Training loss (for one batch) at step 100: 272.0818, Accuracy: 0.7684\n",
      "Training loss (for one batch) at step 110: 294.2516, Accuracy: 0.7679\n",
      "---- Training ----\n",
      "Training loss: 110.9661\n",
      "Training acc over epoch: 0.7678\n",
      "---- Validation ----\n",
      "Validation loss: 48.2137\n",
      "Validation acc: 0.6945\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 338.6435, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 328.3199, Accuracy: 0.6783\n",
      "Training loss (for one batch) at step 20: 292.5147, Accuracy: 0.7243\n",
      "Training loss (for one batch) at step 30: 276.1634, Accuracy: 0.7558\n",
      "Training loss (for one batch) at step 40: 268.1580, Accuracy: 0.7727\n",
      "Training loss (for one batch) at step 50: 287.9877, Accuracy: 0.7834\n",
      "Training loss (for one batch) at step 60: 291.4900, Accuracy: 0.7869\n",
      "Training loss (for one batch) at step 70: 305.8083, Accuracy: 0.7751\n",
      "Training loss (for one batch) at step 80: 302.0603, Accuracy: 0.7606\n",
      "Training loss (for one batch) at step 90: 295.3962, Accuracy: 0.7574\n",
      "Training loss (for one batch) at step 100: 291.9519, Accuracy: 0.7625\n",
      "Training loss (for one batch) at step 110: 314.0554, Accuracy: 0.7632\n",
      "---- Training ----\n",
      "Training loss: 98.1554\n",
      "Training acc over epoch: 0.7638\n",
      "---- Validation ----\n",
      "Validation loss: 39.3559\n",
      "Validation acc: 0.6937\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 340.1909, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 320.8495, Accuracy: 0.6818\n",
      "Training loss (for one batch) at step 20: 272.0752, Accuracy: 0.7228\n",
      "Training loss (for one batch) at step 30: 278.9651, Accuracy: 0.7518\n",
      "Training loss (for one batch) at step 40: 286.2504, Accuracy: 0.7689\n",
      "Training loss (for one batch) at step 50: 277.6106, Accuracy: 0.7829\n",
      "Training loss (for one batch) at step 60: 306.0592, Accuracy: 0.7873\n",
      "Training loss (for one batch) at step 70: 312.0965, Accuracy: 0.7789\n",
      "Training loss (for one batch) at step 80: 323.9883, Accuracy: 0.7641\n",
      "Training loss (for one batch) at step 90: 298.8779, Accuracy: 0.7626\n",
      "Training loss (for one batch) at step 100: 306.0182, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 110: 309.1940, Accuracy: 0.7656\n",
      "---- Training ----\n",
      "Training loss: 95.1459\n",
      "Training acc over epoch: 0.7650\n",
      "---- Validation ----\n",
      "Validation loss: 50.6727\n",
      "Validation acc: 0.6961\n",
      "Time taken: 10.51s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 343.5566, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 321.9327, Accuracy: 0.6939\n",
      "Training loss (for one batch) at step 20: 300.7180, Accuracy: 0.7254\n",
      "Training loss (for one batch) at step 30: 297.1244, Accuracy: 0.7555\n",
      "Training loss (for one batch) at step 40: 283.4514, Accuracy: 0.7725\n",
      "Training loss (for one batch) at step 50: 270.1887, Accuracy: 0.7808\n",
      "Training loss (for one batch) at step 60: 302.6233, Accuracy: 0.7853\n",
      "Training loss (for one batch) at step 70: 313.7210, Accuracy: 0.7776\n",
      "Training loss (for one batch) at step 80: 332.2287, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 90: 304.8216, Accuracy: 0.7605\n",
      "Training loss (for one batch) at step 100: 277.7457, Accuracy: 0.7659\n",
      "Training loss (for one batch) at step 110: 298.5598, Accuracy: 0.7657\n",
      "---- Training ----\n",
      "Training loss: 94.9288\n",
      "Training acc over epoch: 0.7647\n",
      "---- Validation ----\n",
      "Validation loss: 42.8229\n",
      "Validation acc: 0.6934\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 334.4930, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 302.6565, Accuracy: 0.6847\n",
      "Training loss (for one batch) at step 20: 312.4792, Accuracy: 0.7333\n",
      "Training loss (for one batch) at step 30: 293.6142, Accuracy: 0.7586\n",
      "Training loss (for one batch) at step 40: 302.5224, Accuracy: 0.7740\n",
      "Training loss (for one batch) at step 50: 273.2515, Accuracy: 0.7835\n",
      "Training loss (for one batch) at step 60: 278.8286, Accuracy: 0.7905\n",
      "Training loss (for one batch) at step 70: 309.3847, Accuracy: 0.7783\n",
      "Training loss (for one batch) at step 80: 329.5502, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 90: 289.7920, Accuracy: 0.7636\n",
      "Training loss (for one batch) at step 100: 297.3940, Accuracy: 0.7657\n",
      "Training loss (for one batch) at step 110: 292.9354, Accuracy: 0.7680\n",
      "---- Training ----\n",
      "Training loss: 94.9762\n",
      "Training acc over epoch: 0.7665\n",
      "---- Validation ----\n",
      "Validation loss: 41.5203\n",
      "Validation acc: 0.7117\n",
      "Time taken: 10.73s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 350.2545, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 311.4441, Accuracy: 0.6918\n",
      "Training loss (for one batch) at step 20: 274.1198, Accuracy: 0.7206\n",
      "Training loss (for one batch) at step 30: 271.2708, Accuracy: 0.7533\n",
      "Training loss (for one batch) at step 40: 272.8390, Accuracy: 0.7685\n",
      "Training loss (for one batch) at step 50: 290.4589, Accuracy: 0.7786\n",
      "Training loss (for one batch) at step 60: 286.1545, Accuracy: 0.7860\n",
      "Training loss (for one batch) at step 70: 304.7486, Accuracy: 0.7756\n",
      "Training loss (for one batch) at step 80: 320.1755, Accuracy: 0.7639\n",
      "Training loss (for one batch) at step 90: 295.6421, Accuracy: 0.7604\n",
      "Training loss (for one batch) at step 100: 294.4286, Accuracy: 0.7642\n",
      "Training loss (for one batch) at step 110: 323.7513, Accuracy: 0.7641\n",
      "---- Training ----\n",
      "Training loss: 100.3239\n",
      "Training acc over epoch: 0.7639\n",
      "---- Validation ----\n",
      "Validation loss: 45.4993\n",
      "Validation acc: 0.6921\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 306.2062, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 324.8782, Accuracy: 0.6960\n",
      "Training loss (for one batch) at step 20: 299.6825, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 30: 278.9128, Accuracy: 0.7618\n",
      "Training loss (for one batch) at step 40: 273.1058, Accuracy: 0.7752\n",
      "Training loss (for one batch) at step 50: 288.8609, Accuracy: 0.7862\n",
      "Training loss (for one batch) at step 60: 303.1447, Accuracy: 0.7918\n",
      "Training loss (for one batch) at step 70: 312.4411, Accuracy: 0.7841\n",
      "Training loss (for one batch) at step 80: 320.7054, Accuracy: 0.7701\n",
      "Training loss (for one batch) at step 90: 291.1502, Accuracy: 0.7637\n",
      "Training loss (for one batch) at step 100: 278.2640, Accuracy: 0.7676\n",
      "Training loss (for one batch) at step 110: 317.5350, Accuracy: 0.7682\n",
      "---- Training ----\n",
      "Training loss: 120.3856\n",
      "Training acc over epoch: 0.7682\n",
      "---- Validation ----\n",
      "Validation loss: 56.7585\n",
      "Validation acc: 0.7074\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 316.6532, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 312.4282, Accuracy: 0.6776\n",
      "Training loss (for one batch) at step 20: 281.8134, Accuracy: 0.7158\n",
      "Training loss (for one batch) at step 30: 281.0378, Accuracy: 0.7442\n",
      "Training loss (for one batch) at step 40: 282.9072, Accuracy: 0.7614\n",
      "Training loss (for one batch) at step 50: 268.2426, Accuracy: 0.7751\n",
      "Training loss (for one batch) at step 60: 298.9246, Accuracy: 0.7836\n",
      "Training loss (for one batch) at step 70: 316.2135, Accuracy: 0.7737\n",
      "Training loss (for one batch) at step 80: 321.1166, Accuracy: 0.7603\n",
      "Training loss (for one batch) at step 90: 273.7093, Accuracy: 0.7583\n",
      "Training loss (for one batch) at step 100: 271.9445, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 110: 311.3150, Accuracy: 0.7651\n",
      "---- Training ----\n",
      "Training loss: 99.6252\n",
      "Training acc over epoch: 0.7635\n",
      "---- Validation ----\n",
      "Validation loss: 65.3568\n",
      "Validation acc: 0.7031\n",
      "Time taken: 10.61s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 323.5280, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 320.3562, Accuracy: 0.6825\n",
      "Training loss (for one batch) at step 20: 304.4603, Accuracy: 0.7247\n",
      "Training loss (for one batch) at step 30: 300.0118, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 40: 300.3676, Accuracy: 0.7725\n",
      "Training loss (for one batch) at step 50: 278.2060, Accuracy: 0.7851\n",
      "Training loss (for one batch) at step 60: 285.5509, Accuracy: 0.7903\n",
      "Training loss (for one batch) at step 70: 316.7808, Accuracy: 0.7818\n",
      "Training loss (for one batch) at step 80: 318.2979, Accuracy: 0.7629\n",
      "Training loss (for one batch) at step 90: 294.5405, Accuracy: 0.7605\n",
      "Training loss (for one batch) at step 100: 296.5862, Accuracy: 0.7655\n",
      "Training loss (for one batch) at step 110: 303.9142, Accuracy: 0.7678\n",
      "---- Training ----\n",
      "Training loss: 90.2988\n",
      "Training acc over epoch: 0.7661\n",
      "---- Validation ----\n",
      "Validation loss: 43.2127\n",
      "Validation acc: 0.7050\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 326.6885, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 334.2103, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 20: 277.3813, Accuracy: 0.7173\n",
      "Training loss (for one batch) at step 30: 277.2665, Accuracy: 0.7497\n",
      "Training loss (for one batch) at step 40: 291.8460, Accuracy: 0.7685\n",
      "Training loss (for one batch) at step 50: 265.2022, Accuracy: 0.7791\n",
      "Training loss (for one batch) at step 60: 296.4495, Accuracy: 0.7853\n",
      "Training loss (for one batch) at step 70: 314.4222, Accuracy: 0.7766\n",
      "Training loss (for one batch) at step 80: 323.8188, Accuracy: 0.7651\n",
      "Training loss (for one batch) at step 90: 287.4335, Accuracy: 0.7618\n",
      "Training loss (for one batch) at step 100: 280.6861, Accuracy: 0.7657\n",
      "Training loss (for one batch) at step 110: 297.3643, Accuracy: 0.7675\n",
      "---- Training ----\n",
      "Training loss: 84.7558\n",
      "Training acc over epoch: 0.7658\n",
      "---- Validation ----\n",
      "Validation loss: 48.1127\n",
      "Validation acc: 0.7053\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 313.1207, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 308.4626, Accuracy: 0.6932\n",
      "Training loss (for one batch) at step 20: 282.1450, Accuracy: 0.7269\n",
      "Training loss (for one batch) at step 30: 283.6939, Accuracy: 0.7566\n",
      "Training loss (for one batch) at step 40: 285.1623, Accuracy: 0.7753\n",
      "Training loss (for one batch) at step 50: 262.6595, Accuracy: 0.7846\n",
      "Training loss (for one batch) at step 60: 287.8889, Accuracy: 0.7907\n",
      "Training loss (for one batch) at step 70: 324.0776, Accuracy: 0.7795\n",
      "Training loss (for one batch) at step 80: 308.2980, Accuracy: 0.7668\n",
      "Training loss (for one batch) at step 90: 277.4670, Accuracy: 0.7632\n",
      "Training loss (for one batch) at step 100: 301.5074, Accuracy: 0.7668\n",
      "Training loss (for one batch) at step 110: 298.0370, Accuracy: 0.7687\n",
      "---- Training ----\n",
      "Training loss: 83.9315\n",
      "Training acc over epoch: 0.7682\n",
      "---- Validation ----\n",
      "Validation loss: 51.5096\n",
      "Validation acc: 0.7015\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 322.0189, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 314.2659, Accuracy: 0.6854\n",
      "Training loss (for one batch) at step 20: 283.5724, Accuracy: 0.7281\n",
      "Training loss (for one batch) at step 30: 277.9028, Accuracy: 0.7573\n",
      "Training loss (for one batch) at step 40: 274.8564, Accuracy: 0.7725\n",
      "Training loss (for one batch) at step 50: 259.1082, Accuracy: 0.7823\n",
      "Training loss (for one batch) at step 60: 304.3126, Accuracy: 0.7893\n",
      "Training loss (for one batch) at step 70: 297.2839, Accuracy: 0.7793\n",
      "Training loss (for one batch) at step 80: 319.3320, Accuracy: 0.7653\n",
      "Training loss (for one batch) at step 90: 265.5652, Accuracy: 0.7624\n",
      "Training loss (for one batch) at step 100: 268.0980, Accuracy: 0.7659\n",
      "Training loss (for one batch) at step 110: 309.4207, Accuracy: 0.7670\n",
      "---- Training ----\n",
      "Training loss: 105.5749\n",
      "Training acc over epoch: 0.7663\n",
      "---- Validation ----\n",
      "Validation loss: 61.8201\n",
      "Validation acc: 0.7039\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 310.4471, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 299.0570, Accuracy: 0.6896\n",
      "Training loss (for one batch) at step 20: 269.8714, Accuracy: 0.7269\n",
      "Training loss (for one batch) at step 30: 270.6329, Accuracy: 0.7563\n",
      "Training loss (for one batch) at step 40: 273.7123, Accuracy: 0.7712\n",
      "Training loss (for one batch) at step 50: 274.0189, Accuracy: 0.7832\n",
      "Training loss (for one batch) at step 60: 310.8582, Accuracy: 0.7896\n",
      "Training loss (for one batch) at step 70: 300.8504, Accuracy: 0.7815\n",
      "Training loss (for one batch) at step 80: 325.0138, Accuracy: 0.7660\n",
      "Training loss (for one batch) at step 90: 312.1613, Accuracy: 0.7612\n",
      "Training loss (for one batch) at step 100: 282.0011, Accuracy: 0.7659\n",
      "Training loss (for one batch) at step 110: 319.5728, Accuracy: 0.7677\n",
      "---- Training ----\n",
      "Training loss: 97.7451\n",
      "Training acc over epoch: 0.7666\n",
      "---- Validation ----\n",
      "Validation loss: 41.8205\n",
      "Validation acc: 0.6832\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 320.9501, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 308.4154, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 20: 276.7795, Accuracy: 0.7128\n",
      "Training loss (for one batch) at step 30: 285.5897, Accuracy: 0.7477\n",
      "Training loss (for one batch) at step 40: 258.5196, Accuracy: 0.7658\n",
      "Training loss (for one batch) at step 50: 265.0710, Accuracy: 0.7791\n",
      "Training loss (for one batch) at step 60: 295.8217, Accuracy: 0.7856\n",
      "Training loss (for one batch) at step 70: 312.6228, Accuracy: 0.7781\n",
      "Training loss (for one batch) at step 80: 288.1742, Accuracy: 0.7650\n",
      "Training loss (for one batch) at step 90: 273.8457, Accuracy: 0.7622\n",
      "Training loss (for one batch) at step 100: 288.4792, Accuracy: 0.7657\n",
      "Training loss (for one batch) at step 110: 301.2167, Accuracy: 0.7663\n",
      "---- Training ----\n",
      "Training loss: 96.0479\n",
      "Training acc over epoch: 0.7663\n",
      "---- Validation ----\n",
      "Validation loss: 62.9614\n",
      "Validation acc: 0.6921\n",
      "Time taken: 10.31s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.6782\n",
      "Validation AUC: 0.6796\n",
      "Validation Balanced_ACC: 0.4328\n",
      "Validation MI: 0.1091\n",
      "Validation Normalized MI: 0.1594\n",
      "Validation Adjusted MI: 0.1594\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 640.3831, Accuracy: 0.4531\n",
      "Training loss (for one batch) at step 10: 524.1616, Accuracy: 0.5007\n",
      "Training loss (for one batch) at step 20: 480.8433, Accuracy: 0.5052\n",
      "Training loss (for one batch) at step 30: 474.6664, Accuracy: 0.5055\n",
      "Training loss (for one batch) at step 40: 474.6341, Accuracy: 0.5101\n",
      "Training loss (for one batch) at step 50: 476.9593, Accuracy: 0.5118\n",
      "Training loss (for one batch) at step 60: 490.1765, Accuracy: 0.5111\n",
      "Training loss (for one batch) at step 70: 474.3410, Accuracy: 0.5149\n",
      "Training loss (for one batch) at step 80: 478.1397, Accuracy: 0.5129\n",
      "Training loss (for one batch) at step 90: 461.4368, Accuracy: 0.5110\n",
      "Training loss (for one batch) at step 100: 454.7614, Accuracy: 0.5099\n",
      "Training loss (for one batch) at step 110: 454.5378, Accuracy: 0.5097\n",
      "---- Training ----\n",
      "Training loss: 140.2958\n",
      "Training acc over epoch: 0.5088\n",
      "---- Validation ----\n",
      "Validation loss: 34.9446\n",
      "Validation acc: 0.5134\n",
      "Time taken: 12.68s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 461.0795, Accuracy: 0.4688\n",
      "Training loss (for one batch) at step 10: 454.4846, Accuracy: 0.5355\n",
      "Training loss (for one batch) at step 20: 453.2402, Accuracy: 0.5286\n",
      "Training loss (for one batch) at step 30: 448.6741, Accuracy: 0.5265\n",
      "Training loss (for one batch) at step 40: 451.3018, Accuracy: 0.5225\n",
      "Training loss (for one batch) at step 50: 446.0435, Accuracy: 0.5158\n",
      "Training loss (for one batch) at step 60: 451.9798, Accuracy: 0.5168\n",
      "Training loss (for one batch) at step 70: 444.6334, Accuracy: 0.5187\n",
      "Training loss (for one batch) at step 80: 447.9535, Accuracy: 0.5193\n",
      "Training loss (for one batch) at step 90: 445.0858, Accuracy: 0.5189\n",
      "Training loss (for one batch) at step 100: 446.5316, Accuracy: 0.5179\n",
      "Training loss (for one batch) at step 110: 446.0518, Accuracy: 0.5166\n",
      "---- Training ----\n",
      "Training loss: 140.9907\n",
      "Training acc over epoch: 0.5179\n",
      "---- Validation ----\n",
      "Validation loss: 34.7611\n",
      "Validation acc: 0.4919\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 443.4244, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 447.9801, Accuracy: 0.5447\n",
      "Training loss (for one batch) at step 20: 446.6898, Accuracy: 0.5376\n",
      "Training loss (for one batch) at step 30: 448.2636, Accuracy: 0.5343\n",
      "Training loss (for one batch) at step 40: 440.5841, Accuracy: 0.5267\n",
      "Training loss (for one batch) at step 50: 445.8710, Accuracy: 0.5276\n",
      "Training loss (for one batch) at step 60: 443.2610, Accuracy: 0.5269\n",
      "Training loss (for one batch) at step 70: 448.8965, Accuracy: 0.5287\n",
      "Training loss (for one batch) at step 80: 444.5338, Accuracy: 0.5285\n",
      "Training loss (for one batch) at step 90: 446.8545, Accuracy: 0.5283\n",
      "Training loss (for one batch) at step 100: 447.1056, Accuracy: 0.5281\n",
      "Training loss (for one batch) at step 110: 444.2221, Accuracy: 0.5270\n",
      "---- Training ----\n",
      "Training loss: 137.9442\n",
      "Training acc over epoch: 0.5283\n",
      "---- Validation ----\n",
      "Validation loss: 34.2835\n",
      "Validation acc: 0.5236\n",
      "Time taken: 10.68s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 444.3583, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 447.2050, Accuracy: 0.5632\n",
      "Training loss (for one batch) at step 20: 443.9574, Accuracy: 0.5446\n",
      "Training loss (for one batch) at step 30: 441.1359, Accuracy: 0.5486\n",
      "Training loss (for one batch) at step 40: 445.4242, Accuracy: 0.5415\n",
      "Training loss (for one batch) at step 50: 443.2034, Accuracy: 0.5386\n",
      "Training loss (for one batch) at step 60: 442.4628, Accuracy: 0.5373\n",
      "Training loss (for one batch) at step 70: 444.3891, Accuracy: 0.5376\n",
      "Training loss (for one batch) at step 80: 445.6272, Accuracy: 0.5395\n",
      "Training loss (for one batch) at step 90: 443.8334, Accuracy: 0.5397\n",
      "Training loss (for one batch) at step 100: 446.0716, Accuracy: 0.5381\n",
      "Training loss (for one batch) at step 110: 444.4748, Accuracy: 0.5391\n",
      "---- Training ----\n",
      "Training loss: 139.4893\n",
      "Training acc over epoch: 0.5388\n",
      "---- Validation ----\n",
      "Validation loss: 34.9370\n",
      "Validation acc: 0.5519\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 444.0096, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 443.6510, Accuracy: 0.5476\n",
      "Training loss (for one batch) at step 20: 442.1729, Accuracy: 0.5432\n",
      "Training loss (for one batch) at step 30: 438.3408, Accuracy: 0.5416\n",
      "Training loss (for one batch) at step 40: 441.0650, Accuracy: 0.5427\n",
      "Training loss (for one batch) at step 50: 439.4665, Accuracy: 0.5424\n",
      "Training loss (for one batch) at step 60: 444.5002, Accuracy: 0.5447\n",
      "Training loss (for one batch) at step 70: 443.5722, Accuracy: 0.5500\n",
      "Training loss (for one batch) at step 80: 440.6774, Accuracy: 0.5531\n",
      "Training loss (for one batch) at step 90: 445.9167, Accuracy: 0.5555\n",
      "Training loss (for one batch) at step 100: 444.8968, Accuracy: 0.5536\n",
      "Training loss (for one batch) at step 110: 441.2438, Accuracy: 0.5522\n",
      "---- Training ----\n",
      "Training loss: 137.6016\n",
      "Training acc over epoch: 0.5519\n",
      "---- Validation ----\n",
      "Validation loss: 34.8668\n",
      "Validation acc: 0.5406\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 439.4860, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 438.9312, Accuracy: 0.5412\n",
      "Training loss (for one batch) at step 20: 440.3114, Accuracy: 0.5510\n",
      "Training loss (for one batch) at step 30: 445.6500, Accuracy: 0.5552\n",
      "Training loss (for one batch) at step 40: 438.4659, Accuracy: 0.5568\n",
      "Training loss (for one batch) at step 50: 438.2330, Accuracy: 0.5597\n",
      "Training loss (for one batch) at step 60: 439.4005, Accuracy: 0.5588\n",
      "Training loss (for one batch) at step 70: 440.3646, Accuracy: 0.5670\n",
      "Training loss (for one batch) at step 80: 444.8411, Accuracy: 0.5628\n",
      "Training loss (for one batch) at step 90: 439.0789, Accuracy: 0.5622\n",
      "Training loss (for one batch) at step 100: 439.6955, Accuracy: 0.5601\n",
      "Training loss (for one batch) at step 110: 437.9836, Accuracy: 0.5566\n",
      "---- Training ----\n",
      "Training loss: 136.6890\n",
      "Training acc over epoch: 0.5572\n",
      "---- Validation ----\n",
      "Validation loss: 34.1097\n",
      "Validation acc: 0.5621\n",
      "Time taken: 10.66s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 439.2935, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 439.3883, Accuracy: 0.5696\n",
      "Training loss (for one batch) at step 20: 440.2910, Accuracy: 0.5666\n",
      "Training loss (for one batch) at step 30: 439.0664, Accuracy: 0.5660\n",
      "Training loss (for one batch) at step 40: 434.1194, Accuracy: 0.5575\n",
      "Training loss (for one batch) at step 50: 438.8879, Accuracy: 0.5665\n",
      "Training loss (for one batch) at step 60: 435.6122, Accuracy: 0.5695\n",
      "Training loss (for one batch) at step 70: 437.6531, Accuracy: 0.5714\n",
      "Training loss (for one batch) at step 80: 440.6899, Accuracy: 0.5722\n",
      "Training loss (for one batch) at step 90: 436.6035, Accuracy: 0.5701\n",
      "Training loss (for one batch) at step 100: 437.5035, Accuracy: 0.5681\n",
      "Training loss (for one batch) at step 110: 438.3961, Accuracy: 0.5653\n",
      "---- Training ----\n",
      "Training loss: 136.9980\n",
      "Training acc over epoch: 0.5652\n",
      "---- Validation ----\n",
      "Validation loss: 33.7599\n",
      "Validation acc: 0.5656\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 437.8826, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 443.1075, Accuracy: 0.5696\n",
      "Training loss (for one batch) at step 20: 439.6046, Accuracy: 0.5569\n",
      "Training loss (for one batch) at step 30: 442.3909, Accuracy: 0.5549\n",
      "Training loss (for one batch) at step 40: 425.6347, Accuracy: 0.5617\n",
      "Training loss (for one batch) at step 50: 431.1115, Accuracy: 0.5726\n",
      "Training loss (for one batch) at step 60: 442.7233, Accuracy: 0.5753\n",
      "Training loss (for one batch) at step 70: 445.6448, Accuracy: 0.5778\n",
      "Training loss (for one batch) at step 80: 440.8452, Accuracy: 0.5762\n",
      "Training loss (for one batch) at step 90: 437.1498, Accuracy: 0.5734\n",
      "Training loss (for one batch) at step 100: 435.3095, Accuracy: 0.5678\n",
      "Training loss (for one batch) at step 110: 442.2931, Accuracy: 0.5679\n",
      "---- Training ----\n",
      "Training loss: 137.6212\n",
      "Training acc over epoch: 0.5680\n",
      "---- Validation ----\n",
      "Validation loss: 35.0253\n",
      "Validation acc: 0.5771\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 448.3482, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 437.2465, Accuracy: 0.5710\n",
      "Training loss (for one batch) at step 20: 435.8075, Accuracy: 0.5718\n",
      "Training loss (for one batch) at step 30: 439.5851, Accuracy: 0.5691\n",
      "Training loss (for one batch) at step 40: 435.8435, Accuracy: 0.5680\n",
      "Training loss (for one batch) at step 50: 433.2935, Accuracy: 0.5764\n",
      "Training loss (for one batch) at step 60: 444.7415, Accuracy: 0.5813\n",
      "Training loss (for one batch) at step 70: 429.9902, Accuracy: 0.5832\n",
      "Training loss (for one batch) at step 80: 435.6113, Accuracy: 0.5791\n",
      "Training loss (for one batch) at step 90: 436.7578, Accuracy: 0.5739\n",
      "Training loss (for one batch) at step 100: 433.7404, Accuracy: 0.5738\n",
      "Training loss (for one batch) at step 110: 437.6577, Accuracy: 0.5739\n",
      "---- Training ----\n",
      "Training loss: 135.6552\n",
      "Training acc over epoch: 0.5742\n",
      "---- Validation ----\n",
      "Validation loss: 36.3212\n",
      "Validation acc: 0.5830\n",
      "Time taken: 10.65s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 438.9073, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 434.3423, Accuracy: 0.5561\n",
      "Training loss (for one batch) at step 20: 433.7427, Accuracy: 0.5465\n",
      "Training loss (for one batch) at step 30: 438.9428, Accuracy: 0.5610\n",
      "Training loss (for one batch) at step 40: 428.5968, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 50: 426.6089, Accuracy: 0.5743\n",
      "Training loss (for one batch) at step 60: 436.3868, Accuracy: 0.5818\n",
      "Training loss (for one batch) at step 70: 436.6337, Accuracy: 0.5882\n",
      "Training loss (for one batch) at step 80: 436.9944, Accuracy: 0.5869\n",
      "Training loss (for one batch) at step 90: 432.4532, Accuracy: 0.5834\n",
      "Training loss (for one batch) at step 100: 433.1062, Accuracy: 0.5816\n",
      "Training loss (for one batch) at step 110: 432.9456, Accuracy: 0.5816\n",
      "---- Training ----\n",
      "Training loss: 137.9249\n",
      "Training acc over epoch: 0.5809\n",
      "---- Validation ----\n",
      "Validation loss: 35.7790\n",
      "Validation acc: 0.5639\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 436.2821, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 433.4308, Accuracy: 0.5668\n",
      "Training loss (for one batch) at step 20: 438.3777, Accuracy: 0.5569\n",
      "Training loss (for one batch) at step 30: 429.6921, Accuracy: 0.5706\n",
      "Training loss (for one batch) at step 40: 428.5466, Accuracy: 0.5783\n",
      "Training loss (for one batch) at step 50: 430.0659, Accuracy: 0.5879\n",
      "Training loss (for one batch) at step 60: 429.4939, Accuracy: 0.5975\n",
      "Training loss (for one batch) at step 70: 436.5396, Accuracy: 0.5999\n",
      "Training loss (for one batch) at step 80: 437.0604, Accuracy: 0.5984\n",
      "Training loss (for one batch) at step 90: 437.9796, Accuracy: 0.5913\n",
      "Training loss (for one batch) at step 100: 428.9104, Accuracy: 0.5909\n",
      "Training loss (for one batch) at step 110: 437.0079, Accuracy: 0.5911\n",
      "---- Training ----\n",
      "Training loss: 136.1512\n",
      "Training acc over epoch: 0.5918\n",
      "---- Validation ----\n",
      "Validation loss: 32.8250\n",
      "Validation acc: 0.5975\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 433.0761, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 433.2877, Accuracy: 0.6108\n",
      "Training loss (for one batch) at step 20: 434.7550, Accuracy: 0.6027\n",
      "Training loss (for one batch) at step 30: 436.9329, Accuracy: 0.5993\n",
      "Training loss (for one batch) at step 40: 426.2813, Accuracy: 0.6069\n",
      "Training loss (for one batch) at step 50: 424.3555, Accuracy: 0.6141\n",
      "Training loss (for one batch) at step 60: 428.4891, Accuracy: 0.6203\n",
      "Training loss (for one batch) at step 70: 440.1432, Accuracy: 0.6244\n",
      "Training loss (for one batch) at step 80: 434.4791, Accuracy: 0.6186\n",
      "Training loss (for one batch) at step 90: 436.0153, Accuracy: 0.6123\n",
      "Training loss (for one batch) at step 100: 424.1350, Accuracy: 0.6085\n",
      "Training loss (for one batch) at step 110: 442.1481, Accuracy: 0.6085\n",
      "---- Training ----\n",
      "Training loss: 135.8546\n",
      "Training acc over epoch: 0.6077\n",
      "---- Validation ----\n",
      "Validation loss: 33.7733\n",
      "Validation acc: 0.5629\n",
      "Time taken: 10.62s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 438.5443, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 433.0126, Accuracy: 0.5987\n",
      "Training loss (for one batch) at step 20: 432.3424, Accuracy: 0.5952\n",
      "Training loss (for one batch) at step 30: 425.3098, Accuracy: 0.6114\n",
      "Training loss (for one batch) at step 40: 422.8748, Accuracy: 0.6143\n",
      "Training loss (for one batch) at step 50: 419.4405, Accuracy: 0.6258\n",
      "Training loss (for one batch) at step 60: 425.2624, Accuracy: 0.6327\n",
      "Training loss (for one batch) at step 70: 430.2074, Accuracy: 0.6372\n",
      "Training loss (for one batch) at step 80: 426.9294, Accuracy: 0.6309\n",
      "Training loss (for one batch) at step 90: 431.0014, Accuracy: 0.6243\n",
      "Training loss (for one batch) at step 100: 425.0251, Accuracy: 0.6218\n",
      "Training loss (for one batch) at step 110: 429.0507, Accuracy: 0.6205\n",
      "---- Training ----\n",
      "Training loss: 134.8499\n",
      "Training acc over epoch: 0.6198\n",
      "---- Validation ----\n",
      "Validation loss: 33.3505\n",
      "Validation acc: 0.6257\n",
      "Time taken: 10.91s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 435.0557, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 438.8121, Accuracy: 0.6129\n",
      "Training loss (for one batch) at step 20: 424.8913, Accuracy: 0.6053\n",
      "Training loss (for one batch) at step 30: 425.3738, Accuracy: 0.6167\n",
      "Training loss (for one batch) at step 40: 421.2811, Accuracy: 0.6218\n",
      "Training loss (for one batch) at step 50: 410.8153, Accuracy: 0.6325\n",
      "Training loss (for one batch) at step 60: 429.5490, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 70: 422.0627, Accuracy: 0.6415\n",
      "Training loss (for one batch) at step 80: 439.4671, Accuracy: 0.6377\n",
      "Training loss (for one batch) at step 90: 433.3759, Accuracy: 0.6330\n",
      "Training loss (for one batch) at step 100: 426.1382, Accuracy: 0.6295\n",
      "Training loss (for one batch) at step 110: 431.3848, Accuracy: 0.6275\n",
      "---- Training ----\n",
      "Training loss: 130.6596\n",
      "Training acc over epoch: 0.6280\n",
      "---- Validation ----\n",
      "Validation loss: 33.5308\n",
      "Validation acc: 0.5948\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 431.5231, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 433.8334, Accuracy: 0.6072\n",
      "Training loss (for one batch) at step 20: 428.9972, Accuracy: 0.6049\n",
      "Training loss (for one batch) at step 30: 430.0662, Accuracy: 0.6197\n",
      "Training loss (for one batch) at step 40: 410.0986, Accuracy: 0.6349\n",
      "Training loss (for one batch) at step 50: 403.6265, Accuracy: 0.6440\n",
      "Training loss (for one batch) at step 60: 414.2125, Accuracy: 0.6497\n",
      "Training loss (for one batch) at step 70: 430.6430, Accuracy: 0.6491\n",
      "Training loss (for one batch) at step 80: 433.1277, Accuracy: 0.6436\n",
      "Training loss (for one batch) at step 90: 426.5938, Accuracy: 0.6362\n",
      "Training loss (for one batch) at step 100: 429.9294, Accuracy: 0.6314\n",
      "Training loss (for one batch) at step 110: 429.3330, Accuracy: 0.6322\n",
      "---- Training ----\n",
      "Training loss: 129.2150\n",
      "Training acc over epoch: 0.6324\n",
      "---- Validation ----\n",
      "Validation loss: 36.2932\n",
      "Validation acc: 0.6163\n",
      "Time taken: 10.68s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 429.6823, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 434.9441, Accuracy: 0.6335\n",
      "Training loss (for one batch) at step 20: 426.4055, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 30: 423.3228, Accuracy: 0.6321\n",
      "Training loss (for one batch) at step 40: 414.9999, Accuracy: 0.6423\n",
      "Training loss (for one batch) at step 50: 418.7442, Accuracy: 0.6541\n",
      "Training loss (for one batch) at step 60: 414.5417, Accuracy: 0.6612\n",
      "Training loss (for one batch) at step 70: 433.5640, Accuracy: 0.6620\n",
      "Training loss (for one batch) at step 80: 427.7702, Accuracy: 0.6555\n",
      "Training loss (for one batch) at step 90: 428.5740, Accuracy: 0.6494\n",
      "Training loss (for one batch) at step 100: 423.7515, Accuracy: 0.6453\n",
      "Training loss (for one batch) at step 110: 435.1146, Accuracy: 0.6458\n",
      "---- Training ----\n",
      "Training loss: 131.9192\n",
      "Training acc over epoch: 0.6458\n",
      "---- Validation ----\n",
      "Validation loss: 37.2685\n",
      "Validation acc: 0.6126\n",
      "Time taken: 10.14s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 430.1935, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 413.3575, Accuracy: 0.6413\n",
      "Training loss (for one batch) at step 20: 429.4572, Accuracy: 0.6343\n",
      "Training loss (for one batch) at step 30: 416.4626, Accuracy: 0.6348\n",
      "Training loss (for one batch) at step 40: 421.3595, Accuracy: 0.6441\n",
      "Training loss (for one batch) at step 50: 404.9730, Accuracy: 0.6526\n",
      "Training loss (for one batch) at step 60: 405.1724, Accuracy: 0.6627\n",
      "Training loss (for one batch) at step 70: 419.9509, Accuracy: 0.6676\n",
      "Training loss (for one batch) at step 80: 431.9617, Accuracy: 0.6599\n",
      "Training loss (for one batch) at step 90: 415.0598, Accuracy: 0.6556\n",
      "Training loss (for one batch) at step 100: 423.8051, Accuracy: 0.6525\n",
      "Training loss (for one batch) at step 110: 416.9754, Accuracy: 0.6557\n",
      "---- Training ----\n",
      "Training loss: 130.7833\n",
      "Training acc over epoch: 0.6552\n",
      "---- Validation ----\n",
      "Validation loss: 35.6062\n",
      "Validation acc: 0.6389\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 427.3144, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 424.6075, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 20: 418.7474, Accuracy: 0.6384\n",
      "Training loss (for one batch) at step 30: 415.1303, Accuracy: 0.6442\n",
      "Training loss (for one batch) at step 40: 415.8702, Accuracy: 0.6469\n",
      "Training loss (for one batch) at step 50: 394.2610, Accuracy: 0.6619\n",
      "Training loss (for one batch) at step 60: 405.2730, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 70: 428.2262, Accuracy: 0.6732\n",
      "Training loss (for one batch) at step 80: 430.1499, Accuracy: 0.6675\n",
      "Training loss (for one batch) at step 90: 425.3219, Accuracy: 0.6611\n",
      "Training loss (for one batch) at step 100: 414.3621, Accuracy: 0.6590\n",
      "Training loss (for one batch) at step 110: 414.0999, Accuracy: 0.6595\n",
      "---- Training ----\n",
      "Training loss: 130.6752\n",
      "Training acc over epoch: 0.6604\n",
      "---- Validation ----\n",
      "Validation loss: 36.6346\n",
      "Validation acc: 0.6276\n",
      "Time taken: 10.81s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 434.3991, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 414.6397, Accuracy: 0.6321\n",
      "Training loss (for one batch) at step 20: 420.1859, Accuracy: 0.6443\n",
      "Training loss (for one batch) at step 30: 413.0401, Accuracy: 0.6575\n",
      "Training loss (for one batch) at step 40: 406.6647, Accuracy: 0.6604\n",
      "Training loss (for one batch) at step 50: 411.2967, Accuracy: 0.6711\n",
      "Training loss (for one batch) at step 60: 397.0843, Accuracy: 0.6780\n",
      "Training loss (for one batch) at step 70: 421.8049, Accuracy: 0.6765\n",
      "Training loss (for one batch) at step 80: 413.9203, Accuracy: 0.6721\n",
      "Training loss (for one batch) at step 90: 414.7848, Accuracy: 0.6672\n",
      "Training loss (for one batch) at step 100: 416.2416, Accuracy: 0.6643\n",
      "Training loss (for one batch) at step 110: 426.2841, Accuracy: 0.6641\n",
      "---- Training ----\n",
      "Training loss: 127.5326\n",
      "Training acc over epoch: 0.6646\n",
      "---- Validation ----\n",
      "Validation loss: 34.1716\n",
      "Validation acc: 0.6467\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 423.3451, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 416.1630, Accuracy: 0.6143\n",
      "Training loss (for one batch) at step 20: 420.6708, Accuracy: 0.6388\n",
      "Training loss (for one batch) at step 30: 405.3369, Accuracy: 0.6545\n",
      "Training loss (for one batch) at step 40: 402.3418, Accuracy: 0.6637\n",
      "Training loss (for one batch) at step 50: 393.8846, Accuracy: 0.6720\n",
      "Training loss (for one batch) at step 60: 413.3602, Accuracy: 0.6799\n",
      "Training loss (for one batch) at step 70: 409.6490, Accuracy: 0.6810\n",
      "Training loss (for one batch) at step 80: 419.6625, Accuracy: 0.6748\n",
      "Training loss (for one batch) at step 90: 423.2703, Accuracy: 0.6697\n",
      "Training loss (for one batch) at step 100: 401.5822, Accuracy: 0.6679\n",
      "Training loss (for one batch) at step 110: 418.5836, Accuracy: 0.6665\n",
      "---- Training ----\n",
      "Training loss: 127.7125\n",
      "Training acc over epoch: 0.6668\n",
      "---- Validation ----\n",
      "Validation loss: 35.8787\n",
      "Validation acc: 0.6561\n",
      "Time taken: 10.92s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 432.0993, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 421.8401, Accuracy: 0.6236\n",
      "Training loss (for one batch) at step 20: 406.6461, Accuracy: 0.6358\n",
      "Training loss (for one batch) at step 30: 397.9352, Accuracy: 0.6537\n",
      "Training loss (for one batch) at step 40: 404.4961, Accuracy: 0.6618\n",
      "Training loss (for one batch) at step 50: 384.9414, Accuracy: 0.6792\n",
      "Training loss (for one batch) at step 60: 425.4943, Accuracy: 0.6838\n",
      "Training loss (for one batch) at step 70: 416.0066, Accuracy: 0.6835\n",
      "Training loss (for one batch) at step 80: 426.4092, Accuracy: 0.6783\n",
      "Training loss (for one batch) at step 90: 412.0657, Accuracy: 0.6738\n",
      "Training loss (for one batch) at step 100: 402.8476, Accuracy: 0.6705\n",
      "Training loss (for one batch) at step 110: 403.6380, Accuracy: 0.6708\n",
      "---- Training ----\n",
      "Training loss: 131.2104\n",
      "Training acc over epoch: 0.6718\n",
      "---- Validation ----\n",
      "Validation loss: 33.2235\n",
      "Validation acc: 0.6335\n",
      "Time taken: 10.61s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 433.7018, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 422.5246, Accuracy: 0.6378\n",
      "Training loss (for one batch) at step 20: 398.9379, Accuracy: 0.6540\n",
      "Training loss (for one batch) at step 30: 400.9731, Accuracy: 0.6628\n",
      "Training loss (for one batch) at step 40: 375.5084, Accuracy: 0.6704\n",
      "Training loss (for one batch) at step 50: 383.1266, Accuracy: 0.6792\n",
      "Training loss (for one batch) at step 60: 388.2761, Accuracy: 0.6861\n",
      "Training loss (for one batch) at step 70: 420.8889, Accuracy: 0.6882\n",
      "Training loss (for one batch) at step 80: 422.6949, Accuracy: 0.6826\n",
      "Training loss (for one batch) at step 90: 422.0779, Accuracy: 0.6787\n",
      "Training loss (for one batch) at step 100: 400.4181, Accuracy: 0.6795\n",
      "Training loss (for one batch) at step 110: 396.6034, Accuracy: 0.6787\n",
      "---- Training ----\n",
      "Training loss: 128.4369\n",
      "Training acc over epoch: 0.6781\n",
      "---- Validation ----\n",
      "Validation loss: 35.7978\n",
      "Validation acc: 0.6316\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 433.6944, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 414.5092, Accuracy: 0.6470\n",
      "Training loss (for one batch) at step 20: 406.5746, Accuracy: 0.6592\n",
      "Training loss (for one batch) at step 30: 399.3921, Accuracy: 0.6739\n",
      "Training loss (for one batch) at step 40: 383.4706, Accuracy: 0.6841\n",
      "Training loss (for one batch) at step 50: 382.7056, Accuracy: 0.6942\n",
      "Training loss (for one batch) at step 60: 388.1195, Accuracy: 0.6997\n",
      "Training loss (for one batch) at step 70: 412.9237, Accuracy: 0.6952\n",
      "Training loss (for one batch) at step 80: 401.4939, Accuracy: 0.6882\n",
      "Training loss (for one batch) at step 90: 390.8097, Accuracy: 0.6876\n",
      "Training loss (for one batch) at step 100: 396.0881, Accuracy: 0.6861\n",
      "Training loss (for one batch) at step 110: 416.3976, Accuracy: 0.6862\n",
      "---- Training ----\n",
      "Training loss: 126.8336\n",
      "Training acc over epoch: 0.6849\n",
      "---- Validation ----\n",
      "Validation loss: 38.6357\n",
      "Validation acc: 0.6411\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 419.6639, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 405.6529, Accuracy: 0.6385\n",
      "Training loss (for one batch) at step 20: 400.2971, Accuracy: 0.6589\n",
      "Training loss (for one batch) at step 30: 393.9770, Accuracy: 0.6711\n",
      "Training loss (for one batch) at step 40: 385.2906, Accuracy: 0.6784\n",
      "Training loss (for one batch) at step 50: 375.6222, Accuracy: 0.6864\n",
      "Training loss (for one batch) at step 60: 400.5497, Accuracy: 0.6902\n",
      "Training loss (for one batch) at step 70: 414.1187, Accuracy: 0.6883\n",
      "Training loss (for one batch) at step 80: 405.8038, Accuracy: 0.6820\n",
      "Training loss (for one batch) at step 90: 404.2600, Accuracy: 0.6787\n",
      "Training loss (for one batch) at step 100: 381.7273, Accuracy: 0.6778\n",
      "Training loss (for one batch) at step 110: 391.3994, Accuracy: 0.6760\n",
      "---- Training ----\n",
      "Training loss: 125.9294\n",
      "Training acc over epoch: 0.6750\n",
      "---- Validation ----\n",
      "Validation loss: 47.9079\n",
      "Validation acc: 0.6128\n",
      "Time taken: 10.64s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 430.2979, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 413.2490, Accuracy: 0.6555\n",
      "Training loss (for one batch) at step 20: 394.7888, Accuracy: 0.6685\n",
      "Training loss (for one batch) at step 30: 384.2847, Accuracy: 0.6794\n",
      "Training loss (for one batch) at step 40: 387.6020, Accuracy: 0.6871\n",
      "Training loss (for one batch) at step 50: 382.9097, Accuracy: 0.7005\n",
      "Training loss (for one batch) at step 60: 395.6623, Accuracy: 0.7054\n",
      "Training loss (for one batch) at step 70: 409.2077, Accuracy: 0.7018\n",
      "Training loss (for one batch) at step 80: 402.4955, Accuracy: 0.6942\n",
      "Training loss (for one batch) at step 90: 386.9610, Accuracy: 0.6897\n",
      "Training loss (for one batch) at step 100: 395.1416, Accuracy: 0.6861\n",
      "Training loss (for one batch) at step 110: 413.1739, Accuracy: 0.6863\n",
      "---- Training ----\n",
      "Training loss: 121.7707\n",
      "Training acc over epoch: 0.6844\n",
      "---- Validation ----\n",
      "Validation loss: 38.4474\n",
      "Validation acc: 0.6276\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 420.4655, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 416.4294, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 20: 394.4427, Accuracy: 0.6693\n",
      "Training loss (for one batch) at step 30: 386.8160, Accuracy: 0.6837\n",
      "Training loss (for one batch) at step 40: 383.0020, Accuracy: 0.6829\n",
      "Training loss (for one batch) at step 50: 368.2178, Accuracy: 0.6945\n",
      "Training loss (for one batch) at step 60: 381.2906, Accuracy: 0.7013\n",
      "Training loss (for one batch) at step 70: 393.6689, Accuracy: 0.6945\n",
      "Training loss (for one batch) at step 80: 411.7311, Accuracy: 0.6906\n",
      "Training loss (for one batch) at step 90: 386.0027, Accuracy: 0.6888\n",
      "Training loss (for one batch) at step 100: 395.9991, Accuracy: 0.6878\n",
      "Training loss (for one batch) at step 110: 376.5691, Accuracy: 0.6876\n",
      "---- Training ----\n",
      "Training loss: 132.7092\n",
      "Training acc over epoch: 0.6877\n",
      "---- Validation ----\n",
      "Validation loss: 39.6682\n",
      "Validation acc: 0.6351\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 401.8873, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 404.5912, Accuracy: 0.6449\n",
      "Training loss (for one batch) at step 20: 382.8951, Accuracy: 0.6603\n",
      "Training loss (for one batch) at step 30: 398.3470, Accuracy: 0.6860\n",
      "Training loss (for one batch) at step 40: 366.6403, Accuracy: 0.6947\n",
      "Training loss (for one batch) at step 50: 363.2291, Accuracy: 0.7019\n",
      "Training loss (for one batch) at step 60: 371.0706, Accuracy: 0.7059\n",
      "Training loss (for one batch) at step 70: 400.7492, Accuracy: 0.7018\n",
      "Training loss (for one batch) at step 80: 398.9051, Accuracy: 0.6940\n",
      "Training loss (for one batch) at step 90: 382.8846, Accuracy: 0.6903\n",
      "Training loss (for one batch) at step 100: 386.2896, Accuracy: 0.6879\n",
      "Training loss (for one batch) at step 110: 383.8965, Accuracy: 0.6871\n",
      "---- Training ----\n",
      "Training loss: 124.6322\n",
      "Training acc over epoch: 0.6875\n",
      "---- Validation ----\n",
      "Validation loss: 37.9552\n",
      "Validation acc: 0.6209\n",
      "Time taken: 10.77s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 414.7494, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 414.0512, Accuracy: 0.6513\n",
      "Training loss (for one batch) at step 20: 390.7233, Accuracy: 0.6667\n",
      "Training loss (for one batch) at step 30: 384.7047, Accuracy: 0.6880\n",
      "Training loss (for one batch) at step 40: 372.1350, Accuracy: 0.6970\n",
      "Training loss (for one batch) at step 50: 359.4680, Accuracy: 0.7085\n",
      "Training loss (for one batch) at step 60: 389.0225, Accuracy: 0.7164\n",
      "Training loss (for one batch) at step 70: 402.6818, Accuracy: 0.7107\n",
      "Training loss (for one batch) at step 80: 402.7732, Accuracy: 0.7006\n",
      "Training loss (for one batch) at step 90: 383.4584, Accuracy: 0.6955\n",
      "Training loss (for one batch) at step 100: 373.4879, Accuracy: 0.6920\n",
      "Training loss (for one batch) at step 110: 383.0522, Accuracy: 0.6921\n",
      "---- Training ----\n",
      "Training loss: 112.0201\n",
      "Training acc over epoch: 0.6928\n",
      "---- Validation ----\n",
      "Validation loss: 38.3146\n",
      "Validation acc: 0.6376\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 402.6953, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 382.4841, Accuracy: 0.6555\n",
      "Training loss (for one batch) at step 20: 373.2692, Accuracy: 0.6775\n",
      "Training loss (for one batch) at step 30: 375.3987, Accuracy: 0.6860\n",
      "Training loss (for one batch) at step 40: 358.0652, Accuracy: 0.6947\n",
      "Training loss (for one batch) at step 50: 353.9769, Accuracy: 0.7059\n",
      "Training loss (for one batch) at step 60: 355.8951, Accuracy: 0.7144\n",
      "Training loss (for one batch) at step 70: 389.6445, Accuracy: 0.7099\n",
      "Training loss (for one batch) at step 80: 395.1289, Accuracy: 0.7022\n",
      "Training loss (for one batch) at step 90: 368.2222, Accuracy: 0.6981\n",
      "Training loss (for one batch) at step 100: 355.7322, Accuracy: 0.6978\n",
      "Training loss (for one batch) at step 110: 393.0814, Accuracy: 0.6955\n",
      "---- Training ----\n",
      "Training loss: 124.3321\n",
      "Training acc over epoch: 0.6940\n",
      "---- Validation ----\n",
      "Validation loss: 32.0859\n",
      "Validation acc: 0.6343\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 397.2830, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 392.9659, Accuracy: 0.6286\n",
      "Training loss (for one batch) at step 20: 376.0174, Accuracy: 0.6589\n",
      "Training loss (for one batch) at step 30: 365.9074, Accuracy: 0.6769\n",
      "Training loss (for one batch) at step 40: 361.8375, Accuracy: 0.6917\n",
      "Training loss (for one batch) at step 50: 353.6830, Accuracy: 0.7048\n",
      "Training loss (for one batch) at step 60: 363.3981, Accuracy: 0.7112\n",
      "Training loss (for one batch) at step 70: 378.7145, Accuracy: 0.7061\n",
      "Training loss (for one batch) at step 80: 394.6019, Accuracy: 0.6977\n",
      "Training loss (for one batch) at step 90: 374.4723, Accuracy: 0.6926\n",
      "Training loss (for one batch) at step 100: 368.7187, Accuracy: 0.6925\n",
      "Training loss (for one batch) at step 110: 367.1287, Accuracy: 0.6917\n",
      "---- Training ----\n",
      "Training loss: 118.6227\n",
      "Training acc over epoch: 0.6914\n",
      "---- Validation ----\n",
      "Validation loss: 41.8894\n",
      "Validation acc: 0.6351\n",
      "Time taken: 10.76s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 419.7998, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 391.1464, Accuracy: 0.6662\n",
      "Training loss (for one batch) at step 20: 370.8248, Accuracy: 0.6756\n",
      "Training loss (for one batch) at step 30: 362.0857, Accuracy: 0.6978\n",
      "Training loss (for one batch) at step 40: 361.5830, Accuracy: 0.7058\n",
      "Training loss (for one batch) at step 50: 341.1606, Accuracy: 0.7135\n",
      "Training loss (for one batch) at step 60: 358.0263, Accuracy: 0.7161\n",
      "Training loss (for one batch) at step 70: 370.0924, Accuracy: 0.7094\n",
      "Training loss (for one batch) at step 80: 385.8388, Accuracy: 0.6991\n",
      "Training loss (for one batch) at step 90: 372.1921, Accuracy: 0.6968\n",
      "Training loss (for one batch) at step 100: 370.2845, Accuracy: 0.6946\n",
      "Training loss (for one batch) at step 110: 374.3108, Accuracy: 0.6928\n",
      "---- Training ----\n",
      "Training loss: 116.2340\n",
      "Training acc over epoch: 0.6924\n",
      "---- Validation ----\n",
      "Validation loss: 42.9936\n",
      "Validation acc: 0.6518\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 408.3200, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 368.7592, Accuracy: 0.6648\n",
      "Training loss (for one batch) at step 20: 380.4260, Accuracy: 0.6752\n",
      "Training loss (for one batch) at step 30: 364.6209, Accuracy: 0.6880\n",
      "Training loss (for one batch) at step 40: 355.1415, Accuracy: 0.6974\n",
      "Training loss (for one batch) at step 50: 353.1372, Accuracy: 0.7108\n",
      "Training loss (for one batch) at step 60: 341.6678, Accuracy: 0.7140\n",
      "Training loss (for one batch) at step 70: 366.4781, Accuracy: 0.7083\n",
      "Training loss (for one batch) at step 80: 383.5200, Accuracy: 0.6977\n",
      "Training loss (for one batch) at step 90: 367.4967, Accuracy: 0.6918\n",
      "Training loss (for one batch) at step 100: 363.5553, Accuracy: 0.6920\n",
      "Training loss (for one batch) at step 110: 353.4990, Accuracy: 0.6905\n",
      "---- Training ----\n",
      "Training loss: 123.3690\n",
      "Training acc over epoch: 0.6902\n",
      "---- Validation ----\n",
      "Validation loss: 33.2650\n",
      "Validation acc: 0.6392\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 396.3891, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 382.3661, Accuracy: 0.6506\n",
      "Training loss (for one batch) at step 20: 377.4102, Accuracy: 0.6778\n",
      "Training loss (for one batch) at step 30: 362.2179, Accuracy: 0.6923\n",
      "Training loss (for one batch) at step 40: 345.5760, Accuracy: 0.7098\n",
      "Training loss (for one batch) at step 50: 355.8593, Accuracy: 0.7142\n",
      "Training loss (for one batch) at step 60: 376.7827, Accuracy: 0.7195\n",
      "Training loss (for one batch) at step 70: 397.7999, Accuracy: 0.7114\n",
      "Training loss (for one batch) at step 80: 395.9962, Accuracy: 0.7013\n",
      "Training loss (for one batch) at step 90: 364.2005, Accuracy: 0.6975\n",
      "Training loss (for one batch) at step 100: 365.7154, Accuracy: 0.6972\n",
      "Training loss (for one batch) at step 110: 384.0677, Accuracy: 0.6990\n",
      "---- Training ----\n",
      "Training loss: 108.8215\n",
      "Training acc over epoch: 0.6977\n",
      "---- Validation ----\n",
      "Validation loss: 39.8485\n",
      "Validation acc: 0.6617\n",
      "Time taken: 10.58s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 390.0038, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 399.7561, Accuracy: 0.6619\n",
      "Training loss (for one batch) at step 20: 358.1141, Accuracy: 0.6827\n",
      "Training loss (for one batch) at step 30: 340.5353, Accuracy: 0.6976\n",
      "Training loss (for one batch) at step 40: 340.5346, Accuracy: 0.7077\n",
      "Training loss (for one batch) at step 50: 340.3844, Accuracy: 0.7146\n",
      "Training loss (for one batch) at step 60: 359.1234, Accuracy: 0.7204\n",
      "Training loss (for one batch) at step 70: 378.0419, Accuracy: 0.7130\n",
      "Training loss (for one batch) at step 80: 380.2249, Accuracy: 0.7032\n",
      "Training loss (for one batch) at step 90: 356.6075, Accuracy: 0.6999\n",
      "Training loss (for one batch) at step 100: 358.9725, Accuracy: 0.6991\n",
      "Training loss (for one batch) at step 110: 368.4713, Accuracy: 0.6969\n",
      "---- Training ----\n",
      "Training loss: 116.3311\n",
      "Training acc over epoch: 0.6973\n",
      "---- Validation ----\n",
      "Validation loss: 36.6943\n",
      "Validation acc: 0.6515\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 390.4591, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 376.4448, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 20: 371.5931, Accuracy: 0.6626\n",
      "Training loss (for one batch) at step 30: 347.1435, Accuracy: 0.6867\n",
      "Training loss (for one batch) at step 40: 339.3190, Accuracy: 0.7001\n",
      "Training loss (for one batch) at step 50: 336.8403, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 60: 367.4301, Accuracy: 0.7152\n",
      "Training loss (for one batch) at step 70: 363.9303, Accuracy: 0.7090\n",
      "Training loss (for one batch) at step 80: 364.8858, Accuracy: 0.6987\n",
      "Training loss (for one batch) at step 90: 346.9826, Accuracy: 0.6955\n",
      "Training loss (for one batch) at step 100: 345.7607, Accuracy: 0.6934\n",
      "Training loss (for one batch) at step 110: 356.9655, Accuracy: 0.6917\n",
      "---- Training ----\n",
      "Training loss: 111.4440\n",
      "Training acc over epoch: 0.6908\n",
      "---- Validation ----\n",
      "Validation loss: 39.8483\n",
      "Validation acc: 0.6494\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 405.3149, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 371.4014, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 20: 360.9721, Accuracy: 0.6741\n",
      "Training loss (for one batch) at step 30: 344.8087, Accuracy: 0.6900\n",
      "Training loss (for one batch) at step 40: 338.3850, Accuracy: 0.7029\n",
      "Training loss (for one batch) at step 50: 329.7849, Accuracy: 0.7125\n",
      "Training loss (for one batch) at step 60: 369.8441, Accuracy: 0.7159\n",
      "Training loss (for one batch) at step 70: 387.7305, Accuracy: 0.7108\n",
      "Training loss (for one batch) at step 80: 362.4502, Accuracy: 0.7002\n",
      "Training loss (for one batch) at step 90: 354.1528, Accuracy: 0.6973\n",
      "Training loss (for one batch) at step 100: 357.0104, Accuracy: 0.6959\n",
      "Training loss (for one batch) at step 110: 360.4088, Accuracy: 0.6948\n",
      "---- Training ----\n",
      "Training loss: 132.8862\n",
      "Training acc over epoch: 0.6941\n",
      "---- Validation ----\n",
      "Validation loss: 45.0525\n",
      "Validation acc: 0.6260\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 383.2358, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 380.8166, Accuracy: 0.6456\n",
      "Training loss (for one batch) at step 20: 370.7259, Accuracy: 0.6693\n",
      "Training loss (for one batch) at step 30: 344.5418, Accuracy: 0.6903\n",
      "Training loss (for one batch) at step 40: 329.0600, Accuracy: 0.7008\n",
      "Training loss (for one batch) at step 50: 332.0952, Accuracy: 0.7135\n",
      "Training loss (for one batch) at step 60: 351.5945, Accuracy: 0.7149\n",
      "Training loss (for one batch) at step 70: 366.5866, Accuracy: 0.7080\n",
      "Training loss (for one batch) at step 80: 367.4133, Accuracy: 0.6989\n",
      "Training loss (for one batch) at step 90: 362.8475, Accuracy: 0.6937\n",
      "Training loss (for one batch) at step 100: 337.8161, Accuracy: 0.6959\n",
      "Training loss (for one batch) at step 110: 365.1978, Accuracy: 0.6946\n",
      "---- Training ----\n",
      "Training loss: 120.4889\n",
      "Training acc over epoch: 0.6932\n",
      "---- Validation ----\n",
      "Validation loss: 38.0081\n",
      "Validation acc: 0.6478\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 389.8892, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 367.3434, Accuracy: 0.6186\n",
      "Training loss (for one batch) at step 20: 352.8488, Accuracy: 0.6555\n",
      "Training loss (for one batch) at step 30: 353.4034, Accuracy: 0.6825\n",
      "Training loss (for one batch) at step 40: 338.7972, Accuracy: 0.6951\n",
      "Training loss (for one batch) at step 50: 318.6361, Accuracy: 0.7068\n",
      "Training loss (for one batch) at step 60: 359.2589, Accuracy: 0.7106\n",
      "Training loss (for one batch) at step 70: 354.9910, Accuracy: 0.7038\n",
      "Training loss (for one batch) at step 80: 366.5435, Accuracy: 0.6937\n",
      "Training loss (for one batch) at step 90: 345.4251, Accuracy: 0.6887\n",
      "Training loss (for one batch) at step 100: 350.2411, Accuracy: 0.6893\n",
      "Training loss (for one batch) at step 110: 365.5200, Accuracy: 0.6898\n",
      "---- Training ----\n",
      "Training loss: 124.8027\n",
      "Training acc over epoch: 0.6904\n",
      "---- Validation ----\n",
      "Validation loss: 42.3816\n",
      "Validation acc: 0.6346\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 391.4778, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 354.5500, Accuracy: 0.6236\n",
      "Training loss (for one batch) at step 20: 341.1597, Accuracy: 0.6540\n",
      "Training loss (for one batch) at step 30: 320.1305, Accuracy: 0.6872\n",
      "Training loss (for one batch) at step 40: 335.2133, Accuracy: 0.7001\n",
      "Training loss (for one batch) at step 50: 318.1583, Accuracy: 0.7105\n",
      "Training loss (for one batch) at step 60: 340.2736, Accuracy: 0.7172\n",
      "Training loss (for one batch) at step 70: 360.0398, Accuracy: 0.7102\n",
      "Training loss (for one batch) at step 80: 357.1779, Accuracy: 0.6989\n",
      "Training loss (for one batch) at step 90: 334.3538, Accuracy: 0.6979\n",
      "Training loss (for one batch) at step 100: 321.6641, Accuracy: 0.6972\n",
      "Training loss (for one batch) at step 110: 367.1424, Accuracy: 0.6962\n",
      "---- Training ----\n",
      "Training loss: 116.1417\n",
      "Training acc over epoch: 0.6947\n",
      "---- Validation ----\n",
      "Validation loss: 44.0772\n",
      "Validation acc: 0.6634\n",
      "Time taken: 10.62s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 384.8054, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 370.8205, Accuracy: 0.6428\n",
      "Training loss (for one batch) at step 20: 346.4832, Accuracy: 0.6667\n",
      "Training loss (for one batch) at step 30: 335.5494, Accuracy: 0.6842\n",
      "Training loss (for one batch) at step 40: 338.5735, Accuracy: 0.6968\n",
      "Training loss (for one batch) at step 50: 321.8374, Accuracy: 0.7122\n",
      "Training loss (for one batch) at step 60: 349.1335, Accuracy: 0.7159\n",
      "Training loss (for one batch) at step 70: 347.8734, Accuracy: 0.7093\n",
      "Training loss (for one batch) at step 80: 373.5570, Accuracy: 0.7005\n",
      "Training loss (for one batch) at step 90: 336.3210, Accuracy: 0.6975\n",
      "Training loss (for one batch) at step 100: 333.6501, Accuracy: 0.6950\n",
      "Training loss (for one batch) at step 110: 329.1127, Accuracy: 0.6958\n",
      "---- Training ----\n",
      "Training loss: 103.5430\n",
      "Training acc over epoch: 0.6949\n",
      "---- Validation ----\n",
      "Validation loss: 37.7378\n",
      "Validation acc: 0.6381\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 386.5495, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 355.8457, Accuracy: 0.6243\n",
      "Training loss (for one batch) at step 20: 364.4307, Accuracy: 0.6536\n",
      "Training loss (for one batch) at step 30: 329.1757, Accuracy: 0.6815\n",
      "Training loss (for one batch) at step 40: 320.4069, Accuracy: 0.6955\n",
      "Training loss (for one batch) at step 50: 318.7688, Accuracy: 0.7108\n",
      "Training loss (for one batch) at step 60: 342.4093, Accuracy: 0.7140\n",
      "Training loss (for one batch) at step 70: 360.8731, Accuracy: 0.7064\n",
      "Training loss (for one batch) at step 80: 366.6713, Accuracy: 0.6969\n",
      "Training loss (for one batch) at step 90: 342.7742, Accuracy: 0.6938\n",
      "Training loss (for one batch) at step 100: 317.7195, Accuracy: 0.6944\n",
      "Training loss (for one batch) at step 110: 354.1055, Accuracy: 0.6934\n",
      "---- Training ----\n",
      "Training loss: 106.6287\n",
      "Training acc over epoch: 0.6932\n",
      "---- Validation ----\n",
      "Validation loss: 39.9766\n",
      "Validation acc: 0.6435\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 366.5821, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 380.2654, Accuracy: 0.6385\n",
      "Training loss (for one batch) at step 20: 344.2114, Accuracy: 0.6652\n",
      "Training loss (for one batch) at step 30: 331.5855, Accuracy: 0.6845\n",
      "Training loss (for one batch) at step 40: 319.5279, Accuracy: 0.7006\n",
      "Training loss (for one batch) at step 50: 310.9934, Accuracy: 0.7106\n",
      "Training loss (for one batch) at step 60: 342.8531, Accuracy: 0.7145\n",
      "Training loss (for one batch) at step 70: 362.7281, Accuracy: 0.7075\n",
      "Training loss (for one batch) at step 80: 349.8445, Accuracy: 0.6948\n",
      "Training loss (for one batch) at step 90: 337.2046, Accuracy: 0.6902\n",
      "Training loss (for one batch) at step 100: 334.1512, Accuracy: 0.6918\n",
      "Training loss (for one batch) at step 110: 356.0984, Accuracy: 0.6922\n",
      "---- Training ----\n",
      "Training loss: 105.0335\n",
      "Training acc over epoch: 0.6909\n",
      "---- Validation ----\n",
      "Validation loss: 36.7088\n",
      "Validation acc: 0.6521\n",
      "Time taken: 10.79s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 365.5847, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 363.3885, Accuracy: 0.6463\n",
      "Training loss (for one batch) at step 20: 346.2659, Accuracy: 0.6693\n",
      "Training loss (for one batch) at step 30: 346.0449, Accuracy: 0.6930\n",
      "Training loss (for one batch) at step 40: 321.1731, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 50: 311.1034, Accuracy: 0.7194\n",
      "Training loss (for one batch) at step 60: 336.6984, Accuracy: 0.7231\n",
      "Training loss (for one batch) at step 70: 373.3463, Accuracy: 0.7142\n",
      "Training loss (for one batch) at step 80: 360.0358, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 90: 332.1623, Accuracy: 0.7003\n",
      "Training loss (for one batch) at step 100: 340.9402, Accuracy: 0.6997\n",
      "Training loss (for one batch) at step 110: 345.1519, Accuracy: 0.6996\n",
      "---- Training ----\n",
      "Training loss: 125.8131\n",
      "Training acc over epoch: 0.6977\n",
      "---- Validation ----\n",
      "Validation loss: 44.1084\n",
      "Validation acc: 0.6459\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 368.9672, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 361.5163, Accuracy: 0.6357\n",
      "Training loss (for one batch) at step 20: 346.5214, Accuracy: 0.6611\n",
      "Training loss (for one batch) at step 30: 339.0691, Accuracy: 0.6840\n",
      "Training loss (for one batch) at step 40: 320.0331, Accuracy: 0.7003\n",
      "Training loss (for one batch) at step 50: 330.8113, Accuracy: 0.7111\n",
      "Training loss (for one batch) at step 60: 334.6336, Accuracy: 0.7171\n",
      "Training loss (for one batch) at step 70: 349.7491, Accuracy: 0.7095\n",
      "Training loss (for one batch) at step 80: 356.8147, Accuracy: 0.6970\n",
      "Training loss (for one batch) at step 90: 334.2440, Accuracy: 0.6934\n",
      "Training loss (for one batch) at step 100: 324.3597, Accuracy: 0.6929\n",
      "Training loss (for one batch) at step 110: 346.2629, Accuracy: 0.6933\n",
      "---- Training ----\n",
      "Training loss: 120.9141\n",
      "Training acc over epoch: 0.6925\n",
      "---- Validation ----\n",
      "Validation loss: 50.9683\n",
      "Validation acc: 0.6370\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 365.3855, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 357.5387, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 20: 329.3248, Accuracy: 0.6730\n",
      "Training loss (for one batch) at step 30: 334.5254, Accuracy: 0.6928\n",
      "Training loss (for one batch) at step 40: 320.3314, Accuracy: 0.7071\n",
      "Training loss (for one batch) at step 50: 318.4276, Accuracy: 0.7171\n",
      "Training loss (for one batch) at step 60: 323.8052, Accuracy: 0.7198\n",
      "Training loss (for one batch) at step 70: 356.2635, Accuracy: 0.7112\n",
      "Training loss (for one batch) at step 80: 349.9629, Accuracy: 0.7001\n",
      "Training loss (for one batch) at step 90: 320.2809, Accuracy: 0.6976\n",
      "Training loss (for one batch) at step 100: 307.4145, Accuracy: 0.6978\n",
      "Training loss (for one batch) at step 110: 351.0807, Accuracy: 0.6973\n",
      "---- Training ----\n",
      "Training loss: 114.1124\n",
      "Training acc over epoch: 0.6966\n",
      "---- Validation ----\n",
      "Validation loss: 32.7507\n",
      "Validation acc: 0.6537\n",
      "Time taken: 10.71s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 376.2600, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 363.2130, Accuracy: 0.6378\n",
      "Training loss (for one batch) at step 20: 335.4836, Accuracy: 0.6734\n",
      "Training loss (for one batch) at step 30: 311.5251, Accuracy: 0.6943\n",
      "Training loss (for one batch) at step 40: 305.5682, Accuracy: 0.7111\n",
      "Training loss (for one batch) at step 50: 310.3155, Accuracy: 0.7166\n",
      "Training loss (for one batch) at step 60: 324.9958, Accuracy: 0.7226\n",
      "Training loss (for one batch) at step 70: 354.0584, Accuracy: 0.7143\n",
      "Training loss (for one batch) at step 80: 343.5858, Accuracy: 0.7003\n",
      "Training loss (for one batch) at step 90: 333.4502, Accuracy: 0.6966\n",
      "Training loss (for one batch) at step 100: 335.3272, Accuracy: 0.6956\n",
      "Training loss (for one batch) at step 110: 338.0504, Accuracy: 0.6970\n",
      "---- Training ----\n",
      "Training loss: 116.1101\n",
      "Training acc over epoch: 0.6965\n",
      "---- Validation ----\n",
      "Validation loss: 35.5532\n",
      "Validation acc: 0.6397\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 359.3651, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 359.6617, Accuracy: 0.6520\n",
      "Training loss (for one batch) at step 20: 356.9423, Accuracy: 0.6760\n",
      "Training loss (for one batch) at step 30: 323.4594, Accuracy: 0.6948\n",
      "Training loss (for one batch) at step 40: 330.4488, Accuracy: 0.7066\n",
      "Training loss (for one batch) at step 50: 322.3014, Accuracy: 0.7157\n",
      "Training loss (for one batch) at step 60: 308.4926, Accuracy: 0.7225\n",
      "Training loss (for one batch) at step 70: 361.3051, Accuracy: 0.7167\n",
      "Training loss (for one batch) at step 80: 349.8115, Accuracy: 0.7042\n",
      "Training loss (for one batch) at step 90: 336.4884, Accuracy: 0.7010\n",
      "Training loss (for one batch) at step 100: 331.5035, Accuracy: 0.6997\n",
      "Training loss (for one batch) at step 110: 349.9045, Accuracy: 0.6993\n",
      "---- Training ----\n",
      "Training loss: 121.0709\n",
      "Training acc over epoch: 0.6994\n",
      "---- Validation ----\n",
      "Validation loss: 36.0864\n",
      "Validation acc: 0.6483\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 377.4774, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 351.8380, Accuracy: 0.6349\n",
      "Training loss (for one batch) at step 20: 335.5051, Accuracy: 0.6722\n",
      "Training loss (for one batch) at step 30: 323.1698, Accuracy: 0.6903\n",
      "Training loss (for one batch) at step 40: 329.7034, Accuracy: 0.7006\n",
      "Training loss (for one batch) at step 50: 313.3492, Accuracy: 0.7122\n",
      "Training loss (for one batch) at step 60: 313.3599, Accuracy: 0.7186\n",
      "Training loss (for one batch) at step 70: 353.1422, Accuracy: 0.7080\n",
      "Training loss (for one batch) at step 80: 367.3689, Accuracy: 0.6974\n",
      "Training loss (for one batch) at step 90: 330.6279, Accuracy: 0.6934\n",
      "Training loss (for one batch) at step 100: 329.0696, Accuracy: 0.6948\n",
      "Training loss (for one batch) at step 110: 348.8074, Accuracy: 0.6964\n",
      "---- Training ----\n",
      "Training loss: 101.4684\n",
      "Training acc over epoch: 0.6969\n",
      "---- Validation ----\n",
      "Validation loss: 46.1983\n",
      "Validation acc: 0.6459\n",
      "Time taken: 10.70s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 363.7949, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 370.2833, Accuracy: 0.6342\n",
      "Training loss (for one batch) at step 20: 325.2934, Accuracy: 0.6663\n",
      "Training loss (for one batch) at step 30: 315.1473, Accuracy: 0.6930\n",
      "Training loss (for one batch) at step 40: 306.5699, Accuracy: 0.7054\n",
      "Training loss (for one batch) at step 50: 304.5651, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 60: 327.1616, Accuracy: 0.7243\n",
      "Training loss (for one batch) at step 70: 343.1528, Accuracy: 0.7148\n",
      "Training loss (for one batch) at step 80: 346.4978, Accuracy: 0.7029\n",
      "Training loss (for one batch) at step 90: 341.5897, Accuracy: 0.6992\n",
      "Training loss (for one batch) at step 100: 322.0588, Accuracy: 0.7005\n",
      "Training loss (for one batch) at step 110: 348.1479, Accuracy: 0.7009\n",
      "---- Training ----\n",
      "Training loss: 111.6222\n",
      "Training acc over epoch: 0.6992\n",
      "---- Validation ----\n",
      "Validation loss: 39.1449\n",
      "Validation acc: 0.6432\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 375.2970, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 351.1942, Accuracy: 0.6413\n",
      "Training loss (for one batch) at step 20: 335.7980, Accuracy: 0.6656\n",
      "Training loss (for one batch) at step 30: 306.8707, Accuracy: 0.6867\n",
      "Training loss (for one batch) at step 40: 307.4645, Accuracy: 0.7027\n",
      "Training loss (for one batch) at step 50: 311.2366, Accuracy: 0.7154\n",
      "Training loss (for one batch) at step 60: 332.9459, Accuracy: 0.7202\n",
      "Training loss (for one batch) at step 70: 355.5018, Accuracy: 0.7107\n",
      "Training loss (for one batch) at step 80: 319.3197, Accuracy: 0.6974\n",
      "Training loss (for one batch) at step 90: 332.5132, Accuracy: 0.6930\n",
      "Training loss (for one batch) at step 100: 338.2356, Accuracy: 0.6952\n",
      "Training loss (for one batch) at step 110: 331.2155, Accuracy: 0.6948\n",
      "---- Training ----\n",
      "Training loss: 107.8370\n",
      "Training acc over epoch: 0.6934\n",
      "---- Validation ----\n",
      "Validation loss: 50.2848\n",
      "Validation acc: 0.6462\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 343.8622, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 352.6409, Accuracy: 0.6435\n",
      "Training loss (for one batch) at step 20: 322.1598, Accuracy: 0.6700\n",
      "Training loss (for one batch) at step 30: 331.2937, Accuracy: 0.6946\n",
      "Training loss (for one batch) at step 40: 303.3931, Accuracy: 0.7058\n",
      "Training loss (for one batch) at step 50: 323.1633, Accuracy: 0.7158\n",
      "Training loss (for one batch) at step 60: 313.6336, Accuracy: 0.7208\n",
      "Training loss (for one batch) at step 70: 328.2486, Accuracy: 0.7097\n",
      "Training loss (for one batch) at step 80: 338.9626, Accuracy: 0.6969\n",
      "Training loss (for one batch) at step 90: 315.3734, Accuracy: 0.6948\n",
      "Training loss (for one batch) at step 100: 318.0748, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 110: 347.6436, Accuracy: 0.6940\n",
      "---- Training ----\n",
      "Training loss: 105.6461\n",
      "Training acc over epoch: 0.6943\n",
      "---- Validation ----\n",
      "Validation loss: 45.0979\n",
      "Validation acc: 0.6534\n",
      "Time taken: 10.73s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 353.6950, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 360.2821, Accuracy: 0.6548\n",
      "Training loss (for one batch) at step 20: 327.1490, Accuracy: 0.6730\n",
      "Training loss (for one batch) at step 30: 314.1612, Accuracy: 0.6930\n",
      "Training loss (for one batch) at step 40: 326.8262, Accuracy: 0.7064\n",
      "Training loss (for one batch) at step 50: 311.8389, Accuracy: 0.7200\n",
      "Training loss (for one batch) at step 60: 317.4635, Accuracy: 0.7246\n",
      "Training loss (for one batch) at step 70: 338.1192, Accuracy: 0.7139\n",
      "Training loss (for one batch) at step 80: 342.3944, Accuracy: 0.6979\n",
      "Training loss (for one batch) at step 90: 311.2885, Accuracy: 0.6962\n",
      "Training loss (for one batch) at step 100: 318.8772, Accuracy: 0.6974\n",
      "Training loss (for one batch) at step 110: 352.4550, Accuracy: 0.6972\n",
      "---- Training ----\n",
      "Training loss: 97.0246\n",
      "Training acc over epoch: 0.6959\n",
      "---- Validation ----\n",
      "Validation loss: 46.9929\n",
      "Validation acc: 0.6359\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 354.2250, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 364.9331, Accuracy: 0.6207\n",
      "Training loss (for one batch) at step 20: 323.4709, Accuracy: 0.6589\n",
      "Training loss (for one batch) at step 30: 324.0536, Accuracy: 0.6845\n",
      "Training loss (for one batch) at step 40: 308.5301, Accuracy: 0.6968\n",
      "Training loss (for one batch) at step 50: 301.9210, Accuracy: 0.7112\n",
      "Training loss (for one batch) at step 60: 310.6977, Accuracy: 0.7163\n",
      "Training loss (for one batch) at step 70: 348.6004, Accuracy: 0.7104\n",
      "Training loss (for one batch) at step 80: 343.8024, Accuracy: 0.7009\n",
      "Training loss (for one batch) at step 90: 328.9387, Accuracy: 0.6979\n",
      "Training loss (for one batch) at step 100: 331.9386, Accuracy: 0.6974\n",
      "Training loss (for one batch) at step 110: 328.5506, Accuracy: 0.6966\n",
      "---- Training ----\n",
      "Training loss: 109.1807\n",
      "Training acc over epoch: 0.6965\n",
      "---- Validation ----\n",
      "Validation loss: 36.1561\n",
      "Validation acc: 0.6298\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 335.8313, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 328.2871, Accuracy: 0.6435\n",
      "Training loss (for one batch) at step 20: 338.5440, Accuracy: 0.6648\n",
      "Training loss (for one batch) at step 30: 339.5374, Accuracy: 0.6872\n",
      "Training loss (for one batch) at step 40: 302.4235, Accuracy: 0.7029\n",
      "Training loss (for one batch) at step 50: 306.6735, Accuracy: 0.7149\n",
      "Training loss (for one batch) at step 60: 314.0701, Accuracy: 0.7190\n",
      "Training loss (for one batch) at step 70: 332.9185, Accuracy: 0.7105\n",
      "Training loss (for one batch) at step 80: 340.9561, Accuracy: 0.6985\n",
      "Training loss (for one batch) at step 90: 313.8646, Accuracy: 0.6942\n",
      "Training loss (for one batch) at step 100: 327.6758, Accuracy: 0.6942\n",
      "Training loss (for one batch) at step 110: 318.8530, Accuracy: 0.6943\n",
      "---- Training ----\n",
      "Training loss: 101.0805\n",
      "Training acc over epoch: 0.6941\n",
      "---- Validation ----\n",
      "Validation loss: 42.7221\n",
      "Validation acc: 0.6402\n",
      "Time taken: 10.93s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 340.9860, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 345.4875, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 20: 310.6040, Accuracy: 0.6741\n",
      "Training loss (for one batch) at step 30: 317.4843, Accuracy: 0.6910\n",
      "Training loss (for one batch) at step 40: 305.4823, Accuracy: 0.7119\n",
      "Training loss (for one batch) at step 50: 307.5093, Accuracy: 0.7200\n",
      "Training loss (for one batch) at step 60: 314.4726, Accuracy: 0.7222\n",
      "Training loss (for one batch) at step 70: 347.6502, Accuracy: 0.7115\n",
      "Training loss (for one batch) at step 80: 349.7705, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 90: 322.1317, Accuracy: 0.6950\n",
      "Training loss (for one batch) at step 100: 325.6018, Accuracy: 0.6942\n",
      "Training loss (for one batch) at step 110: 318.4212, Accuracy: 0.6962\n",
      "---- Training ----\n",
      "Training loss: 99.3133\n",
      "Training acc over epoch: 0.6963\n",
      "---- Validation ----\n",
      "Validation loss: 44.2917\n",
      "Validation acc: 0.6569\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 350.8094, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 355.3096, Accuracy: 0.6491\n",
      "Training loss (for one batch) at step 20: 313.9082, Accuracy: 0.6745\n",
      "Training loss (for one batch) at step 30: 301.1574, Accuracy: 0.6910\n",
      "Training loss (for one batch) at step 40: 292.0797, Accuracy: 0.7102\n",
      "Training loss (for one batch) at step 50: 314.5567, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 60: 324.4713, Accuracy: 0.7226\n",
      "Training loss (for one batch) at step 70: 350.3817, Accuracy: 0.7134\n",
      "Training loss (for one batch) at step 80: 351.0543, Accuracy: 0.6995\n",
      "Training loss (for one batch) at step 90: 342.3795, Accuracy: 0.6959\n",
      "Training loss (for one batch) at step 100: 300.2565, Accuracy: 0.6973\n",
      "Training loss (for one batch) at step 110: 328.1230, Accuracy: 0.6967\n",
      "---- Training ----\n",
      "Training loss: 108.7725\n",
      "Training acc over epoch: 0.6966\n",
      "---- Validation ----\n",
      "Validation loss: 43.1748\n",
      "Validation acc: 0.6373\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 336.0922, Accuracy: 0.5391\n",
      "Training loss (for one batch) at step 10: 359.6278, Accuracy: 0.6243\n",
      "Training loss (for one batch) at step 20: 327.2383, Accuracy: 0.6577\n",
      "Training loss (for one batch) at step 30: 314.3589, Accuracy: 0.6840\n",
      "Training loss (for one batch) at step 40: 310.8869, Accuracy: 0.6982\n",
      "Training loss (for one batch) at step 50: 301.8064, Accuracy: 0.7134\n",
      "Training loss (for one batch) at step 60: 333.4382, Accuracy: 0.7191\n",
      "Training loss (for one batch) at step 70: 337.8602, Accuracy: 0.7097\n",
      "Training loss (for one batch) at step 80: 343.4002, Accuracy: 0.7000\n",
      "Training loss (for one batch) at step 90: 315.4587, Accuracy: 0.6960\n",
      "Training loss (for one batch) at step 100: 305.2864, Accuracy: 0.6969\n",
      "Training loss (for one batch) at step 110: 342.5679, Accuracy: 0.6978\n",
      "---- Training ----\n",
      "Training loss: 101.5800\n",
      "Training acc over epoch: 0.6955\n",
      "---- Validation ----\n",
      "Validation loss: 56.3350\n",
      "Validation acc: 0.6386\n",
      "Time taken: 10.91s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 342.9965, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 329.0262, Accuracy: 0.6392\n",
      "Training loss (for one batch) at step 20: 321.6434, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 30: 287.2411, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 40: 310.8760, Accuracy: 0.7075\n",
      "Training loss (for one batch) at step 50: 302.6190, Accuracy: 0.7171\n",
      "Training loss (for one batch) at step 60: 326.6569, Accuracy: 0.7191\n",
      "Training loss (for one batch) at step 70: 335.3593, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 80: 347.5534, Accuracy: 0.6997\n",
      "Training loss (for one batch) at step 90: 307.2921, Accuracy: 0.6957\n",
      "Training loss (for one batch) at step 100: 304.0942, Accuracy: 0.6972\n",
      "Training loss (for one batch) at step 110: 347.5405, Accuracy: 0.6963\n",
      "---- Training ----\n",
      "Training loss: 107.4325\n",
      "Training acc over epoch: 0.6963\n",
      "---- Validation ----\n",
      "Validation loss: 55.1997\n",
      "Validation acc: 0.6440\n",
      "Time taken: 10.95s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 365.6613, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 330.9061, Accuracy: 0.6278\n",
      "Training loss (for one batch) at step 20: 298.9633, Accuracy: 0.6611\n",
      "Training loss (for one batch) at step 30: 301.1096, Accuracy: 0.6832\n",
      "Training loss (for one batch) at step 40: 320.2329, Accuracy: 0.6986\n",
      "Training loss (for one batch) at step 50: 285.5548, Accuracy: 0.7111\n",
      "Training loss (for one batch) at step 60: 308.5919, Accuracy: 0.7163\n",
      "Training loss (for one batch) at step 70: 330.7396, Accuracy: 0.7068\n",
      "Training loss (for one batch) at step 80: 320.9405, Accuracy: 0.6943\n",
      "Training loss (for one batch) at step 90: 323.2017, Accuracy: 0.6929\n",
      "Training loss (for one batch) at step 100: 306.1031, Accuracy: 0.6920\n",
      "Training loss (for one batch) at step 110: 334.7587, Accuracy: 0.6921\n",
      "---- Training ----\n",
      "Training loss: 121.9984\n",
      "Training acc over epoch: 0.6922\n",
      "---- Validation ----\n",
      "Validation loss: 57.1885\n",
      "Validation acc: 0.6381\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 343.3381, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 334.9482, Accuracy: 0.6243\n",
      "Training loss (for one batch) at step 20: 311.4285, Accuracy: 0.6637\n",
      "Training loss (for one batch) at step 30: 315.2538, Accuracy: 0.6913\n",
      "Training loss (for one batch) at step 40: 294.7678, Accuracy: 0.7056\n",
      "Training loss (for one batch) at step 50: 283.0649, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 60: 320.7852, Accuracy: 0.7240\n",
      "Training loss (for one batch) at step 70: 318.1705, Accuracy: 0.7141\n",
      "Training loss (for one batch) at step 80: 352.6407, Accuracy: 0.7010\n",
      "Training loss (for one batch) at step 90: 330.6756, Accuracy: 0.6975\n",
      "Training loss (for one batch) at step 100: 320.9896, Accuracy: 0.6982\n",
      "Training loss (for one batch) at step 110: 353.3736, Accuracy: 0.6983\n",
      "---- Training ----\n",
      "Training loss: 107.5274\n",
      "Training acc over epoch: 0.6971\n",
      "---- Validation ----\n",
      "Validation loss: 54.8687\n",
      "Validation acc: 0.6354\n",
      "Time taken: 10.73s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 336.8376, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 333.8694, Accuracy: 0.6236\n",
      "Training loss (for one batch) at step 20: 309.3250, Accuracy: 0.6585\n",
      "Training loss (for one batch) at step 30: 308.4544, Accuracy: 0.6895\n",
      "Training loss (for one batch) at step 40: 290.9681, Accuracy: 0.7050\n",
      "Training loss (for one batch) at step 50: 290.4019, Accuracy: 0.7131\n",
      "Training loss (for one batch) at step 60: 314.5612, Accuracy: 0.7209\n",
      "Training loss (for one batch) at step 70: 331.2205, Accuracy: 0.7096\n",
      "Training loss (for one batch) at step 80: 317.8864, Accuracy: 0.6987\n",
      "Training loss (for one batch) at step 90: 309.9170, Accuracy: 0.6956\n",
      "Training loss (for one batch) at step 100: 287.3427, Accuracy: 0.6957\n",
      "Training loss (for one batch) at step 110: 303.6295, Accuracy: 0.6950\n",
      "---- Training ----\n",
      "Training loss: 103.1555\n",
      "Training acc over epoch: 0.6949\n",
      "---- Validation ----\n",
      "Validation loss: 37.5968\n",
      "Validation acc: 0.6435\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 351.3392, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 345.8920, Accuracy: 0.6364\n",
      "Training loss (for one batch) at step 20: 312.9818, Accuracy: 0.6622\n",
      "Training loss (for one batch) at step 30: 309.3781, Accuracy: 0.6898\n",
      "Training loss (for one batch) at step 40: 290.2691, Accuracy: 0.7048\n",
      "Training loss (for one batch) at step 50: 294.5244, Accuracy: 0.7149\n",
      "Training loss (for one batch) at step 60: 316.6741, Accuracy: 0.7196\n",
      "Training loss (for one batch) at step 70: 308.9182, Accuracy: 0.7096\n",
      "Training loss (for one batch) at step 80: 312.7290, Accuracy: 0.6978\n",
      "Training loss (for one batch) at step 90: 318.1742, Accuracy: 0.6954\n",
      "Training loss (for one batch) at step 100: 311.8974, Accuracy: 0.6959\n",
      "Training loss (for one batch) at step 110: 298.0965, Accuracy: 0.6962\n",
      "---- Training ----\n",
      "Training loss: 98.5067\n",
      "Training acc over epoch: 0.6949\n",
      "---- Validation ----\n",
      "Validation loss: 41.6565\n",
      "Validation acc: 0.6351\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 348.2412, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 337.4221, Accuracy: 0.6399\n",
      "Training loss (for one batch) at step 20: 306.8956, Accuracy: 0.6637\n",
      "Training loss (for one batch) at step 30: 310.8754, Accuracy: 0.6880\n",
      "Training loss (for one batch) at step 40: 295.2830, Accuracy: 0.7027\n",
      "Training loss (for one batch) at step 50: 292.5987, Accuracy: 0.7151\n",
      "Training loss (for one batch) at step 60: 321.1086, Accuracy: 0.7173\n",
      "Training loss (for one batch) at step 70: 332.8376, Accuracy: 0.7087\n",
      "Training loss (for one batch) at step 80: 329.4356, Accuracy: 0.6995\n",
      "Training loss (for one batch) at step 90: 318.4276, Accuracy: 0.6972\n",
      "Training loss (for one batch) at step 100: 311.3404, Accuracy: 0.6970\n",
      "Training loss (for one batch) at step 110: 345.8697, Accuracy: 0.6981\n",
      "---- Training ----\n",
      "Training loss: 96.3468\n",
      "Training acc over epoch: 0.6960\n",
      "---- Validation ----\n",
      "Validation loss: 44.7829\n",
      "Validation acc: 0.6502\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 360.4927, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 318.1212, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 20: 316.2665, Accuracy: 0.6644\n",
      "Training loss (for one batch) at step 30: 305.5607, Accuracy: 0.6908\n",
      "Training loss (for one batch) at step 40: 297.8804, Accuracy: 0.7062\n",
      "Training loss (for one batch) at step 50: 326.1204, Accuracy: 0.7168\n",
      "Training loss (for one batch) at step 60: 313.3019, Accuracy: 0.7239\n",
      "Training loss (for one batch) at step 70: 332.6006, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 80: 341.2550, Accuracy: 0.7004\n",
      "Training loss (for one batch) at step 90: 328.5103, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 100: 307.0342, Accuracy: 0.6979\n",
      "Training loss (for one batch) at step 110: 323.9822, Accuracy: 0.6974\n",
      "---- Training ----\n",
      "Training loss: 92.7532\n",
      "Training acc over epoch: 0.6963\n",
      "---- Validation ----\n",
      "Validation loss: 37.2170\n",
      "Validation acc: 0.6346\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 337.5396, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 324.3318, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 20: 301.2245, Accuracy: 0.6648\n",
      "Training loss (for one batch) at step 30: 296.4724, Accuracy: 0.6943\n",
      "Training loss (for one batch) at step 40: 294.1996, Accuracy: 0.7066\n",
      "Training loss (for one batch) at step 50: 282.7946, Accuracy: 0.7195\n",
      "Training loss (for one batch) at step 60: 300.0942, Accuracy: 0.7236\n",
      "Training loss (for one batch) at step 70: 318.4987, Accuracy: 0.7125\n",
      "Training loss (for one batch) at step 80: 325.9174, Accuracy: 0.6998\n",
      "Training loss (for one batch) at step 90: 299.7476, Accuracy: 0.6966\n",
      "Training loss (for one batch) at step 100: 318.9254, Accuracy: 0.6961\n",
      "Training loss (for one batch) at step 110: 310.1890, Accuracy: 0.6959\n",
      "---- Training ----\n",
      "Training loss: 92.0228\n",
      "Training acc over epoch: 0.6961\n",
      "---- Validation ----\n",
      "Validation loss: 35.3433\n",
      "Validation acc: 0.6298\n",
      "Time taken: 10.63s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 339.5447, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 325.4223, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 20: 298.2866, Accuracy: 0.6745\n",
      "Training loss (for one batch) at step 30: 302.9896, Accuracy: 0.6933\n",
      "Training loss (for one batch) at step 40: 290.9135, Accuracy: 0.7045\n",
      "Training loss (for one batch) at step 50: 301.4241, Accuracy: 0.7151\n",
      "Training loss (for one batch) at step 60: 312.3876, Accuracy: 0.7225\n",
      "Training loss (for one batch) at step 70: 347.8463, Accuracy: 0.7110\n",
      "Training loss (for one batch) at step 80: 328.2097, Accuracy: 0.6998\n",
      "Training loss (for one batch) at step 90: 309.6798, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 100: 298.7924, Accuracy: 0.6977\n",
      "Training loss (for one batch) at step 110: 331.6614, Accuracy: 0.6963\n",
      "---- Training ----\n",
      "Training loss: 100.4547\n",
      "Training acc over epoch: 0.6959\n",
      "---- Validation ----\n",
      "Validation loss: 43.7703\n",
      "Validation acc: 0.6427\n",
      "Time taken: 10.62s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 340.7924, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 318.2529, Accuracy: 0.6271\n",
      "Training loss (for one batch) at step 20: 308.9501, Accuracy: 0.6648\n",
      "Training loss (for one batch) at step 30: 297.8524, Accuracy: 0.6935\n",
      "Training loss (for one batch) at step 40: 300.6720, Accuracy: 0.7060\n",
      "Training loss (for one batch) at step 50: 274.3059, Accuracy: 0.7195\n",
      "Training loss (for one batch) at step 60: 302.7069, Accuracy: 0.7254\n",
      "Training loss (for one batch) at step 70: 336.7427, Accuracy: 0.7158\n",
      "Training loss (for one batch) at step 80: 327.9063, Accuracy: 0.7014\n",
      "Training loss (for one batch) at step 90: 297.8311, Accuracy: 0.6968\n",
      "Training loss (for one batch) at step 100: 283.5931, Accuracy: 0.6981\n",
      "Training loss (for one batch) at step 110: 296.4903, Accuracy: 0.6983\n",
      "---- Training ----\n",
      "Training loss: 116.4087\n",
      "Training acc over epoch: 0.6971\n",
      "---- Validation ----\n",
      "Validation loss: 38.5028\n",
      "Validation acc: 0.6437\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 345.1211, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 337.6246, Accuracy: 0.6236\n",
      "Training loss (for one batch) at step 20: 321.5117, Accuracy: 0.6607\n",
      "Training loss (for one batch) at step 30: 289.4443, Accuracy: 0.6890\n",
      "Training loss (for one batch) at step 40: 286.7602, Accuracy: 0.7008\n",
      "Training loss (for one batch) at step 50: 282.5077, Accuracy: 0.7138\n",
      "Training loss (for one batch) at step 60: 301.0361, Accuracy: 0.7181\n",
      "Training loss (for one batch) at step 70: 323.9853, Accuracy: 0.7059\n",
      "Training loss (for one batch) at step 80: 313.1327, Accuracy: 0.6940\n",
      "Training loss (for one batch) at step 90: 299.6553, Accuracy: 0.6908\n",
      "Training loss (for one batch) at step 100: 293.5420, Accuracy: 0.6927\n",
      "Training loss (for one batch) at step 110: 306.1153, Accuracy: 0.6936\n",
      "---- Training ----\n",
      "Training loss: 95.9130\n",
      "Training acc over epoch: 0.6920\n",
      "---- Validation ----\n",
      "Validation loss: 52.5200\n",
      "Validation acc: 0.6596\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 355.8667, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 323.8098, Accuracy: 0.6399\n",
      "Training loss (for one batch) at step 20: 295.8669, Accuracy: 0.6711\n",
      "Training loss (for one batch) at step 30: 280.7599, Accuracy: 0.6938\n",
      "Training loss (for one batch) at step 40: 289.6220, Accuracy: 0.7081\n",
      "Training loss (for one batch) at step 50: 279.8486, Accuracy: 0.7189\n",
      "Training loss (for one batch) at step 60: 296.4287, Accuracy: 0.7248\n",
      "Training loss (for one batch) at step 70: 295.5070, Accuracy: 0.7163\n",
      "Training loss (for one batch) at step 80: 326.4128, Accuracy: 0.7040\n",
      "Training loss (for one batch) at step 90: 310.8185, Accuracy: 0.6977\n",
      "Training loss (for one batch) at step 100: 302.1942, Accuracy: 0.6976\n",
      "Training loss (for one batch) at step 110: 308.8452, Accuracy: 0.6969\n",
      "---- Training ----\n",
      "Training loss: 105.3398\n",
      "Training acc over epoch: 0.6960\n",
      "---- Validation ----\n",
      "Validation loss: 51.2950\n",
      "Validation acc: 0.6467\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 346.8922, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 320.9224, Accuracy: 0.6385\n",
      "Training loss (for one batch) at step 20: 315.0410, Accuracy: 0.6656\n",
      "Training loss (for one batch) at step 30: 294.3531, Accuracy: 0.6900\n",
      "Training loss (for one batch) at step 40: 296.4561, Accuracy: 0.7060\n",
      "Training loss (for one batch) at step 50: 282.7824, Accuracy: 0.7184\n",
      "Training loss (for one batch) at step 60: 309.9599, Accuracy: 0.7228\n",
      "Training loss (for one batch) at step 70: 316.0041, Accuracy: 0.7107\n",
      "Training loss (for one batch) at step 80: 313.3219, Accuracy: 0.6983\n",
      "Training loss (for one batch) at step 90: 334.1870, Accuracy: 0.6944\n",
      "Training loss (for one batch) at step 100: 300.6057, Accuracy: 0.6943\n",
      "Training loss (for one batch) at step 110: 342.6419, Accuracy: 0.6925\n",
      "---- Training ----\n",
      "Training loss: 114.4697\n",
      "Training acc over epoch: 0.6918\n",
      "---- Validation ----\n",
      "Validation loss: 44.5969\n",
      "Validation acc: 0.6497\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 347.9242, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 320.1107, Accuracy: 0.6271\n",
      "Training loss (for one batch) at step 20: 310.4275, Accuracy: 0.6611\n",
      "Training loss (for one batch) at step 30: 294.7839, Accuracy: 0.6847\n",
      "Training loss (for one batch) at step 40: 302.1364, Accuracy: 0.7033\n",
      "Training loss (for one batch) at step 50: 319.2019, Accuracy: 0.7160\n",
      "Training loss (for one batch) at step 60: 326.0778, Accuracy: 0.7191\n",
      "Training loss (for one batch) at step 70: 329.1121, Accuracy: 0.7125\n",
      "Training loss (for one batch) at step 80: 328.8904, Accuracy: 0.7008\n",
      "Training loss (for one batch) at step 90: 318.7941, Accuracy: 0.6973\n",
      "Training loss (for one batch) at step 100: 319.6002, Accuracy: 0.6989\n",
      "Training loss (for one batch) at step 110: 325.6185, Accuracy: 0.6976\n",
      "---- Training ----\n",
      "Training loss: 121.5319\n",
      "Training acc over epoch: 0.6978\n",
      "---- Validation ----\n",
      "Validation loss: 37.9680\n",
      "Validation acc: 0.6351\n",
      "Time taken: 10.60s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 326.0504, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 342.7659, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 20: 303.0127, Accuracy: 0.6659\n",
      "Training loss (for one batch) at step 30: 303.4100, Accuracy: 0.6885\n",
      "Training loss (for one batch) at step 40: 301.3337, Accuracy: 0.7066\n",
      "Training loss (for one batch) at step 50: 281.9384, Accuracy: 0.7169\n",
      "Training loss (for one batch) at step 60: 291.5601, Accuracy: 0.7253\n",
      "Training loss (for one batch) at step 70: 317.5474, Accuracy: 0.7146\n",
      "Training loss (for one batch) at step 80: 322.6930, Accuracy: 0.7021\n",
      "Training loss (for one batch) at step 90: 320.1776, Accuracy: 0.6977\n",
      "Training loss (for one batch) at step 100: 305.2180, Accuracy: 0.6989\n",
      "Training loss (for one batch) at step 110: 305.4929, Accuracy: 0.6995\n",
      "---- Training ----\n",
      "Training loss: 91.2847\n",
      "Training acc over epoch: 0.6988\n",
      "---- Validation ----\n",
      "Validation loss: 38.1289\n",
      "Validation acc: 0.6472\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 332.4976, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 319.3879, Accuracy: 0.6349\n",
      "Training loss (for one batch) at step 20: 295.3307, Accuracy: 0.6663\n",
      "Training loss (for one batch) at step 30: 264.3499, Accuracy: 0.6946\n",
      "Training loss (for one batch) at step 40: 285.5592, Accuracy: 0.7100\n",
      "Training loss (for one batch) at step 50: 308.7527, Accuracy: 0.7172\n",
      "Training loss (for one batch) at step 60: 305.4532, Accuracy: 0.7213\n",
      "Training loss (for one batch) at step 70: 324.8540, Accuracy: 0.7101\n",
      "Training loss (for one batch) at step 80: 307.4331, Accuracy: 0.7011\n",
      "Training loss (for one batch) at step 90: 295.1383, Accuracy: 0.6975\n",
      "Training loss (for one batch) at step 100: 293.6214, Accuracy: 0.6983\n",
      "Training loss (for one batch) at step 110: 291.7812, Accuracy: 0.6988\n",
      "---- Training ----\n",
      "Training loss: 112.1163\n",
      "Training acc over epoch: 0.6973\n",
      "---- Validation ----\n",
      "Validation loss: 34.4745\n",
      "Validation acc: 0.6365\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 331.7480, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 328.7435, Accuracy: 0.6449\n",
      "Training loss (for one batch) at step 20: 305.7926, Accuracy: 0.6693\n",
      "Training loss (for one batch) at step 30: 289.4513, Accuracy: 0.6968\n",
      "Training loss (for one batch) at step 40: 284.3632, Accuracy: 0.7104\n",
      "Training loss (for one batch) at step 50: 279.5116, Accuracy: 0.7177\n",
      "Training loss (for one batch) at step 60: 276.3875, Accuracy: 0.7220\n",
      "Training loss (for one batch) at step 70: 308.2152, Accuracy: 0.7110\n",
      "Training loss (for one batch) at step 80: 307.6449, Accuracy: 0.6987\n",
      "Training loss (for one batch) at step 90: 308.2369, Accuracy: 0.6945\n",
      "Training loss (for one batch) at step 100: 291.1419, Accuracy: 0.6955\n",
      "Training loss (for one batch) at step 110: 320.9182, Accuracy: 0.6954\n",
      "---- Training ----\n",
      "Training loss: 96.4615\n",
      "Training acc over epoch: 0.6950\n",
      "---- Validation ----\n",
      "Validation loss: 41.5309\n",
      "Validation acc: 0.6421\n",
      "Time taken: 13.27s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 325.3203, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 324.7573, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 20: 285.7446, Accuracy: 0.6529\n",
      "Training loss (for one batch) at step 30: 286.1429, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 40: 283.0664, Accuracy: 0.7027\n",
      "Training loss (for one batch) at step 50: 279.2788, Accuracy: 0.7149\n",
      "Training loss (for one batch) at step 60: 313.7785, Accuracy: 0.7186\n",
      "Training loss (for one batch) at step 70: 323.1812, Accuracy: 0.7083\n",
      "Training loss (for one batch) at step 80: 316.6804, Accuracy: 0.6985\n",
      "Training loss (for one batch) at step 90: 302.8919, Accuracy: 0.6941\n",
      "Training loss (for one batch) at step 100: 292.1479, Accuracy: 0.6952\n",
      "Training loss (for one batch) at step 110: 317.5630, Accuracy: 0.6943\n",
      "---- Training ----\n",
      "Training loss: 127.4289\n",
      "Training acc over epoch: 0.6953\n",
      "---- Validation ----\n",
      "Validation loss: 56.4672\n",
      "Validation acc: 0.6370\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 334.4198, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 314.3127, Accuracy: 0.6186\n",
      "Training loss (for one batch) at step 20: 309.2299, Accuracy: 0.6603\n",
      "Training loss (for one batch) at step 30: 300.8874, Accuracy: 0.6900\n",
      "Training loss (for one batch) at step 40: 283.8040, Accuracy: 0.7016\n",
      "Training loss (for one batch) at step 50: 289.2272, Accuracy: 0.7138\n",
      "Training loss (for one batch) at step 60: 297.9161, Accuracy: 0.7230\n",
      "Training loss (for one batch) at step 70: 315.0752, Accuracy: 0.7103\n",
      "Training loss (for one batch) at step 80: 323.3152, Accuracy: 0.6988\n",
      "Training loss (for one batch) at step 90: 298.0809, Accuracy: 0.6957\n",
      "Training loss (for one batch) at step 100: 319.9907, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 110: 297.9574, Accuracy: 0.6955\n",
      "---- Training ----\n",
      "Training loss: 90.7834\n",
      "Training acc over epoch: 0.6953\n",
      "---- Validation ----\n",
      "Validation loss: 42.1203\n",
      "Validation acc: 0.6451\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 324.1276, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 326.3253, Accuracy: 0.6207\n",
      "Training loss (for one batch) at step 20: 301.8270, Accuracy: 0.6637\n",
      "Training loss (for one batch) at step 30: 293.1960, Accuracy: 0.6923\n",
      "Training loss (for one batch) at step 40: 286.4813, Accuracy: 0.7050\n",
      "Training loss (for one batch) at step 50: 295.7241, Accuracy: 0.7177\n",
      "Training loss (for one batch) at step 60: 295.6260, Accuracy: 0.7200\n",
      "Training loss (for one batch) at step 70: 321.3236, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 80: 332.9210, Accuracy: 0.6969\n",
      "Training loss (for one batch) at step 90: 301.1829, Accuracy: 0.6950\n",
      "Training loss (for one batch) at step 100: 303.7874, Accuracy: 0.6974\n",
      "Training loss (for one batch) at step 110: 307.0408, Accuracy: 0.6959\n",
      "---- Training ----\n",
      "Training loss: 101.8464\n",
      "Training acc over epoch: 0.6942\n",
      "---- Validation ----\n",
      "Validation loss: 64.1858\n",
      "Validation acc: 0.6373\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 348.6042, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 300.1895, Accuracy: 0.6335\n",
      "Training loss (for one batch) at step 20: 288.6448, Accuracy: 0.6656\n",
      "Training loss (for one batch) at step 30: 274.6791, Accuracy: 0.6941\n",
      "Training loss (for one batch) at step 40: 288.3210, Accuracy: 0.7058\n",
      "Training loss (for one batch) at step 50: 301.7232, Accuracy: 0.7160\n",
      "Training loss (for one batch) at step 60: 284.2555, Accuracy: 0.7220\n",
      "Training loss (for one batch) at step 70: 300.3828, Accuracy: 0.7110\n",
      "Training loss (for one batch) at step 80: 328.2298, Accuracy: 0.6966\n",
      "Training loss (for one batch) at step 90: 296.1833, Accuracy: 0.6942\n",
      "Training loss (for one batch) at step 100: 298.5127, Accuracy: 0.6958\n",
      "Training loss (for one batch) at step 110: 306.7554, Accuracy: 0.6947\n",
      "---- Training ----\n",
      "Training loss: 102.7012\n",
      "Training acc over epoch: 0.6945\n",
      "---- Validation ----\n",
      "Validation loss: 54.9223\n",
      "Validation acc: 0.6400\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 313.4882, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 326.9547, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 20: 320.8675, Accuracy: 0.6700\n",
      "Training loss (for one batch) at step 30: 296.5315, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 40: 292.0476, Accuracy: 0.7079\n",
      "Training loss (for one batch) at step 50: 283.7644, Accuracy: 0.7161\n",
      "Training loss (for one batch) at step 60: 288.7701, Accuracy: 0.7203\n",
      "Training loss (for one batch) at step 70: 323.7659, Accuracy: 0.7086\n",
      "Training loss (for one batch) at step 80: 317.8871, Accuracy: 0.6970\n",
      "Training loss (for one batch) at step 90: 277.2432, Accuracy: 0.6933\n",
      "Training loss (for one batch) at step 100: 288.7197, Accuracy: 0.6949\n",
      "Training loss (for one batch) at step 110: 315.7438, Accuracy: 0.6945\n",
      "---- Training ----\n",
      "Training loss: 92.3918\n",
      "Training acc over epoch: 0.6940\n",
      "---- Validation ----\n",
      "Validation loss: 48.9564\n",
      "Validation acc: 0.6451\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 312.8916, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 320.1154, Accuracy: 0.6286\n",
      "Training loss (for one batch) at step 20: 303.3091, Accuracy: 0.6715\n",
      "Training loss (for one batch) at step 30: 282.3100, Accuracy: 0.6933\n",
      "Training loss (for one batch) at step 40: 284.5268, Accuracy: 0.7045\n",
      "Training loss (for one batch) at step 50: 275.1382, Accuracy: 0.7149\n",
      "Training loss (for one batch) at step 60: 305.3424, Accuracy: 0.7207\n",
      "Training loss (for one batch) at step 70: 297.4660, Accuracy: 0.7115\n",
      "Training loss (for one batch) at step 80: 310.5572, Accuracy: 0.6994\n",
      "Training loss (for one batch) at step 90: 290.1684, Accuracy: 0.6980\n",
      "Training loss (for one batch) at step 100: 295.2264, Accuracy: 0.6962\n",
      "Training loss (for one batch) at step 110: 307.9731, Accuracy: 0.6960\n",
      "---- Training ----\n",
      "Training loss: 92.4044\n",
      "Training acc over epoch: 0.6959\n",
      "---- Validation ----\n",
      "Validation loss: 46.2237\n",
      "Validation acc: 0.6435\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 311.2710, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 319.2832, Accuracy: 0.6243\n",
      "Training loss (for one batch) at step 20: 307.2333, Accuracy: 0.6756\n",
      "Training loss (for one batch) at step 30: 298.3473, Accuracy: 0.6920\n",
      "Training loss (for one batch) at step 40: 297.9565, Accuracy: 0.7066\n",
      "Training loss (for one batch) at step 50: 282.8980, Accuracy: 0.7191\n",
      "Training loss (for one batch) at step 60: 311.9223, Accuracy: 0.7220\n",
      "Training loss (for one batch) at step 70: 317.5082, Accuracy: 0.7119\n",
      "Training loss (for one batch) at step 80: 336.9872, Accuracy: 0.6968\n",
      "Training loss (for one batch) at step 90: 297.9886, Accuracy: 0.6944\n",
      "Training loss (for one batch) at step 100: 300.0786, Accuracy: 0.6935\n",
      "Training loss (for one batch) at step 110: 326.3151, Accuracy: 0.6952\n",
      "---- Training ----\n",
      "Training loss: 103.0649\n",
      "Training acc over epoch: 0.6949\n",
      "---- Validation ----\n",
      "Validation loss: 39.0001\n",
      "Validation acc: 0.6591\n",
      "Time taken: 10.66s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 324.0841, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 310.4836, Accuracy: 0.6385\n",
      "Training loss (for one batch) at step 20: 274.9445, Accuracy: 0.6752\n",
      "Training loss (for one batch) at step 30: 297.3052, Accuracy: 0.6951\n",
      "Training loss (for one batch) at step 40: 274.9916, Accuracy: 0.7136\n",
      "Training loss (for one batch) at step 50: 287.7281, Accuracy: 0.7214\n",
      "Training loss (for one batch) at step 60: 314.9055, Accuracy: 0.7235\n",
      "Training loss (for one batch) at step 70: 302.9796, Accuracy: 0.7138\n",
      "Training loss (for one batch) at step 80: 330.0284, Accuracy: 0.7000\n",
      "Training loss (for one batch) at step 90: 294.5779, Accuracy: 0.6974\n",
      "Training loss (for one batch) at step 100: 291.1356, Accuracy: 0.6977\n",
      "Training loss (for one batch) at step 110: 292.3696, Accuracy: 0.6977\n",
      "---- Training ----\n",
      "Training loss: 89.2763\n",
      "Training acc over epoch: 0.6971\n",
      "---- Validation ----\n",
      "Validation loss: 61.2365\n",
      "Validation acc: 0.6362\n",
      "Time taken: 10.25s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 348.4666, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 308.4947, Accuracy: 0.6413\n",
      "Training loss (for one batch) at step 20: 291.0744, Accuracy: 0.6730\n",
      "Training loss (for one batch) at step 30: 299.8156, Accuracy: 0.6933\n",
      "Training loss (for one batch) at step 40: 284.0340, Accuracy: 0.7100\n",
      "Training loss (for one batch) at step 50: 287.3164, Accuracy: 0.7212\n",
      "Training loss (for one batch) at step 60: 291.2882, Accuracy: 0.7269\n",
      "Training loss (for one batch) at step 70: 320.2283, Accuracy: 0.7160\n",
      "Training loss (for one batch) at step 80: 311.7028, Accuracy: 0.7020\n",
      "Training loss (for one batch) at step 90: 309.9426, Accuracy: 0.6996\n",
      "Training loss (for one batch) at step 100: 296.5168, Accuracy: 0.7019\n",
      "Training loss (for one batch) at step 110: 294.2209, Accuracy: 0.7007\n",
      "---- Training ----\n",
      "Training loss: 113.7540\n",
      "Training acc over epoch: 0.7004\n",
      "---- Validation ----\n",
      "Validation loss: 57.9944\n",
      "Validation acc: 0.6472\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 328.1158, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 312.3503, Accuracy: 0.6257\n",
      "Training loss (for one batch) at step 20: 285.3709, Accuracy: 0.6670\n",
      "Training loss (for one batch) at step 30: 301.3989, Accuracy: 0.6910\n",
      "Training loss (for one batch) at step 40: 280.2309, Accuracy: 0.7077\n",
      "Training loss (for one batch) at step 50: 282.3335, Accuracy: 0.7171\n",
      "Training loss (for one batch) at step 60: 286.3448, Accuracy: 0.7223\n",
      "Training loss (for one batch) at step 70: 317.0681, Accuracy: 0.7105\n",
      "Training loss (for one batch) at step 80: 337.9982, Accuracy: 0.6979\n",
      "Training loss (for one batch) at step 90: 297.1475, Accuracy: 0.6948\n",
      "Training loss (for one batch) at step 100: 284.9782, Accuracy: 0.6969\n",
      "Training loss (for one batch) at step 110: 307.2739, Accuracy: 0.6980\n",
      "---- Training ----\n",
      "Training loss: 110.3217\n",
      "Training acc over epoch: 0.6971\n",
      "---- Validation ----\n",
      "Validation loss: 46.1386\n",
      "Validation acc: 0.6429\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 325.1159, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 310.0242, Accuracy: 0.6271\n",
      "Training loss (for one batch) at step 20: 296.7392, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 30: 298.1154, Accuracy: 0.6920\n",
      "Training loss (for one batch) at step 40: 267.3217, Accuracy: 0.7083\n",
      "Training loss (for one batch) at step 50: 278.7430, Accuracy: 0.7206\n",
      "Training loss (for one batch) at step 60: 306.7674, Accuracy: 0.7239\n",
      "Training loss (for one batch) at step 70: 303.7862, Accuracy: 0.7153\n",
      "Training loss (for one batch) at step 80: 325.6978, Accuracy: 0.7024\n",
      "Training loss (for one batch) at step 90: 287.8458, Accuracy: 0.6984\n",
      "Training loss (for one batch) at step 100: 293.9054, Accuracy: 0.6986\n",
      "Training loss (for one batch) at step 110: 291.7992, Accuracy: 0.7004\n",
      "---- Training ----\n",
      "Training loss: 101.5184\n",
      "Training acc over epoch: 0.6981\n",
      "---- Validation ----\n",
      "Validation loss: 66.6796\n",
      "Validation acc: 0.6435\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 324.4199, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 329.6313, Accuracy: 0.6271\n",
      "Training loss (for one batch) at step 20: 291.7794, Accuracy: 0.6678\n",
      "Training loss (for one batch) at step 30: 296.8893, Accuracy: 0.6951\n",
      "Training loss (for one batch) at step 40: 284.6352, Accuracy: 0.7085\n",
      "Training loss (for one batch) at step 50: 275.1140, Accuracy: 0.7186\n",
      "Training loss (for one batch) at step 60: 294.1190, Accuracy: 0.7237\n",
      "Training loss (for one batch) at step 70: 308.4126, Accuracy: 0.7140\n",
      "Training loss (for one batch) at step 80: 323.4958, Accuracy: 0.6995\n",
      "Training loss (for one batch) at step 90: 290.0737, Accuracy: 0.6927\n",
      "Training loss (for one batch) at step 100: 286.5893, Accuracy: 0.6945\n",
      "Training loss (for one batch) at step 110: 295.2768, Accuracy: 0.6955\n",
      "---- Training ----\n",
      "Training loss: 101.2242\n",
      "Training acc over epoch: 0.6946\n",
      "---- Validation ----\n",
      "Validation loss: 42.2348\n",
      "Validation acc: 0.6370\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 324.5616, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 303.9313, Accuracy: 0.6243\n",
      "Training loss (for one batch) at step 20: 293.0216, Accuracy: 0.6626\n",
      "Training loss (for one batch) at step 30: 277.7022, Accuracy: 0.6908\n",
      "Training loss (for one batch) at step 40: 288.8008, Accuracy: 0.7048\n",
      "Training loss (for one batch) at step 50: 290.7323, Accuracy: 0.7181\n",
      "Training loss (for one batch) at step 60: 312.1790, Accuracy: 0.7228\n",
      "Training loss (for one batch) at step 70: 300.1239, Accuracy: 0.7136\n",
      "Training loss (for one batch) at step 80: 304.6296, Accuracy: 0.6997\n",
      "Training loss (for one batch) at step 90: 295.0997, Accuracy: 0.6969\n",
      "Training loss (for one batch) at step 100: 298.2612, Accuracy: 0.6965\n",
      "Training loss (for one batch) at step 110: 287.1866, Accuracy: 0.6971\n",
      "---- Training ----\n",
      "Training loss: 93.0943\n",
      "Training acc over epoch: 0.6973\n",
      "---- Validation ----\n",
      "Validation loss: 51.4990\n",
      "Validation acc: 0.6437\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 308.6674, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 333.7442, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 20: 292.6218, Accuracy: 0.6611\n",
      "Training loss (for one batch) at step 30: 285.7115, Accuracy: 0.6925\n",
      "Training loss (for one batch) at step 40: 286.9153, Accuracy: 0.7121\n",
      "Training loss (for one batch) at step 50: 267.0659, Accuracy: 0.7210\n",
      "Training loss (for one batch) at step 60: 298.8556, Accuracy: 0.7267\n",
      "Training loss (for one batch) at step 70: 303.4258, Accuracy: 0.7150\n",
      "Training loss (for one batch) at step 80: 305.9243, Accuracy: 0.7022\n",
      "Training loss (for one batch) at step 90: 285.0209, Accuracy: 0.6999\n",
      "Training loss (for one batch) at step 100: 285.0936, Accuracy: 0.7010\n",
      "Training loss (for one batch) at step 110: 317.7206, Accuracy: 0.7009\n",
      "---- Training ----\n",
      "Training loss: 108.3168\n",
      "Training acc over epoch: 0.6998\n",
      "---- Validation ----\n",
      "Validation loss: 62.0848\n",
      "Validation acc: 0.6365\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 328.4640, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 331.2285, Accuracy: 0.6222\n",
      "Training loss (for one batch) at step 20: 295.7762, Accuracy: 0.6611\n",
      "Training loss (for one batch) at step 30: 284.3427, Accuracy: 0.6908\n",
      "Training loss (for one batch) at step 40: 283.4607, Accuracy: 0.7069\n",
      "Training loss (for one batch) at step 50: 283.5343, Accuracy: 0.7204\n",
      "Training loss (for one batch) at step 60: 279.0619, Accuracy: 0.7271\n",
      "Training loss (for one batch) at step 70: 312.1626, Accuracy: 0.7134\n",
      "Training loss (for one batch) at step 80: 315.6913, Accuracy: 0.7007\n",
      "Training loss (for one batch) at step 90: 291.7500, Accuracy: 0.6957\n",
      "Training loss (for one batch) at step 100: 291.3828, Accuracy: 0.6969\n",
      "Training loss (for one batch) at step 110: 303.4969, Accuracy: 0.6969\n",
      "---- Training ----\n",
      "Training loss: 84.8689\n",
      "Training acc over epoch: 0.6954\n",
      "---- Validation ----\n",
      "Validation loss: 49.8800\n",
      "Validation acc: 0.6314\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 325.6389, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 320.0706, Accuracy: 0.6385\n",
      "Training loss (for one batch) at step 20: 295.0969, Accuracy: 0.6607\n",
      "Training loss (for one batch) at step 30: 294.9179, Accuracy: 0.6925\n",
      "Training loss (for one batch) at step 40: 282.1149, Accuracy: 0.7046\n",
      "Training loss (for one batch) at step 50: 278.8050, Accuracy: 0.7177\n",
      "Training loss (for one batch) at step 60: 283.4809, Accuracy: 0.7221\n",
      "Training loss (for one batch) at step 70: 310.2262, Accuracy: 0.7108\n",
      "Training loss (for one batch) at step 80: 312.8152, Accuracy: 0.6982\n",
      "Training loss (for one batch) at step 90: 299.9581, Accuracy: 0.6973\n",
      "Training loss (for one batch) at step 100: 305.2958, Accuracy: 0.6985\n",
      "Training loss (for one batch) at step 110: 305.1586, Accuracy: 0.6972\n",
      "---- Training ----\n",
      "Training loss: 97.3013\n",
      "Training acc over epoch: 0.6963\n",
      "---- Validation ----\n",
      "Validation loss: 36.2292\n",
      "Validation acc: 0.6432\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 328.5027, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 313.0614, Accuracy: 0.6335\n",
      "Training loss (for one batch) at step 20: 301.7736, Accuracy: 0.6659\n",
      "Training loss (for one batch) at step 30: 282.5542, Accuracy: 0.6935\n",
      "Training loss (for one batch) at step 40: 261.5118, Accuracy: 0.7062\n",
      "Training loss (for one batch) at step 50: 267.0110, Accuracy: 0.7197\n",
      "Training loss (for one batch) at step 60: 289.2110, Accuracy: 0.7227\n",
      "Training loss (for one batch) at step 70: 312.1231, Accuracy: 0.7147\n",
      "Training loss (for one batch) at step 80: 316.0671, Accuracy: 0.7018\n",
      "Training loss (for one batch) at step 90: 304.3394, Accuracy: 0.6970\n",
      "Training loss (for one batch) at step 100: 288.0945, Accuracy: 0.6975\n",
      "Training loss (for one batch) at step 110: 294.7043, Accuracy: 0.6993\n",
      "---- Training ----\n",
      "Training loss: 91.2970\n",
      "Training acc over epoch: 0.6967\n",
      "---- Validation ----\n",
      "Validation loss: 75.2150\n",
      "Validation acc: 0.6437\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 300.1818, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 334.1635, Accuracy: 0.6158\n",
      "Training loss (for one batch) at step 20: 283.0598, Accuracy: 0.6648\n",
      "Training loss (for one batch) at step 30: 280.9760, Accuracy: 0.6928\n",
      "Training loss (for one batch) at step 40: 266.4396, Accuracy: 0.7048\n",
      "Training loss (for one batch) at step 50: 290.8057, Accuracy: 0.7172\n",
      "Training loss (for one batch) at step 60: 290.2502, Accuracy: 0.7235\n",
      "Training loss (for one batch) at step 70: 296.0047, Accuracy: 0.7142\n",
      "Training loss (for one batch) at step 80: 322.7255, Accuracy: 0.6999\n",
      "Training loss (for one batch) at step 90: 283.4480, Accuracy: 0.6982\n",
      "Training loss (for one batch) at step 100: 301.5402, Accuracy: 0.6986\n",
      "Training loss (for one batch) at step 110: 325.6041, Accuracy: 0.6970\n",
      "---- Training ----\n",
      "Training loss: 110.9522\n",
      "Training acc over epoch: 0.6967\n",
      "---- Validation ----\n",
      "Validation loss: 52.8537\n",
      "Validation acc: 0.6349\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 307.6752, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 320.3072, Accuracy: 0.6442\n",
      "Training loss (for one batch) at step 20: 290.1954, Accuracy: 0.6693\n",
      "Training loss (for one batch) at step 30: 285.0975, Accuracy: 0.6946\n",
      "Training loss (for one batch) at step 40: 292.4574, Accuracy: 0.7102\n",
      "Training loss (for one batch) at step 50: 278.0476, Accuracy: 0.7215\n",
      "Training loss (for one batch) at step 60: 302.0886, Accuracy: 0.7253\n",
      "Training loss (for one batch) at step 70: 317.7039, Accuracy: 0.7143\n",
      "Training loss (for one batch) at step 80: 289.0139, Accuracy: 0.7036\n",
      "Training loss (for one batch) at step 90: 282.1859, Accuracy: 0.7001\n",
      "Training loss (for one batch) at step 100: 291.2664, Accuracy: 0.7001\n",
      "Training loss (for one batch) at step 110: 308.3301, Accuracy: 0.7000\n",
      "---- Training ----\n",
      "Training loss: 96.3269\n",
      "Training acc over epoch: 0.6997\n",
      "---- Validation ----\n",
      "Validation loss: 41.0739\n",
      "Validation acc: 0.6357\n",
      "Time taken: 10.40s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 313.4621, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 322.3838, Accuracy: 0.6385\n",
      "Training loss (for one batch) at step 20: 278.8358, Accuracy: 0.6667\n",
      "Training loss (for one batch) at step 30: 275.4924, Accuracy: 0.6923\n",
      "Training loss (for one batch) at step 40: 262.8107, Accuracy: 0.7073\n",
      "Training loss (for one batch) at step 50: 259.1125, Accuracy: 0.7189\n",
      "Training loss (for one batch) at step 60: 286.4214, Accuracy: 0.7228\n",
      "Training loss (for one batch) at step 70: 321.9901, Accuracy: 0.7131\n",
      "Training loss (for one batch) at step 80: 316.5040, Accuracy: 0.6988\n",
      "Training loss (for one batch) at step 90: 299.6191, Accuracy: 0.6939\n",
      "Training loss (for one batch) at step 100: 289.3594, Accuracy: 0.6960\n",
      "Training loss (for one batch) at step 110: 318.5475, Accuracy: 0.6976\n",
      "---- Training ----\n",
      "Training loss: 98.7632\n",
      "Training acc over epoch: 0.6963\n",
      "---- Validation ----\n",
      "Validation loss: 41.8892\n",
      "Validation acc: 0.6421\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 327.1082, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 318.8321, Accuracy: 0.6314\n",
      "Training loss (for one batch) at step 20: 307.0473, Accuracy: 0.6637\n",
      "Training loss (for one batch) at step 30: 270.3677, Accuracy: 0.6885\n",
      "Training loss (for one batch) at step 40: 295.2947, Accuracy: 0.7054\n",
      "Training loss (for one batch) at step 50: 270.7784, Accuracy: 0.7184\n",
      "Training loss (for one batch) at step 60: 277.4999, Accuracy: 0.7222\n",
      "Training loss (for one batch) at step 70: 335.5640, Accuracy: 0.7106\n",
      "Training loss (for one batch) at step 80: 315.5509, Accuracy: 0.6986\n",
      "Training loss (for one batch) at step 90: 293.9728, Accuracy: 0.6961\n",
      "Training loss (for one batch) at step 100: 290.3231, Accuracy: 0.6962\n",
      "Training loss (for one batch) at step 110: 304.7285, Accuracy: 0.6981\n",
      "---- Training ----\n",
      "Training loss: 112.7454\n",
      "Training acc over epoch: 0.6972\n",
      "---- Validation ----\n",
      "Validation loss: 51.5784\n",
      "Validation acc: 0.6384\n",
      "Time taken: 10.68s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 334.1876, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 323.1144, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 20: 265.8947, Accuracy: 0.6618\n",
      "Training loss (for one batch) at step 30: 291.2642, Accuracy: 0.6951\n",
      "Training loss (for one batch) at step 40: 283.8112, Accuracy: 0.7113\n",
      "Training loss (for one batch) at step 50: 296.2659, Accuracy: 0.7212\n",
      "Training loss (for one batch) at step 60: 290.1620, Accuracy: 0.7264\n",
      "Training loss (for one batch) at step 70: 319.1221, Accuracy: 0.7143\n",
      "Training loss (for one batch) at step 80: 348.7672, Accuracy: 0.6998\n",
      "Training loss (for one batch) at step 90: 298.8074, Accuracy: 0.6964\n",
      "Training loss (for one batch) at step 100: 289.8666, Accuracy: 0.6970\n",
      "Training loss (for one batch) at step 110: 297.7792, Accuracy: 0.6984\n",
      "---- Training ----\n",
      "Training loss: 99.4044\n",
      "Training acc over epoch: 0.6964\n",
      "---- Validation ----\n",
      "Validation loss: 53.4001\n",
      "Validation acc: 0.6392\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 327.3895, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 307.7068, Accuracy: 0.6392\n",
      "Training loss (for one batch) at step 20: 298.4940, Accuracy: 0.6652\n",
      "Training loss (for one batch) at step 30: 285.3894, Accuracy: 0.6933\n",
      "Training loss (for one batch) at step 40: 285.5229, Accuracy: 0.7075\n",
      "Training loss (for one batch) at step 50: 266.0896, Accuracy: 0.7195\n",
      "Training loss (for one batch) at step 60: 292.0389, Accuracy: 0.7241\n",
      "Training loss (for one batch) at step 70: 314.4994, Accuracy: 0.7119\n",
      "Training loss (for one batch) at step 80: 305.8828, Accuracy: 0.6958\n",
      "Training loss (for one batch) at step 90: 284.1944, Accuracy: 0.6921\n",
      "Training loss (for one batch) at step 100: 278.5903, Accuracy: 0.6942\n",
      "Training loss (for one batch) at step 110: 299.0470, Accuracy: 0.6946\n",
      "---- Training ----\n",
      "Training loss: 98.2371\n",
      "Training acc over epoch: 0.6940\n",
      "---- Validation ----\n",
      "Validation loss: 41.1203\n",
      "Validation acc: 0.6459\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 325.9616, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 316.1461, Accuracy: 0.6286\n",
      "Training loss (for one batch) at step 20: 292.1715, Accuracy: 0.6652\n",
      "Training loss (for one batch) at step 30: 267.2806, Accuracy: 0.6885\n",
      "Training loss (for one batch) at step 40: 285.0722, Accuracy: 0.7029\n",
      "Training loss (for one batch) at step 50: 273.6635, Accuracy: 0.7166\n",
      "Training loss (for one batch) at step 60: 279.0140, Accuracy: 0.7193\n",
      "Training loss (for one batch) at step 70: 309.9435, Accuracy: 0.7120\n",
      "Training loss (for one batch) at step 80: 308.7050, Accuracy: 0.6981\n",
      "Training loss (for one batch) at step 90: 292.0417, Accuracy: 0.6947\n",
      "Training loss (for one batch) at step 100: 283.1263, Accuracy: 0.6959\n",
      "Training loss (for one batch) at step 110: 301.4289, Accuracy: 0.6971\n",
      "---- Training ----\n",
      "Training loss: 109.8448\n",
      "Training acc over epoch: 0.6965\n",
      "---- Validation ----\n",
      "Validation loss: 41.8830\n",
      "Validation acc: 0.6368\n",
      "Time taken: 12.46s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 303.7873, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 299.8839, Accuracy: 0.6349\n",
      "Training loss (for one batch) at step 20: 294.1271, Accuracy: 0.6652\n",
      "Training loss (for one batch) at step 30: 290.9564, Accuracy: 0.6908\n",
      "Training loss (for one batch) at step 40: 273.4720, Accuracy: 0.7069\n",
      "Training loss (for one batch) at step 50: 278.8955, Accuracy: 0.7217\n",
      "Training loss (for one batch) at step 60: 292.6820, Accuracy: 0.7263\n",
      "Training loss (for one batch) at step 70: 293.1231, Accuracy: 0.7146\n",
      "Training loss (for one batch) at step 80: 301.8813, Accuracy: 0.7001\n",
      "Training loss (for one batch) at step 90: 307.5096, Accuracy: 0.6978\n",
      "Training loss (for one batch) at step 100: 288.2551, Accuracy: 0.6984\n",
      "Training loss (for one batch) at step 110: 293.3805, Accuracy: 0.6967\n",
      "---- Training ----\n",
      "Training loss: 94.3400\n",
      "Training acc over epoch: 0.6970\n",
      "---- Validation ----\n",
      "Validation loss: 50.9916\n",
      "Validation acc: 0.6427\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 309.6784, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 329.7626, Accuracy: 0.6165\n",
      "Training loss (for one batch) at step 20: 284.3912, Accuracy: 0.6622\n",
      "Training loss (for one batch) at step 30: 280.0644, Accuracy: 0.6822\n",
      "Training loss (for one batch) at step 40: 267.8490, Accuracy: 0.7050\n",
      "Training loss (for one batch) at step 50: 287.0673, Accuracy: 0.7184\n",
      "Training loss (for one batch) at step 60: 290.3000, Accuracy: 0.7209\n",
      "Training loss (for one batch) at step 70: 291.7708, Accuracy: 0.7106\n",
      "Training loss (for one batch) at step 80: 318.1562, Accuracy: 0.6979\n",
      "Training loss (for one batch) at step 90: 291.2012, Accuracy: 0.6942\n",
      "Training loss (for one batch) at step 100: 286.1453, Accuracy: 0.6952\n",
      "Training loss (for one batch) at step 110: 276.9476, Accuracy: 0.6957\n",
      "---- Training ----\n",
      "Training loss: 89.1175\n",
      "Training acc over epoch: 0.6945\n",
      "---- Validation ----\n",
      "Validation loss: 34.7023\n",
      "Validation acc: 0.6556\n",
      "Time taken: 10.29s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.6477\n",
      "Validation AUC: 0.6515\n",
      "Validation Balanced_ACC: 0.4143\n",
      "Validation MI: 0.1022\n",
      "Validation Normalized MI: 0.1500\n",
      "Validation Adjusted MI: 0.1500\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 659.2661, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 598.0963, Accuracy: 0.5241\n",
      "Training loss (for one batch) at step 20: 534.0239, Accuracy: 0.5268\n",
      "Training loss (for one batch) at step 30: 510.9855, Accuracy: 0.5186\n",
      "Training loss (for one batch) at step 40: 505.1681, Accuracy: 0.5133\n",
      "Training loss (for one batch) at step 50: 514.8218, Accuracy: 0.5083\n",
      "Training loss (for one batch) at step 60: 494.1517, Accuracy: 0.5038\n",
      "Training loss (for one batch) at step 70: 462.9579, Accuracy: 0.5031\n",
      "Training loss (for one batch) at step 80: 476.0633, Accuracy: 0.5038\n",
      "Training loss (for one batch) at step 90: 472.6674, Accuracy: 0.5057\n",
      "Training loss (for one batch) at step 100: 469.9251, Accuracy: 0.5033\n",
      "Training loss (for one batch) at step 110: 458.5880, Accuracy: 0.5051\n",
      "---- Training ----\n",
      "Training loss: 149.5964\n",
      "Training acc over epoch: 0.5060\n",
      "---- Validation ----\n",
      "Validation loss: 34.2209\n",
      "Validation acc: 0.5134\n",
      "Time taken: 12.01s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 479.4261, Accuracy: 0.4766\n",
      "Training loss (for one batch) at step 10: 453.1253, Accuracy: 0.5142\n",
      "Training loss (for one batch) at step 20: 455.4781, Accuracy: 0.5160\n",
      "Training loss (for one batch) at step 30: 457.5296, Accuracy: 0.5161\n",
      "Training loss (for one batch) at step 40: 447.2393, Accuracy: 0.5198\n",
      "Training loss (for one batch) at step 50: 448.8092, Accuracy: 0.5150\n",
      "Training loss (for one batch) at step 60: 450.1353, Accuracy: 0.5146\n",
      "Training loss (for one batch) at step 70: 458.0035, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 80: 451.3072, Accuracy: 0.5196\n",
      "Training loss (for one batch) at step 90: 449.9955, Accuracy: 0.5189\n",
      "Training loss (for one batch) at step 100: 447.5936, Accuracy: 0.5187\n",
      "Training loss (for one batch) at step 110: 447.1085, Accuracy: 0.5191\n",
      "---- Training ----\n",
      "Training loss: 139.2545\n",
      "Training acc over epoch: 0.5189\n",
      "---- Validation ----\n",
      "Validation loss: 34.3260\n",
      "Validation acc: 0.5134\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 446.5291, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 448.1352, Accuracy: 0.5376\n",
      "Training loss (for one batch) at step 20: 449.4023, Accuracy: 0.5201\n",
      "Training loss (for one batch) at step 30: 447.3203, Accuracy: 0.5136\n",
      "Training loss (for one batch) at step 40: 445.4265, Accuracy: 0.5231\n",
      "Training loss (for one batch) at step 50: 448.9271, Accuracy: 0.5224\n",
      "Training loss (for one batch) at step 60: 443.7112, Accuracy: 0.5196\n",
      "Training loss (for one batch) at step 70: 445.1891, Accuracy: 0.5212\n",
      "Training loss (for one batch) at step 80: 445.5098, Accuracy: 0.5255\n",
      "Training loss (for one batch) at step 90: 445.4377, Accuracy: 0.5283\n",
      "Training loss (for one batch) at step 100: 449.2394, Accuracy: 0.5256\n",
      "Training loss (for one batch) at step 110: 443.3371, Accuracy: 0.5229\n",
      "---- Training ----\n",
      "Training loss: 138.3780\n",
      "Training acc over epoch: 0.5233\n",
      "---- Validation ----\n",
      "Validation loss: 34.6676\n",
      "Validation acc: 0.5059\n",
      "Time taken: 10.40s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 446.3224, Accuracy: 0.4844\n",
      "Training loss (for one batch) at step 10: 446.6218, Accuracy: 0.5298\n",
      "Training loss (for one batch) at step 20: 443.5327, Accuracy: 0.5413\n",
      "Training loss (for one batch) at step 30: 443.9053, Accuracy: 0.5401\n",
      "Training loss (for one batch) at step 40: 443.9634, Accuracy: 0.5425\n",
      "Training loss (for one batch) at step 50: 450.0912, Accuracy: 0.5374\n",
      "Training loss (for one batch) at step 60: 445.5726, Accuracy: 0.5347\n",
      "Training loss (for one batch) at step 70: 443.8825, Accuracy: 0.5405\n",
      "Training loss (for one batch) at step 80: 446.1832, Accuracy: 0.5398\n",
      "Training loss (for one batch) at step 90: 446.8929, Accuracy: 0.5390\n",
      "Training loss (for one batch) at step 100: 449.5106, Accuracy: 0.5377\n",
      "Training loss (for one batch) at step 110: 445.1063, Accuracy: 0.5379\n",
      "---- Training ----\n",
      "Training loss: 138.3726\n",
      "Training acc over epoch: 0.5381\n",
      "---- Validation ----\n",
      "Validation loss: 34.5757\n",
      "Validation acc: 0.5677\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 443.5945, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 444.3812, Accuracy: 0.5533\n",
      "Training loss (for one batch) at step 20: 441.9395, Accuracy: 0.5584\n",
      "Training loss (for one batch) at step 30: 443.4194, Accuracy: 0.5643\n",
      "Training loss (for one batch) at step 40: 443.2352, Accuracy: 0.5682\n",
      "Training loss (for one batch) at step 50: 442.8915, Accuracy: 0.5680\n",
      "Training loss (for one batch) at step 60: 443.1793, Accuracy: 0.5629\n",
      "Training loss (for one batch) at step 70: 442.2462, Accuracy: 0.5658\n",
      "Training loss (for one batch) at step 80: 443.9046, Accuracy: 0.5679\n",
      "Training loss (for one batch) at step 90: 443.7951, Accuracy: 0.5665\n",
      "Training loss (for one batch) at step 100: 444.3087, Accuracy: 0.5656\n",
      "Training loss (for one batch) at step 110: 446.1163, Accuracy: 0.5648\n",
      "---- Training ----\n",
      "Training loss: 138.4205\n",
      "Training acc over epoch: 0.5651\n",
      "---- Validation ----\n",
      "Validation loss: 34.6310\n",
      "Validation acc: 0.5661\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 445.8791, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 441.1060, Accuracy: 0.5568\n",
      "Training loss (for one batch) at step 20: 442.5002, Accuracy: 0.5692\n",
      "Training loss (for one batch) at step 30: 440.3193, Accuracy: 0.5683\n",
      "Training loss (for one batch) at step 40: 442.5653, Accuracy: 0.5697\n",
      "Training loss (for one batch) at step 50: 441.8710, Accuracy: 0.5706\n",
      "Training loss (for one batch) at step 60: 443.2484, Accuracy: 0.5685\n",
      "Training loss (for one batch) at step 70: 445.9802, Accuracy: 0.5758\n",
      "Training loss (for one batch) at step 80: 444.8715, Accuracy: 0.5735\n",
      "Training loss (for one batch) at step 90: 442.7647, Accuracy: 0.5753\n",
      "Training loss (for one batch) at step 100: 442.9957, Accuracy: 0.5746\n",
      "Training loss (for one batch) at step 110: 440.8532, Accuracy: 0.5745\n",
      "---- Training ----\n",
      "Training loss: 139.9010\n",
      "Training acc over epoch: 0.5732\n",
      "---- Validation ----\n",
      "Validation loss: 34.6676\n",
      "Validation acc: 0.5607\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 443.3983, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 442.5947, Accuracy: 0.5845\n",
      "Training loss (for one batch) at step 20: 443.6851, Accuracy: 0.5978\n",
      "Training loss (for one batch) at step 30: 445.2210, Accuracy: 0.5932\n",
      "Training loss (for one batch) at step 40: 444.2344, Accuracy: 0.5936\n",
      "Training loss (for one batch) at step 50: 442.0686, Accuracy: 0.5921\n",
      "Training loss (for one batch) at step 60: 440.8222, Accuracy: 0.5932\n",
      "Training loss (for one batch) at step 70: 445.5162, Accuracy: 0.5988\n",
      "Training loss (for one batch) at step 80: 442.4877, Accuracy: 0.5965\n",
      "Training loss (for one batch) at step 90: 441.7841, Accuracy: 0.5949\n",
      "Training loss (for one batch) at step 100: 444.3336, Accuracy: 0.5933\n",
      "Training loss (for one batch) at step 110: 441.2357, Accuracy: 0.5931\n",
      "---- Training ----\n",
      "Training loss: 138.6576\n",
      "Training acc over epoch: 0.5918\n",
      "---- Validation ----\n",
      "Validation loss: 33.9780\n",
      "Validation acc: 0.6233\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 443.3447, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 442.3088, Accuracy: 0.5980\n",
      "Training loss (for one batch) at step 20: 439.5493, Accuracy: 0.6079\n",
      "Training loss (for one batch) at step 30: 444.0780, Accuracy: 0.6046\n",
      "Training loss (for one batch) at step 40: 439.1831, Accuracy: 0.6096\n",
      "Training loss (for one batch) at step 50: 440.3087, Accuracy: 0.6109\n",
      "Training loss (for one batch) at step 60: 440.8419, Accuracy: 0.6103\n",
      "Training loss (for one batch) at step 70: 440.9691, Accuracy: 0.6160\n",
      "Training loss (for one batch) at step 80: 443.2129, Accuracy: 0.6132\n",
      "Training loss (for one batch) at step 90: 442.1843, Accuracy: 0.6074\n",
      "Training loss (for one batch) at step 100: 438.5571, Accuracy: 0.6055\n",
      "Training loss (for one batch) at step 110: 443.4138, Accuracy: 0.6063\n",
      "---- Training ----\n",
      "Training loss: 136.8870\n",
      "Training acc over epoch: 0.6059\n",
      "---- Validation ----\n",
      "Validation loss: 33.9777\n",
      "Validation acc: 0.6341\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 443.1421, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 443.8577, Accuracy: 0.6115\n",
      "Training loss (for one batch) at step 20: 444.4390, Accuracy: 0.6068\n",
      "Training loss (for one batch) at step 30: 439.0238, Accuracy: 0.6008\n",
      "Training loss (for one batch) at step 40: 441.5872, Accuracy: 0.6040\n",
      "Training loss (for one batch) at step 50: 437.0387, Accuracy: 0.6112\n",
      "Training loss (for one batch) at step 60: 443.9073, Accuracy: 0.6160\n",
      "Training loss (for one batch) at step 70: 443.6800, Accuracy: 0.6188\n",
      "Training loss (for one batch) at step 80: 444.1510, Accuracy: 0.6187\n",
      "Training loss (for one batch) at step 90: 440.1955, Accuracy: 0.6196\n",
      "Training loss (for one batch) at step 100: 444.4845, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 110: 441.3437, Accuracy: 0.6173\n",
      "---- Training ----\n",
      "Training loss: 136.8904\n",
      "Training acc over epoch: 0.6178\n",
      "---- Validation ----\n",
      "Validation loss: 34.6363\n",
      "Validation acc: 0.6677\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 439.5539, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 441.1226, Accuracy: 0.6307\n",
      "Training loss (for one batch) at step 20: 441.1729, Accuracy: 0.6269\n",
      "Training loss (for one batch) at step 30: 438.4844, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 40: 435.3656, Accuracy: 0.6303\n",
      "Training loss (for one batch) at step 50: 437.6855, Accuracy: 0.6374\n",
      "Training loss (for one batch) at step 60: 443.3166, Accuracy: 0.6405\n",
      "Training loss (for one batch) at step 70: 444.5543, Accuracy: 0.6439\n",
      "Training loss (for one batch) at step 80: 440.0354, Accuracy: 0.6417\n",
      "Training loss (for one batch) at step 90: 440.5361, Accuracy: 0.6379\n",
      "Training loss (for one batch) at step 100: 438.6338, Accuracy: 0.6347\n",
      "Training loss (for one batch) at step 110: 442.5545, Accuracy: 0.6356\n",
      "---- Training ----\n",
      "Training loss: 138.4146\n",
      "Training acc over epoch: 0.6375\n",
      "---- Validation ----\n",
      "Validation loss: 34.2987\n",
      "Validation acc: 0.6572\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 436.9530, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 445.0157, Accuracy: 0.6385\n",
      "Training loss (for one batch) at step 20: 443.2238, Accuracy: 0.6391\n",
      "Training loss (for one batch) at step 30: 438.6967, Accuracy: 0.6505\n",
      "Training loss (for one batch) at step 40: 438.4478, Accuracy: 0.6570\n",
      "Training loss (for one batch) at step 50: 433.6349, Accuracy: 0.6656\n",
      "Training loss (for one batch) at step 60: 435.1302, Accuracy: 0.6670\n",
      "Training loss (for one batch) at step 70: 438.7166, Accuracy: 0.6715\n",
      "Training loss (for one batch) at step 80: 440.3983, Accuracy: 0.6686\n",
      "Training loss (for one batch) at step 90: 441.8190, Accuracy: 0.6622\n",
      "Training loss (for one batch) at step 100: 437.3434, Accuracy: 0.6593\n",
      "Training loss (for one batch) at step 110: 440.3230, Accuracy: 0.6586\n",
      "---- Training ----\n",
      "Training loss: 135.4671\n",
      "Training acc over epoch: 0.6595\n",
      "---- Validation ----\n",
      "Validation loss: 35.4212\n",
      "Validation acc: 0.6588\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 437.0471, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 439.7130, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 20: 442.8040, Accuracy: 0.6674\n",
      "Training loss (for one batch) at step 30: 428.1306, Accuracy: 0.6663\n",
      "Training loss (for one batch) at step 40: 434.1428, Accuracy: 0.6730\n",
      "Training loss (for one batch) at step 50: 430.9037, Accuracy: 0.6840\n",
      "Training loss (for one batch) at step 60: 441.6559, Accuracy: 0.6838\n",
      "Training loss (for one batch) at step 70: 444.9591, Accuracy: 0.6839\n",
      "Training loss (for one batch) at step 80: 440.5140, Accuracy: 0.6817\n",
      "Training loss (for one batch) at step 90: 437.9526, Accuracy: 0.6805\n",
      "Training loss (for one batch) at step 100: 431.5952, Accuracy: 0.6795\n",
      "Training loss (for one batch) at step 110: 434.3929, Accuracy: 0.6800\n",
      "---- Training ----\n",
      "Training loss: 136.2445\n",
      "Training acc over epoch: 0.6810\n",
      "---- Validation ----\n",
      "Validation loss: 35.6351\n",
      "Validation acc: 0.6945\n",
      "Time taken: 10.64s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 440.6284, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 439.2879, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 20: 432.9077, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 30: 430.1025, Accuracy: 0.6782\n",
      "Training loss (for one batch) at step 40: 431.5697, Accuracy: 0.6850\n",
      "Training loss (for one batch) at step 50: 429.2184, Accuracy: 0.6900\n",
      "Training loss (for one batch) at step 60: 426.0573, Accuracy: 0.6972\n",
      "Training loss (for one batch) at step 70: 440.5701, Accuracy: 0.6985\n",
      "Training loss (for one batch) at step 80: 437.4265, Accuracy: 0.6922\n",
      "Training loss (for one batch) at step 90: 437.7296, Accuracy: 0.6923\n",
      "Training loss (for one batch) at step 100: 434.1855, Accuracy: 0.6898\n",
      "Training loss (for one batch) at step 110: 437.9791, Accuracy: 0.6917\n",
      "---- Training ----\n",
      "Training loss: 136.3044\n",
      "Training acc over epoch: 0.6920\n",
      "---- Validation ----\n",
      "Validation loss: 36.7517\n",
      "Validation acc: 0.6639\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 440.7630, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 439.1595, Accuracy: 0.6740\n",
      "Training loss (for one batch) at step 20: 438.8206, Accuracy: 0.6838\n",
      "Training loss (for one batch) at step 30: 427.9734, Accuracy: 0.6993\n",
      "Training loss (for one batch) at step 40: 433.7045, Accuracy: 0.6991\n",
      "Training loss (for one batch) at step 50: 436.0558, Accuracy: 0.7057\n",
      "Training loss (for one batch) at step 60: 431.1773, Accuracy: 0.7122\n",
      "Training loss (for one batch) at step 70: 441.5369, Accuracy: 0.7149\n",
      "Training loss (for one batch) at step 80: 440.5269, Accuracy: 0.7114\n",
      "Training loss (for one batch) at step 90: 437.6111, Accuracy: 0.7093\n",
      "Training loss (for one batch) at step 100: 437.8285, Accuracy: 0.7075\n",
      "Training loss (for one batch) at step 110: 435.7659, Accuracy: 0.7090\n",
      "---- Training ----\n",
      "Training loss: 133.1991\n",
      "Training acc over epoch: 0.7091\n",
      "---- Validation ----\n",
      "Validation loss: 34.5829\n",
      "Validation acc: 0.6996\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 437.6362, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 440.4725, Accuracy: 0.6982\n",
      "Training loss (for one batch) at step 20: 436.0752, Accuracy: 0.6983\n",
      "Training loss (for one batch) at step 30: 432.5243, Accuracy: 0.7069\n",
      "Training loss (for one batch) at step 40: 419.3690, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 50: 406.4357, Accuracy: 0.7305\n",
      "Training loss (for one batch) at step 60: 438.4783, Accuracy: 0.7342\n",
      "Training loss (for one batch) at step 70: 437.0115, Accuracy: 0.7314\n",
      "Training loss (for one batch) at step 80: 439.5238, Accuracy: 0.7282\n",
      "Training loss (for one batch) at step 90: 433.6056, Accuracy: 0.7254\n",
      "Training loss (for one batch) at step 100: 434.2669, Accuracy: 0.7258\n",
      "Training loss (for one batch) at step 110: 434.2770, Accuracy: 0.7253\n",
      "---- Training ----\n",
      "Training loss: 131.4570\n",
      "Training acc over epoch: 0.7262\n",
      "---- Validation ----\n",
      "Validation loss: 35.4223\n",
      "Validation acc: 0.7391\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 439.0375, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 440.2617, Accuracy: 0.7372\n",
      "Training loss (for one batch) at step 20: 437.0164, Accuracy: 0.7254\n",
      "Training loss (for one batch) at step 30: 422.8495, Accuracy: 0.7326\n",
      "Training loss (for one batch) at step 40: 428.7631, Accuracy: 0.7376\n",
      "Training loss (for one batch) at step 50: 412.3899, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 60: 427.2947, Accuracy: 0.7526\n",
      "Training loss (for one batch) at step 70: 440.5793, Accuracy: 0.7531\n",
      "Training loss (for one batch) at step 80: 436.3125, Accuracy: 0.7491\n",
      "Training loss (for one batch) at step 90: 430.2664, Accuracy: 0.7479\n",
      "Training loss (for one batch) at step 100: 424.7660, Accuracy: 0.7453\n",
      "Training loss (for one batch) at step 110: 427.9275, Accuracy: 0.7456\n",
      "---- Training ----\n",
      "Training loss: 138.5300\n",
      "Training acc over epoch: 0.7446\n",
      "---- Validation ----\n",
      "Validation loss: 37.0173\n",
      "Validation acc: 0.7429\n",
      "Time taken: 10.40s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 439.3893, Accuracy: 0.8594\n",
      "Training loss (for one batch) at step 10: 431.1592, Accuracy: 0.7727\n",
      "Training loss (for one batch) at step 20: 425.7464, Accuracy: 0.7667\n",
      "Training loss (for one batch) at step 30: 423.9657, Accuracy: 0.7692\n",
      "Training loss (for one batch) at step 40: 421.7817, Accuracy: 0.7757\n",
      "Training loss (for one batch) at step 50: 412.2620, Accuracy: 0.7748\n",
      "Training loss (for one batch) at step 60: 422.1143, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 70: 431.1620, Accuracy: 0.7792\n",
      "Training loss (for one batch) at step 80: 434.6383, Accuracy: 0.7684\n",
      "Training loss (for one batch) at step 90: 425.6763, Accuracy: 0.7653\n",
      "Training loss (for one batch) at step 100: 435.0959, Accuracy: 0.7626\n",
      "Training loss (for one batch) at step 110: 427.7478, Accuracy: 0.7623\n",
      "---- Training ----\n",
      "Training loss: 136.0399\n",
      "Training acc over epoch: 0.7623\n",
      "---- Validation ----\n",
      "Validation loss: 40.4171\n",
      "Validation acc: 0.7378\n",
      "Time taken: 10.18s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 441.1943, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 432.3594, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 20: 427.0146, Accuracy: 0.7608\n",
      "Training loss (for one batch) at step 30: 417.5154, Accuracy: 0.7636\n",
      "Training loss (for one batch) at step 40: 420.2180, Accuracy: 0.7660\n",
      "Training loss (for one batch) at step 50: 406.3152, Accuracy: 0.7725\n",
      "Training loss (for one batch) at step 60: 412.0268, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 70: 422.8553, Accuracy: 0.7755\n",
      "Training loss (for one batch) at step 80: 436.5132, Accuracy: 0.7704\n",
      "Training loss (for one batch) at step 90: 423.6223, Accuracy: 0.7657\n",
      "Training loss (for one batch) at step 100: 420.3670, Accuracy: 0.7632\n",
      "Training loss (for one batch) at step 110: 423.8437, Accuracy: 0.7658\n",
      "---- Training ----\n",
      "Training loss: 130.1960\n",
      "Training acc over epoch: 0.7655\n",
      "---- Validation ----\n",
      "Validation loss: 36.0781\n",
      "Validation acc: 0.7633\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 441.2263, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 432.8044, Accuracy: 0.7678\n",
      "Training loss (for one batch) at step 20: 431.2231, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 30: 412.8120, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 40: 418.7764, Accuracy: 0.7729\n",
      "Training loss (for one batch) at step 50: 395.4096, Accuracy: 0.7839\n",
      "Training loss (for one batch) at step 60: 435.7275, Accuracy: 0.7866\n",
      "Training loss (for one batch) at step 70: 440.5100, Accuracy: 0.7842\n",
      "Training loss (for one batch) at step 80: 431.6820, Accuracy: 0.7788\n",
      "Training loss (for one batch) at step 90: 421.8918, Accuracy: 0.7750\n",
      "Training loss (for one batch) at step 100: 421.9646, Accuracy: 0.7728\n",
      "Training loss (for one batch) at step 110: 423.1635, Accuracy: 0.7721\n",
      "---- Training ----\n",
      "Training loss: 129.4637\n",
      "Training acc over epoch: 0.7716\n",
      "---- Validation ----\n",
      "Validation loss: 36.2779\n",
      "Validation acc: 0.7305\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 445.0175, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 424.6309, Accuracy: 0.7756\n",
      "Training loss (for one batch) at step 20: 422.7173, Accuracy: 0.7690\n",
      "Training loss (for one batch) at step 30: 432.2202, Accuracy: 0.7747\n",
      "Training loss (for one batch) at step 40: 409.0509, Accuracy: 0.7790\n",
      "Training loss (for one batch) at step 50: 390.4445, Accuracy: 0.7906\n",
      "Training loss (for one batch) at step 60: 410.2044, Accuracy: 0.7930\n",
      "Training loss (for one batch) at step 70: 425.1997, Accuracy: 0.7934\n",
      "Training loss (for one batch) at step 80: 435.1450, Accuracy: 0.7889\n",
      "Training loss (for one batch) at step 90: 422.4080, Accuracy: 0.7831\n",
      "Training loss (for one batch) at step 100: 418.3460, Accuracy: 0.7828\n",
      "Training loss (for one batch) at step 110: 418.3415, Accuracy: 0.7816\n",
      "---- Training ----\n",
      "Training loss: 127.0676\n",
      "Training acc over epoch: 0.7818\n",
      "---- Validation ----\n",
      "Validation loss: 37.8835\n",
      "Validation acc: 0.7789\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 441.8434, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 427.5727, Accuracy: 0.7876\n",
      "Training loss (for one batch) at step 20: 418.9269, Accuracy: 0.7772\n",
      "Training loss (for one batch) at step 30: 406.5488, Accuracy: 0.7833\n",
      "Training loss (for one batch) at step 40: 404.5857, Accuracy: 0.7898\n",
      "Training loss (for one batch) at step 50: 384.8611, Accuracy: 0.7972\n",
      "Training loss (for one batch) at step 60: 411.6541, Accuracy: 0.8051\n",
      "Training loss (for one batch) at step 70: 419.5908, Accuracy: 0.8064\n",
      "Training loss (for one batch) at step 80: 421.3116, Accuracy: 0.8017\n",
      "Training loss (for one batch) at step 90: 417.9508, Accuracy: 0.7980\n",
      "Training loss (for one batch) at step 100: 420.0143, Accuracy: 0.7946\n",
      "Training loss (for one batch) at step 110: 422.0671, Accuracy: 0.7938\n",
      "---- Training ----\n",
      "Training loss: 134.5863\n",
      "Training acc over epoch: 0.7936\n",
      "---- Validation ----\n",
      "Validation loss: 34.9577\n",
      "Validation acc: 0.7254\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 435.3179, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 426.3499, Accuracy: 0.7756\n",
      "Training loss (for one batch) at step 20: 419.2206, Accuracy: 0.7679\n",
      "Training loss (for one batch) at step 30: 408.3536, Accuracy: 0.7797\n",
      "Training loss (for one batch) at step 40: 402.5269, Accuracy: 0.7908\n",
      "Training loss (for one batch) at step 50: 394.7445, Accuracy: 0.7999\n",
      "Training loss (for one batch) at step 60: 396.4877, Accuracy: 0.8061\n",
      "Training loss (for one batch) at step 70: 424.8128, Accuracy: 0.8038\n",
      "Training loss (for one batch) at step 80: 415.1977, Accuracy: 0.7989\n",
      "Training loss (for one batch) at step 90: 416.8615, Accuracy: 0.7946\n",
      "Training loss (for one batch) at step 100: 410.0551, Accuracy: 0.7941\n",
      "Training loss (for one batch) at step 110: 425.5456, Accuracy: 0.7959\n",
      "---- Training ----\n",
      "Training loss: 121.2342\n",
      "Training acc over epoch: 0.7951\n",
      "---- Validation ----\n",
      "Validation loss: 40.7616\n",
      "Validation acc: 0.7477\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 431.9693, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 426.2869, Accuracy: 0.7784\n",
      "Training loss (for one batch) at step 20: 417.4717, Accuracy: 0.7820\n",
      "Training loss (for one batch) at step 30: 415.2690, Accuracy: 0.7883\n",
      "Training loss (for one batch) at step 40: 404.5692, Accuracy: 0.7980\n",
      "Training loss (for one batch) at step 50: 376.4250, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 60: 393.3443, Accuracy: 0.8172\n",
      "Training loss (for one batch) at step 70: 421.6700, Accuracy: 0.8151\n",
      "Training loss (for one batch) at step 80: 417.7085, Accuracy: 0.8077\n",
      "Training loss (for one batch) at step 90: 407.0949, Accuracy: 0.8033\n",
      "Training loss (for one batch) at step 100: 402.4205, Accuracy: 0.8023\n",
      "Training loss (for one batch) at step 110: 402.9961, Accuracy: 0.8012\n",
      "---- Training ----\n",
      "Training loss: 124.8669\n",
      "Training acc over epoch: 0.8012\n",
      "---- Validation ----\n",
      "Validation loss: 37.7657\n",
      "Validation acc: 0.7609\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 428.0154, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 416.4727, Accuracy: 0.7791\n",
      "Training loss (for one batch) at step 20: 408.2724, Accuracy: 0.7865\n",
      "Training loss (for one batch) at step 30: 412.5218, Accuracy: 0.7928\n",
      "Training loss (for one batch) at step 40: 392.0347, Accuracy: 0.8032\n",
      "Training loss (for one batch) at step 50: 385.3737, Accuracy: 0.8148\n",
      "Training loss (for one batch) at step 60: 375.1031, Accuracy: 0.8183\n",
      "Training loss (for one batch) at step 70: 414.6339, Accuracy: 0.8160\n",
      "Training loss (for one batch) at step 80: 428.3134, Accuracy: 0.8111\n",
      "Training loss (for one batch) at step 90: 410.2289, Accuracy: 0.8074\n",
      "Training loss (for one batch) at step 100: 400.3562, Accuracy: 0.8052\n",
      "Training loss (for one batch) at step 110: 412.4587, Accuracy: 0.8051\n",
      "---- Training ----\n",
      "Training loss: 123.6857\n",
      "Training acc over epoch: 0.8049\n",
      "---- Validation ----\n",
      "Validation loss: 37.0335\n",
      "Validation acc: 0.7504\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 439.6550, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 409.5417, Accuracy: 0.7791\n",
      "Training loss (for one batch) at step 20: 404.6449, Accuracy: 0.7954\n",
      "Training loss (for one batch) at step 30: 404.7110, Accuracy: 0.8004\n",
      "Training loss (for one batch) at step 40: 395.3745, Accuracy: 0.8070\n",
      "Training loss (for one batch) at step 50: 368.1838, Accuracy: 0.8150\n",
      "Training loss (for one batch) at step 60: 370.0840, Accuracy: 0.8213\n",
      "Training loss (for one batch) at step 70: 408.7173, Accuracy: 0.8186\n",
      "Training loss (for one batch) at step 80: 431.1438, Accuracy: 0.8109\n",
      "Training loss (for one batch) at step 90: 410.8311, Accuracy: 0.8081\n",
      "Training loss (for one batch) at step 100: 399.6500, Accuracy: 0.8093\n",
      "Training loss (for one batch) at step 110: 390.7332, Accuracy: 0.8089\n",
      "---- Training ----\n",
      "Training loss: 132.8767\n",
      "Training acc over epoch: 0.8082\n",
      "---- Validation ----\n",
      "Validation loss: 35.1636\n",
      "Validation acc: 0.7641\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 424.9326, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 432.0751, Accuracy: 0.7784\n",
      "Training loss (for one batch) at step 20: 389.6048, Accuracy: 0.7831\n",
      "Training loss (for one batch) at step 30: 392.1157, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 40: 382.0680, Accuracy: 0.8072\n",
      "Training loss (for one batch) at step 50: 369.2558, Accuracy: 0.8205\n",
      "Training loss (for one batch) at step 60: 383.4456, Accuracy: 0.8253\n",
      "Training loss (for one batch) at step 70: 416.1918, Accuracy: 0.8241\n",
      "Training loss (for one batch) at step 80: 422.7503, Accuracy: 0.8180\n",
      "Training loss (for one batch) at step 90: 385.0466, Accuracy: 0.8164\n",
      "Training loss (for one batch) at step 100: 394.9105, Accuracy: 0.8138\n",
      "Training loss (for one batch) at step 110: 399.1249, Accuracy: 0.8143\n",
      "---- Training ----\n",
      "Training loss: 120.5554\n",
      "Training acc over epoch: 0.8136\n",
      "---- Validation ----\n",
      "Validation loss: 31.3285\n",
      "Validation acc: 0.7638\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 447.7668, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 424.1089, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 20: 387.2009, Accuracy: 0.7943\n",
      "Training loss (for one batch) at step 30: 390.0623, Accuracy: 0.8014\n",
      "Training loss (for one batch) at step 40: 397.8446, Accuracy: 0.8114\n",
      "Training loss (for one batch) at step 50: 371.2044, Accuracy: 0.8234\n",
      "Training loss (for one batch) at step 60: 380.8816, Accuracy: 0.8325\n",
      "Training loss (for one batch) at step 70: 395.1638, Accuracy: 0.8272\n",
      "Training loss (for one batch) at step 80: 398.3411, Accuracy: 0.8205\n",
      "Training loss (for one batch) at step 90: 385.0369, Accuracy: 0.8172\n",
      "Training loss (for one batch) at step 100: 390.7647, Accuracy: 0.8167\n",
      "Training loss (for one batch) at step 110: 413.8136, Accuracy: 0.8151\n",
      "---- Training ----\n",
      "Training loss: 129.6692\n",
      "Training acc over epoch: 0.8137\n",
      "---- Validation ----\n",
      "Validation loss: 30.6354\n",
      "Validation acc: 0.7534\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 405.9148, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 414.2168, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 20: 402.1489, Accuracy: 0.8017\n",
      "Training loss (for one batch) at step 30: 378.2094, Accuracy: 0.8072\n",
      "Training loss (for one batch) at step 40: 380.4297, Accuracy: 0.8176\n",
      "Training loss (for one batch) at step 50: 357.8190, Accuracy: 0.8304\n",
      "Training loss (for one batch) at step 60: 369.7057, Accuracy: 0.8336\n",
      "Training loss (for one batch) at step 70: 403.7612, Accuracy: 0.8311\n",
      "Training loss (for one batch) at step 80: 431.0742, Accuracy: 0.8221\n",
      "Training loss (for one batch) at step 90: 395.8371, Accuracy: 0.8201\n",
      "Training loss (for one batch) at step 100: 376.2282, Accuracy: 0.8214\n",
      "Training loss (for one batch) at step 110: 402.1505, Accuracy: 0.8203\n",
      "---- Training ----\n",
      "Training loss: 128.9559\n",
      "Training acc over epoch: 0.8195\n",
      "---- Validation ----\n",
      "Validation loss: 38.1968\n",
      "Validation acc: 0.7552\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 430.2042, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 396.6665, Accuracy: 0.7791\n",
      "Training loss (for one batch) at step 20: 385.5297, Accuracy: 0.7883\n",
      "Training loss (for one batch) at step 30: 407.8903, Accuracy: 0.8014\n",
      "Training loss (for one batch) at step 40: 392.3773, Accuracy: 0.8102\n",
      "Training loss (for one batch) at step 50: 373.8337, Accuracy: 0.8234\n",
      "Training loss (for one batch) at step 60: 375.8609, Accuracy: 0.8286\n",
      "Training loss (for one batch) at step 70: 396.2361, Accuracy: 0.8266\n",
      "Training loss (for one batch) at step 80: 417.0653, Accuracy: 0.8186\n",
      "Training loss (for one batch) at step 90: 388.7869, Accuracy: 0.8168\n",
      "Training loss (for one batch) at step 100: 383.6743, Accuracy: 0.8161\n",
      "Training loss (for one batch) at step 110: 383.4807, Accuracy: 0.8174\n",
      "---- Training ----\n",
      "Training loss: 119.1751\n",
      "Training acc over epoch: 0.8148\n",
      "---- Validation ----\n",
      "Validation loss: 40.6697\n",
      "Validation acc: 0.7703\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 406.7613, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 390.9919, Accuracy: 0.7848\n",
      "Training loss (for one batch) at step 20: 381.5289, Accuracy: 0.7980\n",
      "Training loss (for one batch) at step 30: 375.3069, Accuracy: 0.8107\n",
      "Training loss (for one batch) at step 40: 369.0424, Accuracy: 0.8190\n",
      "Training loss (for one batch) at step 50: 354.6823, Accuracy: 0.8287\n",
      "Training loss (for one batch) at step 60: 382.8032, Accuracy: 0.8320\n",
      "Training loss (for one batch) at step 70: 392.2808, Accuracy: 0.8294\n",
      "Training loss (for one batch) at step 80: 405.1298, Accuracy: 0.8230\n",
      "Training loss (for one batch) at step 90: 396.7121, Accuracy: 0.8205\n",
      "Training loss (for one batch) at step 100: 377.9649, Accuracy: 0.8215\n",
      "Training loss (for one batch) at step 110: 393.6131, Accuracy: 0.8214\n",
      "---- Training ----\n",
      "Training loss: 118.4530\n",
      "Training acc over epoch: 0.8210\n",
      "---- Validation ----\n",
      "Validation loss: 38.3427\n",
      "Validation acc: 0.7628\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 426.9449, Accuracy: 0.8516\n",
      "Training loss (for one batch) at step 10: 398.8411, Accuracy: 0.7912\n",
      "Training loss (for one batch) at step 20: 368.9765, Accuracy: 0.8028\n",
      "Training loss (for one batch) at step 30: 379.2066, Accuracy: 0.8130\n",
      "Training loss (for one batch) at step 40: 351.5552, Accuracy: 0.8270\n",
      "Training loss (for one batch) at step 50: 346.5702, Accuracy: 0.8381\n",
      "Training loss (for one batch) at step 60: 366.0934, Accuracy: 0.8439\n",
      "Training loss (for one batch) at step 70: 412.0679, Accuracy: 0.8392\n",
      "Training loss (for one batch) at step 80: 397.1788, Accuracy: 0.8339\n",
      "Training loss (for one batch) at step 90: 385.5557, Accuracy: 0.8289\n",
      "Training loss (for one batch) at step 100: 375.0717, Accuracy: 0.8289\n",
      "Training loss (for one batch) at step 110: 386.2921, Accuracy: 0.8283\n",
      "---- Training ----\n",
      "Training loss: 136.8424\n",
      "Training acc over epoch: 0.8264\n",
      "---- Validation ----\n",
      "Validation loss: 36.3966\n",
      "Validation acc: 0.7504\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 412.5513, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 373.7935, Accuracy: 0.7784\n",
      "Training loss (for one batch) at step 20: 376.5186, Accuracy: 0.7917\n",
      "Training loss (for one batch) at step 30: 351.1578, Accuracy: 0.8042\n",
      "Training loss (for one batch) at step 40: 347.5839, Accuracy: 0.8197\n",
      "Training loss (for one batch) at step 50: 351.3017, Accuracy: 0.8332\n",
      "Training loss (for one batch) at step 60: 358.1600, Accuracy: 0.8399\n",
      "Training loss (for one batch) at step 70: 376.8779, Accuracy: 0.8349\n",
      "Training loss (for one batch) at step 80: 391.6094, Accuracy: 0.8247\n",
      "Training loss (for one batch) at step 90: 372.6961, Accuracy: 0.8219\n",
      "Training loss (for one batch) at step 100: 363.3999, Accuracy: 0.8233\n",
      "Training loss (for one batch) at step 110: 395.0570, Accuracy: 0.8233\n",
      "---- Training ----\n",
      "Training loss: 116.0371\n",
      "Training acc over epoch: 0.8225\n",
      "---- Validation ----\n",
      "Validation loss: 41.5802\n",
      "Validation acc: 0.7719\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 406.6316, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 388.2922, Accuracy: 0.7926\n",
      "Training loss (for one batch) at step 20: 394.0848, Accuracy: 0.8051\n",
      "Training loss (for one batch) at step 30: 375.5799, Accuracy: 0.8155\n",
      "Training loss (for one batch) at step 40: 350.6216, Accuracy: 0.8287\n",
      "Training loss (for one batch) at step 50: 324.1857, Accuracy: 0.8405\n",
      "Training loss (for one batch) at step 60: 348.8026, Accuracy: 0.8466\n",
      "Training loss (for one batch) at step 70: 387.0594, Accuracy: 0.8391\n",
      "Training loss (for one batch) at step 80: 397.4916, Accuracy: 0.8294\n",
      "Training loss (for one batch) at step 90: 362.1671, Accuracy: 0.8258\n",
      "Training loss (for one batch) at step 100: 352.2620, Accuracy: 0.8256\n",
      "Training loss (for one batch) at step 110: 373.9710, Accuracy: 0.8259\n",
      "---- Training ----\n",
      "Training loss: 113.6680\n",
      "Training acc over epoch: 0.8249\n",
      "---- Validation ----\n",
      "Validation loss: 50.0651\n",
      "Validation acc: 0.7544\n",
      "Time taken: 12.23s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 409.1810, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 402.0185, Accuracy: 0.7862\n",
      "Training loss (for one batch) at step 20: 369.7706, Accuracy: 0.8043\n",
      "Training loss (for one batch) at step 30: 369.3379, Accuracy: 0.8153\n",
      "Training loss (for one batch) at step 40: 364.1685, Accuracy: 0.8264\n",
      "Training loss (for one batch) at step 50: 342.3385, Accuracy: 0.8358\n",
      "Training loss (for one batch) at step 60: 372.3004, Accuracy: 0.8407\n",
      "Training loss (for one batch) at step 70: 376.2753, Accuracy: 0.8343\n",
      "Training loss (for one batch) at step 80: 389.4466, Accuracy: 0.8279\n",
      "Training loss (for one batch) at step 90: 367.8063, Accuracy: 0.8243\n",
      "Training loss (for one batch) at step 100: 349.4708, Accuracy: 0.8250\n",
      "Training loss (for one batch) at step 110: 370.3247, Accuracy: 0.8258\n",
      "---- Training ----\n",
      "Training loss: 113.8814\n",
      "Training acc over epoch: 0.8252\n",
      "---- Validation ----\n",
      "Validation loss: 39.8071\n",
      "Validation acc: 0.7644\n",
      "Time taken: 12.76s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 380.1535, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 402.1433, Accuracy: 0.7820\n",
      "Training loss (for one batch) at step 20: 354.0195, Accuracy: 0.7943\n",
      "Training loss (for one batch) at step 30: 359.5295, Accuracy: 0.8110\n",
      "Training loss (for one batch) at step 40: 351.0306, Accuracy: 0.8220\n",
      "Training loss (for one batch) at step 50: 335.9458, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 60: 382.8644, Accuracy: 0.8414\n",
      "Training loss (for one batch) at step 70: 397.8712, Accuracy: 0.8318\n",
      "Training loss (for one batch) at step 80: 419.7193, Accuracy: 0.8233\n",
      "Training loss (for one batch) at step 90: 357.8327, Accuracy: 0.8197\n",
      "Training loss (for one batch) at step 100: 344.0626, Accuracy: 0.8221\n",
      "Training loss (for one batch) at step 110: 351.6327, Accuracy: 0.8249\n",
      "---- Training ----\n",
      "Training loss: 123.5930\n",
      "Training acc over epoch: 0.8230\n",
      "---- Validation ----\n",
      "Validation loss: 35.5921\n",
      "Validation acc: 0.7560\n",
      "Time taken: 11.57s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 385.0190, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 10: 386.1282, Accuracy: 0.7749\n",
      "Training loss (for one batch) at step 20: 350.6890, Accuracy: 0.8054\n",
      "Training loss (for one batch) at step 30: 349.8548, Accuracy: 0.8165\n",
      "Training loss (for one batch) at step 40: 338.1567, Accuracy: 0.8277\n",
      "Training loss (for one batch) at step 50: 329.7301, Accuracy: 0.8367\n",
      "Training loss (for one batch) at step 60: 349.6011, Accuracy: 0.8425\n",
      "Training loss (for one batch) at step 70: 398.0773, Accuracy: 0.8345\n",
      "Training loss (for one batch) at step 80: 386.8148, Accuracy: 0.8280\n",
      "Training loss (for one batch) at step 90: 329.9502, Accuracy: 0.8247\n",
      "Training loss (for one batch) at step 100: 344.3298, Accuracy: 0.8271\n",
      "Training loss (for one batch) at step 110: 353.3024, Accuracy: 0.8275\n",
      "---- Training ----\n",
      "Training loss: 123.7553\n",
      "Training acc over epoch: 0.8266\n",
      "---- Validation ----\n",
      "Validation loss: 37.1829\n",
      "Validation acc: 0.7550\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 398.5286, Accuracy: 0.8438\n",
      "Training loss (for one batch) at step 10: 383.9316, Accuracy: 0.7685\n",
      "Training loss (for one batch) at step 20: 358.1205, Accuracy: 0.8051\n",
      "Training loss (for one batch) at step 30: 348.4557, Accuracy: 0.8185\n",
      "Training loss (for one batch) at step 40: 336.1020, Accuracy: 0.8316\n",
      "Training loss (for one batch) at step 50: 335.9069, Accuracy: 0.8401\n",
      "Training loss (for one batch) at step 60: 357.1631, Accuracy: 0.8449\n",
      "Training loss (for one batch) at step 70: 382.7083, Accuracy: 0.8368\n",
      "Training loss (for one batch) at step 80: 374.7619, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 90: 348.9767, Accuracy: 0.8240\n",
      "Training loss (for one batch) at step 100: 346.1362, Accuracy: 0.8256\n",
      "Training loss (for one batch) at step 110: 353.0916, Accuracy: 0.8257\n",
      "---- Training ----\n",
      "Training loss: 122.4935\n",
      "Training acc over epoch: 0.8260\n",
      "---- Validation ----\n",
      "Validation loss: 37.2511\n",
      "Validation acc: 0.7622\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 398.6628, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 385.7697, Accuracy: 0.7727\n",
      "Training loss (for one batch) at step 20: 347.5135, Accuracy: 0.8021\n",
      "Training loss (for one batch) at step 30: 345.7090, Accuracy: 0.8196\n",
      "Training loss (for one batch) at step 40: 337.2059, Accuracy: 0.8340\n",
      "Training loss (for one batch) at step 50: 335.5177, Accuracy: 0.8441\n",
      "Training loss (for one batch) at step 60: 359.9843, Accuracy: 0.8480\n",
      "Training loss (for one batch) at step 70: 379.1578, Accuracy: 0.8363\n",
      "Training loss (for one batch) at step 80: 377.4723, Accuracy: 0.8272\n",
      "Training loss (for one batch) at step 90: 359.8532, Accuracy: 0.8237\n",
      "Training loss (for one batch) at step 100: 333.2743, Accuracy: 0.8241\n",
      "Training loss (for one batch) at step 110: 356.5164, Accuracy: 0.8246\n",
      "---- Training ----\n",
      "Training loss: 117.9390\n",
      "Training acc over epoch: 0.8234\n",
      "---- Validation ----\n",
      "Validation loss: 43.4934\n",
      "Validation acc: 0.7636\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 386.5427, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 390.1668, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 20: 327.1374, Accuracy: 0.7976\n",
      "Training loss (for one batch) at step 30: 344.1267, Accuracy: 0.8178\n",
      "Training loss (for one batch) at step 40: 336.7575, Accuracy: 0.8306\n",
      "Training loss (for one batch) at step 50: 329.3497, Accuracy: 0.8431\n",
      "Training loss (for one batch) at step 60: 330.7772, Accuracy: 0.8482\n",
      "Training loss (for one batch) at step 70: 367.7049, Accuracy: 0.8426\n",
      "Training loss (for one batch) at step 80: 369.4937, Accuracy: 0.8332\n",
      "Training loss (for one batch) at step 90: 365.5601, Accuracy: 0.8280\n",
      "Training loss (for one batch) at step 100: 327.3151, Accuracy: 0.8291\n",
      "Training loss (for one batch) at step 110: 338.0720, Accuracy: 0.8302\n",
      "---- Training ----\n",
      "Training loss: 119.8689\n",
      "Training acc over epoch: 0.8300\n",
      "---- Validation ----\n",
      "Validation loss: 33.9486\n",
      "Validation acc: 0.7679\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 380.1360, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 377.3032, Accuracy: 0.7820\n",
      "Training loss (for one batch) at step 20: 344.2789, Accuracy: 0.7987\n",
      "Training loss (for one batch) at step 30: 330.9362, Accuracy: 0.8243\n",
      "Training loss (for one batch) at step 40: 337.8031, Accuracy: 0.8375\n",
      "Training loss (for one batch) at step 50: 339.0958, Accuracy: 0.8494\n",
      "Training loss (for one batch) at step 60: 343.5507, Accuracy: 0.8557\n",
      "Training loss (for one batch) at step 70: 364.9202, Accuracy: 0.8431\n",
      "Training loss (for one batch) at step 80: 386.5942, Accuracy: 0.8313\n",
      "Training loss (for one batch) at step 90: 344.0822, Accuracy: 0.8282\n",
      "Training loss (for one batch) at step 100: 334.5871, Accuracy: 0.8306\n",
      "Training loss (for one batch) at step 110: 375.1484, Accuracy: 0.8318\n",
      "---- Training ----\n",
      "Training loss: 101.6561\n",
      "Training acc over epoch: 0.8309\n",
      "---- Validation ----\n",
      "Validation loss: 46.8844\n",
      "Validation acc: 0.7684\n",
      "Time taken: 10.54s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 376.5168, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 385.2565, Accuracy: 0.7699\n",
      "Training loss (for one batch) at step 20: 344.5788, Accuracy: 0.7909\n",
      "Training loss (for one batch) at step 30: 326.5031, Accuracy: 0.8105\n",
      "Training loss (for one batch) at step 40: 335.2802, Accuracy: 0.8302\n",
      "Training loss (for one batch) at step 50: 308.3350, Accuracy: 0.8444\n",
      "Training loss (for one batch) at step 60: 335.8416, Accuracy: 0.8493\n",
      "Training loss (for one batch) at step 70: 373.3791, Accuracy: 0.8428\n",
      "Training loss (for one batch) at step 80: 357.6683, Accuracy: 0.8326\n",
      "Training loss (for one batch) at step 90: 332.3081, Accuracy: 0.8290\n",
      "Training loss (for one batch) at step 100: 340.6358, Accuracy: 0.8300\n",
      "Training loss (for one batch) at step 110: 348.2180, Accuracy: 0.8288\n",
      "---- Training ----\n",
      "Training loss: 108.7456\n",
      "Training acc over epoch: 0.8282\n",
      "---- Validation ----\n",
      "Validation loss: 51.5507\n",
      "Validation acc: 0.7671\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 382.1511, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 385.9024, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 20: 342.5292, Accuracy: 0.8054\n",
      "Training loss (for one batch) at step 30: 326.8053, Accuracy: 0.8228\n",
      "Training loss (for one batch) at step 40: 321.2199, Accuracy: 0.8378\n",
      "Training loss (for one batch) at step 50: 319.3321, Accuracy: 0.8499\n",
      "Training loss (for one batch) at step 60: 337.8447, Accuracy: 0.8540\n",
      "Training loss (for one batch) at step 70: 362.2169, Accuracy: 0.8461\n",
      "Training loss (for one batch) at step 80: 391.1987, Accuracy: 0.8351\n",
      "Training loss (for one batch) at step 90: 344.7827, Accuracy: 0.8309\n",
      "Training loss (for one batch) at step 100: 340.1013, Accuracy: 0.8340\n",
      "Training loss (for one batch) at step 110: 358.8172, Accuracy: 0.8328\n",
      "---- Training ----\n",
      "Training loss: 112.5479\n",
      "Training acc over epoch: 0.8322\n",
      "---- Validation ----\n",
      "Validation loss: 36.6115\n",
      "Validation acc: 0.7614\n",
      "Time taken: 10.54s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 382.3734, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 379.2614, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 20: 340.0223, Accuracy: 0.7954\n",
      "Training loss (for one batch) at step 30: 330.0919, Accuracy: 0.8140\n",
      "Training loss (for one batch) at step 40: 322.7744, Accuracy: 0.8304\n",
      "Training loss (for one batch) at step 50: 315.6282, Accuracy: 0.8422\n",
      "Training loss (for one batch) at step 60: 336.8171, Accuracy: 0.8493\n",
      "Training loss (for one batch) at step 70: 374.4610, Accuracy: 0.8417\n",
      "Training loss (for one batch) at step 80: 373.4292, Accuracy: 0.8295\n",
      "Training loss (for one batch) at step 90: 321.7331, Accuracy: 0.8255\n",
      "Training loss (for one batch) at step 100: 326.0594, Accuracy: 0.8282\n",
      "Training loss (for one batch) at step 110: 348.2444, Accuracy: 0.8284\n",
      "---- Training ----\n",
      "Training loss: 113.7145\n",
      "Training acc over epoch: 0.8274\n",
      "---- Validation ----\n",
      "Validation loss: 39.3380\n",
      "Validation acc: 0.7644\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 366.3627, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 363.3859, Accuracy: 0.7741\n",
      "Training loss (for one batch) at step 20: 325.3647, Accuracy: 0.7965\n",
      "Training loss (for one batch) at step 30: 344.3993, Accuracy: 0.8120\n",
      "Training loss (for one batch) at step 40: 315.5220, Accuracy: 0.8312\n",
      "Training loss (for one batch) at step 50: 303.0357, Accuracy: 0.8448\n",
      "Training loss (for one batch) at step 60: 349.2842, Accuracy: 0.8491\n",
      "Training loss (for one batch) at step 70: 370.4297, Accuracy: 0.8398\n",
      "Training loss (for one batch) at step 80: 371.9517, Accuracy: 0.8289\n",
      "Training loss (for one batch) at step 90: 331.5723, Accuracy: 0.8264\n",
      "Training loss (for one batch) at step 100: 322.9893, Accuracy: 0.8303\n",
      "Training loss (for one batch) at step 110: 353.0498, Accuracy: 0.8305\n",
      "---- Training ----\n",
      "Training loss: 113.7497\n",
      "Training acc over epoch: 0.8299\n",
      "---- Validation ----\n",
      "Validation loss: 45.1175\n",
      "Validation acc: 0.7687\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 384.8402, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 373.2050, Accuracy: 0.7749\n",
      "Training loss (for one batch) at step 20: 345.5745, Accuracy: 0.7984\n",
      "Training loss (for one batch) at step 30: 335.3374, Accuracy: 0.8193\n",
      "Training loss (for one batch) at step 40: 322.8488, Accuracy: 0.8335\n",
      "Training loss (for one batch) at step 50: 318.9592, Accuracy: 0.8482\n",
      "Training loss (for one batch) at step 60: 338.0700, Accuracy: 0.8516\n",
      "Training loss (for one batch) at step 70: 359.1020, Accuracy: 0.8420\n",
      "Training loss (for one batch) at step 80: 368.9113, Accuracy: 0.8316\n",
      "Training loss (for one batch) at step 90: 340.7841, Accuracy: 0.8285\n",
      "Training loss (for one batch) at step 100: 316.3150, Accuracy: 0.8313\n",
      "Training loss (for one batch) at step 110: 338.5720, Accuracy: 0.8309\n",
      "---- Training ----\n",
      "Training loss: 107.0278\n",
      "Training acc over epoch: 0.8298\n",
      "---- Validation ----\n",
      "Validation loss: 54.7448\n",
      "Validation acc: 0.7560\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 381.0266, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 354.1911, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 20: 340.5226, Accuracy: 0.7913\n",
      "Training loss (for one batch) at step 30: 320.8424, Accuracy: 0.8185\n",
      "Training loss (for one batch) at step 40: 324.1332, Accuracy: 0.8357\n",
      "Training loss (for one batch) at step 50: 306.1629, Accuracy: 0.8456\n",
      "Training loss (for one batch) at step 60: 306.4596, Accuracy: 0.8489\n",
      "Training loss (for one batch) at step 70: 340.0827, Accuracy: 0.8412\n",
      "Training loss (for one batch) at step 80: 380.9713, Accuracy: 0.8295\n",
      "Training loss (for one batch) at step 90: 324.1347, Accuracy: 0.8255\n",
      "Training loss (for one batch) at step 100: 313.0023, Accuracy: 0.8298\n",
      "Training loss (for one batch) at step 110: 344.1317, Accuracy: 0.8316\n",
      "---- Training ----\n",
      "Training loss: 111.6102\n",
      "Training acc over epoch: 0.8302\n",
      "---- Validation ----\n",
      "Validation loss: 44.3530\n",
      "Validation acc: 0.7689\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 361.8422, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 339.3021, Accuracy: 0.7784\n",
      "Training loss (for one batch) at step 20: 330.5092, Accuracy: 0.8028\n",
      "Training loss (for one batch) at step 30: 320.0746, Accuracy: 0.8216\n",
      "Training loss (for one batch) at step 40: 304.4786, Accuracy: 0.8371\n",
      "Training loss (for one batch) at step 50: 316.7358, Accuracy: 0.8487\n",
      "Training loss (for one batch) at step 60: 322.1992, Accuracy: 0.8539\n",
      "Training loss (for one batch) at step 70: 359.5956, Accuracy: 0.8419\n",
      "Training loss (for one batch) at step 80: 354.7728, Accuracy: 0.8302\n",
      "Training loss (for one batch) at step 90: 313.6531, Accuracy: 0.8275\n",
      "Training loss (for one batch) at step 100: 335.5600, Accuracy: 0.8306\n",
      "Training loss (for one batch) at step 110: 343.7269, Accuracy: 0.8302\n",
      "---- Training ----\n",
      "Training loss: 100.7391\n",
      "Training acc over epoch: 0.8299\n",
      "---- Validation ----\n",
      "Validation loss: 39.9745\n",
      "Validation acc: 0.7574\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 387.6802, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 372.8944, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 20: 324.5287, Accuracy: 0.7935\n",
      "Training loss (for one batch) at step 30: 331.2612, Accuracy: 0.8170\n",
      "Training loss (for one batch) at step 40: 331.5459, Accuracy: 0.8314\n",
      "Training loss (for one batch) at step 50: 303.4506, Accuracy: 0.8436\n",
      "Training loss (for one batch) at step 60: 334.5593, Accuracy: 0.8472\n",
      "Training loss (for one batch) at step 70: 351.8941, Accuracy: 0.8393\n",
      "Training loss (for one batch) at step 80: 384.7642, Accuracy: 0.8309\n",
      "Training loss (for one batch) at step 90: 342.5274, Accuracy: 0.8264\n",
      "Training loss (for one batch) at step 100: 338.0846, Accuracy: 0.8297\n",
      "Training loss (for one batch) at step 110: 353.9833, Accuracy: 0.8300\n",
      "---- Training ----\n",
      "Training loss: 115.4292\n",
      "Training acc over epoch: 0.8287\n",
      "---- Validation ----\n",
      "Validation loss: 38.5512\n",
      "Validation acc: 0.7630\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 354.1013, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 345.0583, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 20: 344.3099, Accuracy: 0.8058\n",
      "Training loss (for one batch) at step 30: 317.3382, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 40: 293.8051, Accuracy: 0.8415\n",
      "Training loss (for one batch) at step 50: 317.6377, Accuracy: 0.8519\n",
      "Training loss (for one batch) at step 60: 338.0407, Accuracy: 0.8545\n",
      "Training loss (for one batch) at step 70: 345.0709, Accuracy: 0.8426\n",
      "Training loss (for one batch) at step 80: 379.4175, Accuracy: 0.8311\n",
      "Training loss (for one batch) at step 90: 316.1012, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 100: 312.2560, Accuracy: 0.8315\n",
      "Training loss (for one batch) at step 110: 336.8354, Accuracy: 0.8312\n",
      "---- Training ----\n",
      "Training loss: 102.6276\n",
      "Training acc over epoch: 0.8301\n",
      "---- Validation ----\n",
      "Validation loss: 42.3325\n",
      "Validation acc: 0.7585\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 349.6721, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 10: 352.4738, Accuracy: 0.7855\n",
      "Training loss (for one batch) at step 20: 330.5334, Accuracy: 0.8006\n",
      "Training loss (for one batch) at step 30: 333.3254, Accuracy: 0.8228\n",
      "Training loss (for one batch) at step 40: 318.0057, Accuracy: 0.8382\n",
      "Training loss (for one batch) at step 50: 309.8419, Accuracy: 0.8511\n",
      "Training loss (for one batch) at step 60: 316.8146, Accuracy: 0.8562\n",
      "Training loss (for one batch) at step 70: 338.4966, Accuracy: 0.8455\n",
      "Training loss (for one batch) at step 80: 381.8792, Accuracy: 0.8341\n",
      "Training loss (for one batch) at step 90: 322.8448, Accuracy: 0.8303\n",
      "Training loss (for one batch) at step 100: 311.7483, Accuracy: 0.8311\n",
      "Training loss (for one batch) at step 110: 339.9923, Accuracy: 0.8316\n",
      "---- Training ----\n",
      "Training loss: 104.1735\n",
      "Training acc over epoch: 0.8307\n",
      "---- Validation ----\n",
      "Validation loss: 45.7385\n",
      "Validation acc: 0.7590\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 354.3381, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 332.8950, Accuracy: 0.7805\n",
      "Training loss (for one batch) at step 20: 320.1783, Accuracy: 0.8021\n",
      "Training loss (for one batch) at step 30: 322.2815, Accuracy: 0.8196\n",
      "Training loss (for one batch) at step 40: 313.0781, Accuracy: 0.8361\n",
      "Training loss (for one batch) at step 50: 302.9639, Accuracy: 0.8477\n",
      "Training loss (for one batch) at step 60: 311.2834, Accuracy: 0.8528\n",
      "Training loss (for one batch) at step 70: 380.1270, Accuracy: 0.8443\n",
      "Training loss (for one batch) at step 80: 369.5972, Accuracy: 0.8312\n",
      "Training loss (for one batch) at step 90: 307.0517, Accuracy: 0.8283\n",
      "Training loss (for one batch) at step 100: 310.3608, Accuracy: 0.8309\n",
      "Training loss (for one batch) at step 110: 341.4050, Accuracy: 0.8312\n",
      "---- Training ----\n",
      "Training loss: 107.6371\n",
      "Training acc over epoch: 0.8303\n",
      "---- Validation ----\n",
      "Validation loss: 54.5663\n",
      "Validation acc: 0.7555\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 356.2049, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 347.2787, Accuracy: 0.7756\n",
      "Training loss (for one batch) at step 20: 325.1635, Accuracy: 0.7999\n",
      "Training loss (for one batch) at step 30: 319.2826, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 40: 298.7528, Accuracy: 0.8388\n",
      "Training loss (for one batch) at step 50: 302.8277, Accuracy: 0.8493\n",
      "Training loss (for one batch) at step 60: 310.8672, Accuracy: 0.8541\n",
      "Training loss (for one batch) at step 70: 355.7474, Accuracy: 0.8428\n",
      "Training loss (for one batch) at step 80: 353.3492, Accuracy: 0.8316\n",
      "Training loss (for one batch) at step 90: 313.0602, Accuracy: 0.8279\n",
      "Training loss (for one batch) at step 100: 322.2249, Accuracy: 0.8295\n",
      "Training loss (for one batch) at step 110: 325.4424, Accuracy: 0.8307\n",
      "---- Training ----\n",
      "Training loss: 110.7650\n",
      "Training acc over epoch: 0.8296\n",
      "---- Validation ----\n",
      "Validation loss: 36.7580\n",
      "Validation acc: 0.7512\n",
      "Time taken: 10.92s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 363.6883, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 353.1706, Accuracy: 0.7777\n",
      "Training loss (for one batch) at step 20: 322.0643, Accuracy: 0.8062\n",
      "Training loss (for one batch) at step 30: 299.3551, Accuracy: 0.8226\n",
      "Training loss (for one batch) at step 40: 315.9697, Accuracy: 0.8407\n",
      "Training loss (for one batch) at step 50: 302.1304, Accuracy: 0.8542\n",
      "Training loss (for one batch) at step 60: 323.4519, Accuracy: 0.8560\n",
      "Training loss (for one batch) at step 70: 369.4288, Accuracy: 0.8447\n",
      "Training loss (for one batch) at step 80: 371.5956, Accuracy: 0.8347\n",
      "Training loss (for one batch) at step 90: 333.4621, Accuracy: 0.8298\n",
      "Training loss (for one batch) at step 100: 313.8403, Accuracy: 0.8329\n",
      "Training loss (for one batch) at step 110: 327.6395, Accuracy: 0.8326\n",
      "---- Training ----\n",
      "Training loss: 109.6896\n",
      "Training acc over epoch: 0.8311\n",
      "---- Validation ----\n",
      "Validation loss: 42.6369\n",
      "Validation acc: 0.7577\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 345.9060, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 354.0009, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 20: 306.3888, Accuracy: 0.7954\n",
      "Training loss (for one batch) at step 30: 307.5435, Accuracy: 0.8193\n",
      "Training loss (for one batch) at step 40: 306.0880, Accuracy: 0.8378\n",
      "Training loss (for one batch) at step 50: 298.0361, Accuracy: 0.8502\n",
      "Training loss (for one batch) at step 60: 328.3831, Accuracy: 0.8523\n",
      "Training loss (for one batch) at step 70: 347.3732, Accuracy: 0.8412\n",
      "Training loss (for one batch) at step 80: 361.1186, Accuracy: 0.8295\n",
      "Training loss (for one batch) at step 90: 327.4591, Accuracy: 0.8286\n",
      "Training loss (for one batch) at step 100: 314.8869, Accuracy: 0.8318\n",
      "Training loss (for one batch) at step 110: 334.9662, Accuracy: 0.8315\n",
      "---- Training ----\n",
      "Training loss: 106.2930\n",
      "Training acc over epoch: 0.8301\n",
      "---- Validation ----\n",
      "Validation loss: 42.6614\n",
      "Validation acc: 0.7483\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 362.8694, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 340.7518, Accuracy: 0.7699\n",
      "Training loss (for one batch) at step 20: 319.1459, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 30: 301.7896, Accuracy: 0.8178\n",
      "Training loss (for one batch) at step 40: 298.7846, Accuracy: 0.8327\n",
      "Training loss (for one batch) at step 50: 312.4343, Accuracy: 0.8471\n",
      "Training loss (for one batch) at step 60: 323.6581, Accuracy: 0.8509\n",
      "Training loss (for one batch) at step 70: 341.8594, Accuracy: 0.8412\n",
      "Training loss (for one batch) at step 80: 355.2393, Accuracy: 0.8313\n",
      "Training loss (for one batch) at step 90: 317.2661, Accuracy: 0.8274\n",
      "Training loss (for one batch) at step 100: 319.8624, Accuracy: 0.8311\n",
      "Training loss (for one batch) at step 110: 336.9577, Accuracy: 0.8318\n",
      "---- Training ----\n",
      "Training loss: 107.1647\n",
      "Training acc over epoch: 0.8307\n",
      "---- Validation ----\n",
      "Validation loss: 48.4600\n",
      "Validation acc: 0.7603\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 348.7568, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 335.9276, Accuracy: 0.7713\n",
      "Training loss (for one batch) at step 20: 326.1436, Accuracy: 0.8013\n",
      "Training loss (for one batch) at step 30: 319.3230, Accuracy: 0.8266\n",
      "Training loss (for one batch) at step 40: 330.0302, Accuracy: 0.8417\n",
      "Training loss (for one batch) at step 50: 307.4553, Accuracy: 0.8542\n",
      "Training loss (for one batch) at step 60: 317.3954, Accuracy: 0.8577\n",
      "Training loss (for one batch) at step 70: 335.8168, Accuracy: 0.8489\n",
      "Training loss (for one batch) at step 80: 326.1687, Accuracy: 0.8364\n",
      "Training loss (for one batch) at step 90: 316.8812, Accuracy: 0.8319\n",
      "Training loss (for one batch) at step 100: 295.5449, Accuracy: 0.8345\n",
      "Training loss (for one batch) at step 110: 320.1511, Accuracy: 0.8321\n",
      "---- Training ----\n",
      "Training loss: 98.4420\n",
      "Training acc over epoch: 0.8316\n",
      "---- Validation ----\n",
      "Validation loss: 51.2802\n",
      "Validation acc: 0.7577\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 382.9459, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 357.8329, Accuracy: 0.7784\n",
      "Training loss (for one batch) at step 20: 319.8684, Accuracy: 0.8028\n",
      "Training loss (for one batch) at step 30: 295.2116, Accuracy: 0.8236\n",
      "Training loss (for one batch) at step 40: 297.7411, Accuracy: 0.8418\n",
      "Training loss (for one batch) at step 50: 290.5281, Accuracy: 0.8523\n",
      "Training loss (for one batch) at step 60: 306.0402, Accuracy: 0.8580\n",
      "Training loss (for one batch) at step 70: 361.9045, Accuracy: 0.8434\n",
      "Training loss (for one batch) at step 80: 332.9402, Accuracy: 0.8318\n",
      "Training loss (for one batch) at step 90: 303.2932, Accuracy: 0.8279\n",
      "Training loss (for one batch) at step 100: 308.0685, Accuracy: 0.8315\n",
      "Training loss (for one batch) at step 110: 319.8600, Accuracy: 0.8322\n",
      "---- Training ----\n",
      "Training loss: 111.1723\n",
      "Training acc over epoch: 0.8307\n",
      "---- Validation ----\n",
      "Validation loss: 66.8254\n",
      "Validation acc: 0.7638\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 368.2775, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 334.9496, Accuracy: 0.7727\n",
      "Training loss (for one batch) at step 20: 307.9439, Accuracy: 0.8028\n",
      "Training loss (for one batch) at step 30: 316.5306, Accuracy: 0.8238\n",
      "Training loss (for one batch) at step 40: 307.4628, Accuracy: 0.8390\n",
      "Training loss (for one batch) at step 50: 294.1957, Accuracy: 0.8514\n",
      "Training loss (for one batch) at step 60: 325.7660, Accuracy: 0.8564\n",
      "Training loss (for one batch) at step 70: 335.3067, Accuracy: 0.8465\n",
      "Training loss (for one batch) at step 80: 345.9008, Accuracy: 0.8348\n",
      "Training loss (for one batch) at step 90: 298.7709, Accuracy: 0.8314\n",
      "Training loss (for one batch) at step 100: 306.1116, Accuracy: 0.8338\n",
      "Training loss (for one batch) at step 110: 307.1397, Accuracy: 0.8341\n",
      "---- Training ----\n",
      "Training loss: 111.2981\n",
      "Training acc over epoch: 0.8331\n",
      "---- Validation ----\n",
      "Validation loss: 41.6301\n",
      "Validation acc: 0.7638\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 370.3217, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 332.1222, Accuracy: 0.7805\n",
      "Training loss (for one batch) at step 20: 325.1413, Accuracy: 0.8039\n",
      "Training loss (for one batch) at step 30: 304.1892, Accuracy: 0.8281\n",
      "Training loss (for one batch) at step 40: 312.6476, Accuracy: 0.8449\n",
      "Training loss (for one batch) at step 50: 295.9057, Accuracy: 0.8555\n",
      "Training loss (for one batch) at step 60: 307.9169, Accuracy: 0.8587\n",
      "Training loss (for one batch) at step 70: 323.7766, Accuracy: 0.8483\n",
      "Training loss (for one batch) at step 80: 339.1920, Accuracy: 0.8360\n",
      "Training loss (for one batch) at step 90: 321.1409, Accuracy: 0.8322\n",
      "Training loss (for one batch) at step 100: 296.7798, Accuracy: 0.8338\n",
      "Training loss (for one batch) at step 110: 320.4363, Accuracy: 0.8342\n",
      "---- Training ----\n",
      "Training loss: 106.1268\n",
      "Training acc over epoch: 0.8327\n",
      "---- Validation ----\n",
      "Validation loss: 44.8580\n",
      "Validation acc: 0.7663\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 340.1474, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 332.1490, Accuracy: 0.7642\n",
      "Training loss (for one batch) at step 20: 316.6260, Accuracy: 0.7991\n",
      "Training loss (for one batch) at step 30: 306.0619, Accuracy: 0.8201\n",
      "Training loss (for one batch) at step 40: 293.1306, Accuracy: 0.8380\n",
      "Training loss (for one batch) at step 50: 282.4198, Accuracy: 0.8529\n",
      "Training loss (for one batch) at step 60: 319.5907, Accuracy: 0.8592\n",
      "Training loss (for one batch) at step 70: 358.7457, Accuracy: 0.8467\n",
      "Training loss (for one batch) at step 80: 345.4198, Accuracy: 0.8356\n",
      "Training loss (for one batch) at step 90: 291.9045, Accuracy: 0.8328\n",
      "Training loss (for one batch) at step 100: 313.2230, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 110: 316.3957, Accuracy: 0.8366\n",
      "---- Training ----\n",
      "Training loss: 104.1636\n",
      "Training acc over epoch: 0.8346\n",
      "---- Validation ----\n",
      "Validation loss: 46.9753\n",
      "Validation acc: 0.7528\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 366.4660, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 332.0050, Accuracy: 0.7706\n",
      "Training loss (for one batch) at step 20: 320.5469, Accuracy: 0.8028\n",
      "Training loss (for one batch) at step 30: 304.2566, Accuracy: 0.8223\n",
      "Training loss (for one batch) at step 40: 285.5566, Accuracy: 0.8365\n",
      "Training loss (for one batch) at step 50: 294.1404, Accuracy: 0.8496\n",
      "Training loss (for one batch) at step 60: 308.5252, Accuracy: 0.8534\n",
      "Training loss (for one batch) at step 70: 327.0685, Accuracy: 0.8444\n",
      "Training loss (for one batch) at step 80: 329.4633, Accuracy: 0.8318\n",
      "Training loss (for one batch) at step 90: 318.1855, Accuracy: 0.8278\n",
      "Training loss (for one batch) at step 100: 300.9168, Accuracy: 0.8317\n",
      "Training loss (for one batch) at step 110: 309.8107, Accuracy: 0.8315\n",
      "---- Training ----\n",
      "Training loss: 96.1424\n",
      "Training acc over epoch: 0.8313\n",
      "---- Validation ----\n",
      "Validation loss: 42.0633\n",
      "Validation acc: 0.7603\n",
      "Time taken: 10.68s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 367.7414, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 343.4676, Accuracy: 0.7706\n",
      "Training loss (for one batch) at step 20: 321.5784, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 30: 304.3908, Accuracy: 0.8269\n",
      "Training loss (for one batch) at step 40: 302.2507, Accuracy: 0.8405\n",
      "Training loss (for one batch) at step 50: 288.0201, Accuracy: 0.8529\n",
      "Training loss (for one batch) at step 60: 302.8924, Accuracy: 0.8573\n",
      "Training loss (for one batch) at step 70: 363.9539, Accuracy: 0.8486\n",
      "Training loss (for one batch) at step 80: 342.4129, Accuracy: 0.8360\n",
      "Training loss (for one batch) at step 90: 297.9084, Accuracy: 0.8328\n",
      "Training loss (for one batch) at step 100: 304.9484, Accuracy: 0.8356\n",
      "Training loss (for one batch) at step 110: 309.9944, Accuracy: 0.8353\n",
      "---- Training ----\n",
      "Training loss: 112.8106\n",
      "Training acc over epoch: 0.8336\n",
      "---- Validation ----\n",
      "Validation loss: 67.6978\n",
      "Validation acc: 0.7582\n",
      "Time taken: 10.16s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 327.8106, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 331.3161, Accuracy: 0.7869\n",
      "Training loss (for one batch) at step 20: 309.4503, Accuracy: 0.8039\n",
      "Training loss (for one batch) at step 30: 305.5761, Accuracy: 0.8233\n",
      "Training loss (for one batch) at step 40: 295.6908, Accuracy: 0.8397\n",
      "Training loss (for one batch) at step 50: 286.2639, Accuracy: 0.8531\n",
      "Training loss (for one batch) at step 60: 319.3603, Accuracy: 0.8546\n",
      "Training loss (for one batch) at step 70: 333.7785, Accuracy: 0.8439\n",
      "Training loss (for one batch) at step 80: 358.9461, Accuracy: 0.8329\n",
      "Training loss (for one batch) at step 90: 309.4612, Accuracy: 0.8296\n",
      "Training loss (for one batch) at step 100: 296.9341, Accuracy: 0.8324\n",
      "Training loss (for one batch) at step 110: 320.6845, Accuracy: 0.8334\n",
      "---- Training ----\n",
      "Training loss: 104.5678\n",
      "Training acc over epoch: 0.8322\n",
      "---- Validation ----\n",
      "Validation loss: 42.1025\n",
      "Validation acc: 0.7474\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 350.7762, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 353.0241, Accuracy: 0.7685\n",
      "Training loss (for one batch) at step 20: 299.8678, Accuracy: 0.7976\n",
      "Training loss (for one batch) at step 30: 291.1463, Accuracy: 0.8266\n",
      "Training loss (for one batch) at step 40: 303.8850, Accuracy: 0.8396\n",
      "Training loss (for one batch) at step 50: 288.9500, Accuracy: 0.8528\n",
      "Training loss (for one batch) at step 60: 302.6798, Accuracy: 0.8567\n",
      "Training loss (for one batch) at step 70: 349.9065, Accuracy: 0.8475\n",
      "Training loss (for one batch) at step 80: 351.3281, Accuracy: 0.8364\n",
      "Training loss (for one batch) at step 90: 307.3438, Accuracy: 0.8310\n",
      "Training loss (for one batch) at step 100: 288.4560, Accuracy: 0.8339\n",
      "Training loss (for one batch) at step 110: 317.1531, Accuracy: 0.8344\n",
      "---- Training ----\n",
      "Training loss: 101.2813\n",
      "Training acc over epoch: 0.8336\n",
      "---- Validation ----\n",
      "Validation loss: 51.2325\n",
      "Validation acc: 0.7740\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 339.9785, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 332.0352, Accuracy: 0.7571\n",
      "Training loss (for one batch) at step 20: 333.8852, Accuracy: 0.7946\n",
      "Training loss (for one batch) at step 30: 292.2027, Accuracy: 0.8198\n",
      "Training loss (for one batch) at step 40: 280.3877, Accuracy: 0.8371\n",
      "Training loss (for one batch) at step 50: 287.3347, Accuracy: 0.8505\n",
      "Training loss (for one batch) at step 60: 326.9149, Accuracy: 0.8549\n",
      "Training loss (for one batch) at step 70: 349.1414, Accuracy: 0.8458\n",
      "Training loss (for one batch) at step 80: 336.8353, Accuracy: 0.8343\n",
      "Training loss (for one batch) at step 90: 309.6172, Accuracy: 0.8304\n",
      "Training loss (for one batch) at step 100: 302.9470, Accuracy: 0.8338\n",
      "Training loss (for one batch) at step 110: 309.4966, Accuracy: 0.8352\n",
      "---- Training ----\n",
      "Training loss: 101.1453\n",
      "Training acc over epoch: 0.8342\n",
      "---- Validation ----\n",
      "Validation loss: 43.8788\n",
      "Validation acc: 0.7679\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 330.4229, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 343.3500, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 20: 318.2560, Accuracy: 0.7898\n",
      "Training loss (for one batch) at step 30: 312.2626, Accuracy: 0.8193\n",
      "Training loss (for one batch) at step 40: 295.0742, Accuracy: 0.8392\n",
      "Training loss (for one batch) at step 50: 276.8911, Accuracy: 0.8516\n",
      "Training loss (for one batch) at step 60: 299.5099, Accuracy: 0.8562\n",
      "Training loss (for one batch) at step 70: 334.8134, Accuracy: 0.8483\n",
      "Training loss (for one batch) at step 80: 339.9507, Accuracy: 0.8369\n",
      "Training loss (for one batch) at step 90: 305.0880, Accuracy: 0.8326\n",
      "Training loss (for one batch) at step 100: 293.1320, Accuracy: 0.8346\n",
      "Training loss (for one batch) at step 110: 322.6252, Accuracy: 0.8355\n",
      "---- Training ----\n",
      "Training loss: 103.8949\n",
      "Training acc over epoch: 0.8347\n",
      "---- Validation ----\n",
      "Validation loss: 52.0078\n",
      "Validation acc: 0.7684\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 347.3873, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 363.4811, Accuracy: 0.7699\n",
      "Training loss (for one batch) at step 20: 310.1959, Accuracy: 0.7976\n",
      "Training loss (for one batch) at step 30: 296.6210, Accuracy: 0.8248\n",
      "Training loss (for one batch) at step 40: 273.3903, Accuracy: 0.8422\n",
      "Training loss (for one batch) at step 50: 282.1852, Accuracy: 0.8559\n",
      "Training loss (for one batch) at step 60: 287.5271, Accuracy: 0.8584\n",
      "Training loss (for one batch) at step 70: 339.2047, Accuracy: 0.8456\n",
      "Training loss (for one batch) at step 80: 341.1332, Accuracy: 0.8332\n",
      "Training loss (for one batch) at step 90: 324.2651, Accuracy: 0.8284\n",
      "Training loss (for one batch) at step 100: 304.0140, Accuracy: 0.8321\n",
      "Training loss (for one batch) at step 110: 317.4760, Accuracy: 0.8315\n",
      "---- Training ----\n",
      "Training loss: 99.5436\n",
      "Training acc over epoch: 0.8310\n",
      "---- Validation ----\n",
      "Validation loss: 42.6387\n",
      "Validation acc: 0.7609\n",
      "Time taken: 10.61s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 343.7682, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 335.7014, Accuracy: 0.7784\n",
      "Training loss (for one batch) at step 20: 301.3677, Accuracy: 0.7950\n",
      "Training loss (for one batch) at step 30: 303.0613, Accuracy: 0.8198\n",
      "Training loss (for one batch) at step 40: 294.3859, Accuracy: 0.8337\n",
      "Training loss (for one batch) at step 50: 283.9868, Accuracy: 0.8468\n",
      "Training loss (for one batch) at step 60: 307.7134, Accuracy: 0.8525\n",
      "Training loss (for one batch) at step 70: 349.8351, Accuracy: 0.8433\n",
      "Training loss (for one batch) at step 80: 340.6735, Accuracy: 0.8319\n",
      "Training loss (for one batch) at step 90: 320.8335, Accuracy: 0.8290\n",
      "Training loss (for one batch) at step 100: 304.0935, Accuracy: 0.8321\n",
      "Training loss (for one batch) at step 110: 313.6270, Accuracy: 0.8320\n",
      "---- Training ----\n",
      "Training loss: 99.1129\n",
      "Training acc over epoch: 0.8311\n",
      "---- Validation ----\n",
      "Validation loss: 53.5792\n",
      "Validation acc: 0.7652\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 347.9897, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 337.7515, Accuracy: 0.7784\n",
      "Training loss (for one batch) at step 20: 302.3914, Accuracy: 0.8080\n",
      "Training loss (for one batch) at step 30: 301.9489, Accuracy: 0.8286\n",
      "Training loss (for one batch) at step 40: 297.9221, Accuracy: 0.8424\n",
      "Training loss (for one batch) at step 50: 293.3442, Accuracy: 0.8539\n",
      "Training loss (for one batch) at step 60: 314.9214, Accuracy: 0.8586\n",
      "Training loss (for one batch) at step 70: 330.9456, Accuracy: 0.8494\n",
      "Training loss (for one batch) at step 80: 331.2098, Accuracy: 0.8360\n",
      "Training loss (for one batch) at step 90: 294.2641, Accuracy: 0.8328\n",
      "Training loss (for one batch) at step 100: 286.5536, Accuracy: 0.8349\n",
      "Training loss (for one batch) at step 110: 318.7902, Accuracy: 0.8354\n",
      "---- Training ----\n",
      "Training loss: 104.8224\n",
      "Training acc over epoch: 0.8344\n",
      "---- Validation ----\n",
      "Validation loss: 39.0221\n",
      "Validation acc: 0.7585\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 334.1585, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 344.8841, Accuracy: 0.7798\n",
      "Training loss (for one batch) at step 20: 290.6798, Accuracy: 0.8077\n",
      "Training loss (for one batch) at step 30: 277.2834, Accuracy: 0.8319\n",
      "Training loss (for one batch) at step 40: 286.3299, Accuracy: 0.8476\n",
      "Training loss (for one batch) at step 50: 294.1385, Accuracy: 0.8565\n",
      "Training loss (for one batch) at step 60: 299.4717, Accuracy: 0.8609\n",
      "Training loss (for one batch) at step 70: 310.9990, Accuracy: 0.8485\n",
      "Training loss (for one batch) at step 80: 342.1872, Accuracy: 0.8347\n",
      "Training loss (for one batch) at step 90: 314.4240, Accuracy: 0.8316\n",
      "Training loss (for one batch) at step 100: 300.9810, Accuracy: 0.8348\n",
      "Training loss (for one batch) at step 110: 321.4561, Accuracy: 0.8352\n",
      "---- Training ----\n",
      "Training loss: 98.3104\n",
      "Training acc over epoch: 0.8342\n",
      "---- Validation ----\n",
      "Validation loss: 47.7358\n",
      "Validation acc: 0.7598\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 351.0321, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 328.6250, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 20: 305.0446, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 30: 296.9940, Accuracy: 0.8251\n",
      "Training loss (for one batch) at step 40: 294.4661, Accuracy: 0.8401\n",
      "Training loss (for one batch) at step 50: 305.0746, Accuracy: 0.8520\n",
      "Training loss (for one batch) at step 60: 303.3417, Accuracy: 0.8586\n",
      "Training loss (for one batch) at step 70: 317.5489, Accuracy: 0.8467\n",
      "Training loss (for one batch) at step 80: 330.8795, Accuracy: 0.8342\n",
      "Training loss (for one batch) at step 90: 306.2928, Accuracy: 0.8316\n",
      "Training loss (for one batch) at step 100: 290.1372, Accuracy: 0.8333\n",
      "Training loss (for one batch) at step 110: 336.3865, Accuracy: 0.8335\n",
      "---- Training ----\n",
      "Training loss: 93.2807\n",
      "Training acc over epoch: 0.8329\n",
      "---- Validation ----\n",
      "Validation loss: 57.4208\n",
      "Validation acc: 0.7520\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 345.2328, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 333.3032, Accuracy: 0.7720\n",
      "Training loss (for one batch) at step 20: 299.9740, Accuracy: 0.8065\n",
      "Training loss (for one batch) at step 30: 292.2369, Accuracy: 0.8218\n",
      "Training loss (for one batch) at step 40: 271.5883, Accuracy: 0.8399\n",
      "Training loss (for one batch) at step 50: 291.6693, Accuracy: 0.8526\n",
      "Training loss (for one batch) at step 60: 304.1861, Accuracy: 0.8562\n",
      "Training loss (for one batch) at step 70: 331.3146, Accuracy: 0.8462\n",
      "Training loss (for one batch) at step 80: 354.0865, Accuracy: 0.8325\n",
      "Training loss (for one batch) at step 90: 302.0038, Accuracy: 0.8307\n",
      "Training loss (for one batch) at step 100: 308.2296, Accuracy: 0.8339\n",
      "Training loss (for one batch) at step 110: 321.6507, Accuracy: 0.8333\n",
      "---- Training ----\n",
      "Training loss: 106.6568\n",
      "Training acc over epoch: 0.8321\n",
      "---- Validation ----\n",
      "Validation loss: 42.3382\n",
      "Validation acc: 0.7577\n",
      "Time taken: 10.78s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 326.9718, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 10: 362.3618, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 20: 283.9021, Accuracy: 0.8051\n",
      "Training loss (for one batch) at step 30: 303.8698, Accuracy: 0.8221\n",
      "Training loss (for one batch) at step 40: 285.1412, Accuracy: 0.8388\n",
      "Training loss (for one batch) at step 50: 284.9850, Accuracy: 0.8513\n",
      "Training loss (for one batch) at step 60: 289.3906, Accuracy: 0.8564\n",
      "Training loss (for one batch) at step 70: 338.3824, Accuracy: 0.8469\n",
      "Training loss (for one batch) at step 80: 338.4982, Accuracy: 0.8362\n",
      "Training loss (for one batch) at step 90: 296.0189, Accuracy: 0.8320\n",
      "Training loss (for one batch) at step 100: 280.0002, Accuracy: 0.8349\n",
      "Training loss (for one batch) at step 110: 305.2266, Accuracy: 0.8351\n",
      "---- Training ----\n",
      "Training loss: 95.1875\n",
      "Training acc over epoch: 0.8338\n",
      "---- Validation ----\n",
      "Validation loss: 38.4228\n",
      "Validation acc: 0.7663\n",
      "Time taken: 10.61s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 342.8984, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 327.5854, Accuracy: 0.7741\n",
      "Training loss (for one batch) at step 20: 326.3414, Accuracy: 0.8032\n",
      "Training loss (for one batch) at step 30: 302.4065, Accuracy: 0.8269\n",
      "Training loss (for one batch) at step 40: 288.5956, Accuracy: 0.8430\n",
      "Training loss (for one batch) at step 50: 289.6565, Accuracy: 0.8560\n",
      "Training loss (for one batch) at step 60: 307.4967, Accuracy: 0.8591\n",
      "Training loss (for one batch) at step 70: 343.2309, Accuracy: 0.8484\n",
      "Training loss (for one batch) at step 80: 328.8448, Accuracy: 0.8364\n",
      "Training loss (for one batch) at step 90: 308.8660, Accuracy: 0.8310\n",
      "Training loss (for one batch) at step 100: 284.0781, Accuracy: 0.8335\n",
      "Training loss (for one batch) at step 110: 296.0774, Accuracy: 0.8344\n",
      "---- Training ----\n",
      "Training loss: 110.3044\n",
      "Training acc over epoch: 0.8333\n",
      "---- Validation ----\n",
      "Validation loss: 62.2241\n",
      "Validation acc: 0.7638\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 361.8316, Accuracy: 0.8203\n",
      "Training loss (for one batch) at step 10: 329.2029, Accuracy: 0.7763\n",
      "Training loss (for one batch) at step 20: 303.0392, Accuracy: 0.8073\n",
      "Training loss (for one batch) at step 30: 306.8719, Accuracy: 0.8294\n",
      "Training loss (for one batch) at step 40: 278.1553, Accuracy: 0.8476\n",
      "Training loss (for one batch) at step 50: 273.7465, Accuracy: 0.8568\n",
      "Training loss (for one batch) at step 60: 304.1411, Accuracy: 0.8603\n",
      "Training loss (for one batch) at step 70: 325.3640, Accuracy: 0.8477\n",
      "Training loss (for one batch) at step 80: 328.9714, Accuracy: 0.8375\n",
      "Training loss (for one batch) at step 90: 283.9793, Accuracy: 0.8343\n",
      "Training loss (for one batch) at step 100: 297.7829, Accuracy: 0.8366\n",
      "Training loss (for one batch) at step 110: 291.2639, Accuracy: 0.8361\n",
      "---- Training ----\n",
      "Training loss: 101.8816\n",
      "Training acc over epoch: 0.8350\n",
      "---- Validation ----\n",
      "Validation loss: 37.6970\n",
      "Validation acc: 0.7582\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 352.3051, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 10: 335.5359, Accuracy: 0.7834\n",
      "Training loss (for one batch) at step 20: 308.6225, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 30: 307.6238, Accuracy: 0.8248\n",
      "Training loss (for one batch) at step 40: 282.8329, Accuracy: 0.8418\n",
      "Training loss (for one batch) at step 50: 274.5095, Accuracy: 0.8505\n",
      "Training loss (for one batch) at step 60: 305.9213, Accuracy: 0.8560\n",
      "Training loss (for one batch) at step 70: 366.5205, Accuracy: 0.8454\n",
      "Training loss (for one batch) at step 80: 327.3811, Accuracy: 0.8329\n",
      "Training loss (for one batch) at step 90: 289.2767, Accuracy: 0.8296\n",
      "Training loss (for one batch) at step 100: 278.3011, Accuracy: 0.8338\n",
      "Training loss (for one batch) at step 110: 297.5428, Accuracy: 0.8337\n",
      "---- Training ----\n",
      "Training loss: 95.3678\n",
      "Training acc over epoch: 0.8335\n",
      "---- Validation ----\n",
      "Validation loss: 51.5747\n",
      "Validation acc: 0.7579\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 335.9444, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 335.0107, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 20: 279.4221, Accuracy: 0.7939\n",
      "Training loss (for one batch) at step 30: 291.2356, Accuracy: 0.8188\n",
      "Training loss (for one batch) at step 40: 282.0588, Accuracy: 0.8333\n",
      "Training loss (for one batch) at step 50: 291.3433, Accuracy: 0.8493\n",
      "Training loss (for one batch) at step 60: 302.3312, Accuracy: 0.8532\n",
      "Training loss (for one batch) at step 70: 311.2637, Accuracy: 0.8426\n",
      "Training loss (for one batch) at step 80: 329.7950, Accuracy: 0.8314\n",
      "Training loss (for one batch) at step 90: 303.4493, Accuracy: 0.8295\n",
      "Training loss (for one batch) at step 100: 286.5853, Accuracy: 0.8318\n",
      "Training loss (for one batch) at step 110: 292.1808, Accuracy: 0.8333\n",
      "---- Training ----\n",
      "Training loss: 108.2970\n",
      "Training acc over epoch: 0.8329\n",
      "---- Validation ----\n",
      "Validation loss: 50.3250\n",
      "Validation acc: 0.7622\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 333.0537, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 346.9200, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 20: 295.1937, Accuracy: 0.8058\n",
      "Training loss (for one batch) at step 30: 278.5383, Accuracy: 0.8228\n",
      "Training loss (for one batch) at step 40: 283.5769, Accuracy: 0.8411\n",
      "Training loss (for one batch) at step 50: 279.3358, Accuracy: 0.8536\n",
      "Training loss (for one batch) at step 60: 295.8170, Accuracy: 0.8578\n",
      "Training loss (for one batch) at step 70: 316.1357, Accuracy: 0.8478\n",
      "Training loss (for one batch) at step 80: 335.0646, Accuracy: 0.8356\n",
      "Training loss (for one batch) at step 90: 276.5431, Accuracy: 0.8338\n",
      "Training loss (for one batch) at step 100: 288.6100, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 110: 299.0884, Accuracy: 0.8358\n",
      "---- Training ----\n",
      "Training loss: 99.4987\n",
      "Training acc over epoch: 0.8338\n",
      "---- Validation ----\n",
      "Validation loss: 74.8232\n",
      "Validation acc: 0.7558\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 341.8311, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 313.6891, Accuracy: 0.7798\n",
      "Training loss (for one batch) at step 20: 290.7979, Accuracy: 0.8065\n",
      "Training loss (for one batch) at step 30: 284.0136, Accuracy: 0.8306\n",
      "Training loss (for one batch) at step 40: 284.9247, Accuracy: 0.8422\n",
      "Training loss (for one batch) at step 50: 281.3033, Accuracy: 0.8537\n",
      "Training loss (for one batch) at step 60: 297.9055, Accuracy: 0.8581\n",
      "Training loss (for one batch) at step 70: 327.9697, Accuracy: 0.8486\n",
      "Training loss (for one batch) at step 80: 340.8537, Accuracy: 0.8372\n",
      "Training loss (for one batch) at step 90: 298.4128, Accuracy: 0.8339\n",
      "Training loss (for one batch) at step 100: 282.0118, Accuracy: 0.8363\n",
      "Training loss (for one batch) at step 110: 309.1871, Accuracy: 0.8366\n",
      "---- Training ----\n",
      "Training loss: 129.7868\n",
      "Training acc over epoch: 0.8352\n",
      "---- Validation ----\n",
      "Validation loss: 61.5631\n",
      "Validation acc: 0.7542\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 341.6190, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 316.4113, Accuracy: 0.7699\n",
      "Training loss (for one batch) at step 20: 297.0335, Accuracy: 0.8002\n",
      "Training loss (for one batch) at step 30: 275.3525, Accuracy: 0.8266\n",
      "Training loss (for one batch) at step 40: 277.1036, Accuracy: 0.8415\n",
      "Training loss (for one batch) at step 50: 280.2994, Accuracy: 0.8540\n",
      "Training loss (for one batch) at step 60: 310.3402, Accuracy: 0.8580\n",
      "Training loss (for one batch) at step 70: 346.9888, Accuracy: 0.8472\n",
      "Training loss (for one batch) at step 80: 324.5453, Accuracy: 0.8371\n",
      "Training loss (for one batch) at step 90: 292.6582, Accuracy: 0.8338\n",
      "Training loss (for one batch) at step 100: 295.9602, Accuracy: 0.8358\n",
      "Training loss (for one batch) at step 110: 287.2900, Accuracy: 0.8349\n",
      "---- Training ----\n",
      "Training loss: 103.5445\n",
      "Training acc over epoch: 0.8338\n",
      "---- Validation ----\n",
      "Validation loss: 52.3916\n",
      "Validation acc: 0.7552\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 326.8783, Accuracy: 0.8594\n",
      "Training loss (for one batch) at step 10: 315.4590, Accuracy: 0.7805\n",
      "Training loss (for one batch) at step 20: 301.7892, Accuracy: 0.7984\n",
      "Training loss (for one batch) at step 30: 274.2717, Accuracy: 0.8254\n",
      "Training loss (for one batch) at step 40: 275.3726, Accuracy: 0.8434\n",
      "Training loss (for one batch) at step 50: 282.6683, Accuracy: 0.8551\n",
      "Training loss (for one batch) at step 60: 314.1191, Accuracy: 0.8599\n",
      "Training loss (for one batch) at step 70: 304.5375, Accuracy: 0.8489\n",
      "Training loss (for one batch) at step 80: 320.8890, Accuracy: 0.8373\n",
      "Training loss (for one batch) at step 90: 289.3056, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 100: 299.0930, Accuracy: 0.8377\n",
      "Training loss (for one batch) at step 110: 300.6603, Accuracy: 0.8376\n",
      "---- Training ----\n",
      "Training loss: 108.0583\n",
      "Training acc over epoch: 0.8359\n",
      "---- Validation ----\n",
      "Validation loss: 64.8111\n",
      "Validation acc: 0.7595\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 327.6508, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 337.5601, Accuracy: 0.7678\n",
      "Training loss (for one batch) at step 20: 283.4665, Accuracy: 0.7987\n",
      "Training loss (for one batch) at step 30: 284.2156, Accuracy: 0.8241\n",
      "Training loss (for one batch) at step 40: 278.5970, Accuracy: 0.8420\n",
      "Training loss (for one batch) at step 50: 279.5460, Accuracy: 0.8548\n",
      "Training loss (for one batch) at step 60: 302.9994, Accuracy: 0.8594\n",
      "Training loss (for one batch) at step 70: 319.0479, Accuracy: 0.8473\n",
      "Training loss (for one batch) at step 80: 356.8512, Accuracy: 0.8358\n",
      "Training loss (for one batch) at step 90: 293.0823, Accuracy: 0.8326\n",
      "Training loss (for one batch) at step 100: 283.0297, Accuracy: 0.8356\n",
      "Training loss (for one batch) at step 110: 291.1109, Accuracy: 0.8351\n",
      "---- Training ----\n",
      "Training loss: 111.7969\n",
      "Training acc over epoch: 0.8339\n",
      "---- Validation ----\n",
      "Validation loss: 50.8465\n",
      "Validation acc: 0.7585\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 326.6061, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 10: 325.6458, Accuracy: 0.7820\n",
      "Training loss (for one batch) at step 20: 289.9538, Accuracy: 0.8095\n",
      "Training loss (for one batch) at step 30: 281.6462, Accuracy: 0.8334\n",
      "Training loss (for one batch) at step 40: 272.7562, Accuracy: 0.8445\n",
      "Training loss (for one batch) at step 50: 282.7548, Accuracy: 0.8540\n",
      "Training loss (for one batch) at step 60: 298.4497, Accuracy: 0.8584\n",
      "Training loss (for one batch) at step 70: 328.1536, Accuracy: 0.8485\n",
      "Training loss (for one batch) at step 80: 344.8672, Accuracy: 0.8354\n",
      "Training loss (for one batch) at step 90: 278.0832, Accuracy: 0.8321\n",
      "Training loss (for one batch) at step 100: 266.6542, Accuracy: 0.8342\n",
      "Training loss (for one batch) at step 110: 290.4963, Accuracy: 0.8335\n",
      "---- Training ----\n",
      "Training loss: 107.3491\n",
      "Training acc over epoch: 0.8321\n",
      "---- Validation ----\n",
      "Validation loss: 41.6607\n",
      "Validation acc: 0.7526\n",
      "Time taken: 10.14s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 322.2262, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 310.0181, Accuracy: 0.7571\n",
      "Training loss (for one batch) at step 20: 310.7625, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 30: 274.9081, Accuracy: 0.8208\n",
      "Training loss (for one batch) at step 40: 291.4911, Accuracy: 0.8361\n",
      "Training loss (for one batch) at step 50: 281.4532, Accuracy: 0.8496\n",
      "Training loss (for one batch) at step 60: 290.9276, Accuracy: 0.8551\n",
      "Training loss (for one batch) at step 70: 315.6894, Accuracy: 0.8440\n",
      "Training loss (for one batch) at step 80: 341.5851, Accuracy: 0.8329\n",
      "Training loss (for one batch) at step 90: 288.7276, Accuracy: 0.8294\n",
      "Training loss (for one batch) at step 100: 278.5288, Accuracy: 0.8335\n",
      "Training loss (for one batch) at step 110: 332.1732, Accuracy: 0.8335\n",
      "---- Training ----\n",
      "Training loss: 105.8437\n",
      "Training acc over epoch: 0.8325\n",
      "---- Validation ----\n",
      "Validation loss: 65.6806\n",
      "Validation acc: 0.7421\n",
      "Time taken: 10.47s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 335.1342, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 325.4045, Accuracy: 0.7564\n",
      "Training loss (for one batch) at step 20: 302.8513, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 30: 268.5882, Accuracy: 0.8223\n",
      "Training loss (for one batch) at step 40: 279.6042, Accuracy: 0.8384\n",
      "Training loss (for one batch) at step 50: 275.7973, Accuracy: 0.8490\n",
      "Training loss (for one batch) at step 60: 305.1973, Accuracy: 0.8543\n",
      "Training loss (for one batch) at step 70: 335.6053, Accuracy: 0.8441\n",
      "Training loss (for one batch) at step 80: 318.3383, Accuracy: 0.8319\n",
      "Training loss (for one batch) at step 90: 286.6165, Accuracy: 0.8298\n",
      "Training loss (for one batch) at step 100: 277.2845, Accuracy: 0.8334\n",
      "Training loss (for one batch) at step 110: 292.8971, Accuracy: 0.8348\n",
      "---- Training ----\n",
      "Training loss: 101.5656\n",
      "Training acc over epoch: 0.8334\n",
      "---- Validation ----\n",
      "Validation loss: 66.5128\n",
      "Validation acc: 0.7644\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 334.8022, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 316.1822, Accuracy: 0.7642\n",
      "Training loss (for one batch) at step 20: 296.0880, Accuracy: 0.8006\n",
      "Training loss (for one batch) at step 30: 279.2176, Accuracy: 0.8238\n",
      "Training loss (for one batch) at step 40: 272.0745, Accuracy: 0.8384\n",
      "Training loss (for one batch) at step 50: 280.1721, Accuracy: 0.8528\n",
      "Training loss (for one batch) at step 60: 298.1399, Accuracy: 0.8592\n",
      "Training loss (for one batch) at step 70: 318.2410, Accuracy: 0.8447\n",
      "Training loss (for one batch) at step 80: 322.0799, Accuracy: 0.8357\n",
      "Training loss (for one batch) at step 90: 273.2552, Accuracy: 0.8326\n",
      "Training loss (for one batch) at step 100: 300.8710, Accuracy: 0.8342\n",
      "Training loss (for one batch) at step 110: 292.5004, Accuracy: 0.8354\n",
      "---- Training ----\n",
      "Training loss: 97.7702\n",
      "Training acc over epoch: 0.8342\n",
      "---- Validation ----\n",
      "Validation loss: 59.2113\n",
      "Validation acc: 0.7582\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 337.3602, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 314.4830, Accuracy: 0.7599\n",
      "Training loss (for one batch) at step 20: 284.4229, Accuracy: 0.7939\n",
      "Training loss (for one batch) at step 30: 274.9901, Accuracy: 0.8201\n",
      "Training loss (for one batch) at step 40: 267.0928, Accuracy: 0.8377\n",
      "Training loss (for one batch) at step 50: 293.3677, Accuracy: 0.8493\n",
      "Training loss (for one batch) at step 60: 287.6404, Accuracy: 0.8557\n",
      "Training loss (for one batch) at step 70: 323.3525, Accuracy: 0.8475\n",
      "Training loss (for one batch) at step 80: 310.3594, Accuracy: 0.8356\n",
      "Training loss (for one batch) at step 90: 281.0845, Accuracy: 0.8330\n",
      "Training loss (for one batch) at step 100: 276.9810, Accuracy: 0.8357\n",
      "Training loss (for one batch) at step 110: 296.0504, Accuracy: 0.8373\n",
      "---- Training ----\n",
      "Training loss: 94.8597\n",
      "Training acc over epoch: 0.8357\n",
      "---- Validation ----\n",
      "Validation loss: 46.3794\n",
      "Validation acc: 0.7526\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 326.3207, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 310.7327, Accuracy: 0.7763\n",
      "Training loss (for one batch) at step 20: 284.2920, Accuracy: 0.8025\n",
      "Training loss (for one batch) at step 30: 277.3891, Accuracy: 0.8226\n",
      "Training loss (for one batch) at step 40: 292.1134, Accuracy: 0.8390\n",
      "Training loss (for one batch) at step 50: 280.1747, Accuracy: 0.8520\n",
      "Training loss (for one batch) at step 60: 302.0482, Accuracy: 0.8560\n",
      "Training loss (for one batch) at step 70: 313.7919, Accuracy: 0.8474\n",
      "Training loss (for one batch) at step 80: 311.4353, Accuracy: 0.8351\n",
      "Training loss (for one batch) at step 90: 270.3551, Accuracy: 0.8337\n",
      "Training loss (for one batch) at step 100: 291.1477, Accuracy: 0.8357\n",
      "Training loss (for one batch) at step 110: 307.6956, Accuracy: 0.8354\n",
      "---- Training ----\n",
      "Training loss: 89.8743\n",
      "Training acc over epoch: 0.8338\n",
      "---- Validation ----\n",
      "Validation loss: 36.5298\n",
      "Validation acc: 0.7437\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 339.4268, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 320.8250, Accuracy: 0.7820\n",
      "Training loss (for one batch) at step 20: 297.7610, Accuracy: 0.8043\n",
      "Training loss (for one batch) at step 30: 276.1114, Accuracy: 0.8261\n",
      "Training loss (for one batch) at step 40: 292.9518, Accuracy: 0.8413\n",
      "Training loss (for one batch) at step 50: 278.3764, Accuracy: 0.8548\n",
      "Training loss (for one batch) at step 60: 303.3339, Accuracy: 0.8587\n",
      "Training loss (for one batch) at step 70: 308.7238, Accuracy: 0.8480\n",
      "Training loss (for one batch) at step 80: 323.4978, Accuracy: 0.8370\n",
      "Training loss (for one batch) at step 90: 291.9623, Accuracy: 0.8357\n",
      "Training loss (for one batch) at step 100: 293.4053, Accuracy: 0.8373\n",
      "Training loss (for one batch) at step 110: 294.2093, Accuracy: 0.8389\n",
      "---- Training ----\n",
      "Training loss: 92.4506\n",
      "Training acc over epoch: 0.8371\n",
      "---- Validation ----\n",
      "Validation loss: 54.1599\n",
      "Validation acc: 0.7493\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 332.0023, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 316.5005, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 20: 283.0422, Accuracy: 0.7935\n",
      "Training loss (for one batch) at step 30: 269.5894, Accuracy: 0.8223\n",
      "Training loss (for one batch) at step 40: 285.7367, Accuracy: 0.8384\n",
      "Training loss (for one batch) at step 50: 274.4067, Accuracy: 0.8520\n",
      "Training loss (for one batch) at step 60: 285.8964, Accuracy: 0.8555\n",
      "Training loss (for one batch) at step 70: 303.6633, Accuracy: 0.8466\n",
      "Training loss (for one batch) at step 80: 324.6945, Accuracy: 0.8351\n",
      "Training loss (for one batch) at step 90: 285.1062, Accuracy: 0.8316\n",
      "Training loss (for one batch) at step 100: 289.4643, Accuracy: 0.8355\n",
      "Training loss (for one batch) at step 110: 288.3301, Accuracy: 0.8356\n",
      "---- Training ----\n",
      "Training loss: 93.5733\n",
      "Training acc over epoch: 0.8353\n",
      "---- Validation ----\n",
      "Validation loss: 49.2380\n",
      "Validation acc: 0.7671\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 331.7190, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 314.7437, Accuracy: 0.7614\n",
      "Training loss (for one batch) at step 20: 274.6675, Accuracy: 0.7961\n",
      "Training loss (for one batch) at step 30: 288.9633, Accuracy: 0.8264\n",
      "Training loss (for one batch) at step 40: 289.2635, Accuracy: 0.8447\n",
      "Training loss (for one batch) at step 50: 270.1565, Accuracy: 0.8554\n",
      "Training loss (for one batch) at step 60: 308.4292, Accuracy: 0.8589\n",
      "Training loss (for one batch) at step 70: 325.6817, Accuracy: 0.8485\n",
      "Training loss (for one batch) at step 80: 301.5582, Accuracy: 0.8383\n",
      "Training loss (for one batch) at step 90: 292.2171, Accuracy: 0.8346\n",
      "Training loss (for one batch) at step 100: 281.5494, Accuracy: 0.8366\n",
      "Training loss (for one batch) at step 110: 292.3021, Accuracy: 0.8376\n",
      "---- Training ----\n",
      "Training loss: 114.7411\n",
      "Training acc over epoch: 0.8361\n",
      "---- Validation ----\n",
      "Validation loss: 63.8127\n",
      "Validation acc: 0.7654\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 317.5991, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 337.3229, Accuracy: 0.7628\n",
      "Training loss (for one batch) at step 20: 275.7523, Accuracy: 0.7909\n",
      "Training loss (for one batch) at step 30: 261.0652, Accuracy: 0.8196\n",
      "Training loss (for one batch) at step 40: 279.6645, Accuracy: 0.8386\n",
      "Training loss (for one batch) at step 50: 259.3585, Accuracy: 0.8526\n",
      "Training loss (for one batch) at step 60: 312.2944, Accuracy: 0.8585\n",
      "Training loss (for one batch) at step 70: 306.0400, Accuracy: 0.8487\n",
      "Training loss (for one batch) at step 80: 313.3221, Accuracy: 0.8384\n",
      "Training loss (for one batch) at step 90: 274.8227, Accuracy: 0.8339\n",
      "Training loss (for one batch) at step 100: 292.6643, Accuracy: 0.8356\n",
      "Training loss (for one batch) at step 110: 289.2638, Accuracy: 0.8378\n",
      "---- Training ----\n",
      "Training loss: 110.2649\n",
      "Training acc over epoch: 0.8360\n",
      "---- Validation ----\n",
      "Validation loss: 53.2767\n",
      "Validation acc: 0.7517\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 347.6532, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 310.9363, Accuracy: 0.7713\n",
      "Training loss (for one batch) at step 20: 281.7752, Accuracy: 0.7991\n",
      "Training loss (for one batch) at step 30: 276.6320, Accuracy: 0.8241\n",
      "Training loss (for one batch) at step 40: 284.3535, Accuracy: 0.8401\n",
      "Training loss (for one batch) at step 50: 274.8516, Accuracy: 0.8537\n",
      "Training loss (for one batch) at step 60: 288.8384, Accuracy: 0.8586\n",
      "Training loss (for one batch) at step 70: 300.4756, Accuracy: 0.8483\n",
      "Training loss (for one batch) at step 80: 317.5289, Accuracy: 0.8363\n",
      "Training loss (for one batch) at step 90: 288.0626, Accuracy: 0.8331\n",
      "Training loss (for one batch) at step 100: 295.7071, Accuracy: 0.8359\n",
      "Training loss (for one batch) at step 110: 309.7257, Accuracy: 0.8346\n",
      "---- Training ----\n",
      "Training loss: 104.0811\n",
      "Training acc over epoch: 0.8345\n",
      "---- Validation ----\n",
      "Validation loss: 60.2498\n",
      "Validation acc: 0.7496\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 319.7180, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 294.8688, Accuracy: 0.7820\n",
      "Training loss (for one batch) at step 20: 278.5282, Accuracy: 0.8073\n",
      "Training loss (for one batch) at step 30: 291.3822, Accuracy: 0.8271\n",
      "Training loss (for one batch) at step 40: 281.8744, Accuracy: 0.8441\n",
      "Training loss (for one batch) at step 50: 278.8027, Accuracy: 0.8566\n",
      "Training loss (for one batch) at step 60: 288.0269, Accuracy: 0.8601\n",
      "Training loss (for one batch) at step 70: 323.0143, Accuracy: 0.8488\n",
      "Training loss (for one batch) at step 80: 323.5846, Accuracy: 0.8362\n",
      "Training loss (for one batch) at step 90: 279.8154, Accuracy: 0.8326\n",
      "Training loss (for one batch) at step 100: 264.2101, Accuracy: 0.8366\n",
      "Training loss (for one batch) at step 110: 304.6733, Accuracy: 0.8364\n",
      "---- Training ----\n",
      "Training loss: 86.0721\n",
      "Training acc over epoch: 0.8356\n",
      "---- Validation ----\n",
      "Validation loss: 33.1359\n",
      "Validation acc: 0.7488\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 336.9731, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 292.6676, Accuracy: 0.7685\n",
      "Training loss (for one batch) at step 20: 281.0811, Accuracy: 0.8002\n",
      "Training loss (for one batch) at step 30: 275.6748, Accuracy: 0.8243\n",
      "Training loss (for one batch) at step 40: 283.8981, Accuracy: 0.8428\n",
      "Training loss (for one batch) at step 50: 289.5034, Accuracy: 0.8542\n",
      "Training loss (for one batch) at step 60: 292.8645, Accuracy: 0.8598\n",
      "Training loss (for one batch) at step 70: 316.3595, Accuracy: 0.8488\n",
      "Training loss (for one batch) at step 80: 308.0257, Accuracy: 0.8362\n",
      "Training loss (for one batch) at step 90: 281.7557, Accuracy: 0.8315\n",
      "Training loss (for one batch) at step 100: 280.6405, Accuracy: 0.8337\n",
      "Training loss (for one batch) at step 110: 303.2298, Accuracy: 0.8342\n",
      "---- Training ----\n",
      "Training loss: 115.0790\n",
      "Training acc over epoch: 0.8319\n",
      "---- Validation ----\n",
      "Validation loss: 35.2947\n",
      "Validation acc: 0.7571\n",
      "Time taken: 10.19s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 334.0713, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 335.4538, Accuracy: 0.7699\n",
      "Training loss (for one batch) at step 20: 287.8755, Accuracy: 0.7984\n",
      "Training loss (for one batch) at step 30: 288.1205, Accuracy: 0.8226\n",
      "Training loss (for one batch) at step 40: 282.7523, Accuracy: 0.8390\n",
      "Training loss (for one batch) at step 50: 288.4211, Accuracy: 0.8502\n",
      "Training loss (for one batch) at step 60: 277.8154, Accuracy: 0.8541\n",
      "Training loss (for one batch) at step 70: 325.3250, Accuracy: 0.8452\n",
      "Training loss (for one batch) at step 80: 336.1809, Accuracy: 0.8357\n",
      "Training loss (for one batch) at step 90: 295.6733, Accuracy: 0.8318\n",
      "Training loss (for one batch) at step 100: 282.5186, Accuracy: 0.8341\n",
      "Training loss (for one batch) at step 110: 301.4417, Accuracy: 0.8341\n",
      "---- Training ----\n",
      "Training loss: 104.5222\n",
      "Training acc over epoch: 0.8336\n",
      "---- Validation ----\n",
      "Validation loss: 38.0022\n",
      "Validation acc: 0.7587\n",
      "Time taken: 10.59s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 318.8116, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 324.7588, Accuracy: 0.7713\n",
      "Training loss (for one batch) at step 20: 280.4101, Accuracy: 0.8013\n",
      "Training loss (for one batch) at step 30: 286.8006, Accuracy: 0.8201\n",
      "Training loss (for one batch) at step 40: 283.2366, Accuracy: 0.8409\n",
      "Training loss (for one batch) at step 50: 283.2076, Accuracy: 0.8522\n",
      "Training loss (for one batch) at step 60: 293.7375, Accuracy: 0.8566\n",
      "Training loss (for one batch) at step 70: 297.6135, Accuracy: 0.8451\n",
      "Training loss (for one batch) at step 80: 321.0002, Accuracy: 0.8330\n",
      "Training loss (for one batch) at step 90: 267.0161, Accuracy: 0.8321\n",
      "Training loss (for one batch) at step 100: 270.7910, Accuracy: 0.8345\n",
      "Training loss (for one batch) at step 110: 312.6649, Accuracy: 0.8357\n",
      "---- Training ----\n",
      "Training loss: 95.9740\n",
      "Training acc over epoch: 0.8344\n",
      "---- Validation ----\n",
      "Validation loss: 41.5489\n",
      "Validation acc: 0.7582\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 341.2961, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 321.0019, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 20: 298.9968, Accuracy: 0.7958\n",
      "Training loss (for one batch) at step 30: 262.9372, Accuracy: 0.8248\n",
      "Training loss (for one batch) at step 40: 282.2994, Accuracy: 0.8413\n",
      "Training loss (for one batch) at step 50: 280.5160, Accuracy: 0.8531\n",
      "Training loss (for one batch) at step 60: 289.6297, Accuracy: 0.8566\n",
      "Training loss (for one batch) at step 70: 306.5270, Accuracy: 0.8507\n",
      "Training loss (for one batch) at step 80: 308.4695, Accuracy: 0.8367\n",
      "Training loss (for one batch) at step 90: 279.7580, Accuracy: 0.8351\n",
      "Training loss (for one batch) at step 100: 265.4282, Accuracy: 0.8366\n",
      "Training loss (for one batch) at step 110: 293.4292, Accuracy: 0.8374\n",
      "---- Training ----\n",
      "Training loss: 102.4745\n",
      "Training acc over epoch: 0.8354\n",
      "---- Validation ----\n",
      "Validation loss: 50.9975\n",
      "Validation acc: 0.7622\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 339.3499, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 327.0408, Accuracy: 0.7642\n",
      "Training loss (for one batch) at step 20: 281.5120, Accuracy: 0.8051\n",
      "Training loss (for one batch) at step 30: 266.3788, Accuracy: 0.8243\n",
      "Training loss (for one batch) at step 40: 272.0091, Accuracy: 0.8409\n",
      "Training loss (for one batch) at step 50: 271.1111, Accuracy: 0.8528\n",
      "Training loss (for one batch) at step 60: 300.7172, Accuracy: 0.8564\n",
      "Training loss (for one batch) at step 70: 310.5545, Accuracy: 0.8469\n",
      "Training loss (for one batch) at step 80: 296.5833, Accuracy: 0.8352\n",
      "Training loss (for one batch) at step 90: 280.4009, Accuracy: 0.8321\n",
      "Training loss (for one batch) at step 100: 277.9148, Accuracy: 0.8355\n",
      "Training loss (for one batch) at step 110: 288.5097, Accuracy: 0.8358\n",
      "---- Training ----\n",
      "Training loss: 93.0466\n",
      "Training acc over epoch: 0.8347\n",
      "---- Validation ----\n",
      "Validation loss: 37.7292\n",
      "Validation acc: 0.7552\n",
      "Time taken: 10.74s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 314.5821, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 322.5479, Accuracy: 0.7699\n",
      "Training loss (for one batch) at step 20: 312.1002, Accuracy: 0.8021\n",
      "Training loss (for one batch) at step 30: 281.3837, Accuracy: 0.8289\n",
      "Training loss (for one batch) at step 40: 254.1530, Accuracy: 0.8455\n",
      "Training loss (for one batch) at step 50: 255.5358, Accuracy: 0.8594\n",
      "Training loss (for one batch) at step 60: 293.8500, Accuracy: 0.8622\n",
      "Training loss (for one batch) at step 70: 318.5382, Accuracy: 0.8508\n",
      "Training loss (for one batch) at step 80: 328.4325, Accuracy: 0.8402\n",
      "Training loss (for one batch) at step 90: 279.0668, Accuracy: 0.8376\n",
      "Training loss (for one batch) at step 100: 268.6631, Accuracy: 0.8399\n",
      "Training loss (for one batch) at step 110: 302.8218, Accuracy: 0.8397\n",
      "---- Training ----\n",
      "Training loss: 115.1829\n",
      "Training acc over epoch: 0.8389\n",
      "---- Validation ----\n",
      "Validation loss: 75.4301\n",
      "Validation acc: 0.7595\n",
      "Time taken: 10.39s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.7625\n",
      "Validation AUC: 0.7635\n",
      "Validation Balanced_ACC: 0.4284\n",
      "Validation MI: 0.1084\n",
      "Validation Normalized MI: 0.1589\n",
      "Validation Adjusted MI: 0.1589\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 643.0958, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 658.1411, Accuracy: 0.4979\n",
      "Training loss (for one batch) at step 20: 572.3500, Accuracy: 0.5104\n",
      "Training loss (for one batch) at step 30: 532.2385, Accuracy: 0.5108\n",
      "Training loss (for one batch) at step 40: 508.2115, Accuracy: 0.5076\n",
      "Training loss (for one batch) at step 50: 502.2137, Accuracy: 0.5055\n",
      "Training loss (for one batch) at step 60: 498.0054, Accuracy: 0.5065\n",
      "Training loss (for one batch) at step 70: 492.0191, Accuracy: 0.5084\n",
      "Training loss (for one batch) at step 80: 474.4009, Accuracy: 0.5085\n",
      "Training loss (for one batch) at step 90: 475.4165, Accuracy: 0.5086\n",
      "Training loss (for one batch) at step 100: 464.9786, Accuracy: 0.5098\n",
      "Training loss (for one batch) at step 110: 457.6664, Accuracy: 0.5096\n",
      "---- Training ----\n",
      "Training loss: 144.5296\n",
      "Training acc over epoch: 0.5109\n",
      "---- Validation ----\n",
      "Validation loss: 34.6612\n",
      "Validation acc: 0.5105\n",
      "Time taken: 12.21s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 456.6292, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 458.1786, Accuracy: 0.5170\n",
      "Training loss (for one batch) at step 20: 462.9851, Accuracy: 0.5182\n",
      "Training loss (for one batch) at step 30: 455.7194, Accuracy: 0.5189\n",
      "Training loss (for one batch) at step 40: 453.2460, Accuracy: 0.5185\n",
      "Training loss (for one batch) at step 50: 447.4539, Accuracy: 0.5213\n",
      "Training loss (for one batch) at step 60: 448.1710, Accuracy: 0.5232\n",
      "Training loss (for one batch) at step 70: 446.9835, Accuracy: 0.5264\n",
      "Training loss (for one batch) at step 80: 442.3250, Accuracy: 0.5297\n",
      "Training loss (for one batch) at step 90: 445.1976, Accuracy: 0.5292\n",
      "Training loss (for one batch) at step 100: 449.0360, Accuracy: 0.5289\n",
      "Training loss (for one batch) at step 110: 448.5337, Accuracy: 0.5297\n",
      "---- Training ----\n",
      "Training loss: 141.4085\n",
      "Training acc over epoch: 0.5294\n",
      "---- Validation ----\n",
      "Validation loss: 34.6176\n",
      "Validation acc: 0.5132\n",
      "Time taken: 10.54s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 449.6511, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 448.2730, Accuracy: 0.5249\n",
      "Training loss (for one batch) at step 20: 447.4303, Accuracy: 0.5260\n",
      "Training loss (for one batch) at step 30: 446.1346, Accuracy: 0.5292\n",
      "Training loss (for one batch) at step 40: 444.0374, Accuracy: 0.5339\n",
      "Training loss (for one batch) at step 50: 443.4906, Accuracy: 0.5362\n",
      "Training loss (for one batch) at step 60: 443.2271, Accuracy: 0.5409\n",
      "Training loss (for one batch) at step 70: 446.8255, Accuracy: 0.5440\n",
      "Training loss (for one batch) at step 80: 443.0452, Accuracy: 0.5429\n",
      "Training loss (for one batch) at step 90: 441.7513, Accuracy: 0.5413\n",
      "Training loss (for one batch) at step 100: 443.8793, Accuracy: 0.5418\n",
      "Training loss (for one batch) at step 110: 444.3918, Accuracy: 0.5443\n",
      "---- Training ----\n",
      "Training loss: 140.1092\n",
      "Training acc over epoch: 0.5465\n",
      "---- Validation ----\n",
      "Validation loss: 33.9171\n",
      "Validation acc: 0.5099\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 444.0962, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 445.0457, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 20: 442.7304, Accuracy: 0.5603\n",
      "Training loss (for one batch) at step 30: 441.1248, Accuracy: 0.5610\n",
      "Training loss (for one batch) at step 40: 440.7949, Accuracy: 0.5631\n",
      "Training loss (for one batch) at step 50: 442.4897, Accuracy: 0.5637\n",
      "Training loss (for one batch) at step 60: 447.1281, Accuracy: 0.5702\n",
      "Training loss (for one batch) at step 70: 444.6812, Accuracy: 0.5736\n",
      "Training loss (for one batch) at step 80: 443.5744, Accuracy: 0.5730\n",
      "Training loss (for one batch) at step 90: 446.7567, Accuracy: 0.5707\n",
      "Training loss (for one batch) at step 100: 442.1839, Accuracy: 0.5716\n",
      "Training loss (for one batch) at step 110: 444.2266, Accuracy: 0.5761\n",
      "---- Training ----\n",
      "Training loss: 138.0576\n",
      "Training acc over epoch: 0.5773\n",
      "---- Validation ----\n",
      "Validation loss: 34.6618\n",
      "Validation acc: 0.6112\n",
      "Time taken: 10.59s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 444.1062, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 448.3383, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 20: 443.6872, Accuracy: 0.5908\n",
      "Training loss (for one batch) at step 30: 438.1575, Accuracy: 0.5885\n",
      "Training loss (for one batch) at step 40: 438.7854, Accuracy: 0.5920\n",
      "Training loss (for one batch) at step 50: 449.5369, Accuracy: 0.5918\n",
      "Training loss (for one batch) at step 60: 440.8210, Accuracy: 0.5961\n",
      "Training loss (for one batch) at step 70: 444.2176, Accuracy: 0.5991\n",
      "Training loss (for one batch) at step 80: 445.6093, Accuracy: 0.5978\n",
      "Training loss (for one batch) at step 90: 444.2760, Accuracy: 0.5930\n",
      "Training loss (for one batch) at step 100: 439.9444, Accuracy: 0.5927\n",
      "Training loss (for one batch) at step 110: 441.8326, Accuracy: 0.5954\n",
      "---- Training ----\n",
      "Training loss: 136.0701\n",
      "Training acc over epoch: 0.5968\n",
      "---- Validation ----\n",
      "Validation loss: 34.0824\n",
      "Validation acc: 0.6365\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 442.5003, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 443.2271, Accuracy: 0.6200\n",
      "Training loss (for one batch) at step 20: 438.7075, Accuracy: 0.6168\n",
      "Training loss (for one batch) at step 30: 438.5221, Accuracy: 0.6174\n",
      "Training loss (for one batch) at step 40: 437.2954, Accuracy: 0.6277\n",
      "Training loss (for one batch) at step 50: 433.8322, Accuracy: 0.6311\n",
      "Training loss (for one batch) at step 60: 441.1201, Accuracy: 0.6322\n",
      "Training loss (for one batch) at step 70: 437.7541, Accuracy: 0.6346\n",
      "Training loss (for one batch) at step 80: 440.3815, Accuracy: 0.6307\n",
      "Training loss (for one batch) at step 90: 439.9121, Accuracy: 0.6251\n",
      "Training loss (for one batch) at step 100: 438.1245, Accuracy: 0.6218\n",
      "Training loss (for one batch) at step 110: 438.1846, Accuracy: 0.6239\n",
      "---- Training ----\n",
      "Training loss: 139.8192\n",
      "Training acc over epoch: 0.6248\n",
      "---- Validation ----\n",
      "Validation loss: 34.0369\n",
      "Validation acc: 0.6625\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 439.7107, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 439.8492, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 20: 440.3815, Accuracy: 0.6131\n",
      "Training loss (for one batch) at step 30: 437.3905, Accuracy: 0.6147\n",
      "Training loss (for one batch) at step 40: 437.7270, Accuracy: 0.6271\n",
      "Training loss (for one batch) at step 50: 440.0512, Accuracy: 0.6337\n",
      "Training loss (for one batch) at step 60: 438.9185, Accuracy: 0.6356\n",
      "Training loss (for one batch) at step 70: 443.1807, Accuracy: 0.6394\n",
      "Training loss (for one batch) at step 80: 437.5410, Accuracy: 0.6344\n",
      "Training loss (for one batch) at step 90: 437.4958, Accuracy: 0.6265\n",
      "Training loss (for one batch) at step 100: 436.3392, Accuracy: 0.6256\n",
      "Training loss (for one batch) at step 110: 445.0721, Accuracy: 0.6275\n",
      "---- Training ----\n",
      "Training loss: 137.9191\n",
      "Training acc over epoch: 0.6290\n",
      "---- Validation ----\n",
      "Validation loss: 35.5809\n",
      "Validation acc: 0.6542\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 435.2721, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 438.4244, Accuracy: 0.6236\n",
      "Training loss (for one batch) at step 20: 438.3236, Accuracy: 0.6168\n",
      "Training loss (for one batch) at step 30: 431.9580, Accuracy: 0.6283\n",
      "Training loss (for one batch) at step 40: 436.5416, Accuracy: 0.6402\n",
      "Training loss (for one batch) at step 50: 433.8495, Accuracy: 0.6440\n",
      "Training loss (for one batch) at step 60: 437.1813, Accuracy: 0.6488\n",
      "Training loss (for one batch) at step 70: 443.8564, Accuracy: 0.6499\n",
      "Training loss (for one batch) at step 80: 437.0745, Accuracy: 0.6469\n",
      "Training loss (for one batch) at step 90: 434.7689, Accuracy: 0.6417\n",
      "Training loss (for one batch) at step 100: 434.2435, Accuracy: 0.6424\n",
      "Training loss (for one batch) at step 110: 442.4392, Accuracy: 0.6439\n",
      "---- Training ----\n",
      "Training loss: 138.9214\n",
      "Training acc over epoch: 0.6453\n",
      "---- Validation ----\n",
      "Validation loss: 33.7235\n",
      "Validation acc: 0.6550\n",
      "Time taken: 10.65s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 444.5651, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 442.9351, Accuracy: 0.6456\n",
      "Training loss (for one batch) at step 20: 439.6504, Accuracy: 0.6254\n",
      "Training loss (for one batch) at step 30: 433.5302, Accuracy: 0.6361\n",
      "Training loss (for one batch) at step 40: 441.3358, Accuracy: 0.6351\n",
      "Training loss (for one batch) at step 50: 433.2660, Accuracy: 0.6446\n",
      "Training loss (for one batch) at step 60: 430.5636, Accuracy: 0.6542\n",
      "Training loss (for one batch) at step 70: 444.3974, Accuracy: 0.6557\n",
      "Training loss (for one batch) at step 80: 441.7037, Accuracy: 0.6527\n",
      "Training loss (for one batch) at step 90: 440.5787, Accuracy: 0.6503\n",
      "Training loss (for one batch) at step 100: 435.0311, Accuracy: 0.6514\n",
      "Training loss (for one batch) at step 110: 439.9062, Accuracy: 0.6553\n",
      "---- Training ----\n",
      "Training loss: 136.3617\n",
      "Training acc over epoch: 0.6552\n",
      "---- Validation ----\n",
      "Validation loss: 35.7208\n",
      "Validation acc: 0.6574\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 444.6539, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 439.0287, Accuracy: 0.6499\n",
      "Training loss (for one batch) at step 20: 435.5684, Accuracy: 0.6339\n",
      "Training loss (for one batch) at step 30: 438.1788, Accuracy: 0.6381\n",
      "Training loss (for one batch) at step 40: 433.9283, Accuracy: 0.6503\n",
      "Training loss (for one batch) at step 50: 431.6777, Accuracy: 0.6604\n",
      "Training loss (for one batch) at step 60: 435.2153, Accuracy: 0.6618\n",
      "Training loss (for one batch) at step 70: 443.0105, Accuracy: 0.6660\n",
      "Training loss (for one batch) at step 80: 441.6099, Accuracy: 0.6634\n",
      "Training loss (for one batch) at step 90: 436.5672, Accuracy: 0.6556\n",
      "Training loss (for one batch) at step 100: 435.5899, Accuracy: 0.6574\n",
      "Training loss (for one batch) at step 110: 438.0024, Accuracy: 0.6596\n",
      "---- Training ----\n",
      "Training loss: 130.6315\n",
      "Training acc over epoch: 0.6607\n",
      "---- Validation ----\n",
      "Validation loss: 35.9625\n",
      "Validation acc: 0.6926\n",
      "Time taken: 10.70s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 442.1236, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 437.4543, Accuracy: 0.6662\n",
      "Training loss (for one batch) at step 20: 435.8015, Accuracy: 0.6533\n",
      "Training loss (for one batch) at step 30: 431.0709, Accuracy: 0.6515\n",
      "Training loss (for one batch) at step 40: 430.3578, Accuracy: 0.6555\n",
      "Training loss (for one batch) at step 50: 428.6451, Accuracy: 0.6555\n",
      "Training loss (for one batch) at step 60: 433.3953, Accuracy: 0.6628\n",
      "Training loss (for one batch) at step 70: 438.9812, Accuracy: 0.6687\n",
      "Training loss (for one batch) at step 80: 436.6119, Accuracy: 0.6636\n",
      "Training loss (for one batch) at step 90: 438.1021, Accuracy: 0.6599\n",
      "Training loss (for one batch) at step 100: 429.9561, Accuracy: 0.6629\n",
      "Training loss (for one batch) at step 110: 441.8694, Accuracy: 0.6665\n",
      "---- Training ----\n",
      "Training loss: 135.5079\n",
      "Training acc over epoch: 0.6682\n",
      "---- Validation ----\n",
      "Validation loss: 37.7256\n",
      "Validation acc: 0.6736\n",
      "Time taken: 10.57s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 436.9978, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 438.4897, Accuracy: 0.6477\n",
      "Training loss (for one batch) at step 20: 434.2873, Accuracy: 0.6499\n",
      "Training loss (for one batch) at step 30: 435.4529, Accuracy: 0.6668\n",
      "Training loss (for one batch) at step 40: 423.8262, Accuracy: 0.6681\n",
      "Training loss (for one batch) at step 50: 422.8640, Accuracy: 0.6729\n",
      "Training loss (for one batch) at step 60: 430.5059, Accuracy: 0.6805\n",
      "Training loss (for one batch) at step 70: 443.2583, Accuracy: 0.6861\n",
      "Training loss (for one batch) at step 80: 437.3098, Accuracy: 0.6832\n",
      "Training loss (for one batch) at step 90: 435.1461, Accuracy: 0.6762\n",
      "Training loss (for one batch) at step 100: 432.3731, Accuracy: 0.6769\n",
      "Training loss (for one batch) at step 110: 426.0147, Accuracy: 0.6808\n",
      "---- Training ----\n",
      "Training loss: 134.2805\n",
      "Training acc over epoch: 0.6821\n",
      "---- Validation ----\n",
      "Validation loss: 33.9072\n",
      "Validation acc: 0.6913\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 438.9466, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 447.6093, Accuracy: 0.6776\n",
      "Training loss (for one batch) at step 20: 432.4410, Accuracy: 0.6644\n",
      "Training loss (for one batch) at step 30: 426.1964, Accuracy: 0.6731\n",
      "Training loss (for one batch) at step 40: 422.0751, Accuracy: 0.6833\n",
      "Training loss (for one batch) at step 50: 418.5439, Accuracy: 0.6898\n",
      "Training loss (for one batch) at step 60: 429.1694, Accuracy: 0.6983\n",
      "Training loss (for one batch) at step 70: 440.5735, Accuracy: 0.6985\n",
      "Training loss (for one batch) at step 80: 433.0488, Accuracy: 0.6927\n",
      "Training loss (for one batch) at step 90: 432.6266, Accuracy: 0.6879\n",
      "Training loss (for one batch) at step 100: 430.2288, Accuracy: 0.6899\n",
      "Training loss (for one batch) at step 110: 428.0198, Accuracy: 0.6918\n",
      "---- Training ----\n",
      "Training loss: 137.7460\n",
      "Training acc over epoch: 0.6931\n",
      "---- Validation ----\n",
      "Validation loss: 37.3711\n",
      "Validation acc: 0.6564\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 443.0228, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 434.8680, Accuracy: 0.6839\n",
      "Training loss (for one batch) at step 20: 438.9383, Accuracy: 0.6882\n",
      "Training loss (for one batch) at step 30: 424.6284, Accuracy: 0.6948\n",
      "Training loss (for one batch) at step 40: 422.0131, Accuracy: 0.6938\n",
      "Training loss (for one batch) at step 50: 432.7269, Accuracy: 0.7019\n",
      "Training loss (for one batch) at step 60: 424.6758, Accuracy: 0.7086\n",
      "Training loss (for one batch) at step 70: 441.1887, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 80: 434.1682, Accuracy: 0.7058\n",
      "Training loss (for one batch) at step 90: 434.1534, Accuracy: 0.6956\n",
      "Training loss (for one batch) at step 100: 422.9018, Accuracy: 0.6965\n",
      "Training loss (for one batch) at step 110: 427.2037, Accuracy: 0.7000\n",
      "---- Training ----\n",
      "Training loss: 136.4611\n",
      "Training acc over epoch: 0.7010\n",
      "---- Validation ----\n",
      "Validation loss: 35.1066\n",
      "Validation acc: 0.6276\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 444.8062, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 433.5172, Accuracy: 0.6733\n",
      "Training loss (for one batch) at step 20: 434.3525, Accuracy: 0.6760\n",
      "Training loss (for one batch) at step 30: 423.6025, Accuracy: 0.6837\n",
      "Training loss (for one batch) at step 40: 426.0835, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 50: 409.3351, Accuracy: 0.6988\n",
      "Training loss (for one batch) at step 60: 436.9714, Accuracy: 0.7071\n",
      "Training loss (for one batch) at step 70: 429.1488, Accuracy: 0.7175\n",
      "Training loss (for one batch) at step 80: 425.7442, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 90: 429.4286, Accuracy: 0.7043\n",
      "Training loss (for one batch) at step 100: 423.6017, Accuracy: 0.7045\n",
      "Training loss (for one batch) at step 110: 423.7071, Accuracy: 0.7079\n",
      "---- Training ----\n",
      "Training loss: 135.0239\n",
      "Training acc over epoch: 0.7095\n",
      "---- Validation ----\n",
      "Validation loss: 37.0537\n",
      "Validation acc: 0.6983\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 440.0720, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 435.2317, Accuracy: 0.6861\n",
      "Training loss (for one batch) at step 20: 431.6793, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 30: 429.7082, Accuracy: 0.7046\n",
      "Training loss (for one batch) at step 40: 415.8127, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 50: 418.6469, Accuracy: 0.7223\n",
      "Training loss (for one batch) at step 60: 416.5452, Accuracy: 0.7323\n",
      "Training loss (for one batch) at step 70: 442.3928, Accuracy: 0.7325\n",
      "Training loss (for one batch) at step 80: 433.0604, Accuracy: 0.7278\n",
      "Training loss (for one batch) at step 90: 428.5972, Accuracy: 0.7221\n",
      "Training loss (for one batch) at step 100: 434.2851, Accuracy: 0.7208\n",
      "Training loss (for one batch) at step 110: 419.4944, Accuracy: 0.7212\n",
      "---- Training ----\n",
      "Training loss: 132.7593\n",
      "Training acc over epoch: 0.7228\n",
      "---- Validation ----\n",
      "Validation loss: 34.0904\n",
      "Validation acc: 0.7034\n",
      "Time taken: 10.64s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 436.6303, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 430.0107, Accuracy: 0.7308\n",
      "Training loss (for one batch) at step 20: 420.7782, Accuracy: 0.7295\n",
      "Training loss (for one batch) at step 30: 430.1214, Accuracy: 0.7306\n",
      "Training loss (for one batch) at step 40: 407.6196, Accuracy: 0.7334\n",
      "Training loss (for one batch) at step 50: 400.6094, Accuracy: 0.7364\n",
      "Training loss (for one batch) at step 60: 411.7567, Accuracy: 0.7440\n",
      "Training loss (for one batch) at step 70: 434.9908, Accuracy: 0.7476\n",
      "Training loss (for one batch) at step 80: 432.0199, Accuracy: 0.7394\n",
      "Training loss (for one batch) at step 90: 423.3871, Accuracy: 0.7333\n",
      "Training loss (for one batch) at step 100: 413.1948, Accuracy: 0.7324\n",
      "Training loss (for one batch) at step 110: 427.3694, Accuracy: 0.7330\n",
      "---- Training ----\n",
      "Training loss: 133.6018\n",
      "Training acc over epoch: 0.7329\n",
      "---- Validation ----\n",
      "Validation loss: 36.7651\n",
      "Validation acc: 0.7243\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 438.0061, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 436.2667, Accuracy: 0.7159\n",
      "Training loss (for one batch) at step 20: 433.6243, Accuracy: 0.7054\n",
      "Training loss (for one batch) at step 30: 416.5288, Accuracy: 0.7155\n",
      "Training loss (for one batch) at step 40: 407.0227, Accuracy: 0.7254\n",
      "Training loss (for one batch) at step 50: 411.9824, Accuracy: 0.7339\n",
      "Training loss (for one batch) at step 60: 421.2643, Accuracy: 0.7401\n",
      "Training loss (for one batch) at step 70: 437.8159, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 80: 423.4413, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 90: 418.0381, Accuracy: 0.7315\n",
      "Training loss (for one batch) at step 100: 415.4819, Accuracy: 0.7332\n",
      "Training loss (for one batch) at step 110: 414.3229, Accuracy: 0.7359\n",
      "---- Training ----\n",
      "Training loss: 130.5717\n",
      "Training acc over epoch: 0.7364\n",
      "---- Validation ----\n",
      "Validation loss: 33.9671\n",
      "Validation acc: 0.7415\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 445.1607, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 10: 425.7636, Accuracy: 0.7251\n",
      "Training loss (for one batch) at step 20: 430.9559, Accuracy: 0.7158\n",
      "Training loss (for one batch) at step 30: 415.6170, Accuracy: 0.7273\n",
      "Training loss (for one batch) at step 40: 417.4786, Accuracy: 0.7372\n",
      "Training loss (for one batch) at step 50: 391.4363, Accuracy: 0.7503\n",
      "Training loss (for one batch) at step 60: 407.8810, Accuracy: 0.7610\n",
      "Training loss (for one batch) at step 70: 432.2835, Accuracy: 0.7605\n",
      "Training loss (for one batch) at step 80: 431.2913, Accuracy: 0.7545\n",
      "Training loss (for one batch) at step 90: 431.1040, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 100: 406.8445, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 110: 414.2487, Accuracy: 0.7496\n",
      "---- Training ----\n",
      "Training loss: 129.8243\n",
      "Training acc over epoch: 0.7497\n",
      "---- Validation ----\n",
      "Validation loss: 36.2360\n",
      "Validation acc: 0.7061\n",
      "Time taken: 10.56s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 424.8081, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 420.1565, Accuracy: 0.7280\n",
      "Training loss (for one batch) at step 20: 420.8624, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 30: 412.1678, Accuracy: 0.7492\n",
      "Training loss (for one batch) at step 40: 394.3366, Accuracy: 0.7544\n",
      "Training loss (for one batch) at step 50: 403.6976, Accuracy: 0.7615\n",
      "Training loss (for one batch) at step 60: 416.3553, Accuracy: 0.7668\n",
      "Training loss (for one batch) at step 70: 419.2650, Accuracy: 0.7665\n",
      "Training loss (for one batch) at step 80: 423.1608, Accuracy: 0.7600\n",
      "Training loss (for one batch) at step 90: 406.1721, Accuracy: 0.7540\n",
      "Training loss (for one batch) at step 100: 395.0267, Accuracy: 0.7539\n",
      "Training loss (for one batch) at step 110: 413.8685, Accuracy: 0.7563\n",
      "---- Training ----\n",
      "Training loss: 129.3540\n",
      "Training acc over epoch: 0.7573\n",
      "---- Validation ----\n",
      "Validation loss: 35.3746\n",
      "Validation acc: 0.7286\n",
      "Time taken: 10.62s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 431.3723, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 10: 426.1134, Accuracy: 0.7294\n",
      "Training loss (for one batch) at step 20: 425.4809, Accuracy: 0.7277\n",
      "Training loss (for one batch) at step 30: 416.0156, Accuracy: 0.7354\n",
      "Training loss (for one batch) at step 40: 400.1364, Accuracy: 0.7439\n",
      "Training loss (for one batch) at step 50: 395.1130, Accuracy: 0.7580\n",
      "Training loss (for one batch) at step 60: 395.4657, Accuracy: 0.7690\n",
      "Training loss (for one batch) at step 70: 424.7961, Accuracy: 0.7690\n",
      "Training loss (for one batch) at step 80: 429.2979, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 90: 420.5453, Accuracy: 0.7557\n",
      "Training loss (for one batch) at step 100: 407.3025, Accuracy: 0.7544\n",
      "Training loss (for one batch) at step 110: 413.5560, Accuracy: 0.7578\n",
      "---- Training ----\n",
      "Training loss: 130.1958\n",
      "Training acc over epoch: 0.7575\n",
      "---- Validation ----\n",
      "Validation loss: 33.9487\n",
      "Validation acc: 0.7187\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 431.1914, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 414.4283, Accuracy: 0.7273\n",
      "Training loss (for one batch) at step 20: 423.5449, Accuracy: 0.7217\n",
      "Training loss (for one batch) at step 30: 407.8860, Accuracy: 0.7341\n",
      "Training loss (for one batch) at step 40: 394.2773, Accuracy: 0.7443\n",
      "Training loss (for one batch) at step 50: 397.6024, Accuracy: 0.7581\n",
      "Training loss (for one batch) at step 60: 413.8847, Accuracy: 0.7672\n",
      "Training loss (for one batch) at step 70: 433.5557, Accuracy: 0.7674\n",
      "Training loss (for one batch) at step 80: 422.8119, Accuracy: 0.7630\n",
      "Training loss (for one batch) at step 90: 410.8234, Accuracy: 0.7574\n",
      "Training loss (for one batch) at step 100: 392.9265, Accuracy: 0.7573\n",
      "Training loss (for one batch) at step 110: 408.1122, Accuracy: 0.7597\n",
      "---- Training ----\n",
      "Training loss: 122.2300\n",
      "Training acc over epoch: 0.7603\n",
      "---- Validation ----\n",
      "Validation loss: 37.6838\n",
      "Validation acc: 0.7270\n",
      "Time taken: 10.54s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 445.9753, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 409.7455, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 20: 410.6332, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 30: 392.8791, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 40: 391.1666, Accuracy: 0.7599\n",
      "Training loss (for one batch) at step 50: 377.0490, Accuracy: 0.7710\n",
      "Training loss (for one batch) at step 60: 415.3042, Accuracy: 0.7792\n",
      "Training loss (for one batch) at step 70: 431.7601, Accuracy: 0.7757\n",
      "Training loss (for one batch) at step 80: 424.5435, Accuracy: 0.7676\n",
      "Training loss (for one batch) at step 90: 416.3548, Accuracy: 0.7622\n",
      "Training loss (for one batch) at step 100: 390.0275, Accuracy: 0.7629\n",
      "Training loss (for one batch) at step 110: 407.2796, Accuracy: 0.7629\n",
      "---- Training ----\n",
      "Training loss: 127.1440\n",
      "Training acc over epoch: 0.7640\n",
      "---- Validation ----\n",
      "Validation loss: 42.0037\n",
      "Validation acc: 0.7362\n",
      "Time taken: 10.83s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 443.0494, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 10: 411.3563, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 20: 423.7730, Accuracy: 0.7377\n",
      "Training loss (for one batch) at step 30: 412.8175, Accuracy: 0.7475\n",
      "Training loss (for one batch) at step 40: 369.5336, Accuracy: 0.7660\n",
      "Training loss (for one batch) at step 50: 366.8011, Accuracy: 0.7747\n",
      "Training loss (for one batch) at step 60: 416.1539, Accuracy: 0.7809\n",
      "Training loss (for one batch) at step 70: 420.5695, Accuracy: 0.7778\n",
      "Training loss (for one batch) at step 80: 427.6943, Accuracy: 0.7693\n",
      "Training loss (for one batch) at step 90: 397.7791, Accuracy: 0.7630\n",
      "Training loss (for one batch) at step 100: 404.8208, Accuracy: 0.7614\n",
      "Training loss (for one batch) at step 110: 410.3820, Accuracy: 0.7638\n",
      "---- Training ----\n",
      "Training loss: 132.0725\n",
      "Training acc over epoch: 0.7635\n",
      "---- Validation ----\n",
      "Validation loss: 41.9880\n",
      "Validation acc: 0.7265\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 425.2324, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 10: 414.9896, Accuracy: 0.7251\n",
      "Training loss (for one batch) at step 20: 409.2306, Accuracy: 0.7321\n",
      "Training loss (for one batch) at step 30: 387.9812, Accuracy: 0.7487\n",
      "Training loss (for one batch) at step 40: 383.6231, Accuracy: 0.7563\n",
      "Training loss (for one batch) at step 50: 368.9764, Accuracy: 0.7713\n",
      "Training loss (for one batch) at step 60: 384.4450, Accuracy: 0.7795\n",
      "Training loss (for one batch) at step 70: 407.3090, Accuracy: 0.7743\n",
      "Training loss (for one batch) at step 80: 401.3758, Accuracy: 0.7673\n",
      "Training loss (for one batch) at step 90: 383.7591, Accuracy: 0.7620\n",
      "Training loss (for one batch) at step 100: 384.8016, Accuracy: 0.7625\n",
      "Training loss (for one batch) at step 110: 400.7913, Accuracy: 0.7630\n",
      "---- Training ----\n",
      "Training loss: 126.3974\n",
      "Training acc over epoch: 0.7641\n",
      "---- Validation ----\n",
      "Validation loss: 35.7763\n",
      "Validation acc: 0.7192\n",
      "Time taken: 10.58s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 430.9028, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 392.5153, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 20: 404.5222, Accuracy: 0.7452\n",
      "Training loss (for one batch) at step 30: 390.2801, Accuracy: 0.7613\n",
      "Training loss (for one batch) at step 40: 399.5591, Accuracy: 0.7692\n",
      "Training loss (for one batch) at step 50: 378.6903, Accuracy: 0.7820\n",
      "Training loss (for one batch) at step 60: 384.8681, Accuracy: 0.7894\n",
      "Training loss (for one batch) at step 70: 408.3313, Accuracy: 0.7873\n",
      "Training loss (for one batch) at step 80: 414.5512, Accuracy: 0.7791\n",
      "Training loss (for one batch) at step 90: 393.9243, Accuracy: 0.7751\n",
      "Training loss (for one batch) at step 100: 380.5328, Accuracy: 0.7741\n",
      "Training loss (for one batch) at step 110: 398.7564, Accuracy: 0.7752\n",
      "---- Training ----\n",
      "Training loss: 127.5279\n",
      "Training acc over epoch: 0.7765\n",
      "---- Validation ----\n",
      "Validation loss: 42.9342\n",
      "Validation acc: 0.6994\n",
      "Time taken: 10.56s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 432.1237, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 418.5790, Accuracy: 0.7337\n",
      "Training loss (for one batch) at step 20: 387.3852, Accuracy: 0.7366\n",
      "Training loss (for one batch) at step 30: 403.1455, Accuracy: 0.7530\n",
      "Training loss (for one batch) at step 40: 374.0541, Accuracy: 0.7704\n",
      "Training loss (for one batch) at step 50: 372.2541, Accuracy: 0.7826\n",
      "Training loss (for one batch) at step 60: 385.6570, Accuracy: 0.7903\n",
      "Training loss (for one batch) at step 70: 411.4265, Accuracy: 0.7860\n",
      "Training loss (for one batch) at step 80: 410.3019, Accuracy: 0.7761\n",
      "Training loss (for one batch) at step 90: 397.0750, Accuracy: 0.7717\n",
      "Training loss (for one batch) at step 100: 369.1212, Accuracy: 0.7720\n",
      "Training loss (for one batch) at step 110: 393.8250, Accuracy: 0.7736\n",
      "---- Training ----\n",
      "Training loss: 117.3203\n",
      "Training acc over epoch: 0.7737\n",
      "---- Validation ----\n",
      "Validation loss: 39.2470\n",
      "Validation acc: 0.7149\n",
      "Time taken: 10.39s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 421.8935, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 408.8380, Accuracy: 0.7379\n",
      "Training loss (for one batch) at step 20: 398.4672, Accuracy: 0.7359\n",
      "Training loss (for one batch) at step 30: 386.5721, Accuracy: 0.7581\n",
      "Training loss (for one batch) at step 40: 378.8706, Accuracy: 0.7744\n",
      "Training loss (for one batch) at step 50: 356.7571, Accuracy: 0.7872\n",
      "Training loss (for one batch) at step 60: 372.5247, Accuracy: 0.7942\n",
      "Training loss (for one batch) at step 70: 408.6105, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 80: 397.6927, Accuracy: 0.7796\n",
      "Training loss (for one batch) at step 90: 368.4719, Accuracy: 0.7746\n",
      "Training loss (for one batch) at step 100: 367.3102, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 110: 381.9722, Accuracy: 0.7777\n",
      "---- Training ----\n",
      "Training loss: 123.5006\n",
      "Training acc over epoch: 0.7784\n",
      "---- Validation ----\n",
      "Validation loss: 42.1902\n",
      "Validation acc: 0.7160\n",
      "Time taken: 10.70s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 404.8701, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 10: 405.6434, Accuracy: 0.7358\n",
      "Training loss (for one batch) at step 20: 403.7883, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 30: 365.2480, Accuracy: 0.7560\n",
      "Training loss (for one batch) at step 40: 355.3203, Accuracy: 0.7717\n",
      "Training loss (for one batch) at step 50: 382.4810, Accuracy: 0.7835\n",
      "Training loss (for one batch) at step 60: 379.9785, Accuracy: 0.7889\n",
      "Training loss (for one batch) at step 70: 407.1200, Accuracy: 0.7844\n",
      "Training loss (for one batch) at step 80: 407.3883, Accuracy: 0.7728\n",
      "Training loss (for one batch) at step 90: 406.6908, Accuracy: 0.7708\n",
      "Training loss (for one batch) at step 100: 356.4971, Accuracy: 0.7714\n",
      "Training loss (for one batch) at step 110: 373.9180, Accuracy: 0.7732\n",
      "---- Training ----\n",
      "Training loss: 114.8691\n",
      "Training acc over epoch: 0.7729\n",
      "---- Validation ----\n",
      "Validation loss: 48.2244\n",
      "Validation acc: 0.7069\n",
      "Time taken: 10.65s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 409.9994, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 410.3533, Accuracy: 0.7315\n",
      "Training loss (for one batch) at step 20: 382.4105, Accuracy: 0.7467\n",
      "Training loss (for one batch) at step 30: 379.7542, Accuracy: 0.7661\n",
      "Training loss (for one batch) at step 40: 356.0381, Accuracy: 0.7799\n",
      "Training loss (for one batch) at step 50: 355.7997, Accuracy: 0.7932\n",
      "Training loss (for one batch) at step 60: 373.9542, Accuracy: 0.8008\n",
      "Training loss (for one batch) at step 70: 386.3298, Accuracy: 0.7921\n",
      "Training loss (for one batch) at step 80: 382.0334, Accuracy: 0.7807\n",
      "Training loss (for one batch) at step 90: 379.2156, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 100: 365.9760, Accuracy: 0.7798\n",
      "Training loss (for one batch) at step 110: 378.1232, Accuracy: 0.7810\n",
      "---- Training ----\n",
      "Training loss: 123.5026\n",
      "Training acc over epoch: 0.7809\n",
      "---- Validation ----\n",
      "Validation loss: 44.2783\n",
      "Validation acc: 0.7195\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 429.6336, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 392.6832, Accuracy: 0.7301\n",
      "Training loss (for one batch) at step 20: 382.4823, Accuracy: 0.7392\n",
      "Training loss (for one batch) at step 30: 366.1011, Accuracy: 0.7608\n",
      "Training loss (for one batch) at step 40: 337.8529, Accuracy: 0.7769\n",
      "Training loss (for one batch) at step 50: 363.2935, Accuracy: 0.7903\n",
      "Training loss (for one batch) at step 60: 367.1385, Accuracy: 0.7941\n",
      "Training loss (for one batch) at step 70: 395.2780, Accuracy: 0.7868\n",
      "Training loss (for one batch) at step 80: 397.8397, Accuracy: 0.7784\n",
      "Training loss (for one batch) at step 90: 373.1734, Accuracy: 0.7757\n",
      "Training loss (for one batch) at step 100: 348.9901, Accuracy: 0.7790\n",
      "Training loss (for one batch) at step 110: 368.6457, Accuracy: 0.7783\n",
      "---- Training ----\n",
      "Training loss: 122.8299\n",
      "Training acc over epoch: 0.7773\n",
      "---- Validation ----\n",
      "Validation loss: 34.5083\n",
      "Validation acc: 0.6980\n",
      "Time taken: 10.88s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 388.7062, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 400.6895, Accuracy: 0.7116\n",
      "Training loss (for one batch) at step 20: 362.1937, Accuracy: 0.7440\n",
      "Training loss (for one batch) at step 30: 361.5015, Accuracy: 0.7626\n",
      "Training loss (for one batch) at step 40: 362.5355, Accuracy: 0.7809\n",
      "Training loss (for one batch) at step 50: 349.4900, Accuracy: 0.7927\n",
      "Training loss (for one batch) at step 60: 361.7920, Accuracy: 0.8014\n",
      "Training loss (for one batch) at step 70: 370.0259, Accuracy: 0.7927\n",
      "Training loss (for one batch) at step 80: 384.8699, Accuracy: 0.7849\n",
      "Training loss (for one batch) at step 90: 359.1952, Accuracy: 0.7803\n",
      "Training loss (for one batch) at step 100: 339.2300, Accuracy: 0.7830\n",
      "Training loss (for one batch) at step 110: 372.2688, Accuracy: 0.7836\n",
      "---- Training ----\n",
      "Training loss: 119.2874\n",
      "Training acc over epoch: 0.7830\n",
      "---- Validation ----\n",
      "Validation loss: 50.4699\n",
      "Validation acc: 0.7047\n",
      "Time taken: 10.81s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 385.0730, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 395.4112, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 20: 375.5320, Accuracy: 0.7262\n",
      "Training loss (for one batch) at step 30: 346.2856, Accuracy: 0.7548\n",
      "Training loss (for one batch) at step 40: 361.6018, Accuracy: 0.7694\n",
      "Training loss (for one batch) at step 50: 353.8723, Accuracy: 0.7835\n",
      "Training loss (for one batch) at step 60: 359.5021, Accuracy: 0.7928\n",
      "Training loss (for one batch) at step 70: 400.7289, Accuracy: 0.7825\n",
      "Training loss (for one batch) at step 80: 384.8078, Accuracy: 0.7710\n",
      "Training loss (for one batch) at step 90: 368.7057, Accuracy: 0.7675\n",
      "Training loss (for one batch) at step 100: 330.9783, Accuracy: 0.7704\n",
      "Training loss (for one batch) at step 110: 380.6300, Accuracy: 0.7724\n",
      "---- Training ----\n",
      "Training loss: 115.2827\n",
      "Training acc over epoch: 0.7716\n",
      "---- Validation ----\n",
      "Validation loss: 42.9285\n",
      "Validation acc: 0.6905\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 385.7952, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 374.1064, Accuracy: 0.7216\n",
      "Training loss (for one batch) at step 20: 359.1978, Accuracy: 0.7493\n",
      "Training loss (for one batch) at step 30: 356.4453, Accuracy: 0.7681\n",
      "Training loss (for one batch) at step 40: 356.0137, Accuracy: 0.7811\n",
      "Training loss (for one batch) at step 50: 332.8898, Accuracy: 0.7923\n",
      "Training loss (for one batch) at step 60: 370.3083, Accuracy: 0.7997\n",
      "Training loss (for one batch) at step 70: 381.8828, Accuracy: 0.7913\n",
      "Training loss (for one batch) at step 80: 393.0297, Accuracy: 0.7798\n",
      "Training loss (for one batch) at step 90: 351.6152, Accuracy: 0.7741\n",
      "Training loss (for one batch) at step 100: 345.0325, Accuracy: 0.7771\n",
      "Training loss (for one batch) at step 110: 384.0211, Accuracy: 0.7790\n",
      "---- Training ----\n",
      "Training loss: 117.8201\n",
      "Training acc over epoch: 0.7779\n",
      "---- Validation ----\n",
      "Validation loss: 40.9337\n",
      "Validation acc: 0.7053\n",
      "Time taken: 10.56s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 384.4693, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 385.0256, Accuracy: 0.7152\n",
      "Training loss (for one batch) at step 20: 353.5998, Accuracy: 0.7366\n",
      "Training loss (for one batch) at step 30: 356.4639, Accuracy: 0.7558\n",
      "Training loss (for one batch) at step 40: 349.7800, Accuracy: 0.7740\n",
      "Training loss (for one batch) at step 50: 364.5135, Accuracy: 0.7912\n",
      "Training loss (for one batch) at step 60: 343.6141, Accuracy: 0.7976\n",
      "Training loss (for one batch) at step 70: 382.9829, Accuracy: 0.7896\n",
      "Training loss (for one batch) at step 80: 369.1625, Accuracy: 0.7796\n",
      "Training loss (for one batch) at step 90: 353.7472, Accuracy: 0.7765\n",
      "Training loss (for one batch) at step 100: 343.7502, Accuracy: 0.7794\n",
      "Training loss (for one batch) at step 110: 358.3286, Accuracy: 0.7787\n",
      "---- Training ----\n",
      "Training loss: 122.3125\n",
      "Training acc over epoch: 0.7782\n",
      "---- Validation ----\n",
      "Validation loss: 38.3104\n",
      "Validation acc: 0.7117\n",
      "Time taken: 10.70s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 391.9740, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 389.3445, Accuracy: 0.7259\n",
      "Training loss (for one batch) at step 20: 369.5641, Accuracy: 0.7556\n",
      "Training loss (for one batch) at step 30: 328.3883, Accuracy: 0.7681\n",
      "Training loss (for one batch) at step 40: 336.4061, Accuracy: 0.7860\n",
      "Training loss (for one batch) at step 50: 328.0540, Accuracy: 0.8013\n",
      "Training loss (for one batch) at step 60: 350.8343, Accuracy: 0.8081\n",
      "Training loss (for one batch) at step 70: 390.7060, Accuracy: 0.7958\n",
      "Training loss (for one batch) at step 80: 389.4582, Accuracy: 0.7804\n",
      "Training loss (for one batch) at step 90: 352.3785, Accuracy: 0.7788\n",
      "Training loss (for one batch) at step 100: 350.2457, Accuracy: 0.7808\n",
      "Training loss (for one batch) at step 110: 336.6163, Accuracy: 0.7832\n",
      "---- Training ----\n",
      "Training loss: 110.7076\n",
      "Training acc over epoch: 0.7826\n",
      "---- Validation ----\n",
      "Validation loss: 38.1717\n",
      "Validation acc: 0.7120\n",
      "Time taken: 12.51s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 389.4018, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 374.8690, Accuracy: 0.6946\n",
      "Training loss (for one batch) at step 20: 337.6010, Accuracy: 0.7325\n",
      "Training loss (for one batch) at step 30: 343.1714, Accuracy: 0.7581\n",
      "Training loss (for one batch) at step 40: 354.2196, Accuracy: 0.7784\n",
      "Training loss (for one batch) at step 50: 329.4468, Accuracy: 0.7921\n",
      "Training loss (for one batch) at step 60: 349.3566, Accuracy: 0.8011\n",
      "Training loss (for one batch) at step 70: 358.0814, Accuracy: 0.7888\n",
      "Training loss (for one batch) at step 80: 402.5182, Accuracy: 0.7764\n",
      "Training loss (for one batch) at step 90: 357.7094, Accuracy: 0.7740\n",
      "Training loss (for one batch) at step 100: 339.5001, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 110: 366.3405, Accuracy: 0.7786\n",
      "---- Training ----\n",
      "Training loss: 110.5591\n",
      "Training acc over epoch: 0.7768\n",
      "---- Validation ----\n",
      "Validation loss: 34.5859\n",
      "Validation acc: 0.6991\n",
      "Time taken: 10.76s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 380.3286, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 362.2820, Accuracy: 0.6811\n",
      "Training loss (for one batch) at step 20: 343.5209, Accuracy: 0.7206\n",
      "Training loss (for one batch) at step 30: 338.1574, Accuracy: 0.7560\n",
      "Training loss (for one batch) at step 40: 333.4571, Accuracy: 0.7753\n",
      "Training loss (for one batch) at step 50: 322.7700, Accuracy: 0.7903\n",
      "Training loss (for one batch) at step 60: 342.3305, Accuracy: 0.7983\n",
      "Training loss (for one batch) at step 70: 368.8348, Accuracy: 0.7909\n",
      "Training loss (for one batch) at step 80: 368.5157, Accuracy: 0.7766\n",
      "Training loss (for one batch) at step 90: 348.4645, Accuracy: 0.7721\n",
      "Training loss (for one batch) at step 100: 317.4910, Accuracy: 0.7760\n",
      "Training loss (for one batch) at step 110: 346.4428, Accuracy: 0.7770\n",
      "---- Training ----\n",
      "Training loss: 127.6374\n",
      "Training acc over epoch: 0.7763\n",
      "---- Validation ----\n",
      "Validation loss: 55.8295\n",
      "Validation acc: 0.7179\n",
      "Time taken: 10.93s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 388.8091, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 361.3949, Accuracy: 0.7102\n",
      "Training loss (for one batch) at step 20: 368.1980, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 30: 332.4212, Accuracy: 0.7613\n",
      "Training loss (for one batch) at step 40: 336.8388, Accuracy: 0.7811\n",
      "Training loss (for one batch) at step 50: 333.1653, Accuracy: 0.7941\n",
      "Training loss (for one batch) at step 60: 349.2350, Accuracy: 0.7965\n",
      "Training loss (for one batch) at step 70: 362.1364, Accuracy: 0.7882\n",
      "Training loss (for one batch) at step 80: 380.0527, Accuracy: 0.7768\n",
      "Training loss (for one batch) at step 90: 327.5099, Accuracy: 0.7728\n",
      "Training loss (for one batch) at step 100: 321.3276, Accuracy: 0.7757\n",
      "Training loss (for one batch) at step 110: 344.2614, Accuracy: 0.7763\n",
      "---- Training ----\n",
      "Training loss: 120.8432\n",
      "Training acc over epoch: 0.7762\n",
      "---- Validation ----\n",
      "Validation loss: 44.5335\n",
      "Validation acc: 0.7031\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 387.4059, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 376.5968, Accuracy: 0.7017\n",
      "Training loss (for one batch) at step 20: 343.4701, Accuracy: 0.7310\n",
      "Training loss (for one batch) at step 30: 323.3502, Accuracy: 0.7626\n",
      "Training loss (for one batch) at step 40: 317.2394, Accuracy: 0.7786\n",
      "Training loss (for one batch) at step 50: 328.2692, Accuracy: 0.7934\n",
      "Training loss (for one batch) at step 60: 342.3506, Accuracy: 0.8014\n",
      "Training loss (for one batch) at step 70: 352.1225, Accuracy: 0.7912\n",
      "Training loss (for one batch) at step 80: 366.4787, Accuracy: 0.7800\n",
      "Training loss (for one batch) at step 90: 327.8137, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 100: 324.9319, Accuracy: 0.7804\n",
      "Training loss (for one batch) at step 110: 351.2694, Accuracy: 0.7805\n",
      "---- Training ----\n",
      "Training loss: 99.9533\n",
      "Training acc over epoch: 0.7801\n",
      "---- Validation ----\n",
      "Validation loss: 46.9613\n",
      "Validation acc: 0.7152\n",
      "Time taken: 10.75s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 377.2285, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 353.9688, Accuracy: 0.7017\n",
      "Training loss (for one batch) at step 20: 335.9809, Accuracy: 0.7340\n",
      "Training loss (for one batch) at step 30: 335.8379, Accuracy: 0.7603\n",
      "Training loss (for one batch) at step 40: 346.5583, Accuracy: 0.7814\n",
      "Training loss (for one batch) at step 50: 311.6436, Accuracy: 0.7947\n",
      "Training loss (for one batch) at step 60: 341.0863, Accuracy: 0.8025\n",
      "Training loss (for one batch) at step 70: 351.4797, Accuracy: 0.7916\n",
      "Training loss (for one batch) at step 80: 384.9342, Accuracy: 0.7773\n",
      "Training loss (for one batch) at step 90: 342.6911, Accuracy: 0.7736\n",
      "Training loss (for one batch) at step 100: 315.7950, Accuracy: 0.7785\n",
      "Training loss (for one batch) at step 110: 344.4530, Accuracy: 0.7794\n",
      "---- Training ----\n",
      "Training loss: 108.0514\n",
      "Training acc over epoch: 0.7779\n",
      "---- Validation ----\n",
      "Validation loss: 46.6742\n",
      "Validation acc: 0.7235\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 361.9708, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 364.2830, Accuracy: 0.7060\n",
      "Training loss (for one batch) at step 20: 336.8847, Accuracy: 0.7325\n",
      "Training loss (for one batch) at step 30: 316.7070, Accuracy: 0.7616\n",
      "Training loss (for one batch) at step 40: 316.9980, Accuracy: 0.7782\n",
      "Training loss (for one batch) at step 50: 330.1446, Accuracy: 0.7927\n",
      "Training loss (for one batch) at step 60: 354.1249, Accuracy: 0.8011\n",
      "Training loss (for one batch) at step 70: 342.3252, Accuracy: 0.7912\n",
      "Training loss (for one batch) at step 80: 371.6738, Accuracy: 0.7779\n",
      "Training loss (for one batch) at step 90: 338.6508, Accuracy: 0.7740\n",
      "Training loss (for one batch) at step 100: 310.5694, Accuracy: 0.7767\n",
      "Training loss (for one batch) at step 110: 330.3059, Accuracy: 0.7790\n",
      "---- Training ----\n",
      "Training loss: 115.3900\n",
      "Training acc over epoch: 0.7792\n",
      "---- Validation ----\n",
      "Validation loss: 36.7597\n",
      "Validation acc: 0.7141\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 385.1923, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 385.9988, Accuracy: 0.7017\n",
      "Training loss (for one batch) at step 20: 334.4391, Accuracy: 0.7400\n",
      "Training loss (for one batch) at step 30: 309.6627, Accuracy: 0.7654\n",
      "Training loss (for one batch) at step 40: 336.3211, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 50: 320.1609, Accuracy: 0.7976\n",
      "Training loss (for one batch) at step 60: 327.5134, Accuracy: 0.8058\n",
      "Training loss (for one batch) at step 70: 341.6313, Accuracy: 0.7956\n",
      "Training loss (for one batch) at step 80: 359.0818, Accuracy: 0.7796\n",
      "Training loss (for one batch) at step 90: 324.1284, Accuracy: 0.7787\n",
      "Training loss (for one batch) at step 100: 330.8896, Accuracy: 0.7830\n",
      "Training loss (for one batch) at step 110: 352.0702, Accuracy: 0.7860\n",
      "---- Training ----\n",
      "Training loss: 115.0657\n",
      "Training acc over epoch: 0.7847\n",
      "---- Validation ----\n",
      "Validation loss: 53.1377\n",
      "Validation acc: 0.7120\n",
      "Time taken: 10.59s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 368.8932, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 365.0287, Accuracy: 0.6847\n",
      "Training loss (for one batch) at step 20: 337.8888, Accuracy: 0.7329\n",
      "Training loss (for one batch) at step 30: 320.5602, Accuracy: 0.7598\n",
      "Training loss (for one batch) at step 40: 319.6140, Accuracy: 0.7784\n",
      "Training loss (for one batch) at step 50: 306.3089, Accuracy: 0.7950\n",
      "Training loss (for one batch) at step 60: 332.6338, Accuracy: 0.8002\n",
      "Training loss (for one batch) at step 70: 331.0231, Accuracy: 0.7913\n",
      "Training loss (for one batch) at step 80: 363.3105, Accuracy: 0.7775\n",
      "Training loss (for one batch) at step 90: 325.3448, Accuracy: 0.7739\n",
      "Training loss (for one batch) at step 100: 303.5086, Accuracy: 0.7782\n",
      "Training loss (for one batch) at step 110: 328.3663, Accuracy: 0.7792\n",
      "---- Training ----\n",
      "Training loss: 110.8588\n",
      "Training acc over epoch: 0.7777\n",
      "---- Validation ----\n",
      "Validation loss: 36.7654\n",
      "Validation acc: 0.7157\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 393.2852, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 359.8924, Accuracy: 0.6946\n",
      "Training loss (for one batch) at step 20: 325.6713, Accuracy: 0.7292\n",
      "Training loss (for one batch) at step 30: 333.0620, Accuracy: 0.7568\n",
      "Training loss (for one batch) at step 40: 318.5079, Accuracy: 0.7782\n",
      "Training loss (for one batch) at step 50: 321.6104, Accuracy: 0.7920\n",
      "Training loss (for one batch) at step 60: 331.9522, Accuracy: 0.8019\n",
      "Training loss (for one batch) at step 70: 342.2811, Accuracy: 0.7926\n",
      "Training loss (for one batch) at step 80: 353.4791, Accuracy: 0.7800\n",
      "Training loss (for one batch) at step 90: 343.8177, Accuracy: 0.7767\n",
      "Training loss (for one batch) at step 100: 311.7398, Accuracy: 0.7782\n",
      "Training loss (for one batch) at step 110: 334.4355, Accuracy: 0.7806\n",
      "---- Training ----\n",
      "Training loss: 107.8310\n",
      "Training acc over epoch: 0.7794\n",
      "---- Validation ----\n",
      "Validation loss: 42.4081\n",
      "Validation acc: 0.7039\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 375.5891, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 352.8609, Accuracy: 0.6932\n",
      "Training loss (for one batch) at step 20: 327.6613, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 30: 332.0287, Accuracy: 0.7636\n",
      "Training loss (for one batch) at step 40: 297.8314, Accuracy: 0.7847\n",
      "Training loss (for one batch) at step 50: 319.1093, Accuracy: 0.7976\n",
      "Training loss (for one batch) at step 60: 329.9156, Accuracy: 0.8030\n",
      "Training loss (for one batch) at step 70: 341.8596, Accuracy: 0.7925\n",
      "Training loss (for one batch) at step 80: 354.8931, Accuracy: 0.7797\n",
      "Training loss (for one batch) at step 90: 319.8860, Accuracy: 0.7759\n",
      "Training loss (for one batch) at step 100: 325.6822, Accuracy: 0.7790\n",
      "Training loss (for one batch) at step 110: 326.9729, Accuracy: 0.7804\n",
      "---- Training ----\n",
      "Training loss: 117.7509\n",
      "Training acc over epoch: 0.7794\n",
      "---- Validation ----\n",
      "Validation loss: 40.0216\n",
      "Validation acc: 0.7141\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 366.1288, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 347.9291, Accuracy: 0.6889\n",
      "Training loss (for one batch) at step 20: 329.7889, Accuracy: 0.7199\n",
      "Training loss (for one batch) at step 30: 311.1814, Accuracy: 0.7533\n",
      "Training loss (for one batch) at step 40: 318.1247, Accuracy: 0.7769\n",
      "Training loss (for one batch) at step 50: 305.7940, Accuracy: 0.7934\n",
      "Training loss (for one batch) at step 60: 322.2930, Accuracy: 0.7985\n",
      "Training loss (for one batch) at step 70: 350.6656, Accuracy: 0.7882\n",
      "Training loss (for one batch) at step 80: 351.9559, Accuracy: 0.7759\n",
      "Training loss (for one batch) at step 90: 320.0391, Accuracy: 0.7721\n",
      "Training loss (for one batch) at step 100: 312.2351, Accuracy: 0.7769\n",
      "Training loss (for one batch) at step 110: 316.1126, Accuracy: 0.7782\n",
      "---- Training ----\n",
      "Training loss: 109.8115\n",
      "Training acc over epoch: 0.7779\n",
      "---- Validation ----\n",
      "Validation loss: 61.8582\n",
      "Validation acc: 0.7182\n",
      "Time taken: 10.44s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 379.4222, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 359.3938, Accuracy: 0.7060\n",
      "Training loss (for one batch) at step 20: 314.7222, Accuracy: 0.7359\n",
      "Training loss (for one batch) at step 30: 322.9632, Accuracy: 0.7651\n",
      "Training loss (for one batch) at step 40: 336.7844, Accuracy: 0.7847\n",
      "Training loss (for one batch) at step 50: 301.5745, Accuracy: 0.7944\n",
      "Training loss (for one batch) at step 60: 332.0284, Accuracy: 0.8025\n",
      "Training loss (for one batch) at step 70: 325.5169, Accuracy: 0.7884\n",
      "Training loss (for one batch) at step 80: 362.2975, Accuracy: 0.7758\n",
      "Training loss (for one batch) at step 90: 318.9347, Accuracy: 0.7757\n",
      "Training loss (for one batch) at step 100: 311.0677, Accuracy: 0.7786\n",
      "Training loss (for one batch) at step 110: 354.5094, Accuracy: 0.7801\n",
      "---- Training ----\n",
      "Training loss: 114.1919\n",
      "Training acc over epoch: 0.7792\n",
      "---- Validation ----\n",
      "Validation loss: 50.3394\n",
      "Validation acc: 0.7241\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 340.7437, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 351.0481, Accuracy: 0.6960\n",
      "Training loss (for one batch) at step 20: 329.7681, Accuracy: 0.7217\n",
      "Training loss (for one batch) at step 30: 311.2912, Accuracy: 0.7560\n",
      "Training loss (for one batch) at step 40: 302.3350, Accuracy: 0.7776\n",
      "Training loss (for one batch) at step 50: 302.9764, Accuracy: 0.7944\n",
      "Training loss (for one batch) at step 60: 321.3055, Accuracy: 0.8029\n",
      "Training loss (for one batch) at step 70: 335.4033, Accuracy: 0.7926\n",
      "Training loss (for one batch) at step 80: 361.2549, Accuracy: 0.7781\n",
      "Training loss (for one batch) at step 90: 326.8145, Accuracy: 0.7741\n",
      "Training loss (for one batch) at step 100: 300.2412, Accuracy: 0.7793\n",
      "Training loss (for one batch) at step 110: 340.7311, Accuracy: 0.7801\n",
      "---- Training ----\n",
      "Training loss: 96.0329\n",
      "Training acc over epoch: 0.7787\n",
      "---- Validation ----\n",
      "Validation loss: 46.0836\n",
      "Validation acc: 0.7190\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 363.9748, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 351.2826, Accuracy: 0.6896\n",
      "Training loss (for one batch) at step 20: 325.4103, Accuracy: 0.7310\n",
      "Training loss (for one batch) at step 30: 332.8708, Accuracy: 0.7608\n",
      "Training loss (for one batch) at step 40: 302.9237, Accuracy: 0.7822\n",
      "Training loss (for one batch) at step 50: 299.7566, Accuracy: 0.7956\n",
      "Training loss (for one batch) at step 60: 328.9586, Accuracy: 0.8042\n",
      "Training loss (for one batch) at step 70: 342.6222, Accuracy: 0.7924\n",
      "Training loss (for one batch) at step 80: 350.0177, Accuracy: 0.7769\n",
      "Training loss (for one batch) at step 90: 322.9757, Accuracy: 0.7732\n",
      "Training loss (for one batch) at step 100: 301.7644, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 110: 328.6198, Accuracy: 0.7782\n",
      "---- Training ----\n",
      "Training loss: 102.4885\n",
      "Training acc over epoch: 0.7779\n",
      "---- Validation ----\n",
      "Validation loss: 55.2269\n",
      "Validation acc: 0.7069\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 360.8227, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 341.6651, Accuracy: 0.6925\n",
      "Training loss (for one batch) at step 20: 322.2932, Accuracy: 0.7314\n",
      "Training loss (for one batch) at step 30: 292.1812, Accuracy: 0.7659\n",
      "Training loss (for one batch) at step 40: 289.8585, Accuracy: 0.7866\n",
      "Training loss (for one batch) at step 50: 298.8025, Accuracy: 0.8001\n",
      "Training loss (for one batch) at step 60: 335.3829, Accuracy: 0.8072\n",
      "Training loss (for one batch) at step 70: 371.5312, Accuracy: 0.7965\n",
      "Training loss (for one batch) at step 80: 328.3533, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 90: 291.8771, Accuracy: 0.7774\n",
      "Training loss (for one batch) at step 100: 294.6728, Accuracy: 0.7821\n",
      "Training loss (for one batch) at step 110: 318.8958, Accuracy: 0.7824\n",
      "---- Training ----\n",
      "Training loss: 107.7794\n",
      "Training acc over epoch: 0.7824\n",
      "---- Validation ----\n",
      "Validation loss: 50.7228\n",
      "Validation acc: 0.7007\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 374.8338, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 358.1328, Accuracy: 0.6847\n",
      "Training loss (for one batch) at step 20: 309.4337, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 30: 317.0449, Accuracy: 0.7671\n",
      "Training loss (for one batch) at step 40: 292.5438, Accuracy: 0.7877\n",
      "Training loss (for one batch) at step 50: 313.5053, Accuracy: 0.7999\n",
      "Training loss (for one batch) at step 60: 341.2604, Accuracy: 0.8076\n",
      "Training loss (for one batch) at step 70: 356.9466, Accuracy: 0.7943\n",
      "Training loss (for one batch) at step 80: 344.6447, Accuracy: 0.7794\n",
      "Training loss (for one batch) at step 90: 305.0089, Accuracy: 0.7754\n",
      "Training loss (for one batch) at step 100: 332.1664, Accuracy: 0.7811\n",
      "Training loss (for one batch) at step 110: 326.8275, Accuracy: 0.7818\n",
      "---- Training ----\n",
      "Training loss: 94.9962\n",
      "Training acc over epoch: 0.7817\n",
      "---- Validation ----\n",
      "Validation loss: 47.0534\n",
      "Validation acc: 0.7114\n",
      "Time taken: 10.67s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 331.6693, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 336.2158, Accuracy: 0.6932\n",
      "Training loss (for one batch) at step 20: 315.8605, Accuracy: 0.7314\n",
      "Training loss (for one batch) at step 30: 315.8094, Accuracy: 0.7639\n",
      "Training loss (for one batch) at step 40: 294.5550, Accuracy: 0.7830\n",
      "Training loss (for one batch) at step 50: 299.6019, Accuracy: 0.7981\n",
      "Training loss (for one batch) at step 60: 313.3166, Accuracy: 0.8039\n",
      "Training loss (for one batch) at step 70: 326.8362, Accuracy: 0.7926\n",
      "Training loss (for one batch) at step 80: 346.5642, Accuracy: 0.7772\n",
      "Training loss (for one batch) at step 90: 313.9908, Accuracy: 0.7763\n",
      "Training loss (for one batch) at step 100: 303.7657, Accuracy: 0.7799\n",
      "Training loss (for one batch) at step 110: 318.3907, Accuracy: 0.7812\n",
      "---- Training ----\n",
      "Training loss: 101.4307\n",
      "Training acc over epoch: 0.7810\n",
      "---- Validation ----\n",
      "Validation loss: 61.5265\n",
      "Validation acc: 0.7055\n",
      "Time taken: 10.60s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 345.9867, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 340.8435, Accuracy: 0.6861\n",
      "Training loss (for one batch) at step 20: 329.9984, Accuracy: 0.7202\n",
      "Training loss (for one batch) at step 30: 301.2714, Accuracy: 0.7535\n",
      "Training loss (for one batch) at step 40: 305.2346, Accuracy: 0.7759\n",
      "Training loss (for one batch) at step 50: 296.5875, Accuracy: 0.7927\n",
      "Training loss (for one batch) at step 60: 318.4843, Accuracy: 0.8014\n",
      "Training loss (for one batch) at step 70: 322.7288, Accuracy: 0.7901\n",
      "Training loss (for one batch) at step 80: 341.3947, Accuracy: 0.7738\n",
      "Training loss (for one batch) at step 90: 315.6078, Accuracy: 0.7714\n",
      "Training loss (for one batch) at step 100: 313.3101, Accuracy: 0.7754\n",
      "Training loss (for one batch) at step 110: 318.0877, Accuracy: 0.7762\n",
      "---- Training ----\n",
      "Training loss: 93.2086\n",
      "Training acc over epoch: 0.7756\n",
      "---- Validation ----\n",
      "Validation loss: 61.1360\n",
      "Validation acc: 0.7012\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 340.7073, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 344.2166, Accuracy: 0.7003\n",
      "Training loss (for one batch) at step 20: 313.9784, Accuracy: 0.7273\n",
      "Training loss (for one batch) at step 30: 317.7581, Accuracy: 0.7608\n",
      "Training loss (for one batch) at step 40: 290.1808, Accuracy: 0.7824\n",
      "Training loss (for one batch) at step 50: 295.3903, Accuracy: 0.7990\n",
      "Training loss (for one batch) at step 60: 316.7898, Accuracy: 0.8058\n",
      "Training loss (for one batch) at step 70: 343.4443, Accuracy: 0.7927\n",
      "Training loss (for one batch) at step 80: 364.3085, Accuracy: 0.7779\n",
      "Training loss (for one batch) at step 90: 299.6520, Accuracy: 0.7769\n",
      "Training loss (for one batch) at step 100: 312.0225, Accuracy: 0.7809\n",
      "Training loss (for one batch) at step 110: 299.0946, Accuracy: 0.7817\n",
      "---- Training ----\n",
      "Training loss: 101.3943\n",
      "Training acc over epoch: 0.7806\n",
      "---- Validation ----\n",
      "Validation loss: 46.0299\n",
      "Validation acc: 0.7131\n",
      "Time taken: 10.50s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 341.5137, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 348.0922, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 20: 315.8671, Accuracy: 0.7247\n",
      "Training loss (for one batch) at step 30: 303.8832, Accuracy: 0.7558\n",
      "Training loss (for one batch) at step 40: 314.8620, Accuracy: 0.7809\n",
      "Training loss (for one batch) at step 50: 287.4945, Accuracy: 0.7986\n",
      "Training loss (for one batch) at step 60: 316.4482, Accuracy: 0.8061\n",
      "Training loss (for one batch) at step 70: 335.9844, Accuracy: 0.7929\n",
      "Training loss (for one batch) at step 80: 341.1873, Accuracy: 0.7795\n",
      "Training loss (for one batch) at step 90: 330.7591, Accuracy: 0.7757\n",
      "Training loss (for one batch) at step 100: 303.5891, Accuracy: 0.7795\n",
      "Training loss (for one batch) at step 110: 318.8172, Accuracy: 0.7809\n",
      "---- Training ----\n",
      "Training loss: 100.2394\n",
      "Training acc over epoch: 0.7801\n",
      "---- Validation ----\n",
      "Validation loss: 41.5817\n",
      "Validation acc: 0.7149\n",
      "Time taken: 10.75s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 363.4160, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 359.2803, Accuracy: 0.6783\n",
      "Training loss (for one batch) at step 20: 299.6344, Accuracy: 0.7202\n",
      "Training loss (for one batch) at step 30: 283.0634, Accuracy: 0.7583\n",
      "Training loss (for one batch) at step 40: 299.0273, Accuracy: 0.7830\n",
      "Training loss (for one batch) at step 50: 293.3169, Accuracy: 0.7975\n",
      "Training loss (for one batch) at step 60: 318.5185, Accuracy: 0.8076\n",
      "Training loss (for one batch) at step 70: 345.0773, Accuracy: 0.7954\n",
      "Training loss (for one batch) at step 80: 342.8252, Accuracy: 0.7818\n",
      "Training loss (for one batch) at step 90: 330.7019, Accuracy: 0.7768\n",
      "Training loss (for one batch) at step 100: 308.5812, Accuracy: 0.7827\n",
      "Training loss (for one batch) at step 110: 318.6057, Accuracy: 0.7843\n",
      "---- Training ----\n",
      "Training loss: 95.5772\n",
      "Training acc over epoch: 0.7828\n",
      "---- Validation ----\n",
      "Validation loss: 41.8517\n",
      "Validation acc: 0.7187\n",
      "Time taken: 10.43s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 329.4739, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 317.3696, Accuracy: 0.6847\n",
      "Training loss (for one batch) at step 20: 318.9696, Accuracy: 0.7243\n",
      "Training loss (for one batch) at step 30: 291.3179, Accuracy: 0.7540\n",
      "Training loss (for one batch) at step 40: 303.7809, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 50: 302.5825, Accuracy: 0.7960\n",
      "Training loss (for one batch) at step 60: 332.2961, Accuracy: 0.8030\n",
      "Training loss (for one batch) at step 70: 345.2898, Accuracy: 0.7925\n",
      "Training loss (for one batch) at step 80: 336.3352, Accuracy: 0.7792\n",
      "Training loss (for one batch) at step 90: 310.4958, Accuracy: 0.7740\n",
      "Training loss (for one batch) at step 100: 303.8127, Accuracy: 0.7782\n",
      "Training loss (for one batch) at step 110: 317.3404, Accuracy: 0.7791\n",
      "---- Training ----\n",
      "Training loss: 112.9076\n",
      "Training acc over epoch: 0.7783\n",
      "---- Validation ----\n",
      "Validation loss: 42.0870\n",
      "Validation acc: 0.7077\n",
      "Time taken: 10.56s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 335.0432, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 330.6234, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 20: 311.4905, Accuracy: 0.7236\n",
      "Training loss (for one batch) at step 30: 282.5283, Accuracy: 0.7566\n",
      "Training loss (for one batch) at step 40: 300.9258, Accuracy: 0.7820\n",
      "Training loss (for one batch) at step 50: 296.9300, Accuracy: 0.7983\n",
      "Training loss (for one batch) at step 60: 295.1998, Accuracy: 0.8067\n",
      "Training loss (for one batch) at step 70: 336.4951, Accuracy: 0.7942\n",
      "Training loss (for one batch) at step 80: 331.9686, Accuracy: 0.7806\n",
      "Training loss (for one batch) at step 90: 290.8094, Accuracy: 0.7775\n",
      "Training loss (for one batch) at step 100: 285.0382, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 110: 303.8502, Accuracy: 0.7815\n",
      "---- Training ----\n",
      "Training loss: 106.7997\n",
      "Training acc over epoch: 0.7810\n",
      "---- Validation ----\n",
      "Validation loss: 42.1486\n",
      "Validation acc: 0.7015\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 333.2381, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 344.6086, Accuracy: 0.6939\n",
      "Training loss (for one batch) at step 20: 314.7284, Accuracy: 0.7232\n",
      "Training loss (for one batch) at step 30: 301.5694, Accuracy: 0.7616\n",
      "Training loss (for one batch) at step 40: 291.6937, Accuracy: 0.7875\n",
      "Training loss (for one batch) at step 50: 298.5323, Accuracy: 0.7999\n",
      "Training loss (for one batch) at step 60: 310.4613, Accuracy: 0.8074\n",
      "Training loss (for one batch) at step 70: 337.7873, Accuracy: 0.7954\n",
      "Training loss (for one batch) at step 80: 352.8776, Accuracy: 0.7811\n",
      "Training loss (for one batch) at step 90: 291.7599, Accuracy: 0.7763\n",
      "Training loss (for one batch) at step 100: 305.5728, Accuracy: 0.7804\n",
      "Training loss (for one batch) at step 110: 325.0002, Accuracy: 0.7826\n",
      "---- Training ----\n",
      "Training loss: 111.5890\n",
      "Training acc over epoch: 0.7806\n",
      "---- Validation ----\n",
      "Validation loss: 60.6617\n",
      "Validation acc: 0.7045\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 354.2589, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 346.6342, Accuracy: 0.6818\n",
      "Training loss (for one batch) at step 20: 312.4548, Accuracy: 0.7329\n",
      "Training loss (for one batch) at step 30: 301.9088, Accuracy: 0.7636\n",
      "Training loss (for one batch) at step 40: 294.4958, Accuracy: 0.7851\n",
      "Training loss (for one batch) at step 50: 294.6917, Accuracy: 0.7987\n",
      "Training loss (for one batch) at step 60: 308.5012, Accuracy: 0.8053\n",
      "Training loss (for one batch) at step 70: 320.6061, Accuracy: 0.7947\n",
      "Training loss (for one batch) at step 80: 328.4342, Accuracy: 0.7797\n",
      "Training loss (for one batch) at step 90: 325.1395, Accuracy: 0.7776\n",
      "Training loss (for one batch) at step 100: 297.2558, Accuracy: 0.7822\n",
      "Training loss (for one batch) at step 110: 320.2003, Accuracy: 0.7832\n",
      "---- Training ----\n",
      "Training loss: 106.9904\n",
      "Training acc over epoch: 0.7828\n",
      "---- Validation ----\n",
      "Validation loss: 43.3347\n",
      "Validation acc: 0.7109\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 354.4561, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 347.4787, Accuracy: 0.6960\n",
      "Training loss (for one batch) at step 20: 305.4050, Accuracy: 0.7347\n",
      "Training loss (for one batch) at step 30: 306.2110, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 40: 290.5099, Accuracy: 0.7875\n",
      "Training loss (for one batch) at step 50: 278.4071, Accuracy: 0.8010\n",
      "Training loss (for one batch) at step 60: 302.2710, Accuracy: 0.8075\n",
      "Training loss (for one batch) at step 70: 349.6582, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 80: 340.8639, Accuracy: 0.7811\n",
      "Training loss (for one batch) at step 90: 282.7025, Accuracy: 0.7779\n",
      "Training loss (for one batch) at step 100: 299.6919, Accuracy: 0.7819\n",
      "Training loss (for one batch) at step 110: 315.2856, Accuracy: 0.7830\n",
      "---- Training ----\n",
      "Training loss: 114.5128\n",
      "Training acc over epoch: 0.7818\n",
      "---- Validation ----\n",
      "Validation loss: 65.1984\n",
      "Validation acc: 0.7023\n",
      "Time taken: 10.61s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 339.3564, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 344.0780, Accuracy: 0.6832\n",
      "Training loss (for one batch) at step 20: 293.9963, Accuracy: 0.7262\n",
      "Training loss (for one batch) at step 30: 314.1819, Accuracy: 0.7598\n",
      "Training loss (for one batch) at step 40: 291.9756, Accuracy: 0.7864\n",
      "Training loss (for one batch) at step 50: 300.0779, Accuracy: 0.7987\n",
      "Training loss (for one batch) at step 60: 353.3889, Accuracy: 0.8051\n",
      "Training loss (for one batch) at step 70: 343.0991, Accuracy: 0.7916\n",
      "Training loss (for one batch) at step 80: 327.5054, Accuracy: 0.7759\n",
      "Training loss (for one batch) at step 90: 294.3521, Accuracy: 0.7722\n",
      "Training loss (for one batch) at step 100: 297.8583, Accuracy: 0.7775\n",
      "Training loss (for one batch) at step 110: 324.0669, Accuracy: 0.7793\n",
      "---- Training ----\n",
      "Training loss: 102.7784\n",
      "Training acc over epoch: 0.7790\n",
      "---- Validation ----\n",
      "Validation loss: 57.9760\n",
      "Validation acc: 0.7128\n",
      "Time taken: 10.24s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 355.4377, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 321.5680, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 20: 284.2535, Accuracy: 0.7359\n",
      "Training loss (for one batch) at step 30: 282.5345, Accuracy: 0.7679\n",
      "Training loss (for one batch) at step 40: 291.3449, Accuracy: 0.7856\n",
      "Training loss (for one batch) at step 50: 306.3483, Accuracy: 0.8025\n",
      "Training loss (for one batch) at step 60: 290.0178, Accuracy: 0.8083\n",
      "Training loss (for one batch) at step 70: 326.1358, Accuracy: 0.7965\n",
      "Training loss (for one batch) at step 80: 328.1296, Accuracy: 0.7819\n",
      "Training loss (for one batch) at step 90: 308.1405, Accuracy: 0.7792\n",
      "Training loss (for one batch) at step 100: 289.3735, Accuracy: 0.7831\n",
      "Training loss (for one batch) at step 110: 330.7675, Accuracy: 0.7842\n",
      "---- Training ----\n",
      "Training loss: 106.5245\n",
      "Training acc over epoch: 0.7826\n",
      "---- Validation ----\n",
      "Validation loss: 49.7272\n",
      "Validation acc: 0.7026\n",
      "Time taken: 10.68s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 331.3185, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 326.3969, Accuracy: 0.6882\n",
      "Training loss (for one batch) at step 20: 306.3332, Accuracy: 0.7258\n",
      "Training loss (for one batch) at step 30: 291.7556, Accuracy: 0.7613\n",
      "Training loss (for one batch) at step 40: 284.3606, Accuracy: 0.7826\n",
      "Training loss (for one batch) at step 50: 287.5789, Accuracy: 0.7981\n",
      "Training loss (for one batch) at step 60: 314.4185, Accuracy: 0.8065\n",
      "Training loss (for one batch) at step 70: 327.2507, Accuracy: 0.7951\n",
      "Training loss (for one batch) at step 80: 343.1578, Accuracy: 0.7798\n",
      "Training loss (for one batch) at step 90: 283.4935, Accuracy: 0.7747\n",
      "Training loss (for one batch) at step 100: 291.6865, Accuracy: 0.7797\n",
      "Training loss (for one batch) at step 110: 302.5055, Accuracy: 0.7798\n",
      "---- Training ----\n",
      "Training loss: 105.4861\n",
      "Training acc over epoch: 0.7794\n",
      "---- Validation ----\n",
      "Validation loss: 42.6206\n",
      "Validation acc: 0.7020\n",
      "Time taken: 10.56s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 339.2247, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 320.8516, Accuracy: 0.6996\n",
      "Training loss (for one batch) at step 20: 299.2214, Accuracy: 0.7269\n",
      "Training loss (for one batch) at step 30: 287.6227, Accuracy: 0.7629\n",
      "Training loss (for one batch) at step 40: 294.6416, Accuracy: 0.7843\n",
      "Training loss (for one batch) at step 50: 283.4982, Accuracy: 0.7989\n",
      "Training loss (for one batch) at step 60: 282.4826, Accuracy: 0.8064\n",
      "Training loss (for one batch) at step 70: 330.0428, Accuracy: 0.7937\n",
      "Training loss (for one batch) at step 80: 317.7888, Accuracy: 0.7777\n",
      "Training loss (for one batch) at step 90: 292.1003, Accuracy: 0.7765\n",
      "Training loss (for one batch) at step 100: 292.5584, Accuracy: 0.7813\n",
      "Training loss (for one batch) at step 110: 320.5435, Accuracy: 0.7822\n",
      "---- Training ----\n",
      "Training loss: 89.8379\n",
      "Training acc over epoch: 0.7814\n",
      "---- Validation ----\n",
      "Validation loss: 54.2528\n",
      "Validation acc: 0.7063\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 363.3858, Accuracy: 0.5312\n",
      "Training loss (for one batch) at step 10: 330.6199, Accuracy: 0.6889\n",
      "Training loss (for one batch) at step 20: 281.5995, Accuracy: 0.7292\n",
      "Training loss (for one batch) at step 30: 291.1396, Accuracy: 0.7616\n",
      "Training loss (for one batch) at step 40: 301.8282, Accuracy: 0.7875\n",
      "Training loss (for one batch) at step 50: 292.7548, Accuracy: 0.8030\n",
      "Training loss (for one batch) at step 60: 301.0974, Accuracy: 0.8072\n",
      "Training loss (for one batch) at step 70: 317.6344, Accuracy: 0.7969\n",
      "Training loss (for one batch) at step 80: 352.3431, Accuracy: 0.7790\n",
      "Training loss (for one batch) at step 90: 291.5901, Accuracy: 0.7757\n",
      "Training loss (for one batch) at step 100: 280.4912, Accuracy: 0.7811\n",
      "Training loss (for one batch) at step 110: 329.5225, Accuracy: 0.7824\n",
      "---- Training ----\n",
      "Training loss: 100.7605\n",
      "Training acc over epoch: 0.7819\n",
      "---- Validation ----\n",
      "Validation loss: 53.1216\n",
      "Validation acc: 0.7112\n",
      "Time taken: 10.53s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 352.1902, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 10: 340.1583, Accuracy: 0.6882\n",
      "Training loss (for one batch) at step 20: 298.0201, Accuracy: 0.7303\n",
      "Training loss (for one batch) at step 30: 276.7531, Accuracy: 0.7686\n",
      "Training loss (for one batch) at step 40: 274.2062, Accuracy: 0.7900\n",
      "Training loss (for one batch) at step 50: 303.6024, Accuracy: 0.8035\n",
      "Training loss (for one batch) at step 60: 317.3283, Accuracy: 0.8090\n",
      "Training loss (for one batch) at step 70: 320.2380, Accuracy: 0.7956\n",
      "Training loss (for one batch) at step 80: 321.6386, Accuracy: 0.7826\n",
      "Training loss (for one batch) at step 90: 284.3836, Accuracy: 0.7805\n",
      "Training loss (for one batch) at step 100: 279.3178, Accuracy: 0.7837\n",
      "Training loss (for one batch) at step 110: 313.7732, Accuracy: 0.7849\n",
      "---- Training ----\n",
      "Training loss: 99.0508\n",
      "Training acc over epoch: 0.7832\n",
      "---- Validation ----\n",
      "Validation loss: 40.0132\n",
      "Validation acc: 0.7171\n",
      "Time taken: 10.73s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 340.5956, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 330.3020, Accuracy: 0.6825\n",
      "Training loss (for one batch) at step 20: 302.9366, Accuracy: 0.7214\n",
      "Training loss (for one batch) at step 30: 303.3041, Accuracy: 0.7566\n",
      "Training loss (for one batch) at step 40: 276.5844, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 50: 280.4546, Accuracy: 0.7949\n",
      "Training loss (for one batch) at step 60: 297.4035, Accuracy: 0.8014\n",
      "Training loss (for one batch) at step 70: 309.2677, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 80: 338.0978, Accuracy: 0.7765\n",
      "Training loss (for one batch) at step 90: 291.9224, Accuracy: 0.7738\n",
      "Training loss (for one batch) at step 100: 288.4810, Accuracy: 0.7792\n",
      "Training loss (for one batch) at step 110: 286.9516, Accuracy: 0.7809\n",
      "---- Training ----\n",
      "Training loss: 99.1015\n",
      "Training acc over epoch: 0.7802\n",
      "---- Validation ----\n",
      "Validation loss: 42.9717\n",
      "Validation acc: 0.7128\n",
      "Time taken: 10.21s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 321.2129, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 321.8459, Accuracy: 0.6903\n",
      "Training loss (for one batch) at step 20: 299.7117, Accuracy: 0.7273\n",
      "Training loss (for one batch) at step 30: 291.1308, Accuracy: 0.7684\n",
      "Training loss (for one batch) at step 40: 276.5583, Accuracy: 0.7889\n",
      "Training loss (for one batch) at step 50: 282.1213, Accuracy: 0.8004\n",
      "Training loss (for one batch) at step 60: 289.5803, Accuracy: 0.8090\n",
      "Training loss (for one batch) at step 70: 303.3855, Accuracy: 0.7965\n",
      "Training loss (for one batch) at step 80: 324.4796, Accuracy: 0.7785\n",
      "Training loss (for one batch) at step 90: 282.4539, Accuracy: 0.7763\n",
      "Training loss (for one batch) at step 100: 298.1330, Accuracy: 0.7790\n",
      "Training loss (for one batch) at step 110: 293.9706, Accuracy: 0.7819\n",
      "---- Training ----\n",
      "Training loss: 114.9725\n",
      "Training acc over epoch: 0.7803\n",
      "---- Validation ----\n",
      "Validation loss: 41.0975\n",
      "Validation acc: 0.7069\n",
      "Time taken: 10.69s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 344.2266, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 356.6466, Accuracy: 0.6861\n",
      "Training loss (for one batch) at step 20: 291.4865, Accuracy: 0.7333\n",
      "Training loss (for one batch) at step 30: 295.0704, Accuracy: 0.7644\n",
      "Training loss (for one batch) at step 40: 272.7418, Accuracy: 0.7862\n",
      "Training loss (for one batch) at step 50: 279.7708, Accuracy: 0.8001\n",
      "Training loss (for one batch) at step 60: 316.3881, Accuracy: 0.8062\n",
      "Training loss (for one batch) at step 70: 342.4787, Accuracy: 0.7904\n",
      "Training loss (for one batch) at step 80: 328.3944, Accuracy: 0.7742\n",
      "Training loss (for one batch) at step 90: 316.4875, Accuracy: 0.7706\n",
      "Training loss (for one batch) at step 100: 290.9453, Accuracy: 0.7755\n",
      "Training loss (for one batch) at step 110: 281.4759, Accuracy: 0.7770\n",
      "---- Training ----\n",
      "Training loss: 100.2698\n",
      "Training acc over epoch: 0.7770\n",
      "---- Validation ----\n",
      "Validation loss: 52.8568\n",
      "Validation acc: 0.7157\n",
      "Time taken: 10.58s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 328.4326, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 327.4641, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 20: 303.5027, Accuracy: 0.7333\n",
      "Training loss (for one batch) at step 30: 289.8022, Accuracy: 0.7631\n",
      "Training loss (for one batch) at step 40: 285.3116, Accuracy: 0.7879\n",
      "Training loss (for one batch) at step 50: 281.3257, Accuracy: 0.8010\n",
      "Training loss (for one batch) at step 60: 280.2772, Accuracy: 0.8093\n",
      "Training loss (for one batch) at step 70: 323.8723, Accuracy: 0.7970\n",
      "Training loss (for one batch) at step 80: 310.9565, Accuracy: 0.7837\n",
      "Training loss (for one batch) at step 90: 297.2763, Accuracy: 0.7803\n",
      "Training loss (for one batch) at step 100: 275.4177, Accuracy: 0.7839\n",
      "Training loss (for one batch) at step 110: 277.4645, Accuracy: 0.7851\n",
      "---- Training ----\n",
      "Training loss: 107.9217\n",
      "Training acc over epoch: 0.7834\n",
      "---- Validation ----\n",
      "Validation loss: 49.5818\n",
      "Validation acc: 0.7179\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 319.7811, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 315.1038, Accuracy: 0.6989\n",
      "Training loss (for one batch) at step 20: 289.2358, Accuracy: 0.7396\n",
      "Training loss (for one batch) at step 30: 290.1313, Accuracy: 0.7694\n",
      "Training loss (for one batch) at step 40: 287.7694, Accuracy: 0.7887\n",
      "Training loss (for one batch) at step 50: 284.3339, Accuracy: 0.8007\n",
      "Training loss (for one batch) at step 60: 295.4062, Accuracy: 0.8069\n",
      "Training loss (for one batch) at step 70: 320.1577, Accuracy: 0.7967\n",
      "Training loss (for one batch) at step 80: 339.8550, Accuracy: 0.7818\n",
      "Training loss (for one batch) at step 90: 301.9725, Accuracy: 0.7774\n",
      "Training loss (for one batch) at step 100: 294.6743, Accuracy: 0.7802\n",
      "Training loss (for one batch) at step 110: 306.1132, Accuracy: 0.7835\n",
      "---- Training ----\n",
      "Training loss: 102.0060\n",
      "Training acc over epoch: 0.7828\n",
      "---- Validation ----\n",
      "Validation loss: 35.4665\n",
      "Validation acc: 0.7058\n",
      "Time taken: 10.46s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 342.9973, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 317.4465, Accuracy: 0.6818\n",
      "Training loss (for one batch) at step 20: 302.6165, Accuracy: 0.7251\n",
      "Training loss (for one batch) at step 30: 273.3104, Accuracy: 0.7593\n",
      "Training loss (for one batch) at step 40: 287.7343, Accuracy: 0.7847\n",
      "Training loss (for one batch) at step 50: 262.3593, Accuracy: 0.7967\n",
      "Training loss (for one batch) at step 60: 299.6448, Accuracy: 0.8044\n",
      "Training loss (for one batch) at step 70: 318.3419, Accuracy: 0.7919\n",
      "Training loss (for one batch) at step 80: 329.7488, Accuracy: 0.7777\n",
      "Training loss (for one batch) at step 90: 291.4143, Accuracy: 0.7753\n",
      "Training loss (for one batch) at step 100: 277.1991, Accuracy: 0.7798\n",
      "Training loss (for one batch) at step 110: 307.0007, Accuracy: 0.7827\n",
      "---- Training ----\n",
      "Training loss: 98.2251\n",
      "Training acc over epoch: 0.7812\n",
      "---- Validation ----\n",
      "Validation loss: 50.3022\n",
      "Validation acc: 0.7227\n",
      "Time taken: 10.54s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 337.3849, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 338.9084, Accuracy: 0.6832\n",
      "Training loss (for one batch) at step 20: 309.0458, Accuracy: 0.7325\n",
      "Training loss (for one batch) at step 30: 280.4229, Accuracy: 0.7618\n",
      "Training loss (for one batch) at step 40: 277.3988, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 50: 279.4085, Accuracy: 0.7979\n",
      "Training loss (for one batch) at step 60: 287.8755, Accuracy: 0.8066\n",
      "Training loss (for one batch) at step 70: 316.3871, Accuracy: 0.7953\n",
      "Training loss (for one batch) at step 80: 334.1307, Accuracy: 0.7809\n",
      "Training loss (for one batch) at step 90: 280.0021, Accuracy: 0.7766\n",
      "Training loss (for one batch) at step 100: 290.7303, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 110: 320.0213, Accuracy: 0.7812\n",
      "---- Training ----\n",
      "Training loss: 80.9888\n",
      "Training acc over epoch: 0.7812\n",
      "---- Validation ----\n",
      "Validation loss: 56.0813\n",
      "Validation acc: 0.7131\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 333.6558, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 328.9569, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 20: 294.7642, Accuracy: 0.7243\n",
      "Training loss (for one batch) at step 30: 289.4606, Accuracy: 0.7616\n",
      "Training loss (for one batch) at step 40: 277.2305, Accuracy: 0.7866\n",
      "Training loss (for one batch) at step 50: 284.2727, Accuracy: 0.8022\n",
      "Training loss (for one batch) at step 60: 303.2856, Accuracy: 0.8090\n",
      "Training loss (for one batch) at step 70: 337.3605, Accuracy: 0.7962\n",
      "Training loss (for one batch) at step 80: 333.8604, Accuracy: 0.7809\n",
      "Training loss (for one batch) at step 90: 265.3701, Accuracy: 0.7787\n",
      "Training loss (for one batch) at step 100: 298.1796, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 110: 323.4777, Accuracy: 0.7827\n",
      "---- Training ----\n",
      "Training loss: 103.9449\n",
      "Training acc over epoch: 0.7818\n",
      "---- Validation ----\n",
      "Validation loss: 54.3286\n",
      "Validation acc: 0.7053\n",
      "Time taken: 10.56s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 343.4392, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 314.8376, Accuracy: 0.6832\n",
      "Training loss (for one batch) at step 20: 291.8033, Accuracy: 0.7295\n",
      "Training loss (for one batch) at step 30: 282.6483, Accuracy: 0.7629\n",
      "Training loss (for one batch) at step 40: 289.4935, Accuracy: 0.7864\n",
      "Training loss (for one batch) at step 50: 278.1846, Accuracy: 0.8019\n",
      "Training loss (for one batch) at step 60: 288.7638, Accuracy: 0.8071\n",
      "Training loss (for one batch) at step 70: 341.4920, Accuracy: 0.7942\n",
      "Training loss (for one batch) at step 80: 307.0817, Accuracy: 0.7826\n",
      "Training loss (for one batch) at step 90: 290.9235, Accuracy: 0.7772\n",
      "Training loss (for one batch) at step 100: 263.4465, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 110: 308.3828, Accuracy: 0.7839\n",
      "---- Training ----\n",
      "Training loss: 116.0884\n",
      "Training acc over epoch: 0.7829\n",
      "---- Validation ----\n",
      "Validation loss: 61.2813\n",
      "Validation acc: 0.7171\n",
      "Time taken: 10.75s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 332.7539, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 323.9384, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 20: 292.1468, Accuracy: 0.7329\n",
      "Training loss (for one batch) at step 30: 271.4525, Accuracy: 0.7611\n",
      "Training loss (for one batch) at step 40: 293.3129, Accuracy: 0.7854\n",
      "Training loss (for one batch) at step 50: 268.6650, Accuracy: 0.7998\n",
      "Training loss (for one batch) at step 60: 295.8847, Accuracy: 0.8058\n",
      "Training loss (for one batch) at step 70: 311.9885, Accuracy: 0.7958\n",
      "Training loss (for one batch) at step 80: 304.6474, Accuracy: 0.7802\n",
      "Training loss (for one batch) at step 90: 300.3542, Accuracy: 0.7764\n",
      "Training loss (for one batch) at step 100: 277.3202, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 110: 279.4243, Accuracy: 0.7831\n",
      "---- Training ----\n",
      "Training loss: 89.0210\n",
      "Training acc over epoch: 0.7830\n",
      "---- Validation ----\n",
      "Validation loss: 33.2770\n",
      "Validation acc: 0.7125\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 316.9370, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 314.1612, Accuracy: 0.6847\n",
      "Training loss (for one batch) at step 20: 288.8231, Accuracy: 0.7295\n",
      "Training loss (for one batch) at step 30: 293.2635, Accuracy: 0.7651\n",
      "Training loss (for one batch) at step 40: 291.7122, Accuracy: 0.7881\n",
      "Training loss (for one batch) at step 50: 298.9139, Accuracy: 0.8007\n",
      "Training loss (for one batch) at step 60: 275.9265, Accuracy: 0.8069\n",
      "Training loss (for one batch) at step 70: 321.1360, Accuracy: 0.7939\n",
      "Training loss (for one batch) at step 80: 336.8650, Accuracy: 0.7780\n",
      "Training loss (for one batch) at step 90: 287.8150, Accuracy: 0.7758\n",
      "Training loss (for one batch) at step 100: 283.7073, Accuracy: 0.7813\n",
      "Training loss (for one batch) at step 110: 299.9073, Accuracy: 0.7819\n",
      "---- Training ----\n",
      "Training loss: 97.8725\n",
      "Training acc over epoch: 0.7808\n",
      "---- Validation ----\n",
      "Validation loss: 45.0758\n",
      "Validation acc: 0.7002\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 314.9084, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 325.4778, Accuracy: 0.6882\n",
      "Training loss (for one batch) at step 20: 302.2171, Accuracy: 0.7247\n",
      "Training loss (for one batch) at step 30: 292.0455, Accuracy: 0.7563\n",
      "Training loss (for one batch) at step 40: 254.2716, Accuracy: 0.7824\n",
      "Training loss (for one batch) at step 50: 281.1438, Accuracy: 0.7987\n",
      "Training loss (for one batch) at step 60: 299.5641, Accuracy: 0.8048\n",
      "Training loss (for one batch) at step 70: 304.4002, Accuracy: 0.7939\n",
      "Training loss (for one batch) at step 80: 299.0196, Accuracy: 0.7788\n",
      "Training loss (for one batch) at step 90: 291.0103, Accuracy: 0.7741\n",
      "Training loss (for one batch) at step 100: 283.1577, Accuracy: 0.7778\n",
      "Training loss (for one batch) at step 110: 307.8388, Accuracy: 0.7801\n",
      "---- Training ----\n",
      "Training loss: 93.0056\n",
      "Training acc over epoch: 0.7797\n",
      "---- Validation ----\n",
      "Validation loss: 53.9201\n",
      "Validation acc: 0.7225\n",
      "Time taken: 10.58s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 328.3776, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 333.4330, Accuracy: 0.6932\n",
      "Training loss (for one batch) at step 20: 288.6937, Accuracy: 0.7336\n",
      "Training loss (for one batch) at step 30: 265.8256, Accuracy: 0.7707\n",
      "Training loss (for one batch) at step 40: 272.5335, Accuracy: 0.7870\n",
      "Training loss (for one batch) at step 50: 261.8423, Accuracy: 0.8042\n",
      "Training loss (for one batch) at step 60: 293.0016, Accuracy: 0.8107\n",
      "Training loss (for one batch) at step 70: 325.4192, Accuracy: 0.7983\n",
      "Training loss (for one batch) at step 80: 318.1862, Accuracy: 0.7840\n",
      "Training loss (for one batch) at step 90: 293.6000, Accuracy: 0.7793\n",
      "Training loss (for one batch) at step 100: 280.4716, Accuracy: 0.7836\n",
      "Training loss (for one batch) at step 110: 291.3114, Accuracy: 0.7845\n",
      "---- Training ----\n",
      "Training loss: 91.3334\n",
      "Training acc over epoch: 0.7839\n",
      "---- Validation ----\n",
      "Validation loss: 79.6890\n",
      "Validation acc: 0.7254\n",
      "Time taken: 10.65s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 336.7738, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 324.1690, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 20: 278.9396, Accuracy: 0.7221\n",
      "Training loss (for one batch) at step 30: 288.0799, Accuracy: 0.7586\n",
      "Training loss (for one batch) at step 40: 288.7822, Accuracy: 0.7841\n",
      "Training loss (for one batch) at step 50: 297.1905, Accuracy: 0.7996\n",
      "Training loss (for one batch) at step 60: 322.2222, Accuracy: 0.8060\n",
      "Training loss (for one batch) at step 70: 312.7525, Accuracy: 0.7920\n",
      "Training loss (for one batch) at step 80: 305.7746, Accuracy: 0.7792\n",
      "Training loss (for one batch) at step 90: 306.6925, Accuracy: 0.7769\n",
      "Training loss (for one batch) at step 100: 283.0516, Accuracy: 0.7806\n",
      "Training loss (for one batch) at step 110: 303.4098, Accuracy: 0.7826\n",
      "---- Training ----\n",
      "Training loss: 100.6013\n",
      "Training acc over epoch: 0.7812\n",
      "---- Validation ----\n",
      "Validation loss: 53.2034\n",
      "Validation acc: 0.7042\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 333.8811, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 297.3489, Accuracy: 0.6776\n",
      "Training loss (for one batch) at step 20: 280.3752, Accuracy: 0.7273\n",
      "Training loss (for one batch) at step 30: 272.5826, Accuracy: 0.7601\n",
      "Training loss (for one batch) at step 40: 290.3991, Accuracy: 0.7847\n",
      "Training loss (for one batch) at step 50: 268.0463, Accuracy: 0.7993\n",
      "Training loss (for one batch) at step 60: 304.0194, Accuracy: 0.8057\n",
      "Training loss (for one batch) at step 70: 301.2360, Accuracy: 0.7923\n",
      "Training loss (for one batch) at step 80: 288.3055, Accuracy: 0.7813\n",
      "Training loss (for one batch) at step 90: 285.5814, Accuracy: 0.7761\n",
      "Training loss (for one batch) at step 100: 272.6587, Accuracy: 0.7816\n",
      "Training loss (for one batch) at step 110: 302.7258, Accuracy: 0.7837\n",
      "---- Training ----\n",
      "Training loss: 104.2419\n",
      "Training acc over epoch: 0.7821\n",
      "---- Validation ----\n",
      "Validation loss: 55.9096\n",
      "Validation acc: 0.7082\n",
      "Time taken: 10.54s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 335.2704, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 304.8343, Accuracy: 0.6768\n",
      "Training loss (for one batch) at step 20: 287.4704, Accuracy: 0.7314\n",
      "Training loss (for one batch) at step 30: 271.2527, Accuracy: 0.7681\n",
      "Training loss (for one batch) at step 40: 284.2181, Accuracy: 0.7893\n",
      "Training loss (for one batch) at step 50: 275.6402, Accuracy: 0.8051\n",
      "Training loss (for one batch) at step 60: 303.4603, Accuracy: 0.8128\n",
      "Training loss (for one batch) at step 70: 318.0440, Accuracy: 0.7998\n",
      "Training loss (for one batch) at step 80: 303.8945, Accuracy: 0.7842\n",
      "Training loss (for one batch) at step 90: 293.8372, Accuracy: 0.7810\n",
      "Training loss (for one batch) at step 100: 275.5133, Accuracy: 0.7848\n",
      "Training loss (for one batch) at step 110: 310.4999, Accuracy: 0.7860\n",
      "---- Training ----\n",
      "Training loss: 115.2415\n",
      "Training acc over epoch: 0.7857\n",
      "---- Validation ----\n",
      "Validation loss: 56.9263\n",
      "Validation acc: 0.7042\n",
      "Time taken: 10.40s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 322.2609, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 308.9818, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 20: 310.5332, Accuracy: 0.7295\n",
      "Training loss (for one batch) at step 30: 267.5776, Accuracy: 0.7634\n",
      "Training loss (for one batch) at step 40: 290.8026, Accuracy: 0.7828\n",
      "Training loss (for one batch) at step 50: 274.0910, Accuracy: 0.7996\n",
      "Training loss (for one batch) at step 60: 285.9378, Accuracy: 0.8087\n",
      "Training loss (for one batch) at step 70: 325.6598, Accuracy: 0.7937\n",
      "Training loss (for one batch) at step 80: 300.5322, Accuracy: 0.7784\n",
      "Training loss (for one batch) at step 90: 287.4342, Accuracy: 0.7756\n",
      "Training loss (for one batch) at step 100: 258.2444, Accuracy: 0.7807\n",
      "Training loss (for one batch) at step 110: 307.3629, Accuracy: 0.7817\n",
      "---- Training ----\n",
      "Training loss: 90.7942\n",
      "Training acc over epoch: 0.7812\n",
      "---- Validation ----\n",
      "Validation loss: 35.8807\n",
      "Validation acc: 0.7182\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 324.8397, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 317.1876, Accuracy: 0.6903\n",
      "Training loss (for one batch) at step 20: 261.3159, Accuracy: 0.7303\n",
      "Training loss (for one batch) at step 30: 279.0027, Accuracy: 0.7644\n",
      "Training loss (for one batch) at step 40: 269.6711, Accuracy: 0.7885\n",
      "Training loss (for one batch) at step 50: 267.7037, Accuracy: 0.8028\n",
      "Training loss (for one batch) at step 60: 274.7549, Accuracy: 0.8078\n",
      "Training loss (for one batch) at step 70: 314.2854, Accuracy: 0.7942\n",
      "Training loss (for one batch) at step 80: 307.4659, Accuracy: 0.7788\n",
      "Training loss (for one batch) at step 90: 272.9321, Accuracy: 0.7757\n",
      "Training loss (for one batch) at step 100: 285.4901, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 110: 284.0991, Accuracy: 0.7834\n",
      "---- Training ----\n",
      "Training loss: 83.8965\n",
      "Training acc over epoch: 0.7822\n",
      "---- Validation ----\n",
      "Validation loss: 42.5741\n",
      "Validation acc: 0.7112\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 305.1501, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 319.4198, Accuracy: 0.6854\n",
      "Training loss (for one batch) at step 20: 273.1939, Accuracy: 0.7333\n",
      "Training loss (for one batch) at step 30: 284.5072, Accuracy: 0.7659\n",
      "Training loss (for one batch) at step 40: 267.6937, Accuracy: 0.7894\n",
      "Training loss (for one batch) at step 50: 278.1418, Accuracy: 0.8056\n",
      "Training loss (for one batch) at step 60: 282.5585, Accuracy: 0.8131\n",
      "Training loss (for one batch) at step 70: 317.7114, Accuracy: 0.8014\n",
      "Training loss (for one batch) at step 80: 303.1898, Accuracy: 0.7844\n",
      "Training loss (for one batch) at step 90: 275.6161, Accuracy: 0.7801\n",
      "Training loss (for one batch) at step 100: 275.9998, Accuracy: 0.7843\n",
      "Training loss (for one batch) at step 110: 304.6780, Accuracy: 0.7857\n",
      "---- Training ----\n",
      "Training loss: 100.3123\n",
      "Training acc over epoch: 0.7851\n",
      "---- Validation ----\n",
      "Validation loss: 42.7500\n",
      "Validation acc: 0.7225\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 323.8942, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 314.8211, Accuracy: 0.6832\n",
      "Training loss (for one batch) at step 20: 276.9009, Accuracy: 0.7214\n",
      "Training loss (for one batch) at step 30: 265.6716, Accuracy: 0.7548\n",
      "Training loss (for one batch) at step 40: 278.8163, Accuracy: 0.7795\n",
      "Training loss (for one batch) at step 50: 269.4441, Accuracy: 0.7960\n",
      "Training loss (for one batch) at step 60: 298.9009, Accuracy: 0.8048\n",
      "Training loss (for one batch) at step 70: 304.5366, Accuracy: 0.7949\n",
      "Training loss (for one batch) at step 80: 302.7864, Accuracy: 0.7800\n",
      "Training loss (for one batch) at step 90: 293.6765, Accuracy: 0.7764\n",
      "Training loss (for one batch) at step 100: 277.7757, Accuracy: 0.7792\n",
      "Training loss (for one batch) at step 110: 294.8857, Accuracy: 0.7808\n",
      "---- Training ----\n",
      "Training loss: 91.9148\n",
      "Training acc over epoch: 0.7816\n",
      "---- Validation ----\n",
      "Validation loss: 48.2365\n",
      "Validation acc: 0.7096\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 315.2285, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 310.2712, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 20: 287.1754, Accuracy: 0.7295\n",
      "Training loss (for one batch) at step 30: 265.6517, Accuracy: 0.7636\n",
      "Training loss (for one batch) at step 40: 273.4632, Accuracy: 0.7866\n",
      "Training loss (for one batch) at step 50: 265.1055, Accuracy: 0.8016\n",
      "Training loss (for one batch) at step 60: 326.1788, Accuracy: 0.8090\n",
      "Training loss (for one batch) at step 70: 292.2677, Accuracy: 0.7960\n",
      "Training loss (for one batch) at step 80: 329.5005, Accuracy: 0.7828\n",
      "Training loss (for one batch) at step 90: 267.1856, Accuracy: 0.7784\n",
      "Training loss (for one batch) at step 100: 291.6594, Accuracy: 0.7838\n",
      "Training loss (for one batch) at step 110: 287.4136, Accuracy: 0.7853\n",
      "---- Training ----\n",
      "Training loss: 94.2028\n",
      "Training acc over epoch: 0.7839\n",
      "---- Validation ----\n",
      "Validation loss: 41.9541\n",
      "Validation acc: 0.7128\n",
      "Time taken: 10.75s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 333.6340, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 317.7423, Accuracy: 0.6911\n",
      "Training loss (for one batch) at step 20: 302.0923, Accuracy: 0.7333\n",
      "Training loss (for one batch) at step 30: 286.8214, Accuracy: 0.7651\n",
      "Training loss (for one batch) at step 40: 281.0799, Accuracy: 0.7856\n",
      "Training loss (for one batch) at step 50: 258.6212, Accuracy: 0.8027\n",
      "Training loss (for one batch) at step 60: 287.7168, Accuracy: 0.8071\n",
      "Training loss (for one batch) at step 70: 301.2600, Accuracy: 0.7952\n",
      "Training loss (for one batch) at step 80: 322.0656, Accuracy: 0.7812\n",
      "Training loss (for one batch) at step 90: 269.1472, Accuracy: 0.7782\n",
      "Training loss (for one batch) at step 100: 291.6364, Accuracy: 0.7828\n",
      "Training loss (for one batch) at step 110: 277.8268, Accuracy: 0.7836\n",
      "---- Training ----\n",
      "Training loss: 102.1264\n",
      "Training acc over epoch: 0.7826\n",
      "---- Validation ----\n",
      "Validation loss: 47.3193\n",
      "Validation acc: 0.7063\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 320.6358, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 339.4035, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 20: 273.2795, Accuracy: 0.7333\n",
      "Training loss (for one batch) at step 30: 265.5742, Accuracy: 0.7661\n",
      "Training loss (for one batch) at step 40: 267.4957, Accuracy: 0.7919\n",
      "Training loss (for one batch) at step 50: 292.3539, Accuracy: 0.8064\n",
      "Training loss (for one batch) at step 60: 306.9865, Accuracy: 0.8122\n",
      "Training loss (for one batch) at step 70: 301.3654, Accuracy: 0.7986\n",
      "Training loss (for one batch) at step 80: 299.3928, Accuracy: 0.7826\n",
      "Training loss (for one batch) at step 90: 278.3593, Accuracy: 0.7804\n",
      "Training loss (for one batch) at step 100: 263.0861, Accuracy: 0.7856\n",
      "Training loss (for one batch) at step 110: 280.3535, Accuracy: 0.7855\n",
      "---- Training ----\n",
      "Training loss: 92.3533\n",
      "Training acc over epoch: 0.7845\n",
      "---- Validation ----\n",
      "Validation loss: 43.5366\n",
      "Validation acc: 0.7042\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 331.3804, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 305.7332, Accuracy: 0.7003\n",
      "Training loss (for one batch) at step 20: 292.1870, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 30: 278.7147, Accuracy: 0.7659\n",
      "Training loss (for one batch) at step 40: 270.9251, Accuracy: 0.7864\n",
      "Training loss (for one batch) at step 50: 265.1229, Accuracy: 0.8027\n",
      "Training loss (for one batch) at step 60: 281.4717, Accuracy: 0.8085\n",
      "Training loss (for one batch) at step 70: 310.2205, Accuracy: 0.7972\n",
      "Training loss (for one batch) at step 80: 330.1284, Accuracy: 0.7807\n",
      "Training loss (for one batch) at step 90: 278.3067, Accuracy: 0.7780\n",
      "Training loss (for one batch) at step 100: 270.9674, Accuracy: 0.7819\n",
      "Training loss (for one batch) at step 110: 291.0082, Accuracy: 0.7843\n",
      "---- Training ----\n",
      "Training loss: 90.1717\n",
      "Training acc over epoch: 0.7837\n",
      "---- Validation ----\n",
      "Validation loss: 52.2842\n",
      "Validation acc: 0.7093\n",
      "Time taken: 10.61s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 327.7609, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 305.2808, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 20: 274.3708, Accuracy: 0.7407\n",
      "Training loss (for one batch) at step 30: 272.2593, Accuracy: 0.7676\n",
      "Training loss (for one batch) at step 40: 269.6830, Accuracy: 0.7908\n",
      "Training loss (for one batch) at step 50: 258.4416, Accuracy: 0.8042\n",
      "Training loss (for one batch) at step 60: 264.4858, Accuracy: 0.8125\n",
      "Training loss (for one batch) at step 70: 301.6758, Accuracy: 0.7993\n",
      "Training loss (for one batch) at step 80: 317.8095, Accuracy: 0.7840\n",
      "Training loss (for one batch) at step 90: 302.1377, Accuracy: 0.7801\n",
      "Training loss (for one batch) at step 100: 258.2797, Accuracy: 0.7833\n",
      "Training loss (for one batch) at step 110: 279.3233, Accuracy: 0.7851\n",
      "---- Training ----\n",
      "Training loss: 98.1301\n",
      "Training acc over epoch: 0.7833\n",
      "---- Validation ----\n",
      "Validation loss: 54.8089\n",
      "Validation acc: 0.7157\n",
      "Time taken: 10.42s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 314.2547, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 299.3699, Accuracy: 0.7038\n",
      "Training loss (for one batch) at step 20: 285.1914, Accuracy: 0.7288\n",
      "Training loss (for one batch) at step 30: 281.6494, Accuracy: 0.7656\n",
      "Training loss (for one batch) at step 40: 262.0457, Accuracy: 0.7883\n",
      "Training loss (for one batch) at step 50: 272.3162, Accuracy: 0.8039\n",
      "Training loss (for one batch) at step 60: 296.1483, Accuracy: 0.8112\n",
      "Training loss (for one batch) at step 70: 325.4334, Accuracy: 0.7959\n",
      "Training loss (for one batch) at step 80: 299.7021, Accuracy: 0.7807\n",
      "Training loss (for one batch) at step 90: 274.9814, Accuracy: 0.7767\n",
      "Training loss (for one batch) at step 100: 285.5075, Accuracy: 0.7819\n",
      "Training loss (for one batch) at step 110: 288.9691, Accuracy: 0.7833\n",
      "---- Training ----\n",
      "Training loss: 99.7337\n",
      "Training acc over epoch: 0.7833\n",
      "---- Validation ----\n",
      "Validation loss: 43.3266\n",
      "Validation acc: 0.7090\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 326.6433, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 324.0239, Accuracy: 0.6911\n",
      "Training loss (for one batch) at step 20: 279.5256, Accuracy: 0.7388\n",
      "Training loss (for one batch) at step 30: 276.9496, Accuracy: 0.7644\n",
      "Training loss (for one batch) at step 40: 277.4935, Accuracy: 0.7868\n",
      "Training loss (for one batch) at step 50: 279.7244, Accuracy: 0.8025\n",
      "Training loss (for one batch) at step 60: 292.1929, Accuracy: 0.8107\n",
      "Training loss (for one batch) at step 70: 294.4376, Accuracy: 0.7982\n",
      "Training loss (for one batch) at step 80: 321.8315, Accuracy: 0.7797\n",
      "Training loss (for one batch) at step 90: 282.9345, Accuracy: 0.7793\n",
      "Training loss (for one batch) at step 100: 281.3165, Accuracy: 0.7851\n",
      "Training loss (for one batch) at step 110: 279.3973, Accuracy: 0.7857\n",
      "---- Training ----\n",
      "Training loss: 104.4727\n",
      "Training acc over epoch: 0.7846\n",
      "---- Validation ----\n",
      "Validation loss: 72.3835\n",
      "Validation acc: 0.7120\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 308.2424, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 323.6713, Accuracy: 0.6996\n",
      "Training loss (for one batch) at step 20: 281.6028, Accuracy: 0.7292\n",
      "Training loss (for one batch) at step 30: 280.4436, Accuracy: 0.7634\n",
      "Training loss (for one batch) at step 40: 258.4183, Accuracy: 0.7853\n",
      "Training loss (for one batch) at step 50: 257.8430, Accuracy: 0.8030\n",
      "Training loss (for one batch) at step 60: 286.4427, Accuracy: 0.8097\n",
      "Training loss (for one batch) at step 70: 308.0123, Accuracy: 0.7961\n",
      "Training loss (for one batch) at step 80: 288.1455, Accuracy: 0.7837\n",
      "Training loss (for one batch) at step 90: 285.8238, Accuracy: 0.7803\n",
      "Training loss (for one batch) at step 100: 274.0946, Accuracy: 0.7851\n",
      "Training loss (for one batch) at step 110: 295.6232, Accuracy: 0.7850\n",
      "---- Training ----\n",
      "Training loss: 96.4705\n",
      "Training acc over epoch: 0.7851\n",
      "---- Validation ----\n",
      "Validation loss: 58.7016\n",
      "Validation acc: 0.7192\n",
      "Time taken: 10.58s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 289.8508, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 317.9062, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 20: 264.8902, Accuracy: 0.7292\n",
      "Training loss (for one batch) at step 30: 258.2066, Accuracy: 0.7671\n",
      "Training loss (for one batch) at step 40: 276.6964, Accuracy: 0.7904\n",
      "Training loss (for one batch) at step 50: 297.3189, Accuracy: 0.8033\n",
      "Training loss (for one batch) at step 60: 303.9314, Accuracy: 0.8108\n",
      "Training loss (for one batch) at step 70: 304.1445, Accuracy: 0.7982\n",
      "Training loss (for one batch) at step 80: 304.8938, Accuracy: 0.7811\n",
      "Training loss (for one batch) at step 90: 307.9823, Accuracy: 0.7789\n",
      "Training loss (for one batch) at step 100: 268.5569, Accuracy: 0.7834\n",
      "Training loss (for one batch) at step 110: 292.9780, Accuracy: 0.7834\n",
      "---- Training ----\n",
      "Training loss: 85.9423\n",
      "Training acc over epoch: 0.7833\n",
      "---- Validation ----\n",
      "Validation loss: 70.7772\n",
      "Validation acc: 0.7155\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 323.2980, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 298.4331, Accuracy: 0.6733\n",
      "Training loss (for one batch) at step 20: 267.7126, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 30: 268.6348, Accuracy: 0.7661\n",
      "Training loss (for one batch) at step 40: 260.7712, Accuracy: 0.7906\n",
      "Training loss (for one batch) at step 50: 259.6229, Accuracy: 0.8042\n",
      "Training loss (for one batch) at step 60: 275.1117, Accuracy: 0.8134\n",
      "Training loss (for one batch) at step 70: 299.6949, Accuracy: 0.8007\n",
      "Training loss (for one batch) at step 80: 297.7895, Accuracy: 0.7847\n",
      "Training loss (for one batch) at step 90: 273.4712, Accuracy: 0.7803\n",
      "Training loss (for one batch) at step 100: 266.4529, Accuracy: 0.7868\n",
      "Training loss (for one batch) at step 110: 258.5491, Accuracy: 0.7878\n",
      "---- Training ----\n",
      "Training loss: 96.9476\n",
      "Training acc over epoch: 0.7857\n",
      "---- Validation ----\n",
      "Validation loss: 49.0970\n",
      "Validation acc: 0.7117\n",
      "Time taken: 10.54s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 320.2278, Accuracy: 0.7266\n",
      "Training loss (for one batch) at step 10: 313.0222, Accuracy: 0.6939\n",
      "Training loss (for one batch) at step 20: 287.2866, Accuracy: 0.7381\n",
      "Training loss (for one batch) at step 30: 282.6695, Accuracy: 0.7699\n",
      "Training loss (for one batch) at step 40: 277.1462, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 50: 263.0241, Accuracy: 0.8047\n",
      "Training loss (for one batch) at step 60: 297.9761, Accuracy: 0.8117\n",
      "Training loss (for one batch) at step 70: 298.1209, Accuracy: 0.7948\n",
      "Training loss (for one batch) at step 80: 295.9417, Accuracy: 0.7813\n",
      "Training loss (for one batch) at step 90: 264.5125, Accuracy: 0.7781\n",
      "Training loss (for one batch) at step 100: 278.7303, Accuracy: 0.7836\n",
      "Training loss (for one batch) at step 110: 289.0193, Accuracy: 0.7843\n",
      "---- Training ----\n",
      "Training loss: 90.3151\n",
      "Training acc over epoch: 0.7830\n",
      "---- Validation ----\n",
      "Validation loss: 45.1199\n",
      "Validation acc: 0.7050\n",
      "Time taken: 10.55s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 317.7900, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 303.1720, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 20: 270.4875, Accuracy: 0.7221\n",
      "Training loss (for one batch) at step 30: 276.5914, Accuracy: 0.7606\n",
      "Training loss (for one batch) at step 40: 251.3433, Accuracy: 0.7868\n",
      "Training loss (for one batch) at step 50: 244.3852, Accuracy: 0.8033\n",
      "Training loss (for one batch) at step 60: 277.0467, Accuracy: 0.8102\n",
      "Training loss (for one batch) at step 70: 315.8945, Accuracy: 0.7978\n",
      "Training loss (for one batch) at step 80: 311.6301, Accuracy: 0.7815\n",
      "Training loss (for one batch) at step 90: 279.8536, Accuracy: 0.7771\n",
      "Training loss (for one batch) at step 100: 281.3217, Accuracy: 0.7823\n",
      "Training loss (for one batch) at step 110: 268.4342, Accuracy: 0.7824\n",
      "---- Training ----\n",
      "Training loss: 113.9757\n",
      "Training acc over epoch: 0.7813\n",
      "---- Validation ----\n",
      "Validation loss: 48.0257\n",
      "Validation acc: 0.7112\n",
      "Time taken: 10.44s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.7145\n",
      "Validation AUC: 0.7157\n",
      "Validation Balanced_ACC: 0.4288\n",
      "Validation MI: 0.1077\n",
      "Validation Normalized MI: 0.1578\n",
      "Validation Adjusted MI: 0.1578\n",
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 628.4440, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 590.2397, Accuracy: 0.5036\n",
      "Training loss (for one batch) at step 20: 565.8301, Accuracy: 0.4970\n",
      "Training loss (for one batch) at step 30: 534.7332, Accuracy: 0.4962\n",
      "Training loss (for one batch) at step 40: 503.4869, Accuracy: 0.4937\n",
      "Training loss (for one batch) at step 50: 517.4158, Accuracy: 0.4992\n",
      "Training loss (for one batch) at step 60: 496.8973, Accuracy: 0.5029\n",
      "Training loss (for one batch) at step 70: 466.6150, Accuracy: 0.5057\n",
      "Training loss (for one batch) at step 80: 481.3030, Accuracy: 0.5065\n",
      "Training loss (for one batch) at step 90: 477.0554, Accuracy: 0.5072\n",
      "Training loss (for one batch) at step 100: 473.0565, Accuracy: 0.5082\n",
      "Training loss (for one batch) at step 110: 461.3055, Accuracy: 0.5102\n",
      "---- Training ----\n",
      "Training loss: 145.5480\n",
      "Training acc over epoch: 0.5108\n",
      "---- Validation ----\n",
      "Validation loss: 34.8882\n",
      "Validation acc: 0.5134\n",
      "Time taken: 12.38s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 469.7980, Accuracy: 0.4922\n",
      "Training loss (for one batch) at step 10: 460.8850, Accuracy: 0.5149\n",
      "Training loss (for one batch) at step 20: 470.6975, Accuracy: 0.5026\n",
      "Training loss (for one batch) at step 30: 460.7130, Accuracy: 0.5076\n",
      "Training loss (for one batch) at step 40: 465.9964, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 50: 455.1950, Accuracy: 0.5199\n",
      "Training loss (for one batch) at step 60: 454.4700, Accuracy: 0.5251\n",
      "Training loss (for one batch) at step 70: 454.6590, Accuracy: 0.5246\n",
      "Training loss (for one batch) at step 80: 448.4974, Accuracy: 0.5240\n",
      "Training loss (for one batch) at step 90: 448.0042, Accuracy: 0.5270\n",
      "Training loss (for one batch) at step 100: 452.6222, Accuracy: 0.5256\n",
      "Training loss (for one batch) at step 110: 451.0503, Accuracy: 0.5258\n",
      "---- Training ----\n",
      "Training loss: 138.7836\n",
      "Training acc over epoch: 0.5260\n",
      "---- Validation ----\n",
      "Validation loss: 34.5174\n",
      "Validation acc: 0.5140\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 453.2232, Accuracy: 0.5078\n",
      "Training loss (for one batch) at step 10: 446.5374, Accuracy: 0.5320\n",
      "Training loss (for one batch) at step 20: 446.3201, Accuracy: 0.5376\n",
      "Training loss (for one batch) at step 30: 444.8570, Accuracy: 0.5418\n",
      "Training loss (for one batch) at step 40: 443.1640, Accuracy: 0.5452\n",
      "Training loss (for one batch) at step 50: 446.6570, Accuracy: 0.5499\n",
      "Training loss (for one batch) at step 60: 446.8941, Accuracy: 0.5494\n",
      "Training loss (for one batch) at step 70: 443.7859, Accuracy: 0.5531\n",
      "Training loss (for one batch) at step 80: 445.1138, Accuracy: 0.5532\n",
      "Training loss (for one batch) at step 90: 444.6631, Accuracy: 0.5503\n",
      "Training loss (for one batch) at step 100: 445.4709, Accuracy: 0.5490\n",
      "Training loss (for one batch) at step 110: 447.5863, Accuracy: 0.5510\n",
      "---- Training ----\n",
      "Training loss: 138.5775\n",
      "Training acc over epoch: 0.5496\n",
      "---- Validation ----\n",
      "Validation loss: 34.6415\n",
      "Validation acc: 0.5301\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 441.3699, Accuracy: 0.5000\n",
      "Training loss (for one batch) at step 10: 444.3925, Accuracy: 0.5241\n",
      "Training loss (for one batch) at step 20: 443.1485, Accuracy: 0.5346\n",
      "Training loss (for one batch) at step 30: 447.7426, Accuracy: 0.5355\n",
      "Training loss (for one batch) at step 40: 439.4034, Accuracy: 0.5448\n",
      "Training loss (for one batch) at step 50: 446.9204, Accuracy: 0.5492\n",
      "Training loss (for one batch) at step 60: 443.1394, Accuracy: 0.5532\n",
      "Training loss (for one batch) at step 70: 444.3475, Accuracy: 0.5589\n",
      "Training loss (for one batch) at step 80: 448.1696, Accuracy: 0.5577\n",
      "Training loss (for one batch) at step 90: 442.8224, Accuracy: 0.5551\n",
      "Training loss (for one batch) at step 100: 444.2703, Accuracy: 0.5572\n",
      "Training loss (for one batch) at step 110: 445.6199, Accuracy: 0.5602\n",
      "---- Training ----\n",
      "Training loss: 139.2822\n",
      "Training acc over epoch: 0.5625\n",
      "---- Validation ----\n",
      "Validation loss: 34.6167\n",
      "Validation acc: 0.5486\n",
      "Time taken: 10.74s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 444.1556, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 445.8487, Accuracy: 0.5582\n",
      "Training loss (for one batch) at step 20: 441.6125, Accuracy: 0.5510\n",
      "Training loss (for one batch) at step 30: 438.4360, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 40: 433.9943, Accuracy: 0.5566\n",
      "Training loss (for one batch) at step 50: 444.7961, Accuracy: 0.5676\n",
      "Training loss (for one batch) at step 60: 441.7134, Accuracy: 0.5785\n",
      "Training loss (for one batch) at step 70: 440.4702, Accuracy: 0.5780\n",
      "Training loss (for one batch) at step 80: 441.5477, Accuracy: 0.5785\n",
      "Training loss (for one batch) at step 90: 442.2136, Accuracy: 0.5767\n",
      "Training loss (for one batch) at step 100: 442.0241, Accuracy: 0.5762\n",
      "Training loss (for one batch) at step 110: 442.4578, Accuracy: 0.5750\n",
      "---- Training ----\n",
      "Training loss: 136.4830\n",
      "Training acc over epoch: 0.5750\n",
      "---- Validation ----\n",
      "Validation loss: 34.0171\n",
      "Validation acc: 0.5905\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 443.3196, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 441.9443, Accuracy: 0.5902\n",
      "Training loss (for one batch) at step 20: 444.3723, Accuracy: 0.5763\n",
      "Training loss (for one batch) at step 30: 442.3523, Accuracy: 0.5806\n",
      "Training loss (for one batch) at step 40: 440.7560, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 50: 441.1694, Accuracy: 0.5882\n",
      "Training loss (for one batch) at step 60: 444.2198, Accuracy: 0.5912\n",
      "Training loss (for one batch) at step 70: 441.5111, Accuracy: 0.5951\n",
      "Training loss (for one batch) at step 80: 441.6506, Accuracy: 0.5918\n",
      "Training loss (for one batch) at step 90: 445.7710, Accuracy: 0.5884\n",
      "Training loss (for one batch) at step 100: 439.4579, Accuracy: 0.5873\n",
      "Training loss (for one batch) at step 110: 445.4872, Accuracy: 0.5905\n",
      "---- Training ----\n",
      "Training loss: 139.5290\n",
      "Training acc over epoch: 0.5905\n",
      "---- Validation ----\n",
      "Validation loss: 34.0946\n",
      "Validation acc: 0.5825\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 440.1432, Accuracy: 0.5156\n",
      "Training loss (for one batch) at step 10: 441.4377, Accuracy: 0.5760\n",
      "Training loss (for one batch) at step 20: 441.2275, Accuracy: 0.5774\n",
      "Training loss (for one batch) at step 30: 439.0147, Accuracy: 0.5824\n",
      "Training loss (for one batch) at step 40: 440.0325, Accuracy: 0.5873\n",
      "Training loss (for one batch) at step 50: 441.4174, Accuracy: 0.5895\n",
      "Training loss (for one batch) at step 60: 438.9201, Accuracy: 0.5970\n",
      "Training loss (for one batch) at step 70: 447.7039, Accuracy: 0.6019\n",
      "Training loss (for one batch) at step 80: 441.0786, Accuracy: 0.6022\n",
      "Training loss (for one batch) at step 90: 444.2216, Accuracy: 0.5956\n",
      "Training loss (for one batch) at step 100: 439.3436, Accuracy: 0.5948\n",
      "Training loss (for one batch) at step 110: 441.0961, Accuracy: 0.5955\n",
      "---- Training ----\n",
      "Training loss: 137.7330\n",
      "Training acc over epoch: 0.5958\n",
      "---- Validation ----\n",
      "Validation loss: 34.7849\n",
      "Validation acc: 0.6142\n",
      "Time taken: 10.82s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 440.1061, Accuracy: 0.5547\n",
      "Training loss (for one batch) at step 10: 440.5079, Accuracy: 0.5838\n",
      "Training loss (for one batch) at step 20: 439.2245, Accuracy: 0.5874\n",
      "Training loss (for one batch) at step 30: 443.2712, Accuracy: 0.5978\n",
      "Training loss (for one batch) at step 40: 439.1674, Accuracy: 0.6044\n",
      "Training loss (for one batch) at step 50: 430.8085, Accuracy: 0.6114\n",
      "Training loss (for one batch) at step 60: 438.0470, Accuracy: 0.6142\n",
      "Training loss (for one batch) at step 70: 443.9641, Accuracy: 0.6197\n",
      "Training loss (for one batch) at step 80: 443.5671, Accuracy: 0.6184\n",
      "Training loss (for one batch) at step 90: 439.5149, Accuracy: 0.6129\n",
      "Training loss (for one batch) at step 100: 437.1714, Accuracy: 0.6140\n",
      "Training loss (for one batch) at step 110: 437.2332, Accuracy: 0.6146\n",
      "---- Training ----\n",
      "Training loss: 140.3441\n",
      "Training acc over epoch: 0.6158\n",
      "---- Validation ----\n",
      "Validation loss: 33.7320\n",
      "Validation acc: 0.6236\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 443.2218, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 440.0372, Accuracy: 0.6108\n",
      "Training loss (for one batch) at step 20: 436.5969, Accuracy: 0.6142\n",
      "Training loss (for one batch) at step 30: 440.7677, Accuracy: 0.6210\n",
      "Training loss (for one batch) at step 40: 434.8384, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 50: 443.2195, Accuracy: 0.6324\n",
      "Training loss (for one batch) at step 60: 443.6714, Accuracy: 0.6372\n",
      "Training loss (for one batch) at step 70: 441.0628, Accuracy: 0.6418\n",
      "Training loss (for one batch) at step 80: 436.4879, Accuracy: 0.6388\n",
      "Training loss (for one batch) at step 90: 439.5342, Accuracy: 0.6357\n",
      "Training loss (for one batch) at step 100: 433.6545, Accuracy: 0.6320\n",
      "Training loss (for one batch) at step 110: 439.7629, Accuracy: 0.6316\n",
      "---- Training ----\n",
      "Training loss: 139.3695\n",
      "Training acc over epoch: 0.6323\n",
      "---- Validation ----\n",
      "Validation loss: 34.9263\n",
      "Validation acc: 0.6470\n",
      "Time taken: 10.22s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 439.8299, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 439.8266, Accuracy: 0.6222\n",
      "Training loss (for one batch) at step 20: 440.6782, Accuracy: 0.6209\n",
      "Training loss (for one batch) at step 30: 437.4494, Accuracy: 0.6265\n",
      "Training loss (for one batch) at step 40: 440.7192, Accuracy: 0.6315\n",
      "Training loss (for one batch) at step 50: 426.7097, Accuracy: 0.6419\n",
      "Training loss (for one batch) at step 60: 440.9224, Accuracy: 0.6520\n",
      "Training loss (for one batch) at step 70: 431.5067, Accuracy: 0.6520\n",
      "Training loss (for one batch) at step 80: 435.9554, Accuracy: 0.6455\n",
      "Training loss (for one batch) at step 90: 440.1736, Accuracy: 0.6372\n",
      "Training loss (for one batch) at step 100: 438.0635, Accuracy: 0.6345\n",
      "Training loss (for one batch) at step 110: 439.8970, Accuracy: 0.6349\n",
      "---- Training ----\n",
      "Training loss: 139.7157\n",
      "Training acc over epoch: 0.6337\n",
      "---- Validation ----\n",
      "Validation loss: 35.8462\n",
      "Validation acc: 0.6373\n",
      "Time taken: 11.03s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 445.0820, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 442.1870, Accuracy: 0.6222\n",
      "Training loss (for one batch) at step 20: 436.4574, Accuracy: 0.6164\n",
      "Training loss (for one batch) at step 30: 435.4207, Accuracy: 0.6237\n",
      "Training loss (for one batch) at step 40: 429.6889, Accuracy: 0.6326\n",
      "Training loss (for one batch) at step 50: 430.4568, Accuracy: 0.6460\n",
      "Training loss (for one batch) at step 60: 434.9319, Accuracy: 0.6538\n",
      "Training loss (for one batch) at step 70: 437.6288, Accuracy: 0.6577\n",
      "Training loss (for one batch) at step 80: 443.4108, Accuracy: 0.6516\n",
      "Training loss (for one batch) at step 90: 439.6308, Accuracy: 0.6428\n",
      "Training loss (for one batch) at step 100: 436.9679, Accuracy: 0.6399\n",
      "Training loss (for one batch) at step 110: 440.2280, Accuracy: 0.6401\n",
      "---- Training ----\n",
      "Training loss: 137.5088\n",
      "Training acc over epoch: 0.6408\n",
      "---- Validation ----\n",
      "Validation loss: 32.7057\n",
      "Validation acc: 0.6311\n",
      "Time taken: 10.60s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 439.0037, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 442.0923, Accuracy: 0.6165\n",
      "Training loss (for one batch) at step 20: 442.7176, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 30: 434.8135, Accuracy: 0.6285\n",
      "Training loss (for one batch) at step 40: 430.4831, Accuracy: 0.6401\n",
      "Training loss (for one batch) at step 50: 421.7529, Accuracy: 0.6540\n",
      "Training loss (for one batch) at step 60: 447.1027, Accuracy: 0.6651\n",
      "Training loss (for one batch) at step 70: 439.9865, Accuracy: 0.6653\n",
      "Training loss (for one batch) at step 80: 442.8829, Accuracy: 0.6597\n",
      "Training loss (for one batch) at step 90: 440.2568, Accuracy: 0.6514\n",
      "Training loss (for one batch) at step 100: 435.7792, Accuracy: 0.6482\n",
      "Training loss (for one batch) at step 110: 428.3817, Accuracy: 0.6524\n",
      "---- Training ----\n",
      "Training loss: 134.8683\n",
      "Training acc over epoch: 0.6522\n",
      "---- Validation ----\n",
      "Validation loss: 35.2858\n",
      "Validation acc: 0.6300\n",
      "Time taken: 10.20s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 441.4262, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 438.7735, Accuracy: 0.6371\n",
      "Training loss (for one batch) at step 20: 433.8098, Accuracy: 0.6321\n",
      "Training loss (for one batch) at step 30: 428.9805, Accuracy: 0.6399\n",
      "Training loss (for one batch) at step 40: 421.6410, Accuracy: 0.6521\n",
      "Training loss (for one batch) at step 50: 425.6155, Accuracy: 0.6602\n",
      "Training loss (for one batch) at step 60: 423.2846, Accuracy: 0.6716\n",
      "Training loss (for one batch) at step 70: 436.9503, Accuracy: 0.6723\n",
      "Training loss (for one batch) at step 80: 439.1777, Accuracy: 0.6667\n",
      "Training loss (for one batch) at step 90: 433.1384, Accuracy: 0.6601\n",
      "Training loss (for one batch) at step 100: 432.5543, Accuracy: 0.6580\n",
      "Training loss (for one batch) at step 110: 434.7853, Accuracy: 0.6579\n",
      "---- Training ----\n",
      "Training loss: 132.8040\n",
      "Training acc over epoch: 0.6587\n",
      "---- Validation ----\n",
      "Validation loss: 34.0887\n",
      "Validation acc: 0.6376\n",
      "Time taken: 10.79s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 441.3862, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 435.3965, Accuracy: 0.6335\n",
      "Training loss (for one batch) at step 20: 430.5764, Accuracy: 0.6421\n",
      "Training loss (for one batch) at step 30: 430.5758, Accuracy: 0.6557\n",
      "Training loss (for one batch) at step 40: 423.6476, Accuracy: 0.6683\n",
      "Training loss (for one batch) at step 50: 422.9598, Accuracy: 0.6768\n",
      "Training loss (for one batch) at step 60: 424.3281, Accuracy: 0.6848\n",
      "Training loss (for one batch) at step 70: 432.2322, Accuracy: 0.6882\n",
      "Training loss (for one batch) at step 80: 437.2529, Accuracy: 0.6819\n",
      "Training loss (for one batch) at step 90: 435.8327, Accuracy: 0.6760\n",
      "Training loss (for one batch) at step 100: 431.4619, Accuracy: 0.6755\n",
      "Training loss (for one batch) at step 110: 426.9203, Accuracy: 0.6754\n",
      "---- Training ----\n",
      "Training loss: 135.2541\n",
      "Training acc over epoch: 0.6738\n",
      "---- Validation ----\n",
      "Validation loss: 36.1434\n",
      "Validation acc: 0.6483\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 433.9725, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 431.6487, Accuracy: 0.6371\n",
      "Training loss (for one batch) at step 20: 428.7157, Accuracy: 0.6403\n",
      "Training loss (for one batch) at step 30: 421.9608, Accuracy: 0.6467\n",
      "Training loss (for one batch) at step 40: 416.0457, Accuracy: 0.6612\n",
      "Training loss (for one batch) at step 50: 413.7588, Accuracy: 0.6754\n",
      "Training loss (for one batch) at step 60: 414.0482, Accuracy: 0.6824\n",
      "Training loss (for one batch) at step 70: 433.0338, Accuracy: 0.6836\n",
      "Training loss (for one batch) at step 80: 435.2221, Accuracy: 0.6789\n",
      "Training loss (for one batch) at step 90: 441.9666, Accuracy: 0.6743\n",
      "Training loss (for one batch) at step 100: 426.2034, Accuracy: 0.6722\n",
      "Training loss (for one batch) at step 110: 428.8181, Accuracy: 0.6750\n",
      "---- Training ----\n",
      "Training loss: 135.8435\n",
      "Training acc over epoch: 0.6752\n",
      "---- Validation ----\n",
      "Validation loss: 34.1398\n",
      "Validation acc: 0.6900\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 440.2751, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 437.3861, Accuracy: 0.6733\n",
      "Training loss (for one batch) at step 20: 431.3863, Accuracy: 0.6551\n",
      "Training loss (for one batch) at step 30: 422.0622, Accuracy: 0.6683\n",
      "Training loss (for one batch) at step 40: 416.3728, Accuracy: 0.6787\n",
      "Training loss (for one batch) at step 50: 411.3703, Accuracy: 0.6922\n",
      "Training loss (for one batch) at step 60: 411.8026, Accuracy: 0.7002\n",
      "Training loss (for one batch) at step 70: 426.9734, Accuracy: 0.6991\n",
      "Training loss (for one batch) at step 80: 428.7052, Accuracy: 0.6936\n",
      "Training loss (for one batch) at step 90: 431.4456, Accuracy: 0.6888\n",
      "Training loss (for one batch) at step 100: 417.1861, Accuracy: 0.6883\n",
      "Training loss (for one batch) at step 110: 433.4785, Accuracy: 0.6876\n",
      "---- Training ----\n",
      "Training loss: 129.6501\n",
      "Training acc over epoch: 0.6889\n",
      "---- Validation ----\n",
      "Validation loss: 34.4825\n",
      "Validation acc: 0.6730\n",
      "Time taken: 10.71s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 439.8122, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 428.6275, Accuracy: 0.6705\n",
      "Training loss (for one batch) at step 20: 422.2980, Accuracy: 0.6682\n",
      "Training loss (for one batch) at step 30: 421.6518, Accuracy: 0.6809\n",
      "Training loss (for one batch) at step 40: 415.1192, Accuracy: 0.6890\n",
      "Training loss (for one batch) at step 50: 392.9247, Accuracy: 0.7042\n",
      "Training loss (for one batch) at step 60: 410.7352, Accuracy: 0.7130\n",
      "Training loss (for one batch) at step 70: 430.2858, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 80: 435.8642, Accuracy: 0.7052\n",
      "Training loss (for one batch) at step 90: 431.4427, Accuracy: 0.7000\n",
      "Training loss (for one batch) at step 100: 425.2639, Accuracy: 0.6993\n",
      "Training loss (for one batch) at step 110: 428.4764, Accuracy: 0.7000\n",
      "---- Training ----\n",
      "Training loss: 136.8681\n",
      "Training acc over epoch: 0.7012\n",
      "---- Validation ----\n",
      "Validation loss: 36.0704\n",
      "Validation acc: 0.6843\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 440.2730, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 434.9897, Accuracy: 0.6584\n",
      "Training loss (for one batch) at step 20: 428.3278, Accuracy: 0.6566\n",
      "Training loss (for one batch) at step 30: 410.8579, Accuracy: 0.6734\n",
      "Training loss (for one batch) at step 40: 401.8240, Accuracy: 0.6915\n",
      "Training loss (for one batch) at step 50: 400.6408, Accuracy: 0.7040\n",
      "Training loss (for one batch) at step 60: 408.9615, Accuracy: 0.7140\n",
      "Training loss (for one batch) at step 70: 429.2914, Accuracy: 0.7164\n",
      "Training loss (for one batch) at step 80: 432.9414, Accuracy: 0.7115\n",
      "Training loss (for one batch) at step 90: 421.7003, Accuracy: 0.7059\n",
      "Training loss (for one batch) at step 100: 411.7242, Accuracy: 0.7060\n",
      "Training loss (for one batch) at step 110: 429.0231, Accuracy: 0.7072\n",
      "---- Training ----\n",
      "Training loss: 134.5907\n",
      "Training acc over epoch: 0.7065\n",
      "---- Validation ----\n",
      "Validation loss: 34.2441\n",
      "Validation acc: 0.6822\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 441.6774, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 433.1538, Accuracy: 0.6513\n",
      "Training loss (for one batch) at step 20: 426.3094, Accuracy: 0.6629\n",
      "Training loss (for one batch) at step 30: 415.2012, Accuracy: 0.6842\n",
      "Training loss (for one batch) at step 40: 403.5866, Accuracy: 0.6993\n",
      "Training loss (for one batch) at step 50: 408.6260, Accuracy: 0.7138\n",
      "Training loss (for one batch) at step 60: 417.6821, Accuracy: 0.7289\n",
      "Training loss (for one batch) at step 70: 421.2949, Accuracy: 0.7278\n",
      "Training loss (for one batch) at step 80: 429.0950, Accuracy: 0.7197\n",
      "Training loss (for one batch) at step 90: 421.6910, Accuracy: 0.7121\n",
      "Training loss (for one batch) at step 100: 413.6443, Accuracy: 0.7103\n",
      "Training loss (for one batch) at step 110: 423.2495, Accuracy: 0.7100\n",
      "---- Training ----\n",
      "Training loss: 132.1051\n",
      "Training acc over epoch: 0.7099\n",
      "---- Validation ----\n",
      "Validation loss: 34.8096\n",
      "Validation acc: 0.6773\n",
      "Time taken: 10.76s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 434.9678, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 429.4808, Accuracy: 0.6776\n",
      "Training loss (for one batch) at step 20: 407.6185, Accuracy: 0.6760\n",
      "Training loss (for one batch) at step 30: 401.8791, Accuracy: 0.6961\n",
      "Training loss (for one batch) at step 40: 402.3031, Accuracy: 0.7077\n",
      "Training loss (for one batch) at step 50: 396.4155, Accuracy: 0.7207\n",
      "Training loss (for one batch) at step 60: 404.4969, Accuracy: 0.7336\n",
      "Training loss (for one batch) at step 70: 415.1810, Accuracy: 0.7331\n",
      "Training loss (for one batch) at step 80: 423.4799, Accuracy: 0.7263\n",
      "Training loss (for one batch) at step 90: 427.6346, Accuracy: 0.7182\n",
      "Training loss (for one batch) at step 100: 423.4800, Accuracy: 0.7177\n",
      "Training loss (for one batch) at step 110: 414.7736, Accuracy: 0.7183\n",
      "---- Training ----\n",
      "Training loss: 131.6502\n",
      "Training acc over epoch: 0.7189\n",
      "---- Validation ----\n",
      "Validation loss: 41.8958\n",
      "Validation acc: 0.6865\n",
      "Time taken: 10.38s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 428.1893, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 424.0785, Accuracy: 0.6776\n",
      "Training loss (for one batch) at step 20: 413.9885, Accuracy: 0.6830\n",
      "Training loss (for one batch) at step 30: 412.1316, Accuracy: 0.7077\n",
      "Training loss (for one batch) at step 40: 392.8449, Accuracy: 0.7228\n",
      "Training loss (for one batch) at step 50: 387.2014, Accuracy: 0.7376\n",
      "Training loss (for one batch) at step 60: 402.9802, Accuracy: 0.7471\n",
      "Training loss (for one batch) at step 70: 413.1172, Accuracy: 0.7449\n",
      "Training loss (for one batch) at step 80: 425.1343, Accuracy: 0.7366\n",
      "Training loss (for one batch) at step 90: 420.1125, Accuracy: 0.7283\n",
      "Training loss (for one batch) at step 100: 404.1674, Accuracy: 0.7273\n",
      "Training loss (for one batch) at step 110: 410.1050, Accuracy: 0.7279\n",
      "---- Training ----\n",
      "Training loss: 127.2825\n",
      "Training acc over epoch: 0.7268\n",
      "---- Validation ----\n",
      "Validation loss: 35.2985\n",
      "Validation acc: 0.6655\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 438.2149, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 439.2555, Accuracy: 0.6712\n",
      "Training loss (for one batch) at step 20: 403.6779, Accuracy: 0.6741\n",
      "Training loss (for one batch) at step 30: 393.2273, Accuracy: 0.6993\n",
      "Training loss (for one batch) at step 40: 384.5061, Accuracy: 0.7153\n",
      "Training loss (for one batch) at step 50: 376.3264, Accuracy: 0.7315\n",
      "Training loss (for one batch) at step 60: 388.0069, Accuracy: 0.7431\n",
      "Training loss (for one batch) at step 70: 430.2922, Accuracy: 0.7395\n",
      "Training loss (for one batch) at step 80: 436.1526, Accuracy: 0.7330\n",
      "Training loss (for one batch) at step 90: 403.4558, Accuracy: 0.7288\n",
      "Training loss (for one batch) at step 100: 397.2940, Accuracy: 0.7297\n",
      "Training loss (for one batch) at step 110: 406.0875, Accuracy: 0.7318\n",
      "---- Training ----\n",
      "Training loss: 120.6704\n",
      "Training acc over epoch: 0.7316\n",
      "---- Validation ----\n",
      "Validation loss: 38.6643\n",
      "Validation acc: 0.6918\n",
      "Time taken: 10.75s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 430.9953, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 419.1155, Accuracy: 0.6861\n",
      "Training loss (for one batch) at step 20: 399.5994, Accuracy: 0.6968\n",
      "Training loss (for one batch) at step 30: 385.1050, Accuracy: 0.7137\n",
      "Training loss (for one batch) at step 40: 380.4548, Accuracy: 0.7248\n",
      "Training loss (for one batch) at step 50: 363.6570, Accuracy: 0.7391\n",
      "Training loss (for one batch) at step 60: 392.0198, Accuracy: 0.7478\n",
      "Training loss (for one batch) at step 70: 406.0429, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 80: 422.6069, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 90: 405.8604, Accuracy: 0.7294\n",
      "Training loss (for one batch) at step 100: 392.9826, Accuracy: 0.7292\n",
      "Training loss (for one batch) at step 110: 404.6420, Accuracy: 0.7295\n",
      "---- Training ----\n",
      "Training loss: 122.4892\n",
      "Training acc over epoch: 0.7306\n",
      "---- Validation ----\n",
      "Validation loss: 40.7897\n",
      "Validation acc: 0.6921\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 432.5248, Accuracy: 0.7891\n",
      "Training loss (for one batch) at step 10: 422.5057, Accuracy: 0.6967\n",
      "Training loss (for one batch) at step 20: 402.3932, Accuracy: 0.6979\n",
      "Training loss (for one batch) at step 30: 389.5373, Accuracy: 0.7127\n",
      "Training loss (for one batch) at step 40: 374.6360, Accuracy: 0.7252\n",
      "Training loss (for one batch) at step 50: 359.6731, Accuracy: 0.7410\n",
      "Training loss (for one batch) at step 60: 400.4933, Accuracy: 0.7483\n",
      "Training loss (for one batch) at step 70: 401.9834, Accuracy: 0.7465\n",
      "Training loss (for one batch) at step 80: 423.5038, Accuracy: 0.7365\n",
      "Training loss (for one batch) at step 90: 404.3653, Accuracy: 0.7354\n",
      "Training loss (for one batch) at step 100: 389.7820, Accuracy: 0.7365\n",
      "Training loss (for one batch) at step 110: 393.5537, Accuracy: 0.7355\n",
      "---- Training ----\n",
      "Training loss: 133.1349\n",
      "Training acc over epoch: 0.7352\n",
      "---- Validation ----\n",
      "Validation loss: 35.3327\n",
      "Validation acc: 0.6738\n",
      "Time taken: 10.23s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 422.3341, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 413.1359, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 20: 395.3391, Accuracy: 0.7005\n",
      "Training loss (for one batch) at step 30: 405.0614, Accuracy: 0.7198\n",
      "Training loss (for one batch) at step 40: 373.7539, Accuracy: 0.7391\n",
      "Training loss (for one batch) at step 50: 356.9201, Accuracy: 0.7534\n",
      "Training loss (for one batch) at step 60: 380.7386, Accuracy: 0.7605\n",
      "Training loss (for one batch) at step 70: 401.9305, Accuracy: 0.7542\n",
      "Training loss (for one batch) at step 80: 404.6653, Accuracy: 0.7453\n",
      "Training loss (for one batch) at step 90: 408.5253, Accuracy: 0.7400\n",
      "Training loss (for one batch) at step 100: 386.2532, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 110: 409.0385, Accuracy: 0.7401\n",
      "---- Training ----\n",
      "Training loss: 129.0302\n",
      "Training acc over epoch: 0.7397\n",
      "---- Validation ----\n",
      "Validation loss: 36.6627\n",
      "Validation acc: 0.6666\n",
      "Time taken: 10.83s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 411.4284, Accuracy: 0.7500\n",
      "Training loss (for one batch) at step 10: 425.3940, Accuracy: 0.6818\n",
      "Training loss (for one batch) at step 20: 389.9975, Accuracy: 0.6912\n",
      "Training loss (for one batch) at step 30: 389.6190, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 40: 367.5945, Accuracy: 0.7281\n",
      "Training loss (for one batch) at step 50: 365.8846, Accuracy: 0.7430\n",
      "Training loss (for one batch) at step 60: 381.1989, Accuracy: 0.7532\n",
      "Training loss (for one batch) at step 70: 396.9203, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 80: 410.8880, Accuracy: 0.7394\n",
      "Training loss (for one batch) at step 90: 386.6559, Accuracy: 0.7361\n",
      "Training loss (for one batch) at step 100: 399.2290, Accuracy: 0.7390\n",
      "Training loss (for one batch) at step 110: 393.9411, Accuracy: 0.7393\n",
      "---- Training ----\n",
      "Training loss: 121.1545\n",
      "Training acc over epoch: 0.7400\n",
      "---- Validation ----\n",
      "Validation loss: 43.8574\n",
      "Validation acc: 0.6980\n",
      "Time taken: 10.40s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 387.8069, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 412.9489, Accuracy: 0.6697\n",
      "Training loss (for one batch) at step 20: 377.3165, Accuracy: 0.6942\n",
      "Training loss (for one batch) at step 30: 369.5190, Accuracy: 0.7150\n",
      "Training loss (for one batch) at step 40: 372.1331, Accuracy: 0.7357\n",
      "Training loss (for one batch) at step 50: 344.9512, Accuracy: 0.7483\n",
      "Training loss (for one batch) at step 60: 385.6425, Accuracy: 0.7567\n",
      "Training loss (for one batch) at step 70: 418.6171, Accuracy: 0.7503\n",
      "Training loss (for one batch) at step 80: 402.5606, Accuracy: 0.7413\n",
      "Training loss (for one batch) at step 90: 386.9423, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 100: 385.1748, Accuracy: 0.7392\n",
      "Training loss (for one batch) at step 110: 395.2924, Accuracy: 0.7388\n",
      "---- Training ----\n",
      "Training loss: 118.4463\n",
      "Training acc over epoch: 0.7379\n",
      "---- Validation ----\n",
      "Validation loss: 44.7738\n",
      "Validation acc: 0.6663\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 412.1271, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 411.7766, Accuracy: 0.6598\n",
      "Training loss (for one batch) at step 20: 378.8713, Accuracy: 0.6834\n",
      "Training loss (for one batch) at step 30: 385.2674, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 40: 365.9009, Accuracy: 0.7298\n",
      "Training loss (for one batch) at step 50: 358.9760, Accuracy: 0.7482\n",
      "Training loss (for one batch) at step 60: 383.9369, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 70: 389.8192, Accuracy: 0.7521\n",
      "Training loss (for one batch) at step 80: 403.8640, Accuracy: 0.7387\n",
      "Training loss (for one batch) at step 90: 365.7460, Accuracy: 0.7336\n",
      "Training loss (for one batch) at step 100: 379.7954, Accuracy: 0.7351\n",
      "Training loss (for one batch) at step 110: 384.0186, Accuracy: 0.7362\n",
      "---- Training ----\n",
      "Training loss: 122.3951\n",
      "Training acc over epoch: 0.7357\n",
      "---- Validation ----\n",
      "Validation loss: 36.1573\n",
      "Validation acc: 0.6819\n",
      "Time taken: 10.77s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 408.8599, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 417.3318, Accuracy: 0.6733\n",
      "Training loss (for one batch) at step 20: 387.2201, Accuracy: 0.6905\n",
      "Training loss (for one batch) at step 30: 368.9451, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 40: 357.1992, Accuracy: 0.7355\n",
      "Training loss (for one batch) at step 50: 358.0888, Accuracy: 0.7518\n",
      "Training loss (for one batch) at step 60: 349.4619, Accuracy: 0.7637\n",
      "Training loss (for one batch) at step 70: 399.4066, Accuracy: 0.7551\n",
      "Training loss (for one batch) at step 80: 402.8845, Accuracy: 0.7458\n",
      "Training loss (for one batch) at step 90: 374.8622, Accuracy: 0.7404\n",
      "Training loss (for one batch) at step 100: 380.0140, Accuracy: 0.7438\n",
      "Training loss (for one batch) at step 110: 383.1388, Accuracy: 0.7452\n",
      "---- Training ----\n",
      "Training loss: 118.1163\n",
      "Training acc over epoch: 0.7447\n",
      "---- Validation ----\n",
      "Validation loss: 35.5447\n",
      "Validation acc: 0.6961\n",
      "Time taken: 10.17s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 408.3190, Accuracy: 0.7344\n",
      "Training loss (for one batch) at step 10: 409.7519, Accuracy: 0.6548\n",
      "Training loss (for one batch) at step 20: 370.6364, Accuracy: 0.6849\n",
      "Training loss (for one batch) at step 30: 360.7885, Accuracy: 0.7182\n",
      "Training loss (for one batch) at step 40: 352.9118, Accuracy: 0.7384\n",
      "Training loss (for one batch) at step 50: 341.6221, Accuracy: 0.7538\n",
      "Training loss (for one batch) at step 60: 355.1520, Accuracy: 0.7631\n",
      "Training loss (for one batch) at step 70: 372.3141, Accuracy: 0.7548\n",
      "Training loss (for one batch) at step 80: 378.0531, Accuracy: 0.7438\n",
      "Training loss (for one batch) at step 90: 370.7932, Accuracy: 0.7404\n",
      "Training loss (for one batch) at step 100: 361.5370, Accuracy: 0.7426\n",
      "Training loss (for one batch) at step 110: 365.2604, Accuracy: 0.7423\n",
      "---- Training ----\n",
      "Training loss: 123.8923\n",
      "Training acc over epoch: 0.7411\n",
      "---- Validation ----\n",
      "Validation loss: 41.1034\n",
      "Validation acc: 0.6926\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 378.2684, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 407.4211, Accuracy: 0.6662\n",
      "Training loss (for one batch) at step 20: 374.0850, Accuracy: 0.6905\n",
      "Training loss (for one batch) at step 30: 350.5096, Accuracy: 0.7185\n",
      "Training loss (for one batch) at step 40: 362.9460, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 50: 355.2643, Accuracy: 0.7537\n",
      "Training loss (for one batch) at step 60: 355.4252, Accuracy: 0.7606\n",
      "Training loss (for one batch) at step 70: 373.8437, Accuracy: 0.7547\n",
      "Training loss (for one batch) at step 80: 366.6933, Accuracy: 0.7432\n",
      "Training loss (for one batch) at step 90: 351.8340, Accuracy: 0.7388\n",
      "Training loss (for one batch) at step 100: 354.0018, Accuracy: 0.7425\n",
      "Training loss (for one batch) at step 110: 379.3001, Accuracy: 0.7432\n",
      "---- Training ----\n",
      "Training loss: 113.7068\n",
      "Training acc over epoch: 0.7414\n",
      "---- Validation ----\n",
      "Validation loss: 44.1466\n",
      "Validation acc: 0.6644\n",
      "Time taken: 10.64s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 404.8326, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 389.2392, Accuracy: 0.6392\n",
      "Training loss (for one batch) at step 20: 368.5300, Accuracy: 0.6771\n",
      "Training loss (for one batch) at step 30: 338.2318, Accuracy: 0.7079\n",
      "Training loss (for one batch) at step 40: 346.1753, Accuracy: 0.7342\n",
      "Training loss (for one batch) at step 50: 341.2843, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 60: 369.3574, Accuracy: 0.7574\n",
      "Training loss (for one batch) at step 70: 390.7503, Accuracy: 0.7523\n",
      "Training loss (for one batch) at step 80: 384.8973, Accuracy: 0.7384\n",
      "Training loss (for one batch) at step 90: 354.2248, Accuracy: 0.7355\n",
      "Training loss (for one batch) at step 100: 346.0273, Accuracy: 0.7384\n",
      "Training loss (for one batch) at step 110: 387.6149, Accuracy: 0.7413\n",
      "---- Training ----\n",
      "Training loss: 109.1213\n",
      "Training acc over epoch: 0.7399\n",
      "---- Validation ----\n",
      "Validation loss: 34.7088\n",
      "Validation acc: 0.6819\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 393.9970, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 394.9322, Accuracy: 0.6577\n",
      "Training loss (for one batch) at step 20: 358.5139, Accuracy: 0.6938\n",
      "Training loss (for one batch) at step 30: 341.0032, Accuracy: 0.7220\n",
      "Training loss (for one batch) at step 40: 343.0874, Accuracy: 0.7445\n",
      "Training loss (for one batch) at step 50: 337.5525, Accuracy: 0.7607\n",
      "Training loss (for one batch) at step 60: 340.3199, Accuracy: 0.7677\n",
      "Training loss (for one batch) at step 70: 378.8753, Accuracy: 0.7590\n",
      "Training loss (for one batch) at step 80: 382.2769, Accuracy: 0.7432\n",
      "Training loss (for one batch) at step 90: 369.2614, Accuracy: 0.7375\n",
      "Training loss (for one batch) at step 100: 348.1639, Accuracy: 0.7407\n",
      "Training loss (for one batch) at step 110: 360.6133, Accuracy: 0.7418\n",
      "---- Training ----\n",
      "Training loss: 113.3032\n",
      "Training acc over epoch: 0.7409\n",
      "---- Validation ----\n",
      "Validation loss: 38.0634\n",
      "Validation acc: 0.6857\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 386.2018, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 407.2573, Accuracy: 0.6520\n",
      "Training loss (for one batch) at step 20: 375.9821, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 30: 350.0385, Accuracy: 0.7150\n",
      "Training loss (for one batch) at step 40: 336.4284, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 50: 338.7807, Accuracy: 0.7528\n",
      "Training loss (for one batch) at step 60: 353.4055, Accuracy: 0.7645\n",
      "Training loss (for one batch) at step 70: 375.7646, Accuracy: 0.7539\n",
      "Training loss (for one batch) at step 80: 383.8119, Accuracy: 0.7410\n",
      "Training loss (for one batch) at step 90: 337.0338, Accuracy: 0.7371\n",
      "Training loss (for one batch) at step 100: 344.0146, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 110: 366.4510, Accuracy: 0.7421\n",
      "---- Training ----\n",
      "Training loss: 110.8473\n",
      "Training acc over epoch: 0.7419\n",
      "---- Validation ----\n",
      "Validation loss: 41.2716\n",
      "Validation acc: 0.6827\n",
      "Time taken: 10.84s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 394.6515, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 388.4348, Accuracy: 0.6555\n",
      "Training loss (for one batch) at step 20: 352.4010, Accuracy: 0.6834\n",
      "Training loss (for one batch) at step 30: 331.8199, Accuracy: 0.7170\n",
      "Training loss (for one batch) at step 40: 331.1018, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 50: 332.7090, Accuracy: 0.7566\n",
      "Training loss (for one batch) at step 60: 347.1777, Accuracy: 0.7669\n",
      "Training loss (for one batch) at step 70: 379.3810, Accuracy: 0.7567\n",
      "Training loss (for one batch) at step 80: 384.4189, Accuracy: 0.7427\n",
      "Training loss (for one batch) at step 90: 345.3587, Accuracy: 0.7396\n",
      "Training loss (for one batch) at step 100: 366.4030, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 110: 350.1638, Accuracy: 0.7428\n",
      "---- Training ----\n",
      "Training loss: 103.5646\n",
      "Training acc over epoch: 0.7409\n",
      "---- Validation ----\n",
      "Validation loss: 37.6162\n",
      "Validation acc: 0.7007\n",
      "Time taken: 10.26s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 404.1583, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 378.6738, Accuracy: 0.6584\n",
      "Training loss (for one batch) at step 20: 363.6366, Accuracy: 0.6771\n",
      "Training loss (for one batch) at step 30: 345.3639, Accuracy: 0.7172\n",
      "Training loss (for one batch) at step 40: 330.8154, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 50: 322.3564, Accuracy: 0.7541\n",
      "Training loss (for one batch) at step 60: 350.4910, Accuracy: 0.7634\n",
      "Training loss (for one batch) at step 70: 383.3319, Accuracy: 0.7568\n",
      "Training loss (for one batch) at step 80: 374.2685, Accuracy: 0.7421\n",
      "Training loss (for one batch) at step 90: 340.4849, Accuracy: 0.7377\n",
      "Training loss (for one batch) at step 100: 350.9967, Accuracy: 0.7420\n",
      "Training loss (for one batch) at step 110: 362.5769, Accuracy: 0.7442\n",
      "---- Training ----\n",
      "Training loss: 113.2459\n",
      "Training acc over epoch: 0.7427\n",
      "---- Validation ----\n",
      "Validation loss: 43.4045\n",
      "Validation acc: 0.6945\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 383.7205, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 398.5726, Accuracy: 0.6577\n",
      "Training loss (for one batch) at step 20: 351.7895, Accuracy: 0.6842\n",
      "Training loss (for one batch) at step 30: 327.4681, Accuracy: 0.7208\n",
      "Training loss (for one batch) at step 40: 322.4610, Accuracy: 0.7452\n",
      "Training loss (for one batch) at step 50: 321.3861, Accuracy: 0.7627\n",
      "Training loss (for one batch) at step 60: 341.8488, Accuracy: 0.7722\n",
      "Training loss (for one batch) at step 70: 367.8843, Accuracy: 0.7630\n",
      "Training loss (for one batch) at step 80: 364.7535, Accuracy: 0.7464\n",
      "Training loss (for one batch) at step 90: 324.0531, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 100: 319.4700, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 110: 336.4351, Accuracy: 0.7480\n",
      "---- Training ----\n",
      "Training loss: 111.8017\n",
      "Training acc over epoch: 0.7479\n",
      "---- Validation ----\n",
      "Validation loss: 34.1654\n",
      "Validation acc: 0.6943\n",
      "Time taken: 10.95s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 379.9333, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 356.9955, Accuracy: 0.6520\n",
      "Training loss (for one batch) at step 20: 355.0706, Accuracy: 0.6834\n",
      "Training loss (for one batch) at step 30: 326.6237, Accuracy: 0.7218\n",
      "Training loss (for one batch) at step 40: 337.7126, Accuracy: 0.7475\n",
      "Training loss (for one batch) at step 50: 332.5260, Accuracy: 0.7630\n",
      "Training loss (for one batch) at step 60: 341.8951, Accuracy: 0.7699\n",
      "Training loss (for one batch) at step 70: 346.7463, Accuracy: 0.7606\n",
      "Training loss (for one batch) at step 80: 404.0162, Accuracy: 0.7421\n",
      "Training loss (for one batch) at step 90: 346.2952, Accuracy: 0.7373\n",
      "Training loss (for one batch) at step 100: 330.9043, Accuracy: 0.7410\n",
      "Training loss (for one batch) at step 110: 338.0427, Accuracy: 0.7419\n",
      "---- Training ----\n",
      "Training loss: 116.1138\n",
      "Training acc over epoch: 0.7401\n",
      "---- Validation ----\n",
      "Validation loss: 40.9747\n",
      "Validation acc: 0.6878\n",
      "Time taken: 10.37s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 376.9217, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 374.9737, Accuracy: 0.6428\n",
      "Training loss (for one batch) at step 20: 331.1927, Accuracy: 0.6882\n",
      "Training loss (for one batch) at step 30: 308.4612, Accuracy: 0.7180\n",
      "Training loss (for one batch) at step 40: 327.4190, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 50: 320.5186, Accuracy: 0.7595\n",
      "Training loss (for one batch) at step 60: 350.1588, Accuracy: 0.7697\n",
      "Training loss (for one batch) at step 70: 384.2400, Accuracy: 0.7586\n",
      "Training loss (for one batch) at step 80: 370.4021, Accuracy: 0.7458\n",
      "Training loss (for one batch) at step 90: 337.7057, Accuracy: 0.7436\n",
      "Training loss (for one batch) at step 100: 331.0229, Accuracy: 0.7480\n",
      "Training loss (for one batch) at step 110: 340.5015, Accuracy: 0.7505\n",
      "---- Training ----\n",
      "Training loss: 106.1805\n",
      "Training acc over epoch: 0.7487\n",
      "---- Validation ----\n",
      "Validation loss: 50.3400\n",
      "Validation acc: 0.6768\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 381.6754, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 358.3475, Accuracy: 0.6449\n",
      "Training loss (for one batch) at step 20: 333.0173, Accuracy: 0.6782\n",
      "Training loss (for one batch) at step 30: 327.3176, Accuracy: 0.7198\n",
      "Training loss (for one batch) at step 40: 321.1784, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 50: 327.6983, Accuracy: 0.7639\n",
      "Training loss (for one batch) at step 60: 338.1087, Accuracy: 0.7699\n",
      "Training loss (for one batch) at step 70: 363.9278, Accuracy: 0.7579\n",
      "Training loss (for one batch) at step 80: 384.4991, Accuracy: 0.7421\n",
      "Training loss (for one batch) at step 90: 322.9999, Accuracy: 0.7383\n",
      "Training loss (for one batch) at step 100: 341.4614, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 110: 342.8490, Accuracy: 0.7437\n",
      "---- Training ----\n",
      "Training loss: 110.1318\n",
      "Training acc over epoch: 0.7433\n",
      "---- Validation ----\n",
      "Validation loss: 54.2968\n",
      "Validation acc: 0.6918\n",
      "Time taken: 10.85s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 370.3801, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 385.0327, Accuracy: 0.6293\n",
      "Training loss (for one batch) at step 20: 330.6106, Accuracy: 0.6771\n",
      "Training loss (for one batch) at step 30: 315.2665, Accuracy: 0.7172\n",
      "Training loss (for one batch) at step 40: 310.9659, Accuracy: 0.7490\n",
      "Training loss (for one batch) at step 50: 309.4727, Accuracy: 0.7675\n",
      "Training loss (for one batch) at step 60: 338.8641, Accuracy: 0.7739\n",
      "Training loss (for one batch) at step 70: 364.0856, Accuracy: 0.7618\n",
      "Training loss (for one batch) at step 80: 374.5232, Accuracy: 0.7458\n",
      "Training loss (for one batch) at step 90: 344.0194, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 100: 341.0778, Accuracy: 0.7444\n",
      "Training loss (for one batch) at step 110: 331.6525, Accuracy: 0.7473\n",
      "---- Training ----\n",
      "Training loss: 106.7088\n",
      "Training acc over epoch: 0.7457\n",
      "---- Validation ----\n",
      "Validation loss: 44.1398\n",
      "Validation acc: 0.6878\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 375.7067, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 369.1469, Accuracy: 0.6470\n",
      "Training loss (for one batch) at step 20: 329.6388, Accuracy: 0.6868\n",
      "Training loss (for one batch) at step 30: 314.0004, Accuracy: 0.7205\n",
      "Training loss (for one batch) at step 40: 318.9836, Accuracy: 0.7479\n",
      "Training loss (for one batch) at step 50: 322.6061, Accuracy: 0.7626\n",
      "Training loss (for one batch) at step 60: 342.3979, Accuracy: 0.7710\n",
      "Training loss (for one batch) at step 70: 357.8010, Accuracy: 0.7602\n",
      "Training loss (for one batch) at step 80: 374.1765, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 90: 326.7126, Accuracy: 0.7420\n",
      "Training loss (for one batch) at step 100: 317.0950, Accuracy: 0.7461\n",
      "Training loss (for one batch) at step 110: 345.1675, Accuracy: 0.7473\n",
      "---- Training ----\n",
      "Training loss: 113.5318\n",
      "Training acc over epoch: 0.7455\n",
      "---- Validation ----\n",
      "Validation loss: 44.0522\n",
      "Validation acc: 0.6717\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 369.5688, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 10: 368.4986, Accuracy: 0.6541\n",
      "Training loss (for one batch) at step 20: 330.6652, Accuracy: 0.6920\n",
      "Training loss (for one batch) at step 30: 319.0911, Accuracy: 0.7218\n",
      "Training loss (for one batch) at step 40: 302.5033, Accuracy: 0.7496\n",
      "Training loss (for one batch) at step 50: 312.2421, Accuracy: 0.7679\n",
      "Training loss (for one batch) at step 60: 320.4742, Accuracy: 0.7737\n",
      "Training loss (for one batch) at step 70: 356.2297, Accuracy: 0.7599\n",
      "Training loss (for one batch) at step 80: 371.2438, Accuracy: 0.7438\n",
      "Training loss (for one batch) at step 90: 347.0244, Accuracy: 0.7406\n",
      "Training loss (for one batch) at step 100: 337.9684, Accuracy: 0.7456\n",
      "Training loss (for one batch) at step 110: 336.8542, Accuracy: 0.7460\n",
      "---- Training ----\n",
      "Training loss: 106.5063\n",
      "Training acc over epoch: 0.7450\n",
      "---- Validation ----\n",
      "Validation loss: 39.4148\n",
      "Validation acc: 0.6695\n",
      "Time taken: 10.75s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 371.9649, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 355.2630, Accuracy: 0.6321\n",
      "Training loss (for one batch) at step 20: 329.7423, Accuracy: 0.6771\n",
      "Training loss (for one batch) at step 30: 317.5882, Accuracy: 0.7182\n",
      "Training loss (for one batch) at step 40: 313.5977, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 50: 303.5285, Accuracy: 0.7615\n",
      "Training loss (for one batch) at step 60: 344.6996, Accuracy: 0.7699\n",
      "Training loss (for one batch) at step 70: 356.6861, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 80: 351.8937, Accuracy: 0.7446\n",
      "Training loss (for one batch) at step 90: 347.1358, Accuracy: 0.7394\n",
      "Training loss (for one batch) at step 100: 324.6079, Accuracy: 0.7445\n",
      "Training loss (for one batch) at step 110: 343.4202, Accuracy: 0.7450\n",
      "---- Training ----\n",
      "Training loss: 112.5202\n",
      "Training acc over epoch: 0.7436\n",
      "---- Validation ----\n",
      "Validation loss: 46.4966\n",
      "Validation acc: 0.6725\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 380.4158, Accuracy: 0.7031\n",
      "Training loss (for one batch) at step 10: 366.6414, Accuracy: 0.6392\n",
      "Training loss (for one batch) at step 20: 326.2634, Accuracy: 0.6804\n",
      "Training loss (for one batch) at step 30: 311.7190, Accuracy: 0.7160\n",
      "Training loss (for one batch) at step 40: 308.5135, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 50: 302.4588, Accuracy: 0.7612\n",
      "Training loss (for one batch) at step 60: 313.1318, Accuracy: 0.7696\n",
      "Training loss (for one batch) at step 70: 342.5303, Accuracy: 0.7572\n",
      "Training loss (for one batch) at step 80: 358.8282, Accuracy: 0.7404\n",
      "Training loss (for one batch) at step 90: 309.1499, Accuracy: 0.7385\n",
      "Training loss (for one batch) at step 100: 316.2738, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 110: 336.1190, Accuracy: 0.7442\n",
      "---- Training ----\n",
      "Training loss: 107.2105\n",
      "Training acc over epoch: 0.7423\n",
      "---- Validation ----\n",
      "Validation loss: 45.0266\n",
      "Validation acc: 0.6943\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 365.2777, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 361.1830, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 20: 337.2252, Accuracy: 0.6756\n",
      "Training loss (for one batch) at step 30: 323.9459, Accuracy: 0.7137\n",
      "Training loss (for one batch) at step 40: 305.4948, Accuracy: 0.7357\n",
      "Training loss (for one batch) at step 50: 307.1285, Accuracy: 0.7603\n",
      "Training loss (for one batch) at step 60: 333.6948, Accuracy: 0.7690\n",
      "Training loss (for one batch) at step 70: 357.0964, Accuracy: 0.7553\n",
      "Training loss (for one batch) at step 80: 340.3388, Accuracy: 0.7413\n",
      "Training loss (for one batch) at step 90: 331.5270, Accuracy: 0.7380\n",
      "Training loss (for one batch) at step 100: 302.1705, Accuracy: 0.7393\n",
      "Training loss (for one batch) at step 110: 323.7434, Accuracy: 0.7414\n",
      "---- Training ----\n",
      "Training loss: 116.5875\n",
      "Training acc over epoch: 0.7403\n",
      "---- Validation ----\n",
      "Validation loss: 44.0053\n",
      "Validation acc: 0.6902\n",
      "Time taken: 10.99s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 377.9833, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 367.4939, Accuracy: 0.6293\n",
      "Training loss (for one batch) at step 20: 321.8146, Accuracy: 0.6801\n",
      "Training loss (for one batch) at step 30: 316.8044, Accuracy: 0.7180\n",
      "Training loss (for one batch) at step 40: 301.2924, Accuracy: 0.7414\n",
      "Training loss (for one batch) at step 50: 290.9138, Accuracy: 0.7586\n",
      "Training loss (for one batch) at step 60: 312.6982, Accuracy: 0.7693\n",
      "Training loss (for one batch) at step 70: 343.2443, Accuracy: 0.7595\n",
      "Training loss (for one batch) at step 80: 340.3960, Accuracy: 0.7446\n",
      "Training loss (for one batch) at step 90: 319.1105, Accuracy: 0.7423\n",
      "Training loss (for one batch) at step 100: 336.6101, Accuracy: 0.7465\n",
      "Training loss (for one batch) at step 110: 318.5628, Accuracy: 0.7467\n",
      "---- Training ----\n",
      "Training loss: 133.9550\n",
      "Training acc over epoch: 0.7452\n",
      "---- Validation ----\n",
      "Validation loss: 44.0820\n",
      "Validation acc: 0.6830\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 370.9081, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 377.6129, Accuracy: 0.6278\n",
      "Training loss (for one batch) at step 20: 325.4445, Accuracy: 0.6730\n",
      "Training loss (for one batch) at step 30: 307.3306, Accuracy: 0.7145\n",
      "Training loss (for one batch) at step 40: 298.3419, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 50: 300.2834, Accuracy: 0.7632\n",
      "Training loss (for one batch) at step 60: 309.2256, Accuracy: 0.7706\n",
      "Training loss (for one batch) at step 70: 347.8098, Accuracy: 0.7575\n",
      "Training loss (for one batch) at step 80: 341.2033, Accuracy: 0.7414\n",
      "Training loss (for one batch) at step 90: 315.8207, Accuracy: 0.7385\n",
      "Training loss (for one batch) at step 100: 324.5360, Accuracy: 0.7407\n",
      "Training loss (for one batch) at step 110: 320.3960, Accuracy: 0.7424\n",
      "---- Training ----\n",
      "Training loss: 102.7098\n",
      "Training acc over epoch: 0.7410\n",
      "---- Validation ----\n",
      "Validation loss: 40.5690\n",
      "Validation acc: 0.6784\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 373.1567, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 346.2798, Accuracy: 0.6271\n",
      "Training loss (for one batch) at step 20: 314.7376, Accuracy: 0.6834\n",
      "Training loss (for one batch) at step 30: 316.7682, Accuracy: 0.7238\n",
      "Training loss (for one batch) at step 40: 295.3792, Accuracy: 0.7513\n",
      "Training loss (for one batch) at step 50: 288.6741, Accuracy: 0.7704\n",
      "Training loss (for one batch) at step 60: 318.5327, Accuracy: 0.7732\n",
      "Training loss (for one batch) at step 70: 337.9786, Accuracy: 0.7602\n",
      "Training loss (for one batch) at step 80: 352.0395, Accuracy: 0.7483\n",
      "Training loss (for one batch) at step 90: 313.9634, Accuracy: 0.7448\n",
      "Training loss (for one batch) at step 100: 318.5052, Accuracy: 0.7471\n",
      "Training loss (for one batch) at step 110: 335.3512, Accuracy: 0.7474\n",
      "---- Training ----\n",
      "Training loss: 111.4890\n",
      "Training acc over epoch: 0.7460\n",
      "---- Validation ----\n",
      "Validation loss: 46.4561\n",
      "Validation acc: 0.6846\n",
      "Time taken: 10.86s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 360.2672, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 344.2397, Accuracy: 0.6143\n",
      "Training loss (for one batch) at step 20: 329.0466, Accuracy: 0.6685\n",
      "Training loss (for one batch) at step 30: 309.9308, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 40: 305.3414, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 50: 299.1544, Accuracy: 0.7589\n",
      "Training loss (for one batch) at step 60: 316.7416, Accuracy: 0.7677\n",
      "Training loss (for one batch) at step 70: 365.5339, Accuracy: 0.7565\n",
      "Training loss (for one batch) at step 80: 349.5438, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 90: 336.8754, Accuracy: 0.7353\n",
      "Training loss (for one batch) at step 100: 298.2284, Accuracy: 0.7423\n",
      "Training loss (for one batch) at step 110: 339.5746, Accuracy: 0.7433\n",
      "---- Training ----\n",
      "Training loss: 106.4854\n",
      "Training acc over epoch: 0.7410\n",
      "---- Validation ----\n",
      "Validation loss: 61.7630\n",
      "Validation acc: 0.6977\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 359.4131, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 355.1493, Accuracy: 0.6271\n",
      "Training loss (for one batch) at step 20: 329.3352, Accuracy: 0.6782\n",
      "Training loss (for one batch) at step 30: 319.6713, Accuracy: 0.7193\n",
      "Training loss (for one batch) at step 40: 326.6295, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 50: 308.5523, Accuracy: 0.7653\n",
      "Training loss (for one batch) at step 60: 314.5512, Accuracy: 0.7745\n",
      "Training loss (for one batch) at step 70: 339.7444, Accuracy: 0.7622\n",
      "Training loss (for one batch) at step 80: 339.1927, Accuracy: 0.7476\n",
      "Training loss (for one batch) at step 90: 329.0107, Accuracy: 0.7439\n",
      "Training loss (for one batch) at step 100: 315.9609, Accuracy: 0.7465\n",
      "Training loss (for one batch) at step 110: 329.3141, Accuracy: 0.7471\n",
      "---- Training ----\n",
      "Training loss: 91.9273\n",
      "Training acc over epoch: 0.7461\n",
      "---- Validation ----\n",
      "Validation loss: 51.7907\n",
      "Validation acc: 0.6784\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 354.8115, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 345.7722, Accuracy: 0.6428\n",
      "Training loss (for one batch) at step 20: 309.2578, Accuracy: 0.6849\n",
      "Training loss (for one batch) at step 30: 304.7303, Accuracy: 0.7238\n",
      "Training loss (for one batch) at step 40: 283.1240, Accuracy: 0.7470\n",
      "Training loss (for one batch) at step 50: 300.6718, Accuracy: 0.7636\n",
      "Training loss (for one batch) at step 60: 327.6546, Accuracy: 0.7699\n",
      "Training loss (for one batch) at step 70: 338.2264, Accuracy: 0.7587\n",
      "Training loss (for one batch) at step 80: 375.8933, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 90: 316.0339, Accuracy: 0.7395\n",
      "Training loss (for one batch) at step 100: 317.0598, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 110: 323.3401, Accuracy: 0.7444\n",
      "---- Training ----\n",
      "Training loss: 108.2084\n",
      "Training acc over epoch: 0.7431\n",
      "---- Validation ----\n",
      "Validation loss: 58.6957\n",
      "Validation acc: 0.6827\n",
      "Time taken: 10.71s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 342.2520, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 352.2850, Accuracy: 0.6541\n",
      "Training loss (for one batch) at step 20: 319.3013, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 30: 313.5780, Accuracy: 0.7245\n",
      "Training loss (for one batch) at step 40: 291.9906, Accuracy: 0.7511\n",
      "Training loss (for one batch) at step 50: 296.4180, Accuracy: 0.7665\n",
      "Training loss (for one batch) at step 60: 316.8741, Accuracy: 0.7731\n",
      "Training loss (for one batch) at step 70: 351.6922, Accuracy: 0.7622\n",
      "Training loss (for one batch) at step 80: 344.4589, Accuracy: 0.7432\n",
      "Training loss (for one batch) at step 90: 309.5476, Accuracy: 0.7400\n",
      "Training loss (for one batch) at step 100: 310.6724, Accuracy: 0.7444\n",
      "Training loss (for one batch) at step 110: 343.2460, Accuracy: 0.7457\n",
      "---- Training ----\n",
      "Training loss: 115.6510\n",
      "Training acc over epoch: 0.7455\n",
      "---- Validation ----\n",
      "Validation loss: 47.9538\n",
      "Validation acc: 0.6867\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 361.0329, Accuracy: 0.6953\n",
      "Training loss (for one batch) at step 10: 348.6786, Accuracy: 0.6314\n",
      "Training loss (for one batch) at step 20: 293.7578, Accuracy: 0.6763\n",
      "Training loss (for one batch) at step 30: 298.5591, Accuracy: 0.7195\n",
      "Training loss (for one batch) at step 40: 305.0193, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 50: 288.4192, Accuracy: 0.7575\n",
      "Training loss (for one batch) at step 60: 300.8112, Accuracy: 0.7681\n",
      "Training loss (for one batch) at step 70: 332.1554, Accuracy: 0.7587\n",
      "Training loss (for one batch) at step 80: 337.0146, Accuracy: 0.7402\n",
      "Training loss (for one batch) at step 90: 314.7782, Accuracy: 0.7378\n",
      "Training loss (for one batch) at step 100: 288.1605, Accuracy: 0.7432\n",
      "Training loss (for one batch) at step 110: 317.5549, Accuracy: 0.7450\n",
      "---- Training ----\n",
      "Training loss: 108.6807\n",
      "Training acc over epoch: 0.7426\n",
      "---- Validation ----\n",
      "Validation loss: 58.9006\n",
      "Validation acc: 0.6924\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 347.8064, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 340.4440, Accuracy: 0.6186\n",
      "Training loss (for one batch) at step 20: 311.4232, Accuracy: 0.6689\n",
      "Training loss (for one batch) at step 30: 311.9080, Accuracy: 0.7142\n",
      "Training loss (for one batch) at step 40: 293.7538, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 50: 277.1700, Accuracy: 0.7586\n",
      "Training loss (for one batch) at step 60: 317.6254, Accuracy: 0.7688\n",
      "Training loss (for one batch) at step 70: 334.1969, Accuracy: 0.7575\n",
      "Training loss (for one batch) at step 80: 337.0294, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 90: 322.9401, Accuracy: 0.7406\n",
      "Training loss (for one batch) at step 100: 304.9249, Accuracy: 0.7444\n",
      "Training loss (for one batch) at step 110: 338.8777, Accuracy: 0.7463\n",
      "---- Training ----\n",
      "Training loss: 104.4461\n",
      "Training acc over epoch: 0.7447\n",
      "---- Validation ----\n",
      "Validation loss: 64.7998\n",
      "Validation acc: 0.6827\n",
      "Time taken: 10.75s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 352.2747, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 342.5537, Accuracy: 0.6371\n",
      "Training loss (for one batch) at step 20: 313.1597, Accuracy: 0.6815\n",
      "Training loss (for one batch) at step 30: 306.9025, Accuracy: 0.7195\n",
      "Training loss (for one batch) at step 40: 302.1526, Accuracy: 0.7443\n",
      "Training loss (for one batch) at step 50: 292.2776, Accuracy: 0.7624\n",
      "Training loss (for one batch) at step 60: 315.3658, Accuracy: 0.7731\n",
      "Training loss (for one batch) at step 70: 328.8903, Accuracy: 0.7592\n",
      "Training loss (for one batch) at step 80: 329.9715, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 90: 305.1548, Accuracy: 0.7402\n",
      "Training loss (for one batch) at step 100: 307.0914, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 110: 322.0402, Accuracy: 0.7440\n",
      "---- Training ----\n",
      "Training loss: 118.2636\n",
      "Training acc over epoch: 0.7419\n",
      "---- Validation ----\n",
      "Validation loss: 32.4574\n",
      "Validation acc: 0.6988\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 341.1173, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 345.8472, Accuracy: 0.6243\n",
      "Training loss (for one batch) at step 20: 314.8222, Accuracy: 0.6819\n",
      "Training loss (for one batch) at step 30: 294.7689, Accuracy: 0.7243\n",
      "Training loss (for one batch) at step 40: 284.7795, Accuracy: 0.7508\n",
      "Training loss (for one batch) at step 50: 303.7704, Accuracy: 0.7687\n",
      "Training loss (for one batch) at step 60: 340.8033, Accuracy: 0.7770\n",
      "Training loss (for one batch) at step 70: 328.4905, Accuracy: 0.7645\n",
      "Training loss (for one batch) at step 80: 336.6748, Accuracy: 0.7471\n",
      "Training loss (for one batch) at step 90: 320.9257, Accuracy: 0.7442\n",
      "Training loss (for one batch) at step 100: 301.7106, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 110: 332.8440, Accuracy: 0.7493\n",
      "---- Training ----\n",
      "Training loss: 107.6462\n",
      "Training acc over epoch: 0.7470\n",
      "---- Validation ----\n",
      "Validation loss: 47.2188\n",
      "Validation acc: 0.6848\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 339.1389, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 344.0859, Accuracy: 0.6278\n",
      "Training loss (for one batch) at step 20: 323.8722, Accuracy: 0.6715\n",
      "Training loss (for one batch) at step 30: 322.1091, Accuracy: 0.7124\n",
      "Training loss (for one batch) at step 40: 298.2047, Accuracy: 0.7445\n",
      "Training loss (for one batch) at step 50: 303.3201, Accuracy: 0.7627\n",
      "Training loss (for one batch) at step 60: 309.1076, Accuracy: 0.7701\n",
      "Training loss (for one batch) at step 70: 350.0973, Accuracy: 0.7562\n",
      "Training loss (for one batch) at step 80: 342.5304, Accuracy: 0.7401\n",
      "Training loss (for one batch) at step 90: 315.9965, Accuracy: 0.7387\n",
      "Training loss (for one batch) at step 100: 291.0899, Accuracy: 0.7434\n",
      "Training loss (for one batch) at step 110: 310.4059, Accuracy: 0.7447\n",
      "---- Training ----\n",
      "Training loss: 102.6423\n",
      "Training acc over epoch: 0.7423\n",
      "---- Validation ----\n",
      "Validation loss: 41.4121\n",
      "Validation acc: 0.6639\n",
      "Time taken: 10.69s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 375.8687, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 314.6328, Accuracy: 0.6243\n",
      "Training loss (for one batch) at step 20: 328.1508, Accuracy: 0.6696\n",
      "Training loss (for one batch) at step 30: 305.8214, Accuracy: 0.7130\n",
      "Training loss (for one batch) at step 40: 305.8425, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 50: 273.0260, Accuracy: 0.7616\n",
      "Training loss (for one batch) at step 60: 309.4015, Accuracy: 0.7691\n",
      "Training loss (for one batch) at step 70: 324.4760, Accuracy: 0.7567\n",
      "Training loss (for one batch) at step 80: 327.4486, Accuracy: 0.7424\n",
      "Training loss (for one batch) at step 90: 307.7678, Accuracy: 0.7406\n",
      "Training loss (for one batch) at step 100: 293.8188, Accuracy: 0.7434\n",
      "Training loss (for one batch) at step 110: 324.5215, Accuracy: 0.7453\n",
      "---- Training ----\n",
      "Training loss: 102.5251\n",
      "Training acc over epoch: 0.7437\n",
      "---- Validation ----\n",
      "Validation loss: 36.5821\n",
      "Validation acc: 0.6857\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 348.0540, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 331.1037, Accuracy: 0.6278\n",
      "Training loss (for one batch) at step 20: 315.7519, Accuracy: 0.6778\n",
      "Training loss (for one batch) at step 30: 296.5034, Accuracy: 0.7182\n",
      "Training loss (for one batch) at step 40: 288.8884, Accuracy: 0.7462\n",
      "Training loss (for one batch) at step 50: 291.3318, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 60: 312.2513, Accuracy: 0.7742\n",
      "Training loss (for one batch) at step 70: 326.6676, Accuracy: 0.7617\n",
      "Training loss (for one batch) at step 80: 334.2278, Accuracy: 0.7463\n",
      "Training loss (for one batch) at step 90: 313.1764, Accuracy: 0.7414\n",
      "Training loss (for one batch) at step 100: 290.8135, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 110: 308.7370, Accuracy: 0.7483\n",
      "---- Training ----\n",
      "Training loss: 98.0624\n",
      "Training acc over epoch: 0.7468\n",
      "---- Validation ----\n",
      "Validation loss: 53.0463\n",
      "Validation acc: 0.6937\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 358.5070, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 331.0332, Accuracy: 0.6236\n",
      "Training loss (for one batch) at step 20: 301.3442, Accuracy: 0.6685\n",
      "Training loss (for one batch) at step 30: 305.8274, Accuracy: 0.7137\n",
      "Training loss (for one batch) at step 40: 294.1714, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 50: 295.7692, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 60: 307.6479, Accuracy: 0.7679\n",
      "Training loss (for one batch) at step 70: 345.0628, Accuracy: 0.7575\n",
      "Training loss (for one batch) at step 80: 323.4285, Accuracy: 0.7394\n",
      "Training loss (for one batch) at step 90: 329.9748, Accuracy: 0.7364\n",
      "Training loss (for one batch) at step 100: 305.4181, Accuracy: 0.7416\n",
      "Training loss (for one batch) at step 110: 321.1613, Accuracy: 0.7425\n",
      "---- Training ----\n",
      "Training loss: 104.9876\n",
      "Training acc over epoch: 0.7406\n",
      "---- Validation ----\n",
      "Validation loss: 53.2305\n",
      "Validation acc: 0.6929\n",
      "Time taken: 10.79s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 349.0977, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 360.1472, Accuracy: 0.6307\n",
      "Training loss (for one batch) at step 20: 306.7869, Accuracy: 0.6767\n",
      "Training loss (for one batch) at step 30: 307.6220, Accuracy: 0.7150\n",
      "Training loss (for one batch) at step 40: 293.5069, Accuracy: 0.7420\n",
      "Training loss (for one batch) at step 50: 299.4205, Accuracy: 0.7615\n",
      "Training loss (for one batch) at step 60: 302.6672, Accuracy: 0.7688\n",
      "Training loss (for one batch) at step 70: 346.5426, Accuracy: 0.7565\n",
      "Training loss (for one batch) at step 80: 335.0936, Accuracy: 0.7399\n",
      "Training loss (for one batch) at step 90: 305.7151, Accuracy: 0.7368\n",
      "Training loss (for one batch) at step 100: 290.5906, Accuracy: 0.7406\n",
      "Training loss (for one batch) at step 110: 317.9924, Accuracy: 0.7430\n",
      "---- Training ----\n",
      "Training loss: 90.5957\n",
      "Training acc over epoch: 0.7417\n",
      "---- Validation ----\n",
      "Validation loss: 41.2204\n",
      "Validation acc: 0.6854\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 337.9885, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 359.6828, Accuracy: 0.6072\n",
      "Training loss (for one batch) at step 20: 294.2183, Accuracy: 0.6715\n",
      "Training loss (for one batch) at step 30: 297.4950, Accuracy: 0.7140\n",
      "Training loss (for one batch) at step 40: 301.7610, Accuracy: 0.7393\n",
      "Training loss (for one batch) at step 50: 294.1772, Accuracy: 0.7613\n",
      "Training loss (for one batch) at step 60: 309.0402, Accuracy: 0.7711\n",
      "Training loss (for one batch) at step 70: 332.0147, Accuracy: 0.7602\n",
      "Training loss (for one batch) at step 80: 331.3414, Accuracy: 0.7444\n",
      "Training loss (for one batch) at step 90: 295.7648, Accuracy: 0.7415\n",
      "Training loss (for one batch) at step 100: 296.8639, Accuracy: 0.7464\n",
      "Training loss (for one batch) at step 110: 289.9081, Accuracy: 0.7463\n",
      "---- Training ----\n",
      "Training loss: 105.1492\n",
      "Training acc over epoch: 0.7446\n",
      "---- Validation ----\n",
      "Validation loss: 35.2587\n",
      "Validation acc: 0.6900\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 359.6834, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 331.0743, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 20: 311.5149, Accuracy: 0.6823\n",
      "Training loss (for one batch) at step 30: 290.5422, Accuracy: 0.7258\n",
      "Training loss (for one batch) at step 40: 286.4737, Accuracy: 0.7515\n",
      "Training loss (for one batch) at step 50: 280.7332, Accuracy: 0.7652\n",
      "Training loss (for one batch) at step 60: 300.2572, Accuracy: 0.7742\n",
      "Training loss (for one batch) at step 70: 319.4490, Accuracy: 0.7611\n",
      "Training loss (for one batch) at step 80: 339.0180, Accuracy: 0.7442\n",
      "Training loss (for one batch) at step 90: 309.8663, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 100: 293.8656, Accuracy: 0.7449\n",
      "Training loss (for one batch) at step 110: 323.6724, Accuracy: 0.7474\n",
      "---- Training ----\n",
      "Training loss: 101.0533\n",
      "Training acc over epoch: 0.7452\n",
      "---- Validation ----\n",
      "Validation loss: 58.8217\n",
      "Validation acc: 0.6870\n",
      "Time taken: 10.92s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 343.5321, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 343.8733, Accuracy: 0.6420\n",
      "Training loss (for one batch) at step 20: 309.7976, Accuracy: 0.6827\n",
      "Training loss (for one batch) at step 30: 301.3055, Accuracy: 0.7190\n",
      "Training loss (for one batch) at step 40: 278.7257, Accuracy: 0.7456\n",
      "Training loss (for one batch) at step 50: 287.0888, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 60: 326.2986, Accuracy: 0.7723\n",
      "Training loss (for one batch) at step 70: 337.0951, Accuracy: 0.7559\n",
      "Training loss (for one batch) at step 80: 327.1602, Accuracy: 0.7410\n",
      "Training loss (for one batch) at step 90: 330.8637, Accuracy: 0.7371\n",
      "Training loss (for one batch) at step 100: 297.4607, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 110: 306.7714, Accuracy: 0.7432\n",
      "---- Training ----\n",
      "Training loss: 93.2953\n",
      "Training acc over epoch: 0.7427\n",
      "---- Validation ----\n",
      "Validation loss: 59.1743\n",
      "Validation acc: 0.6910\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 363.2029, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 322.4167, Accuracy: 0.6278\n",
      "Training loss (for one batch) at step 20: 310.6273, Accuracy: 0.6760\n",
      "Training loss (for one batch) at step 30: 298.5474, Accuracy: 0.7195\n",
      "Training loss (for one batch) at step 40: 287.6122, Accuracy: 0.7439\n",
      "Training loss (for one batch) at step 50: 282.9009, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 60: 299.5215, Accuracy: 0.7719\n",
      "Training loss (for one batch) at step 70: 331.6386, Accuracy: 0.7596\n",
      "Training loss (for one batch) at step 80: 313.4779, Accuracy: 0.7445\n",
      "Training loss (for one batch) at step 90: 295.2798, Accuracy: 0.7410\n",
      "Training loss (for one batch) at step 100: 269.7129, Accuracy: 0.7451\n",
      "Training loss (for one batch) at step 110: 310.0607, Accuracy: 0.7456\n",
      "---- Training ----\n",
      "Training loss: 102.4660\n",
      "Training acc over epoch: 0.7454\n",
      "---- Validation ----\n",
      "Validation loss: 43.2433\n",
      "Validation acc: 0.6838\n",
      "Time taken: 10.32s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 329.0263, Accuracy: 0.6797\n",
      "Training loss (for one batch) at step 10: 332.4048, Accuracy: 0.6243\n",
      "Training loss (for one batch) at step 20: 307.3586, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 30: 272.7334, Accuracy: 0.7175\n",
      "Training loss (for one batch) at step 40: 292.0273, Accuracy: 0.7443\n",
      "Training loss (for one batch) at step 50: 276.6495, Accuracy: 0.7616\n",
      "Training loss (for one batch) at step 60: 317.1637, Accuracy: 0.7705\n",
      "Training loss (for one batch) at step 70: 321.7739, Accuracy: 0.7569\n",
      "Training loss (for one batch) at step 80: 319.2431, Accuracy: 0.7421\n",
      "Training loss (for one batch) at step 90: 305.6596, Accuracy: 0.7380\n",
      "Training loss (for one batch) at step 100: 285.0782, Accuracy: 0.7427\n",
      "Training loss (for one batch) at step 110: 299.4645, Accuracy: 0.7428\n",
      "---- Training ----\n",
      "Training loss: 91.0452\n",
      "Training acc over epoch: 0.7418\n",
      "---- Validation ----\n",
      "Validation loss: 50.1787\n",
      "Validation acc: 0.6762\n",
      "Time taken: 10.78s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 341.3257, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 359.7144, Accuracy: 0.6207\n",
      "Training loss (for one batch) at step 20: 304.5110, Accuracy: 0.6789\n",
      "Training loss (for one batch) at step 30: 293.1686, Accuracy: 0.7172\n",
      "Training loss (for one batch) at step 40: 269.2837, Accuracy: 0.7460\n",
      "Training loss (for one batch) at step 50: 288.5170, Accuracy: 0.7649\n",
      "Training loss (for one batch) at step 60: 284.4006, Accuracy: 0.7719\n",
      "Training loss (for one batch) at step 70: 327.4816, Accuracy: 0.7594\n",
      "Training loss (for one batch) at step 80: 302.5205, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 90: 279.5975, Accuracy: 0.7382\n",
      "Training loss (for one batch) at step 100: 304.4115, Accuracy: 0.7414\n",
      "Training loss (for one batch) at step 110: 304.0261, Accuracy: 0.7432\n",
      "---- Training ----\n",
      "Training loss: 99.3107\n",
      "Training acc over epoch: 0.7417\n",
      "---- Validation ----\n",
      "Validation loss: 40.9790\n",
      "Validation acc: 0.6757\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 335.0654, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 320.6836, Accuracy: 0.6321\n",
      "Training loss (for one batch) at step 20: 291.7643, Accuracy: 0.6775\n",
      "Training loss (for one batch) at step 30: 271.9202, Accuracy: 0.7195\n",
      "Training loss (for one batch) at step 40: 279.2803, Accuracy: 0.7485\n",
      "Training loss (for one batch) at step 50: 268.7137, Accuracy: 0.7664\n",
      "Training loss (for one batch) at step 60: 294.7986, Accuracy: 0.7757\n",
      "Training loss (for one batch) at step 70: 304.1268, Accuracy: 0.7629\n",
      "Training loss (for one batch) at step 80: 319.9223, Accuracy: 0.7449\n",
      "Training loss (for one batch) at step 90: 284.1855, Accuracy: 0.7417\n",
      "Training loss (for one batch) at step 100: 293.9039, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 110: 288.0670, Accuracy: 0.7461\n",
      "---- Training ----\n",
      "Training loss: 96.4247\n",
      "Training acc over epoch: 0.7450\n",
      "---- Validation ----\n",
      "Validation loss: 36.5003\n",
      "Validation acc: 0.6900\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 358.6035, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 313.1490, Accuracy: 0.6293\n",
      "Training loss (for one batch) at step 20: 300.0300, Accuracy: 0.6838\n",
      "Training loss (for one batch) at step 30: 296.3777, Accuracy: 0.7223\n",
      "Training loss (for one batch) at step 40: 293.4132, Accuracy: 0.7492\n",
      "Training loss (for one batch) at step 50: 282.2011, Accuracy: 0.7647\n",
      "Training loss (for one batch) at step 60: 296.4602, Accuracy: 0.7723\n",
      "Training loss (for one batch) at step 70: 321.7356, Accuracy: 0.7587\n",
      "Training loss (for one batch) at step 80: 340.2055, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 90: 300.8788, Accuracy: 0.7396\n",
      "Training loss (for one batch) at step 100: 300.6163, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 110: 313.0123, Accuracy: 0.7463\n",
      "---- Training ----\n",
      "Training loss: 91.3773\n",
      "Training acc over epoch: 0.7448\n",
      "---- Validation ----\n",
      "Validation loss: 40.3785\n",
      "Validation acc: 0.6977\n",
      "Time taken: 10.72s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 328.9194, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 337.8115, Accuracy: 0.6257\n",
      "Training loss (for one batch) at step 20: 303.6411, Accuracy: 0.6793\n",
      "Training loss (for one batch) at step 30: 276.4494, Accuracy: 0.7240\n",
      "Training loss (for one batch) at step 40: 283.5948, Accuracy: 0.7475\n",
      "Training loss (for one batch) at step 50: 297.5559, Accuracy: 0.7659\n",
      "Training loss (for one batch) at step 60: 296.7162, Accuracy: 0.7741\n",
      "Training loss (for one batch) at step 70: 316.7485, Accuracy: 0.7617\n",
      "Training loss (for one batch) at step 80: 331.0470, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 90: 291.9655, Accuracy: 0.7420\n",
      "Training loss (for one batch) at step 100: 288.1500, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 110: 300.3994, Accuracy: 0.7476\n",
      "---- Training ----\n",
      "Training loss: 100.8205\n",
      "Training acc over epoch: 0.7456\n",
      "---- Validation ----\n",
      "Validation loss: 58.1285\n",
      "Validation acc: 0.6851\n",
      "Time taken: 10.28s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 322.5428, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 336.6393, Accuracy: 0.6200\n",
      "Training loss (for one batch) at step 20: 294.8635, Accuracy: 0.6685\n",
      "Training loss (for one batch) at step 30: 286.8538, Accuracy: 0.7099\n",
      "Training loss (for one batch) at step 40: 285.4338, Accuracy: 0.7399\n",
      "Training loss (for one batch) at step 50: 262.2078, Accuracy: 0.7613\n",
      "Training loss (for one batch) at step 60: 298.3494, Accuracy: 0.7696\n",
      "Training loss (for one batch) at step 70: 330.9868, Accuracy: 0.7584\n",
      "Training loss (for one batch) at step 80: 310.0228, Accuracy: 0.7415\n",
      "Training loss (for one batch) at step 90: 295.9023, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 100: 296.6252, Accuracy: 0.7434\n",
      "Training loss (for one batch) at step 110: 295.7995, Accuracy: 0.7440\n",
      "---- Training ----\n",
      "Training loss: 94.6004\n",
      "Training acc over epoch: 0.7427\n",
      "---- Validation ----\n",
      "Validation loss: 85.0882\n",
      "Validation acc: 0.6803\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 343.1925, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 311.7635, Accuracy: 0.6222\n",
      "Training loss (for one batch) at step 20: 292.0090, Accuracy: 0.6737\n",
      "Training loss (for one batch) at step 30: 282.0099, Accuracy: 0.7210\n",
      "Training loss (for one batch) at step 40: 271.4853, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 50: 269.6772, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 60: 302.6137, Accuracy: 0.7745\n",
      "Training loss (for one batch) at step 70: 318.9230, Accuracy: 0.7635\n",
      "Training loss (for one batch) at step 80: 317.7682, Accuracy: 0.7432\n",
      "Training loss (for one batch) at step 90: 291.7795, Accuracy: 0.7403\n",
      "Training loss (for one batch) at step 100: 291.4889, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 110: 316.4530, Accuracy: 0.7458\n",
      "---- Training ----\n",
      "Training loss: 95.0650\n",
      "Training acc over epoch: 0.7444\n",
      "---- Validation ----\n",
      "Validation loss: 64.8173\n",
      "Validation acc: 0.6830\n",
      "Time taken: 10.79s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 342.0787, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 10: 315.7544, Accuracy: 0.6342\n",
      "Training loss (for one batch) at step 20: 298.7275, Accuracy: 0.6715\n",
      "Training loss (for one batch) at step 30: 281.8973, Accuracy: 0.7172\n",
      "Training loss (for one batch) at step 40: 299.4231, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 50: 291.0225, Accuracy: 0.7615\n",
      "Training loss (for one batch) at step 60: 314.2286, Accuracy: 0.7728\n",
      "Training loss (for one batch) at step 70: 326.8144, Accuracy: 0.7585\n",
      "Training loss (for one batch) at step 80: 313.2918, Accuracy: 0.7428\n",
      "Training loss (for one batch) at step 90: 299.7733, Accuracy: 0.7381\n",
      "Training loss (for one batch) at step 100: 284.8152, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 110: 296.5078, Accuracy: 0.7442\n",
      "---- Training ----\n",
      "Training loss: 92.6370\n",
      "Training acc over epoch: 0.7436\n",
      "---- Validation ----\n",
      "Validation loss: 58.5945\n",
      "Validation acc: 0.6773\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 331.0788, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 343.3631, Accuracy: 0.6243\n",
      "Training loss (for one batch) at step 20: 287.1367, Accuracy: 0.6734\n",
      "Training loss (for one batch) at step 30: 278.5575, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 40: 287.3321, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 50: 296.3307, Accuracy: 0.7584\n",
      "Training loss (for one batch) at step 60: 299.9030, Accuracy: 0.7675\n",
      "Training loss (for one batch) at step 70: 318.0132, Accuracy: 0.7579\n",
      "Training loss (for one batch) at step 80: 322.9474, Accuracy: 0.7389\n",
      "Training loss (for one batch) at step 90: 291.8632, Accuracy: 0.7355\n",
      "Training loss (for one batch) at step 100: 283.3701, Accuracy: 0.7406\n",
      "Training loss (for one batch) at step 110: 286.1169, Accuracy: 0.7423\n",
      "---- Training ----\n",
      "Training loss: 94.2639\n",
      "Training acc over epoch: 0.7403\n",
      "---- Validation ----\n",
      "Validation loss: 45.1260\n",
      "Validation acc: 0.6803\n",
      "Time taken: 10.31s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 324.9745, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 319.8483, Accuracy: 0.6293\n",
      "Training loss (for one batch) at step 20: 299.3896, Accuracy: 0.6782\n",
      "Training loss (for one batch) at step 30: 294.3147, Accuracy: 0.7245\n",
      "Training loss (for one batch) at step 40: 288.2562, Accuracy: 0.7445\n",
      "Training loss (for one batch) at step 50: 287.6566, Accuracy: 0.7629\n",
      "Training loss (for one batch) at step 60: 273.0527, Accuracy: 0.7714\n",
      "Training loss (for one batch) at step 70: 322.8783, Accuracy: 0.7608\n",
      "Training loss (for one batch) at step 80: 294.7278, Accuracy: 0.7430\n",
      "Training loss (for one batch) at step 90: 316.1036, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 100: 281.3078, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 110: 302.1250, Accuracy: 0.7441\n",
      "---- Training ----\n",
      "Training loss: 101.7113\n",
      "Training acc over epoch: 0.7427\n",
      "---- Validation ----\n",
      "Validation loss: 57.1340\n",
      "Validation acc: 0.6765\n",
      "Time taken: 10.90s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 339.7516, Accuracy: 0.6328\n",
      "Training loss (for one batch) at step 10: 334.1686, Accuracy: 0.6357\n",
      "Training loss (for one batch) at step 20: 298.3766, Accuracy: 0.6815\n",
      "Training loss (for one batch) at step 30: 275.9365, Accuracy: 0.7225\n",
      "Training loss (for one batch) at step 40: 302.1840, Accuracy: 0.7460\n",
      "Training loss (for one batch) at step 50: 278.3402, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 60: 313.2224, Accuracy: 0.7742\n",
      "Training loss (for one batch) at step 70: 322.0812, Accuracy: 0.7606\n",
      "Training loss (for one batch) at step 80: 337.1241, Accuracy: 0.7447\n",
      "Training loss (for one batch) at step 90: 296.3747, Accuracy: 0.7400\n",
      "Training loss (for one batch) at step 100: 279.0924, Accuracy: 0.7453\n",
      "Training loss (for one batch) at step 110: 282.2013, Accuracy: 0.7462\n",
      "---- Training ----\n",
      "Training loss: 102.7688\n",
      "Training acc over epoch: 0.7440\n",
      "---- Validation ----\n",
      "Validation loss: 41.4596\n",
      "Validation acc: 0.6805\n",
      "Time taken: 10.33s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 372.6093, Accuracy: 0.5234\n",
      "Training loss (for one batch) at step 10: 335.9840, Accuracy: 0.6143\n",
      "Training loss (for one batch) at step 20: 279.3526, Accuracy: 0.6760\n",
      "Training loss (for one batch) at step 30: 295.8018, Accuracy: 0.7200\n",
      "Training loss (for one batch) at step 40: 268.7210, Accuracy: 0.7468\n",
      "Training loss (for one batch) at step 50: 274.4056, Accuracy: 0.7641\n",
      "Training loss (for one batch) at step 60: 297.2109, Accuracy: 0.7751\n",
      "Training loss (for one batch) at step 70: 324.4698, Accuracy: 0.7598\n",
      "Training loss (for one batch) at step 80: 308.8705, Accuracy: 0.7430\n",
      "Training loss (for one batch) at step 90: 283.0307, Accuracy: 0.7385\n",
      "Training loss (for one batch) at step 100: 291.6433, Accuracy: 0.7439\n",
      "Training loss (for one batch) at step 110: 329.2231, Accuracy: 0.7442\n",
      "---- Training ----\n",
      "Training loss: 91.3105\n",
      "Training acc over epoch: 0.7421\n",
      "---- Validation ----\n",
      "Validation loss: 44.2068\n",
      "Validation acc: 0.6873\n",
      "Time taken: 10.45s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 366.8969, Accuracy: 0.5938\n",
      "Training loss (for one batch) at step 10: 316.5228, Accuracy: 0.6236\n",
      "Training loss (for one batch) at step 20: 290.9376, Accuracy: 0.6793\n",
      "Training loss (for one batch) at step 30: 283.7632, Accuracy: 0.7172\n",
      "Training loss (for one batch) at step 40: 264.7573, Accuracy: 0.7441\n",
      "Training loss (for one batch) at step 50: 285.7253, Accuracy: 0.7624\n",
      "Training loss (for one batch) at step 60: 302.2209, Accuracy: 0.7711\n",
      "Training loss (for one batch) at step 70: 308.2696, Accuracy: 0.7574\n",
      "Training loss (for one batch) at step 80: 318.7778, Accuracy: 0.7418\n",
      "Training loss (for one batch) at step 90: 292.8154, Accuracy: 0.7386\n",
      "Training loss (for one batch) at step 100: 286.7481, Accuracy: 0.7430\n",
      "Training loss (for one batch) at step 110: 309.8874, Accuracy: 0.7449\n",
      "---- Training ----\n",
      "Training loss: 124.7721\n",
      "Training acc over epoch: 0.7427\n",
      "---- Validation ----\n",
      "Validation loss: 44.5478\n",
      "Validation acc: 0.6733\n",
      "Time taken: 11.00s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 325.6992, Accuracy: 0.5781\n",
      "Training loss (for one batch) at step 10: 351.2816, Accuracy: 0.6278\n",
      "Training loss (for one batch) at step 20: 286.8203, Accuracy: 0.6767\n",
      "Training loss (for one batch) at step 30: 299.8443, Accuracy: 0.7152\n",
      "Training loss (for one batch) at step 40: 292.6271, Accuracy: 0.7441\n",
      "Training loss (for one batch) at step 50: 272.8899, Accuracy: 0.7618\n",
      "Training loss (for one batch) at step 60: 292.7153, Accuracy: 0.7720\n",
      "Training loss (for one batch) at step 70: 299.4185, Accuracy: 0.7618\n",
      "Training loss (for one batch) at step 80: 311.1224, Accuracy: 0.7439\n",
      "Training loss (for one batch) at step 90: 276.6333, Accuracy: 0.7400\n",
      "Training loss (for one batch) at step 100: 275.9720, Accuracy: 0.7449\n",
      "Training loss (for one batch) at step 110: 276.9998, Accuracy: 0.7473\n",
      "---- Training ----\n",
      "Training loss: 95.7188\n",
      "Training acc over epoch: 0.7436\n",
      "---- Validation ----\n",
      "Validation loss: 43.2228\n",
      "Validation acc: 0.6787\n",
      "Time taken: 10.49s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 384.2695, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 334.9791, Accuracy: 0.6165\n",
      "Training loss (for one batch) at step 20: 292.2963, Accuracy: 0.6786\n",
      "Training loss (for one batch) at step 30: 274.4844, Accuracy: 0.7230\n",
      "Training loss (for one batch) at step 40: 278.3639, Accuracy: 0.7475\n",
      "Training loss (for one batch) at step 50: 264.4963, Accuracy: 0.7636\n",
      "Training loss (for one batch) at step 60: 283.0599, Accuracy: 0.7716\n",
      "Training loss (for one batch) at step 70: 314.5784, Accuracy: 0.7572\n",
      "Training loss (for one batch) at step 80: 322.6286, Accuracy: 0.7427\n",
      "Training loss (for one batch) at step 90: 286.4360, Accuracy: 0.7389\n",
      "Training loss (for one batch) at step 100: 286.4260, Accuracy: 0.7423\n",
      "Training loss (for one batch) at step 110: 307.4339, Accuracy: 0.7433\n",
      "---- Training ----\n",
      "Training loss: 103.9155\n",
      "Training acc over epoch: 0.7410\n",
      "---- Validation ----\n",
      "Validation loss: 50.6936\n",
      "Validation acc: 0.6814\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 321.4179, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 330.9893, Accuracy: 0.6257\n",
      "Training loss (for one batch) at step 20: 283.7859, Accuracy: 0.6763\n",
      "Training loss (for one batch) at step 30: 295.0752, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 40: 301.2696, Accuracy: 0.7445\n",
      "Training loss (for one batch) at step 50: 293.1476, Accuracy: 0.7629\n",
      "Training loss (for one batch) at step 60: 300.2575, Accuracy: 0.7715\n",
      "Training loss (for one batch) at step 70: 314.3157, Accuracy: 0.7585\n",
      "Training loss (for one batch) at step 80: 343.8185, Accuracy: 0.7408\n",
      "Training loss (for one batch) at step 90: 281.7803, Accuracy: 0.7370\n",
      "Training loss (for one batch) at step 100: 310.9164, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 110: 314.6359, Accuracy: 0.7425\n",
      "---- Training ----\n",
      "Training loss: 88.3254\n",
      "Training acc over epoch: 0.7417\n",
      "---- Validation ----\n",
      "Validation loss: 55.1768\n",
      "Validation acc: 0.6679\n",
      "Time taken: 11.02s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 331.6117, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 345.5530, Accuracy: 0.6165\n",
      "Training loss (for one batch) at step 20: 285.2061, Accuracy: 0.6730\n",
      "Training loss (for one batch) at step 30: 265.2274, Accuracy: 0.7170\n",
      "Training loss (for one batch) at step 40: 283.0762, Accuracy: 0.7429\n",
      "Training loss (for one batch) at step 50: 279.2600, Accuracy: 0.7610\n",
      "Training loss (for one batch) at step 60: 288.0265, Accuracy: 0.7707\n",
      "Training loss (for one batch) at step 70: 336.2535, Accuracy: 0.7572\n",
      "Training loss (for one batch) at step 80: 303.7571, Accuracy: 0.7408\n",
      "Training loss (for one batch) at step 90: 273.3474, Accuracy: 0.7357\n",
      "Training loss (for one batch) at step 100: 295.9373, Accuracy: 0.7391\n",
      "Training loss (for one batch) at step 110: 295.7671, Accuracy: 0.7416\n",
      "---- Training ----\n",
      "Training loss: 100.2951\n",
      "Training acc over epoch: 0.7409\n",
      "---- Validation ----\n",
      "Validation loss: 55.3536\n",
      "Validation acc: 0.6585\n",
      "Time taken: 10.41s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 323.4666, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 315.1980, Accuracy: 0.6314\n",
      "Training loss (for one batch) at step 20: 292.5818, Accuracy: 0.6730\n",
      "Training loss (for one batch) at step 30: 279.4451, Accuracy: 0.7170\n",
      "Training loss (for one batch) at step 40: 281.5523, Accuracy: 0.7447\n",
      "Training loss (for one batch) at step 50: 279.8107, Accuracy: 0.7610\n",
      "Training loss (for one batch) at step 60: 279.1385, Accuracy: 0.7718\n",
      "Training loss (for one batch) at step 70: 334.0915, Accuracy: 0.7588\n",
      "Training loss (for one batch) at step 80: 339.5693, Accuracy: 0.7420\n",
      "Training loss (for one batch) at step 90: 270.9877, Accuracy: 0.7395\n",
      "Training loss (for one batch) at step 100: 293.7555, Accuracy: 0.7450\n",
      "Training loss (for one batch) at step 110: 289.8488, Accuracy: 0.7463\n",
      "---- Training ----\n",
      "Training loss: 103.5545\n",
      "Training acc over epoch: 0.7450\n",
      "---- Validation ----\n",
      "Validation loss: 59.6894\n",
      "Validation acc: 0.6725\n",
      "Time taken: 10.35s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 332.7610, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 319.1279, Accuracy: 0.6115\n",
      "Training loss (for one batch) at step 20: 284.7443, Accuracy: 0.6715\n",
      "Training loss (for one batch) at step 30: 283.1059, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 40: 293.1908, Accuracy: 0.7466\n",
      "Training loss (for one batch) at step 50: 281.1830, Accuracy: 0.7621\n",
      "Training loss (for one batch) at step 60: 286.6473, Accuracy: 0.7714\n",
      "Training loss (for one batch) at step 70: 323.7250, Accuracy: 0.7584\n",
      "Training loss (for one batch) at step 80: 275.9529, Accuracy: 0.7409\n",
      "Training loss (for one batch) at step 90: 279.7646, Accuracy: 0.7395\n",
      "Training loss (for one batch) at step 100: 309.0548, Accuracy: 0.7430\n",
      "Training loss (for one batch) at step 110: 295.2244, Accuracy: 0.7430\n",
      "---- Training ----\n",
      "Training loss: 114.1020\n",
      "Training acc over epoch: 0.7414\n",
      "---- Validation ----\n",
      "Validation loss: 36.7852\n",
      "Validation acc: 0.6722\n",
      "Time taken: 10.99s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 322.6508, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 10: 321.1667, Accuracy: 0.6207\n",
      "Training loss (for one batch) at step 20: 287.4026, Accuracy: 0.6700\n",
      "Training loss (for one batch) at step 30: 285.6266, Accuracy: 0.7167\n",
      "Training loss (for one batch) at step 40: 304.6536, Accuracy: 0.7376\n",
      "Training loss (for one batch) at step 50: 259.8514, Accuracy: 0.7578\n",
      "Training loss (for one batch) at step 60: 290.9679, Accuracy: 0.7679\n",
      "Training loss (for one batch) at step 70: 325.8140, Accuracy: 0.7553\n",
      "Training loss (for one batch) at step 80: 301.7926, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 90: 312.7571, Accuracy: 0.7333\n",
      "Training loss (for one batch) at step 100: 311.6251, Accuracy: 0.7384\n",
      "Training loss (for one batch) at step 110: 318.5631, Accuracy: 0.7411\n",
      "---- Training ----\n",
      "Training loss: 97.7207\n",
      "Training acc over epoch: 0.7400\n",
      "---- Validation ----\n",
      "Validation loss: 61.9697\n",
      "Validation acc: 0.6776\n",
      "Time taken: 10.34s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 313.6167, Accuracy: 0.6562\n",
      "Training loss (for one batch) at step 10: 328.1617, Accuracy: 0.6080\n",
      "Training loss (for one batch) at step 20: 277.0655, Accuracy: 0.6708\n",
      "Training loss (for one batch) at step 30: 282.4450, Accuracy: 0.7162\n",
      "Training loss (for one batch) at step 40: 274.5138, Accuracy: 0.7454\n",
      "Training loss (for one batch) at step 50: 276.0796, Accuracy: 0.7626\n",
      "Training loss (for one batch) at step 60: 280.0897, Accuracy: 0.7702\n",
      "Training loss (for one batch) at step 70: 309.3680, Accuracy: 0.7576\n",
      "Training loss (for one batch) at step 80: 326.7132, Accuracy: 0.7383\n",
      "Training loss (for one batch) at step 90: 292.9199, Accuracy: 0.7350\n",
      "Training loss (for one batch) at step 100: 275.7849, Accuracy: 0.7388\n",
      "Training loss (for one batch) at step 110: 301.7932, Accuracy: 0.7404\n",
      "---- Training ----\n",
      "Training loss: 97.9769\n",
      "Training acc over epoch: 0.7402\n",
      "---- Validation ----\n",
      "Validation loss: 52.8137\n",
      "Validation acc: 0.6776\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 315.4859, Accuracy: 0.6094\n",
      "Training loss (for one batch) at step 10: 300.2862, Accuracy: 0.6151\n",
      "Training loss (for one batch) at step 20: 269.4605, Accuracy: 0.6711\n",
      "Training loss (for one batch) at step 30: 302.5170, Accuracy: 0.7155\n",
      "Training loss (for one batch) at step 40: 273.5858, Accuracy: 0.7439\n",
      "Training loss (for one batch) at step 50: 283.1694, Accuracy: 0.7619\n",
      "Training loss (for one batch) at step 60: 320.4068, Accuracy: 0.7723\n",
      "Training loss (for one batch) at step 70: 324.3046, Accuracy: 0.7574\n",
      "Training loss (for one batch) at step 80: 313.7959, Accuracy: 0.7402\n",
      "Training loss (for one batch) at step 90: 275.7899, Accuracy: 0.7372\n",
      "Training loss (for one batch) at step 100: 296.1965, Accuracy: 0.7431\n",
      "Training loss (for one batch) at step 110: 317.7930, Accuracy: 0.7443\n",
      "---- Training ----\n",
      "Training loss: 109.3953\n",
      "Training acc over epoch: 0.7431\n",
      "---- Validation ----\n",
      "Validation loss: 47.7018\n",
      "Validation acc: 0.6650\n",
      "Time taken: 10.95s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 332.1458, Accuracy: 0.5859\n",
      "Training loss (for one batch) at step 10: 313.3643, Accuracy: 0.6136\n",
      "Training loss (for one batch) at step 20: 288.6904, Accuracy: 0.6652\n",
      "Training loss (for one batch) at step 30: 277.9237, Accuracy: 0.7109\n",
      "Training loss (for one batch) at step 40: 291.0661, Accuracy: 0.7393\n",
      "Training loss (for one batch) at step 50: 281.3903, Accuracy: 0.7551\n",
      "Training loss (for one batch) at step 60: 294.2386, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 70: 306.8398, Accuracy: 0.7540\n",
      "Training loss (for one batch) at step 80: 304.3467, Accuracy: 0.7362\n",
      "Training loss (for one batch) at step 90: 290.1690, Accuracy: 0.7348\n",
      "Training loss (for one batch) at step 100: 277.0764, Accuracy: 0.7402\n",
      "Training loss (for one batch) at step 110: 286.1135, Accuracy: 0.7413\n",
      "---- Training ----\n",
      "Training loss: 84.3614\n",
      "Training acc over epoch: 0.7402\n",
      "---- Validation ----\n",
      "Validation loss: 49.3678\n",
      "Validation acc: 0.6741\n",
      "Time taken: 10.27s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 322.5158, Accuracy: 0.6016\n",
      "Training loss (for one batch) at step 10: 311.6770, Accuracy: 0.6172\n",
      "Training loss (for one batch) at step 20: 286.2098, Accuracy: 0.6745\n",
      "Training loss (for one batch) at step 30: 272.3862, Accuracy: 0.7117\n",
      "Training loss (for one batch) at step 40: 263.0289, Accuracy: 0.7428\n",
      "Training loss (for one batch) at step 50: 272.0694, Accuracy: 0.7624\n",
      "Training loss (for one batch) at step 60: 285.4891, Accuracy: 0.7691\n",
      "Training loss (for one batch) at step 70: 333.9666, Accuracy: 0.7583\n",
      "Training loss (for one batch) at step 80: 296.0023, Accuracy: 0.7421\n",
      "Training loss (for one batch) at step 90: 265.0043, Accuracy: 0.7377\n",
      "Training loss (for one batch) at step 100: 270.3953, Accuracy: 0.7431\n",
      "Training loss (for one batch) at step 110: 293.3194, Accuracy: 0.7417\n",
      "---- Training ----\n",
      "Training loss: 114.6960\n",
      "Training acc over epoch: 0.7401\n",
      "---- Validation ----\n",
      "Validation loss: 60.5398\n",
      "Validation acc: 0.6760\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 307.4401, Accuracy: 0.6484\n",
      "Training loss (for one batch) at step 10: 319.0253, Accuracy: 0.6122\n",
      "Training loss (for one batch) at step 20: 280.1897, Accuracy: 0.6745\n",
      "Training loss (for one batch) at step 30: 289.6273, Accuracy: 0.7188\n",
      "Training loss (for one batch) at step 40: 277.6513, Accuracy: 0.7441\n",
      "Training loss (for one batch) at step 50: 266.6654, Accuracy: 0.7630\n",
      "Training loss (for one batch) at step 60: 296.3190, Accuracy: 0.7736\n",
      "Training loss (for one batch) at step 70: 314.9458, Accuracy: 0.7588\n",
      "Training loss (for one batch) at step 80: 292.4070, Accuracy: 0.7428\n",
      "Training loss (for one batch) at step 90: 280.9223, Accuracy: 0.7405\n",
      "Training loss (for one batch) at step 100: 279.3784, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 110: 303.0188, Accuracy: 0.7441\n",
      "---- Training ----\n",
      "Training loss: 103.1108\n",
      "Training acc over epoch: 0.7433\n",
      "---- Validation ----\n",
      "Validation loss: 65.8697\n",
      "Validation acc: 0.6623\n",
      "Time taken: 10.97s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 322.3223, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 299.2236, Accuracy: 0.6236\n",
      "Training loss (for one batch) at step 20: 275.8360, Accuracy: 0.6771\n",
      "Training loss (for one batch) at step 30: 277.4025, Accuracy: 0.7182\n",
      "Training loss (for one batch) at step 40: 291.5523, Accuracy: 0.7481\n",
      "Training loss (for one batch) at step 50: 266.6397, Accuracy: 0.7658\n",
      "Training loss (for one batch) at step 60: 292.6149, Accuracy: 0.7763\n",
      "Training loss (for one batch) at step 70: 306.7396, Accuracy: 0.7634\n",
      "Training loss (for one batch) at step 80: 318.1796, Accuracy: 0.7432\n",
      "Training loss (for one batch) at step 90: 286.8502, Accuracy: 0.7397\n",
      "Training loss (for one batch) at step 100: 266.4057, Accuracy: 0.7433\n",
      "Training loss (for one batch) at step 110: 293.7480, Accuracy: 0.7453\n",
      "---- Training ----\n",
      "Training loss: 101.4982\n",
      "Training acc over epoch: 0.7433\n",
      "---- Validation ----\n",
      "Validation loss: 65.5579\n",
      "Validation acc: 0.6647\n",
      "Time taken: 10.29s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 306.7117, Accuracy: 0.6406\n",
      "Training loss (for one batch) at step 10: 327.6274, Accuracy: 0.6129\n",
      "Training loss (for one batch) at step 20: 283.1458, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 30: 266.2069, Accuracy: 0.7228\n",
      "Training loss (for one batch) at step 40: 262.9272, Accuracy: 0.7460\n",
      "Training loss (for one batch) at step 50: 263.0480, Accuracy: 0.7646\n",
      "Training loss (for one batch) at step 60: 288.5459, Accuracy: 0.7720\n",
      "Training loss (for one batch) at step 70: 300.5518, Accuracy: 0.7589\n",
      "Training loss (for one batch) at step 80: 296.3932, Accuracy: 0.7435\n",
      "Training loss (for one batch) at step 90: 279.0748, Accuracy: 0.7372\n",
      "Training loss (for one batch) at step 100: 299.6171, Accuracy: 0.7422\n",
      "Training loss (for one batch) at step 110: 279.5696, Accuracy: 0.7437\n",
      "---- Training ----\n",
      "Training loss: 97.8789\n",
      "Training acc over epoch: 0.7415\n",
      "---- Validation ----\n",
      "Validation loss: 65.7039\n",
      "Validation acc: 0.6840\n",
      "Time taken: 10.36s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 324.2560, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 306.7096, Accuracy: 0.6158\n",
      "Training loss (for one batch) at step 20: 295.7118, Accuracy: 0.6708\n",
      "Training loss (for one batch) at step 30: 285.4455, Accuracy: 0.7167\n",
      "Training loss (for one batch) at step 40: 273.6909, Accuracy: 0.7462\n",
      "Training loss (for one batch) at step 50: 269.6133, Accuracy: 0.7664\n",
      "Training loss (for one batch) at step 60: 279.3516, Accuracy: 0.7741\n",
      "Training loss (for one batch) at step 70: 295.0922, Accuracy: 0.7605\n",
      "Training loss (for one batch) at step 80: 330.2700, Accuracy: 0.7432\n",
      "Training loss (for one batch) at step 90: 265.0397, Accuracy: 0.7401\n",
      "Training loss (for one batch) at step 100: 259.0748, Accuracy: 0.7439\n",
      "Training loss (for one batch) at step 110: 271.4938, Accuracy: 0.7457\n",
      "---- Training ----\n",
      "Training loss: 84.3325\n",
      "Training acc over epoch: 0.7434\n",
      "---- Validation ----\n",
      "Validation loss: 29.7400\n",
      "Validation acc: 0.6685\n",
      "Time taken: 10.77s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 342.7714, Accuracy: 0.5625\n",
      "Training loss (for one batch) at step 10: 295.6937, Accuracy: 0.6101\n",
      "Training loss (for one batch) at step 20: 285.6516, Accuracy: 0.6730\n",
      "Training loss (for one batch) at step 30: 292.2438, Accuracy: 0.7114\n",
      "Training loss (for one batch) at step 40: 293.6034, Accuracy: 0.7418\n",
      "Training loss (for one batch) at step 50: 267.0180, Accuracy: 0.7606\n",
      "Training loss (for one batch) at step 60: 305.0492, Accuracy: 0.7729\n",
      "Training loss (for one batch) at step 70: 299.4505, Accuracy: 0.7588\n",
      "Training loss (for one batch) at step 80: 304.7147, Accuracy: 0.7413\n",
      "Training loss (for one batch) at step 90: 301.8560, Accuracy: 0.7392\n",
      "Training loss (for one batch) at step 100: 277.8865, Accuracy: 0.7437\n",
      "Training loss (for one batch) at step 110: 315.6711, Accuracy: 0.7446\n",
      "---- Training ----\n",
      "Training loss: 97.0879\n",
      "Training acc over epoch: 0.7440\n",
      "---- Validation ----\n",
      "Validation loss: 65.5232\n",
      "Validation acc: 0.6797\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 302.8810, Accuracy: 0.5703\n",
      "Training loss (for one batch) at step 10: 308.2393, Accuracy: 0.6115\n",
      "Training loss (for one batch) at step 20: 286.3443, Accuracy: 0.6648\n",
      "Training loss (for one batch) at step 30: 276.1104, Accuracy: 0.7084\n",
      "Training loss (for one batch) at step 40: 286.3007, Accuracy: 0.7384\n",
      "Training loss (for one batch) at step 50: 269.0505, Accuracy: 0.7623\n",
      "Training loss (for one batch) at step 60: 282.8284, Accuracy: 0.7700\n",
      "Training loss (for one batch) at step 70: 306.9268, Accuracy: 0.7575\n",
      "Training loss (for one batch) at step 80: 326.8889, Accuracy: 0.7400\n",
      "Training loss (for one batch) at step 90: 293.7553, Accuracy: 0.7373\n",
      "Training loss (for one batch) at step 100: 258.5867, Accuracy: 0.7414\n",
      "Training loss (for one batch) at step 110: 275.6415, Accuracy: 0.7442\n",
      "---- Training ----\n",
      "Training loss: 92.5669\n",
      "Training acc over epoch: 0.7423\n",
      "---- Validation ----\n",
      "Validation loss: 44.9099\n",
      "Validation acc: 0.6881\n",
      "Time taken: 10.30s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 303.5519, Accuracy: 0.6250\n",
      "Training loss (for one batch) at step 10: 317.3649, Accuracy: 0.6158\n",
      "Training loss (for one batch) at step 20: 291.8306, Accuracy: 0.6760\n",
      "Training loss (for one batch) at step 30: 271.8139, Accuracy: 0.7210\n",
      "Training loss (for one batch) at step 40: 293.7141, Accuracy: 0.7464\n",
      "Training loss (for one batch) at step 50: 264.3254, Accuracy: 0.7655\n",
      "Training loss (for one batch) at step 60: 297.3934, Accuracy: 0.7734\n",
      "Training loss (for one batch) at step 70: 300.1020, Accuracy: 0.7585\n",
      "Training loss (for one batch) at step 80: 301.9954, Accuracy: 0.7408\n",
      "Training loss (for one batch) at step 90: 289.8459, Accuracy: 0.7404\n",
      "Training loss (for one batch) at step 100: 268.1400, Accuracy: 0.7447\n",
      "Training loss (for one batch) at step 110: 284.1953, Accuracy: 0.7456\n",
      "---- Training ----\n",
      "Training loss: 86.5000\n",
      "Training acc over epoch: 0.7425\n",
      "---- Validation ----\n",
      "Validation loss: 58.7228\n",
      "Validation acc: 0.6843\n",
      "Time taken: 10.52s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 299.7581, Accuracy: 0.6875\n",
      "Training loss (for one batch) at step 10: 310.3421, Accuracy: 0.6243\n",
      "Training loss (for one batch) at step 20: 280.2142, Accuracy: 0.6737\n",
      "Training loss (for one batch) at step 30: 270.7104, Accuracy: 0.7119\n",
      "Training loss (for one batch) at step 40: 265.5055, Accuracy: 0.7412\n",
      "Training loss (for one batch) at step 50: 272.6861, Accuracy: 0.7616\n",
      "Training loss (for one batch) at step 60: 278.9734, Accuracy: 0.7714\n",
      "Training loss (for one batch) at step 70: 312.5247, Accuracy: 0.7550\n",
      "Training loss (for one batch) at step 80: 306.2155, Accuracy: 0.7390\n",
      "Training loss (for one batch) at step 90: 281.4853, Accuracy: 0.7353\n",
      "Training loss (for one batch) at step 100: 270.7712, Accuracy: 0.7423\n",
      "Training loss (for one batch) at step 110: 291.7708, Accuracy: 0.7434\n",
      "---- Training ----\n",
      "Training loss: 89.9496\n",
      "Training acc over epoch: 0.7419\n",
      "---- Validation ----\n",
      "Validation loss: 43.8746\n",
      "Validation acc: 0.6859\n",
      "Time taken: 10.62s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 311.3901, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 10: 312.6206, Accuracy: 0.6186\n",
      "Training loss (for one batch) at step 20: 281.9807, Accuracy: 0.6719\n",
      "Training loss (for one batch) at step 30: 267.5016, Accuracy: 0.7155\n",
      "Training loss (for one batch) at step 40: 304.1921, Accuracy: 0.7464\n",
      "Training loss (for one batch) at step 50: 273.2202, Accuracy: 0.7633\n",
      "Training loss (for one batch) at step 60: 282.0925, Accuracy: 0.7732\n",
      "Training loss (for one batch) at step 70: 297.5768, Accuracy: 0.7614\n",
      "Training loss (for one batch) at step 80: 289.8690, Accuracy: 0.7426\n",
      "Training loss (for one batch) at step 90: 273.5302, Accuracy: 0.7385\n",
      "Training loss (for one batch) at step 100: 261.6213, Accuracy: 0.7416\n",
      "Training loss (for one batch) at step 110: 301.1524, Accuracy: 0.7427\n",
      "---- Training ----\n",
      "Training loss: 97.9272\n",
      "Training acc over epoch: 0.7415\n",
      "---- Validation ----\n",
      "Validation loss: 55.9390\n",
      "Validation acc: 0.6771\n",
      "Time taken: 10.48s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 310.7786, Accuracy: 0.6641\n",
      "Training loss (for one batch) at step 10: 311.9303, Accuracy: 0.6122\n",
      "Training loss (for one batch) at step 20: 284.0770, Accuracy: 0.6726\n",
      "Training loss (for one batch) at step 30: 263.7583, Accuracy: 0.7185\n",
      "Training loss (for one batch) at step 40: 256.3025, Accuracy: 0.7449\n",
      "Training loss (for one batch) at step 50: 275.2467, Accuracy: 0.7632\n",
      "Training loss (for one batch) at step 60: 264.1819, Accuracy: 0.7729\n",
      "Training loss (for one batch) at step 70: 309.9383, Accuracy: 0.7594\n",
      "Training loss (for one batch) at step 80: 308.0895, Accuracy: 0.7407\n",
      "Training loss (for one batch) at step 90: 265.4507, Accuracy: 0.7394\n",
      "Training loss (for one batch) at step 100: 270.9872, Accuracy: 0.7438\n",
      "Training loss (for one batch) at step 110: 293.7102, Accuracy: 0.7467\n",
      "---- Training ----\n",
      "Training loss: 98.1297\n",
      "Training acc over epoch: 0.7444\n",
      "---- Validation ----\n",
      "Validation loss: 27.0723\n",
      "Validation acc: 0.6725\n",
      "Time taken: 10.56s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.6730\n",
      "Validation AUC: 0.6754\n",
      "Validation Balanced_ACC: 0.4217\n",
      "Validation MI: 0.1047\n",
      "Validation Normalized MI: 0.1535\n",
      "Validation Adjusted MI: 0.1535\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "from sklearn.metrics import normalized_mutual_info_score, mutual_info_score, adjusted_mutual_info_score\n",
    "#l1 =0.0001\n",
    "NUM_RUNS = 10\n",
    "N_EPOCHS = 100\n",
    "ACC = np.zeros(NUM_RUNS)\n",
    "AUC = np.zeros(NUM_RUNS)\n",
    "MI = np.zeros(NUM_RUNS)\n",
    "NMI = np.zeros(NUM_RUNS)\n",
    "AMI = np.zeros(NUM_RUNS)\n",
    "BACC = np.zeros(NUM_RUNS)\n",
    "BACC1 = []\n",
    "MI1 = []\n",
    "NMI1 =[]\n",
    "AMI1 = []\n",
    "#model = create_model()\n",
    "K=2\n",
    "R=5\n",
    "\n",
    "val_acc = np.zeros(NUM_RUNS)\n",
    "AUC= np.zeros(NUM_RUNS)\n",
    "\n",
    "for i in range(NUM_RUNS):\n",
    "  MA = MultipleAnnotators_Classification(2, 5, 0.1)\n",
    "  model =  create_model()\n",
    "  model = MA.fit(model, train_batches_MA, val_batches_MA, N_EPOCHS)\n",
    "  #model = MA.fit(model, Data_train_MA, N_EPOCHS)\n",
    "  ACC[i] = MA.eval_model(test_batches_MA)\n",
    "  print(\"Validation acc: %.4f\" % (float(ACC[i]),))\n",
    "    \n",
    " #AUC =======================\n",
    "  val_AUC_metric = tf.keras.metrics.AUC( from_logits = True)\n",
    "  for x_batch_val, y_batch_val in test_batches_MA:\n",
    "      val_logits = model(x_batch_val.numpy(), training=False)\n",
    "      # tf.print(y_batch_val)\n",
    "      val_AUC_metric.update_state(y_batch_val, val_logits[:,:K].numpy().argmax(axis=1).astype('float'))\n",
    "      BACC1.append(balanced_accuracy_score(y_batch_val.numpy().squeeze(), val_logits[:,:K].numpy().argmax(axis=1).squeeze(), adjusted=True))\n",
    "      MI1.append(mutual_info_score(y_batch_val.numpy().squeeze(), val_logits[:,:K].numpy().argmax(axis=1).squeeze()))\n",
    "      NMI1.append(normalized_mutual_info_score(y_batch_val.numpy().squeeze(), val_logits[:,:K].numpy().argmax(axis=1).squeeze()))\n",
    "      AMI1.append(normalized_mutual_info_score(y_batch_val.numpy().squeeze(), val_logits[:,:K].numpy().argmax(axis=1).squeeze()))\n",
    "\n",
    "  val_AUC = val_AUC_metric.result()\n",
    "  val_AUC_metric.reset_states()\n",
    "  val_AUC = val_AUC.numpy()\n",
    "  print(\"Validation AUC: %.4f\" % (float(val_AUC),))\n",
    "  AUC[i] = val_AUC\n",
    "  #===================================================\n",
    "    \n",
    "    \n",
    "\n",
    "  # balanced. Accurcy\n",
    "  BACC[i] = np.array(BACC1).mean() # balanced_accuracy_score(Y_true_test.squeeze(), val_logits[:,:K].numpy().argmax(axis=1).squeeze(), adjusted=True)\n",
    "  print(\"Validation Balanced_ACC: %.4f\" % (float(BACC[i])))\n",
    "\n",
    "  #MI\n",
    "  \n",
    "  MI[i] =  np.array(MI1).mean()  #mutual_info_score(Y_true_test.squeeze(), val_logits[:,:K].numpy().argmax(axis=1).squeeze())\n",
    "  print(\"Validation MI: %.4f\" % (float(MI[i]),))\n",
    "  NMI[i] =  np.array(NMI1).mean()   #normalized_mutual_info_score(Y_true_test.squeeze(), val_logits[:,:K].numpy().argmax(axis=1).squeeze())\n",
    "  print(\"Validation Normalized MI: %.4f\" % (float(NMI[i]),))\n",
    "  AMI[i]= np.array(AMI1).mean()  #adjusted_mutual_info_score(Y_true_test.squeeze(), val_logits[:,:K].numpy().argmax(axis=1).squeeze())\n",
    "  print(\"Validation Adjusted MI: %.4f\" % (float(AMI[i]),))\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(val_acc)\n",
    "#df.to_csv('/content/CatDogs_MA_InceptionV3.csv',index=False) # save to notebook output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac59f3a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T08:00:30.984993Z",
     "iopub.status.busy": "2023-02-04T08:00:30.984616Z",
     "iopub.status.idle": "2023-02-04T08:00:30.997317Z",
     "shell.execute_reply": "2023-02-04T08:00:30.996274Z"
    },
    "papermill": {
     "duration": 0.677463,
     "end_time": "2023-02-04T08:00:31.000391",
     "exception": false,
     "start_time": "2023-02-04T08:00:30.322928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy:  70.84\n",
      "Average std:  4.71\n",
      "==============================================\n",
      "Average AUC:  70.98\n",
      "Average AUC std:  4.569999999999999\n",
      "==============================================\n",
      "Average Balanced Accuracy:  43.51\n",
      "Average std:  2.08\n",
      "==============================================\n",
      "Average MI:  10.96\n",
      "Average std:  0.75\n",
      "==============================================\n",
      "Average Normalized MI:  16.02\n",
      "Average std:  1.04\n",
      "==============================================\n",
      "Average Ajdusted MI:  16.02\n",
      "Average std:  1.04\n"
     ]
    }
   ],
   "source": [
    "print('Average Accuracy: ', np.round( ACC.mean(),4)*100) \n",
    "print('Average std: ',np.round(np.std( ACC),4)*100)\n",
    "print('==============================================')\n",
    "print('Average AUC: ', np.round( AUC.mean(),4)*100) \n",
    "print('Average AUC std: ',np.round(np.std( AUC),4)*100)\n",
    "print('==============================================')\n",
    "print('Average Balanced Accuracy: ', np.round( BACC.mean(),4)*100) \n",
    "print('Average std: ',np.round(np.std( BACC),4)*100)\n",
    "print('==============================================')\n",
    "print('Average MI: ', np.round( MI.mean(),4)*100) \n",
    "print('Average std: ',np.round(np.std(MI),4)*100)\n",
    "print('==============================================')\n",
    "print('Average Normalized MI: ', np.round( NMI.mean(),4)*100) \n",
    "print('Average std: ',np.round(np.std(NMI),4)*100)\n",
    "print('==============================================')\n",
    "print('Average Ajdusted MI: ', np.round( AMI.mean(),4)*100) \n",
    "print('Average std: ',np.round(np.std(AMI),4)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10689.603206,
   "end_time": "2023-02-04T08:00:34.833548",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-02-04T05:02:25.230342",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
